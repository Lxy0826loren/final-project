[
    {
        "paperId": "b1b8c3e47f44158d22fb70bb453d2494ed013b70",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "On Second Thought, Let\u2019s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2212.08061",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.08061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2022-12-15",
        "authors": [
            {
                "authorId": "1380218838",
                "name": "Omar Shaikh"
            },
            {
                "authorId": "2118083343",
                "name": "Hongxin Zhang"
            },
            {
                "authorId": "46552910",
                "name": "William B. Held"
            },
            {
                "authorId": "2057372634",
                "name": "Michael Bernstein"
            },
            {
                "authorId": "2143919864",
                "name": "Diyi Yang"
            }
        ],
        "abstract": "Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zero-shot CoT reasoning in sensitive domains significantly increases a model\u2019s likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved."
    },
    {
        "paperId": "35922cd0d6b17e45320917338e9f98cb5c1a4f6f",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2212.10001",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.10001, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2022-12-20",
        "authors": [
            {
                "authorId": "7425689",
                "name": "Boshi Wang"
            },
            {
                "authorId": "48872685",
                "name": "Sewon Min"
            },
            {
                "authorId": "145924070",
                "name": "Xiang Deng"
            },
            {
                "authorId": "3363642",
                "name": "Jiaming Shen"
            },
            {
                "authorId": "1557391861",
                "name": "You Wu"
            },
            {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
            },
            {
                "authorId": "1515546612",
                "name": "Huan Sun"
            }
        ],
        "abstract": "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs\u2019 capability to learn to reason in context."
    },
    {
        "paperId": "db4ab91d5675c37795e719e997a2827d3d83cd45",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Towards Reasoning in Large Language Models: A Survey",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2212.10403",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.10403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2022-12-20",
        "authors": [
            {
                "authorId": "1490651934",
                "name": "Jie Huang"
            },
            {
                "authorId": "143922493",
                "name": "K. Chang"
            }
        ],
        "abstract": "Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work."
    },
    {
        "paperId": "84a440eddc86e4649cce158b788a57f6eb1d9e41",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Evaluating Psychological Safety of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.10529, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2022-12-20",
        "authors": [
            {
                "authorId": "2155447436",
                "name": "Xingxuan Li"
            },
            {
                "authorId": "2116873664",
                "name": "Yutong Li"
            },
            {
                "authorId": "2708940",
                "name": "Shafiq R. Joty"
            },
            {
                "authorId": "2145314839",
                "name": "Linlin Liu"
            },
            {
                "authorId": "2216688423",
                "name": "Fei Huang"
            },
            {
                "authorId": "2215965911",
                "name": "Linlin Qiu"
            },
            {
                "authorId": "1996394",
                "name": "Lidong Bing"
            }
        ],
        "abstract": "In this work, we designed unbiased prompts to systematically evaluate the psychological safety of large language models (LLMs). First, we tested five different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored higher than the human average on SD-3, suggesting a relatively darker personality pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and GPT-4 still showed dark personality patterns; these models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT series by using well-being tests to study the impact of fine-tuning with more training data. We observed a continuous increase in the well-being scores of GPT models. Following these observations, we showed that fine-tuning Llama-2-chat-7B with responses from BFI using direct preference optimization could effectively reduce the psychological toxicity of the model. Based on the findings, we recommended the application of systematic and comprehensive psychological metrics to further evaluate and improve the safety of LLMs."
    },
    {
        "paperId": "e325fe41c8c1d547ccd102ac82be3ec8b23960f2",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Parsel\ud83e\udd86: Algorithmic Reasoning with Language Models by Composing Decompositions",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.10561, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2022-12-20",
        "authors": [
            {
                "authorId": "49456763",
                "name": "E. Zelikman"
            },
            {
                "authorId": "144862341",
                "name": "Qian Huang"
            },
            {
                "authorId": "2113249490",
                "name": "Gabriel Poesia"
            },
            {
                "authorId": "144002017",
                "name": "Noah D. Goodman"
            },
            {
                "authorId": "32551479",
                "name": "Nick Haber"
            }
        ],
        "abstract": "Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75\\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67\\% to 85\\%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel"
    },
    {
        "paperId": "76f54657eb0893a0b203da57dcf0b4fffeebfc2c",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.832.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.10754, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2022-12-21",
        "authors": [
            {
                "authorId": "2156155948",
                "name": "Yi Dong"
            },
            {
                "authorId": "145262322",
                "name": "Lara J. Martin"
            },
            {
                "authorId": "1763608",
                "name": "Chris Callison-Burch"
            }
        ],
        "abstract": "Story generation and understanding -- as with all NLG/NLU tasks -- has seen a surge in neurosymbolic work. Researchers have recognized that, while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for any flaws that the neural networks might have. However, symbolic methods are extremely costly in terms of the amount of time and expertise needed to create them. In this work, we capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use of symbolic methods for tracking the state of stories and aiding in story understanding. We show that our CoRRPUS system and abstracted prompting procedures can beat current state-of-the-art structured LLM techniques on pre-existing story understanding tasks (bAbI Task 2 and Re^3) with minimal hand engineering. We hope that this work can help highlight the importance of symbolic representations and specialized prompting for LLMs as these models require some guidance for performing reasoning tasks properly."
    },
    {
        "paperId": "30c0cdc414f68211d5d0514df027cec22e005174",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "A Survey on In-context Learning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.00234, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2022-12-31",
        "authors": [
            {
                "authorId": "2047143813",
                "name": "Qingxiu Dong"
            },
            {
                "authorId": "49192881",
                "name": "Lei Li"
            },
            {
                "authorId": "10780897",
                "name": "Damai Dai"
            },
            {
                "authorId": "2113919886",
                "name": "Ce Zheng"
            },
            {
                "authorId": "150358371",
                "name": "Zhiyong Wu"
            },
            {
                "authorId": "7267809",
                "name": "Baobao Chang"
            },
            {
                "authorId": "2116530295",
                "name": "Xu Sun"
            },
            {
                "authorId": "47883405",
                "name": "Jingjing Xu"
            },
            {
                "authorId": "143900005",
                "name": "Lei Li"
            },
            {
                "authorId": "3335836",
                "name": "Zhifang Sui"
            }
        ],
        "abstract": "With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL."
    },
    {
        "paperId": "2a3213cb3c755f036d5dfec7261d726a819c78c1",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2301.00704",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.00704, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-01-02",
        "authors": [
            {
                "authorId": "2914394",
                "name": "Huiwen Chang"
            },
            {
                "authorId": "2146204239",
                "name": "Han Zhang"
            },
            {
                "authorId": "152630175",
                "name": "Jarred Barber"
            },
            {
                "authorId": "2199119286",
                "name": "AJ Maschinot"
            },
            {
                "authorId": "143923528",
                "name": "Jos\u00e9 Lezama"
            },
            {
                "authorId": "39978626",
                "name": "Lu Jiang"
            },
            {
                "authorId": "152790163",
                "name": "Ming Yang"
            },
            {
                "authorId": "1702318",
                "name": "K. Murphy"
            },
            {
                "authorId": "1768236",
                "name": "W. Freeman"
            },
            {
                "authorId": "144544291",
                "name": "Michael Rubinstein"
            },
            {
                "authorId": "2167749913",
                "name": "Yuanzhen Li"
            },
            {
                "authorId": "1707347",
                "name": "Dilip Krishnan"
            }
        ],
        "abstract": "We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io"
    },
    {
        "paperId": "9a7ac45eafe11ca003db3a300505f3b5c3f9009a",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2301.11305",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.11305, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-01-26",
        "authors": [
            {
                "authorId": "49688913",
                "name": "E. Mitchell"
            },
            {
                "authorId": "2110392124",
                "name": "Yoonho Lee"
            },
            {
                "authorId": "121873407",
                "name": "Alexander Khazatsky"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            },
            {
                "authorId": "46881670",
                "name": "Chelsea Finn"
            }
        ],
        "abstract": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information."
    },
    {
        "paperId": "05deb6c1862b2f129d6652a09eaedbc1f655cc8f",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "STEP: Learning N: M Structured Sparsity Masks from Scratch with Precondition",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2302.01172",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.01172, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-02-02",
        "authors": [
            {
                "authorId": "1454006122",
                "name": "Yucheng Lu"
            },
            {
                "authorId": "3504647",
                "name": "Shivani Agrawal"
            },
            {
                "authorId": "1929462",
                "name": "Suvinay Subramanian"
            },
            {
                "authorId": "145573927",
                "name": "Oleg Rybakov"
            },
            {
                "authorId": "2081393182",
                "name": "Chris De Sa"
            },
            {
                "authorId": "2112229",
                "name": "A. Yazdanbakhsh"
            }
        ],
        "abstract": "Recent innovations on hardware (e.g. Nvidia A100) have motivated learning N:M structured sparsity masks from scratch for fast model inference. However, state-of-the-art learning recipes in this regime (e.g. SR-STE) are proposed for non-adaptive optimizers like momentum SGD, while incurring non-trivial accuracy drop for Adam-trained models like attention-based LLMs. In this paper, we first demonstrate such gap origins from poorly estimated second moment (i.e. variance) in Adam states given by the masked weights. We conjecture that learning N:M masks with Adam should take the critical regime of variance estimation into account. In light of this, we propose STEP, an Adam-aware recipe that learns N:M masks with two phases: first, STEP calculates a reliable variance estimate (precondition phase) and subsequently, the variance remains fixed and is used as a precondition to learn N:M masks (mask-learning phase). STEP automatically identifies the switching point of two phases by dynamically sampling variance changes over the training trajectory and testing the sample concentration. Empirically, we evaluate STEP and other baselines such as ASP and SR-STE on multiple tasks including CIFAR classification, machine translation and LLM fine-tuning (BERT-Base, GPT-2). We show STEP mitigates the accuracy drop of baseline recipes and is robust to aggressive structured sparsity ratios."
    },
    {
        "paperId": "0b58f4ec8cbf6f63fb65b7e3c368cf511eadecd3",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2302.02662",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.02662, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-02-06",
        "authors": [
            {
                "authorId": "2003745905",
                "name": "Thomas Carta"
            },
            {
                "authorId": "112906667",
                "name": "Cl\u00e9ment Romac"
            },
            {
                "authorId": "1407538116",
                "name": "Thomas Wolf"
            },
            {
                "authorId": "1782552",
                "name": "S. Lamprier"
            },
            {
                "authorId": "97009622",
                "name": "Olivier Sigaud"
            },
            {
                "authorId": "1720664",
                "name": "Pierre-Yves Oudeyer"
            }
        ],
        "abstract": "Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functionally grounding several variants (size, architecture) of FLAN-T5."
    },
    {
        "paperId": "89e184d2bc830af568e439db9476caa0c047e11a",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Guiding Pretraining in Reinforcement Learning with Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2302.06692",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.06692, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-02-13",
        "authors": [
            {
                "authorId": "144894286",
                "name": "Yuqing Du"
            },
            {
                "authorId": "145695607",
                "name": "Olivia Watkins"
            },
            {
                "authorId": "2140051107",
                "name": "Wang"
            },
            {
                "authorId": "102281182",
                "name": "C\u00e9dric Colas"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            },
            {
                "authorId": "144150274",
                "name": "Abhishek Gupta"
            },
            {
                "authorId": "2112400",
                "name": "Jacob Andreas"
            }
        ],
        "abstract": "Reinforcement learning algorithms typically struggle in the absence of a dense, well-shaped reward function. Intrinsically motivated exploration methods address this limitation by rewarding agents for visiting novel states or transitions, but these methods offer limited benefits in large environments where most discovered novelty is irrelevant for downstream tasks. We describe a method that uses background knowledge from text corpora to shape exploration. This method, called ELLM (Exploring with LLMs) rewards an agent for achieving goals suggested by a language model prompted with a description of the agent's current state. By leveraging large-scale language model pretraining, ELLM guides agents toward human-meaningful and plausibly useful behaviors without requiring a human in the loop. We evaluate ELLM in the Crafter game environment and the Housekeep robotic simulator, showing that ELLM-trained agents have better coverage of common-sense behaviors during pretraining and usually match or improve performance on a range of downstream tasks. Code available at https://github.com/yuqingd/ellm."
    },
    {
        "paperId": "59fe7cb560651281cfc5db6b8940da0e3ba9dea6",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "LEVER: Learning to Verify Language-to-Code Generation with Execution",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2302.08468",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.08468, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-02-16",
        "authors": [
            {
                "authorId": "33981736",
                "name": "Ansong Ni"
            },
            {
                "authorId": "1900163",
                "name": "Srini Iyer"
            },
            {
                "authorId": "9215251",
                "name": "Dragomir R. Radev"
            },
            {
                "authorId": "1389924486",
                "name": "Ves Stoyanov"
            },
            {
                "authorId": "2072801764",
                "name": "Wen-tau Yih"
            },
            {
                "authorId": "8729431",
                "name": "Sida I. Wang"
            },
            {
                "authorId": "143724481",
                "name": "Xi Victoria Lin"
            }
        ],
        "abstract": "The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generation probability, and marginalizing over programs with the same execution results. On four datasets across the domains of table QA, math QA and basic Python programming, LEVER consistently improves over the base code LLMs(4.6% to 10.9% with code-davinci-002) and achieves new state-of-the-art results on all of them."
    },
    {
        "paperId": "b0435af3063195e8ae880489e64ccde64e6d7563",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Guiding Large Language Models via Directional Stimulus Prompting",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2302.11520",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.11520, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-02-22",
        "authors": [
            {
                "authorId": "2109964198",
                "name": "Zekun Li"
            },
            {
                "authorId": "1780690",
                "name": "Baolin Peng"
            },
            {
                "authorId": "50462546",
                "name": "Pengcheng He"
            },
            {
                "authorId": "1947267",
                "name": "Michel Galley"
            },
            {
                "authorId": "48441311",
                "name": "Jianfeng Gao"
            },
            {
                "authorId": "145026971",
                "name": "Xi Yan"
            }
        ],
        "abstract": "We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs. Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced, instance-specific hints and clues to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output. We assess our method across summarization, dialogue response generation, and chain-of-thought reasoning tasks. Our experiments demonstrate that the framework consistently improves LLMs' (e.g., ChatGPT, Codex, InstructGPT) performance on these supervised tasks using minimal labeled data. Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances ChatGPT's performance by an impressive 41.4%, matching or surpassing some fully supervised start-of-the-art models. Additionally, the instance-specific chain-of-thought prompt generated by our approach improves InstructGPT's reasoning accuracy compared to human-crafted or automatically generated prompts. The code and data are publicly available at \\url{https://github.com/Leezekun/Directional-Stimulus-Prompting}."
    },
    {
        "paperId": "d78eb600f987334b051d0a2ba69c72f9f01849ea",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2302.12343",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.12343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-02-23",
        "authors": [
            {
                "authorId": "1621628526",
                "name": "Denis Jered McInerney"
            },
            {
                "authorId": "2106234673",
                "name": "Geoffrey S. Young"
            },
            {
                "authorId": "2086966519",
                "name": "Jan-Willem van de Meent"
            },
            {
                "authorId": "1912476",
                "name": "Byron C. Wallace"
            }
        ],
        "abstract": "We propose CHiLL (Crafting High-Level Latents), an approach for natural-language specification of features for linear models. CHiLL prompts LLMs with expert-crafted queries to generate interpretable features from health records. The resulting noisy labels are then used to train a simple linear classifier. Generating features based on queries to an LLM can empower physicians to use their domain expertise to craft features that are clinically meaningful for a downstream task of interest, without having to manually extract these from raw EHR. We are motivated by a real-world risk prediction task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and standard predictive tasks (e.g., 30-day readmission) to evaluate this approach. We find that linear models using automatically extracted features are comparably performant to models using reference features, and provide greater interpretability than linear models using\"Bag-of-Words\"features. We verify that learned feature weights align well with clinical expectations."
    },
    {
        "paperId": "1358f90705b05cdb20ebe6799b02196205e7e9f0",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2302.12822",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.12822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-02-24",
        "authors": [
            {
                "authorId": "2121340452",
                "name": "Kashun Shum"
            },
            {
                "authorId": "50826757",
                "name": "Shizhe Diao"
            },
            {
                "authorId": "2146324423",
                "name": "Tong Zhang"
            }
        ],
        "abstract": "Chain-of-thought (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in complex reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications where labeled data is available without rational chains. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoT by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy to estimate the significance of each example. Automate-CoT enables a quick adaptation of the CoT technique to different tasks. Experimental results demonstrate the effectiveness of our method, where competitive results are achieved on arithmetic reasoning (+2.7%), commonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning tasks (+2.5%). The code is available at https://github.com/SHUMKASHUN/Automate-CoT."
    },
    {
        "paperId": "d318e0169f649656c71f02a1f84194a734fe1962",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Reward Design with Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2303.00001",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.00001, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-02-27",
        "authors": [
            {
                "authorId": "37909625",
                "name": "Minae Kwon"
            },
            {
                "authorId": "46215055",
                "name": "Sang Michael Xie"
            },
            {
                "authorId": "2302656",
                "name": "Kalesha Bullard"
            },
            {
                "authorId": "1779671",
                "name": "Dorsa Sadigh"
            }
        ],
        "abstract": "Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by prompting a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of the desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperform RL agents trained with reward functions learned via supervised learning"
    },
    {
        "paperId": "c4d65688c54154e01bb4fc18e4a58ef4ed6ea46b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2303.01248",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.01248, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-03-01",
        "authors": [
            {
                "authorId": "1810691540",
                "name": "Haocong Rao"
            },
            {
                "authorId": "2047458009",
                "name": "Cyril Leung"
            },
            {
                "authorId": "1679209",
                "name": "C. Miao"
            }
        ],
        "abstract": "Large Language Models (LLMs) especially ChatGPT have produced impressive results in various areas, but their potential human-like psychology is still largely unexplored. Existing works study the virtual personalities of LLMs but rarely explore the possibility of analyzing human personalities via LLMs. This paper presents a generic evaluation framework for LLMs to assess human personalities based on Myers Briggs Type Indicator (MBTI) tests. Specifically, we first devise unbiased prompts by randomly permuting options in MBTI questions and adopt the average testing result to encourage more impartial answer generation. Then, we propose to replace the subject in question statements to enable flexible queries and assessments on different subjects from LLMs. Finally, we re-formulate the question instructions in a manner of correctness evaluation to facilitate LLMs to generate clearer responses. The proposed framework enables LLMs to flexibly assess personalities of different groups of people. We further propose three evaluation metrics to measure the consistency, robustness, and fairness of assessment results from state-of-the-art LLMs including ChatGPT and GPT-4. Our experiments reveal ChatGPT's ability to assess human personalities, and the average results demonstrate that it can achieve more consistent and fairer assessments in spite of lower robustness against prompt biases compared with InstructGPT."
    },
    {
        "paperId": "3d60a54a47b346608430344ff37935d897a14c09",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Gradient-Free Structured Pruning with Unlabeled Data",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2303.04185",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.04185, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-03-07",
        "authors": [
            {
                "authorId": "2176782672",
                "name": "Azade Nova"
            },
            {
                "authorId": "2791430",
                "name": "H. Dai"
            },
            {
                "authorId": "50319359",
                "name": "D. Schuurmans"
            }
        ],
        "abstract": "Large Language Models (LLMs) have achieved great success in solving difficult tasks across many domains, but such success comes with a high computation cost, and inference latency. As developers and third parties customize these models, the need to provide efficient inference has increased. Many efforts have attempted to reduce inference cost through model compression techniques such as pruning and distillation. However, these techniques either require labeled data, or are time-consuming as they require the compressed model to be retrained to regain accuracy. In this paper, we propose a gradient-free structured pruning framework that uses only unlabeled data. An evaluation on the GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates the effectiveness of the proposed approach. By only using the weights of the pre-trained model and unlabeled data, in a matter of a few minutes on a single GPU, up to 40% of the original FLOP count can be reduced with less than a 4% accuracy loss across all tasks considered."
    },
    {
        "paperId": "b626560f19f815808a289ef5c24a17c57320da70",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "MathPrompter: Mathematical Reasoning using Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2303.05398",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.05398, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-03-04",
        "authors": [
            {
                "authorId": "40203626",
                "name": "Shima Imani"
            },
            {
                "authorId": "1669396676",
                "name": "Liang Du"
            },
            {
                "authorId": "1442157164",
                "name": "H. Shrivastava"
            }
        ],
        "abstract": "Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose \u2018MathPrompter\u2019, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the \u2018MultiArith\u2019 dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM."
    },
    {
        "paperId": "f93d5d62a227d0c4ae85c08d7de07d7c2ce28a28",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Consistency Analysis of ChatGPT",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2303.06273",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.06273, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-03-11",
        "authors": [
            {
                "authorId": "35756238",
                "name": "Myeongjun Jang"
            },
            {
                "authorId": "1690572",
                "name": "Thomas Lukasiewicz"
            }
        ],
        "abstract": "ChatGPT has gained a huge popularity since its introduction. Its positive aspects have been reported through many media platforms, and some analyses even showed that ChatGPT achieved a decent grade in professional exams, adding extra support to the claim that AI can now assist and even replace humans in industrial fields. Others, however, doubt its reliability and trustworthiness. This paper investigates the trustworthiness of ChatGPT and GPT-4 regarding logically consistent behaviour, focusing specifically on semantic consistency and the properties of negation, symmetric, and transitive consistency. Our findings suggest that while both models appear to show an enhanced language understanding and reasoning ability, they still frequently fall short of generating logically consistent predictions. We also ascertain via experiments that prompt designing, few-shot learning and employing larger large language models (LLMs) are unlikely to be the ultimate solution to resolve the inconsistency issue of LLMs."
    },
    {
        "paperId": "7c1707db9aafd209aa93db3251e7ebd593d55876",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2303.08896",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.08896, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-03-16",
        "authors": [
            {
                "authorId": "89355510",
                "name": "Potsawee Manakul"
            },
            {
                "authorId": "2190750613",
                "name": "Adian Liusie"
            },
            {
                "authorId": "1740397",
                "name": "M. Gales"
            }
        ],
        "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods."
    },
    {
        "paperId": "70da4fb798a86cbe8cad96c27ced0415885bbd9d",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2303.16854",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.16854, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-03-29",
        "authors": [
            {
                "authorId": "1754500",
                "name": "Xingwei He"
            },
            {
                "authorId": "31113759",
                "name": "Zheng-Wen Lin"
            },
            {
                "authorId": "2171182",
                "name": "Yeyun Gong"
            },
            {
                "authorId": "15796861",
                "name": "Alex Jin"
            },
            {
                "authorId": "2119077859",
                "name": "Hang Zhang"
            },
            {
                "authorId": "2186278677",
                "name": "Chen Lin"
            },
            {
                "authorId": "2143968416",
                "name": "Jian Jiao"
            },
            {
                "authorId": "2184000509",
                "name": "S. Yiu"
            },
            {
                "authorId": "46429989",
                "name": "Nan Duan"
            },
            {
                "authorId": "2109136147",
                "name": "Weizhu Chen"
            }
        ],
        "abstract": "Many natural language processing (NLP) tasks rely on labeled data to train machine learning models with high performance. However, data annotation is time-consuming and expensive, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator when provided with sufficient guidance and demonstrated examples. Accordingly, we propose AnnoLLM, an annotation system powered by LLMs, which adopts a two-step approach, explain-then-annotate. Concretely, we first prompt LLMs to provide explanations for why the specific ground truth answer/label was assigned for a given example. Then, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data with LLMs. Our experiment results on three tasks, including user input and keyword relevance assessment, BoolQ, and WiC, demonstrate that AnnoLLM surpasses or performs on par with crowdsourced annotators. Furthermore, we build the first conversation-based information retrieval dataset employing AnnoLLM. This dataset is designed to facilitate the development of retrieval models capable of retrieving pertinent documents for conversational text. Human evaluation has validated the dataset's high quality."
    },
    {
        "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Self-Refine: Iterative Refinement with Self-Feedback",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2303.17651",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.17651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-03-30",
        "authors": [
            {
                "authorId": "21626987",
                "name": "Aman Madaan"
            },
            {
                "authorId": "1721168",
                "name": "Niket Tandon"
            },
            {
                "authorId": "1491232062",
                "name": "Prakhar Gupta"
            },
            {
                "authorId": "1474550731",
                "name": "Skyler Hallinan"
            },
            {
                "authorId": "49715441",
                "name": "Luyu Gao"
            },
            {
                "authorId": "35823986",
                "name": "Sarah Wiegreffe"
            },
            {
                "authorId": "47051926",
                "name": "Uri Alon"
            },
            {
                "authorId": "46217681",
                "name": "Nouha Dziri"
            },
            {
                "authorId": "9358910",
                "name": "Shrimai Prabhumoye"
            },
            {
                "authorId": "46286308",
                "name": "Yiming Yang"
            },
            {
                "authorId": "2129663",
                "name": "S. Welleck"
            },
            {
                "authorId": "3165738",
                "name": "Bodhisattwa Prasad Majumder"
            },
            {
                "authorId": "2152953535",
                "name": "Shashank Gupta"
            },
            {
                "authorId": "2112229",
                "name": "A. Yazdanbakhsh"
            },
            {
                "authorId": "48323507",
                "name": "Peter Clark"
            }
        ],
        "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach."
    },
    {
        "paperId": "3a69769d2d0d259299373698ae73c940a255e932",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Conceptual structure coheres in human cognition but not in large language models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.47.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.02754, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-04-05",
        "authors": [
            {
                "authorId": "1596903751",
                "name": "Siddharth Suresh"
            },
            {
                "authorId": "1382262357",
                "name": "Kushin Mukherjee"
            },
            {
                "authorId": "2266328048",
                "name": "Xizheng Yu"
            },
            {
                "authorId": "2266384824",
                "name": "Wei-Chun Huang"
            },
            {
                "authorId": "2213417593",
                "name": "Lisa Padua"
            },
            {
                "authorId": "2647414",
                "name": "T. Rogers"
            }
        ],
        "abstract": "Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior in various semantic tasks. Contemporary large language models (LLMs), however, make it possible to interrogate the latent structure of conceptual representations using experimental methods nearly identical to those commonly used with human participants. The current work utilizes three common techniques borrowed from cognitive psychology to estimate and compare the structure of concepts in humans and a suite of LLMs. In humans, we show that conceptual structure is robust to differences in culture, language, and method of estimation. Structures estimated from LLM behavior, while individually fairly consistent with those estimated from human behavior, vary much more depending upon the particular task used to generate responses--across tasks, estimates of conceptual structure from the very same model cohere less with one another than do human structure estimates. These results highlight an important difference between contemporary LLMs and human cognition, with implications for understanding some fundamental limitations of contemporary machine language."
    },
    {
        "paperId": "5d879530c443dd06d3686f31d32cfe34c7ade9bc",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Towards Interpretable Mental Health Analysis with Large Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.370.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.03347, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-04-06",
        "authors": [
            {
                "authorId": "2003396186",
                "name": "Kailai Yang"
            },
            {
                "authorId": "51394448",
                "name": "Shaoxiong Ji"
            },
            {
                "authorId": "9416328",
                "name": "Tianlin Zhang"
            },
            {
                "authorId": "2257034586",
                "name": "Qianqian Xie"
            },
            {
                "authorId": "13627871",
                "name": "Zi-Zhou Kuang"
            },
            {
                "authorId": "2240623492",
                "name": "Sophia Ananiadou"
            }
        ],
        "abstract": "The latest large language models (LLMs) such as ChatGPT, exhibit strong capabilities in automated mental health analysis. However, existing relevant studies bear several limitations, including inadequate evaluations, lack of prompting strategies, and ignorance of exploring LLMs for explainability. To bridge these gaps, we comprehensively evaluate the mental health analysis and emotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore the effects of different prompting strategies with unsupervised and distantly supervised emotional information. Based on these prompts, we explore LLMs for interpretable mental health analysis by instructing them to generate explanations for each of their decisions. We convey strict human evaluations to assess the quality of the generated explanations, leading to a novel dataset with 163 human-assessed explanations. We benchmark existing automatic evaluation metrics on this dataset to guide future related works. According to the results, ChatGPT shows strong in-context learning ability but still has a significant gap with advanced task-specific methods. Careful prompt engineering with emotional cues and expert-written few-shot examples can also effectively improve performance on mental health analysis. In addition, ChatGPT generates explanations that approach human performance, showing its great potential in explainable mental health analysis."
    },
    {
        "paperId": "281a7a99c16ce8f53bfbfb7aeb460dbd28648d28",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2304.05335",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.05335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-04-11",
        "authors": [
            {
                "authorId": "33341943",
                "name": "A. Deshpande"
            },
            {
                "authorId": "46258988",
                "name": "Vishvak Murahari"
            },
            {
                "authorId": "2590556",
                "name": "Tanmay Rajpurohit"
            },
            {
                "authorId": "51043791",
                "name": "A. Kalyan"
            },
            {
                "authorId": "2135381714",
                "name": "Karthik Narasimhan"
            }
        ],
        "abstract": "Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and harmful to an unsuspecting user. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems."
    },
    {
        "paperId": "035d7fce7690d61df76d7479d294e697a4319b7a",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Entity Tracking in Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2305.02363",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.02363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-03",
        "authors": [
            {
                "authorId": "8756748",
                "name": "Najoung Kim"
            },
            {
                "authorId": "145157639",
                "name": "Sebastian Schuster"
            }
        ],
        "abstract": "Keeping track of how states of entities change as a text or dialog unfolds is a key prerequisite to discourse understanding. Yet, there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities. In this work, we present a task probing to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations. We use this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track the state of entities, and find that only GPT-3.5 models, which have been pretrained on large amounts of code, exhibit this ability. We then investigate whether smaller models pretrained primarily on text can learn to track entities, through finetuning T5 on several training/evaluation splits. While performance degrades for more complex splits, we find that even when evaluated on a different set of entities from training or longer operation sequences, a finetuned model can perform non-trivial entity tracking. Taken together, these results suggest that language models can learn to track entities but pretraining on text corpora alone does not make this capacity surface."
    },
    {
        "paperId": "7b569454e5e866af863eaedf883018c5a0b168d7",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.33.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.02615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-04",
        "authors": [
            {
                "authorId": "2183269339",
                "name": "Hang Chen"
            },
            {
                "authorId": "2116740369",
                "name": "Jing Luo"
            },
            {
                "authorId": "2150439354",
                "name": "Xinyu Yang"
            },
            {
                "authorId": "2111465278",
                "name": "Wenjing Zhu"
            }
        ],
        "abstract": "Our investigation into the Affective Reasoning in Conversation (ARC) task highlights the challenge of causal discrimination. Almost all existing models, including large language models (LLMs), excel at capturing semantic correlations within utterance embeddings but fall short in determining the specific causal relationships. To overcome this limitation, we propose the incorporation of \\textit{i.i.d.} noise terms into the conversation process, thereby constructing a structural causal model (SCM). It explores how distinct causal relationships of fitted embeddings can be discerned through independent conditions. To facilitate the implementation of deep learning, we introduce the cogn frameworks to handle unstructured conversation data, and employ an autoencoder architecture to regard the unobservable noise as learnable\"implicit causes.\"Moreover, we curate a synthetic dataset that includes i.i.d. noise. Through comprehensive experiments, we validate the effectiveness and interpretability of our approach. Our code is available in https://github.com/Zodiark-ch/mater-of-our-EMNLP2023-paper."
    },
    {
        "paperId": "b6ccdd0eb776eee6b317d235e457f20175f380ff",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.03353",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.03353, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-05",
        "authors": [
            {
                "authorId": "2313917975",
                "name": "Damien Sileo"
            },
            {
                "authorId": "2216486082",
                "name": "Antoine Lernould"
            }
        ],
        "abstract": "Theory of Mind (ToM) is a critical component of intelligence, yet accurately measuring it continues to be a subject of debate. Prior research has attempted to apply human ToM assessments to natural language processing models using either human-created standardized tests or rule-based templates. However, these methods primarily focus on simplistic reasoning and require further validation. In this study, we utilize dynamic epistemic logic, which has established overlaps with ToM, to generate more intricate problems. We also introduce novel verbalization techniques to express these problems using natural language. Our findings indicate that certain language model scaling (from 70M to 6B and 350M to 174B) does not consistently yield results better than random chance. While GPT-4 demonstrates improved epistemic reasoning capabilities, there is still room for enhancement. Our code and datasets are publicly available https://github.com/antoinelrnld/modlog https://huggingface.co/datasets/sileod/mindgames"
    },
    {
        "paperId": "9702aa281204e7a692fb3ecc83981198426ff70d",
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "title": "Exploring Human-Like Translation Strategy with Large Language Models",
        "openAccessPdf": {
            "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00642/2346100/tacl_a_00642.pdf",
            "status": "GOLD",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.04118, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-06",
        "authors": [
            {
                "authorId": "2610876",
                "name": "Zhiwei He"
            },
            {
                "authorId": "31395252",
                "name": "Tian Liang"
            },
            {
                "authorId": "12386833",
                "name": "Wenxiang Jiao"
            },
            {
                "authorId": "3322871",
                "name": "Zhuosheng Zhang"
            },
            {
                "authorId": "3001727",
                "name": "Yujiu Yang"
            },
            {
                "authorId": "2151039787",
                "name": "Rui Wang"
            },
            {
                "authorId": "2909321",
                "name": "Zhaopeng Tu"
            },
            {
                "authorId": "2072684668",
                "name": "Shuming Shi"
            },
            {
                "authorId": "48631170",
                "name": "Xing Wang"
            }
        ],
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence. Among their numerous skills, the translation abilities of LLMs have received considerable attention. Compared to typical machine translation that focuses solely on source-to-target mapping, LLM-based translation can potentially mimic the human translation process, which might take preparatory steps to ensure high-quality translation. This work explores this possibility by proposing the MAPS framework, which stands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs first to analyze the given source sentence and induce three aspects of translation-related knowledge (keywords, topics, and relevant demonstrations) to guide the final translation process. Moreover, we employ a selection mechanism based on quality estimation to filter out noisy and unhelpful knowledge. Both automatic (3 LLMs \u00d7 11 directions \u00d7 2 automatic metrics) and human evaluation (preference study and MQM) demonstrate the effectiveness of MAPS. Further analysis shows that by mimicking the human translation process, MAPS reduces various translation errors such as hallucination, ambiguity, mistranslation, awkward style, untranslated text, and omission. Source code is available at https://github.com/zwhe99/MAPS-mt."
    },
    {
        "paperId": "7dc928f41e15f65f1267bd87b0fcfcc7e715cb56",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.04388",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.04388, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-07",
        "authors": [
            {
                "authorId": "144196816",
                "name": "Miles Turpin"
            },
            {
                "authorId": "38614754",
                "name": "Julian Michael"
            },
            {
                "authorId": "3439053",
                "name": "Ethan Perez"
            },
            {
                "authorId": "1799822",
                "name": "Sam Bowman"
            }
        ],
        "abstract": "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always\"(A)\"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods."
    },
    {
        "paperId": "fc09ee18ab94884cb8026a7db645dd0f0fc04d38",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "ANALOGICAL - A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.05050",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.05050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-08",
        "authors": [
            {
                "authorId": "2066216451",
                "name": "Thilini Wijesiriwardene"
            },
            {
                "authorId": "70510564",
                "name": "Ruwan Wickramarachchi"
            },
            {
                "authorId": "2216606354",
                "name": "Bimal Gajera"
            },
            {
                "authorId": "2216605596",
                "name": "S. Gowaikar"
            },
            {
                "authorId": "153232070",
                "name": "Chandan Gupta"
            },
            {
                "authorId": "40016108",
                "name": "Aman Chadha"
            },
            {
                "authorId": "8856206",
                "name": "Aishwarya N. Reganti"
            },
            {
                "authorId": "2064342742",
                "name": "Amit P. Sheth"
            },
            {
                "authorId": "48806891",
                "name": "Amitava Das"
            }
        ],
        "abstract": "Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity -- (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space. Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy."
    },
    {
        "paperId": "73207b9fd2dcfeead7fe086cfdb097e4929a7b44",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MoT: Memory-of-Thought Enables ChatGPT to Self-Improve",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.392.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.05181, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-09",
        "authors": [
            {
                "authorId": "50080067",
                "name": "Xiaonan Li"
            },
            {
                "authorId": "2188058565",
                "name": "Xipeng Qiu"
            }
        ],
        "abstract": "Large Language Models (LLMs) have shown impressive abilities in various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, humans can easily improve themselves by self-thinking and memory, without external resources. In this paper, we propose a framework, MoT, to let the LLM self-improve through Memory-of-Thought, without annotated datasets and parameter updates. Specifically, MoT is divided into two stages: 1. before the test stage, the LLM pre-thinks on the unlabeled dataset and saves the high-confidence thoughts as external memory; 2. During the test stage, given a test question, the LLM recalls relevant memory to help itself reason and answer it. Experimental results show that MoT can help ChatGPT significantly improve its abilities in arithmetic reasoning, commonsense reasoning, factual reasoning, and natural language inference. Further analyses show that each component contributes critically to the improvements and MoT can lead to consistent improvements across various CoT methods and LLMs."
    },
    {
        "paperId": "f06c38a0fd49dd1468e72696913169c0d5588fc3",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Distilling Script Knowledge from Large Language Models for Constrained Language Planning",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.05252",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.05252, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-09",
        "authors": [
            {
                "authorId": "2145968425",
                "name": "Siyu Yuan"
            },
            {
                "authorId": "5040052",
                "name": "Jiangjie Chen"
            },
            {
                "authorId": "2165331745",
                "name": "Ziquan Fu"
            },
            {
                "authorId": "2284692027",
                "name": "Xuyang Ge"
            },
            {
                "authorId": "2112624137",
                "name": "Soham Shah"
            },
            {
                "authorId": "3335921",
                "name": "C. R. Jankowski"
            },
            {
                "authorId": "1944126000",
                "name": "Deqing Yang"
            },
            {
                "authorId": "2116642640",
                "name": "Yanghua Xiao"
            }
        ],
        "abstract": "In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., \u201cmake a cake\u201d), but leaves more specific goals with multi-facet constraints understudied (e.g., \u201cmake a cake for diabetics\u201d). In this paper, we define the task of constrained language planning for the first time. We propose an over-generate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, Coscript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, Coscript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability."
    },
    {
        "paperId": "83fe73b5f35ab77444b80e2bf6fbd66b55531ad1",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "COKE: A Cognitive Knowledge Graph for Machine Theory of Mind",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.05390",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.05390, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-09",
        "authors": [
            {
                "authorId": "2109217402",
                "name": "Jincenzi Wu"
            },
            {
                "authorId": null,
                "name": "Zhuang Chen"
            },
            {
                "authorId": "2090444914",
                "name": "Jiawen Deng"
            },
            {
                "authorId": "2106627931",
                "name": "Sahand Sabour"
            },
            {
                "authorId": "2196817617",
                "name": "Minlie Huang"
            }
        ],
        "abstract": "Theory of mind (ToM) refers to humans' ability to understand and infer the desires, beliefs, and intentions of others. The acquisition of ToM plays a key role in humans' social cognition and interpersonal relations. Though indispensable for social intelligence, ToM is still lacking for modern AI and NLP systems since they cannot access the human mental state and cognitive process beneath the training corpus. To empower AI systems with the ToM ability and narrow the gap between them and humans, in this paper, we propose COKE: the first cognitive knowledge graph for machine theory of mind. Specifically, COKE formalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize human mental activities and subsequent behavioral/affective responses when facing specific social circumstances. In addition, we further generalize COKE using LLMs and build a powerful generation model COLM tailored for cognitive reasoning. Experimental results in both automatic and human evaluation demonstrate the high quality of COKE, the superior ToM ability of COLM, and its potential to significantly enhance social applications."
    },
    {
        "paperId": "7d722ec75cf4cde30156e71fffec6f8f08f91600",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.05976",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.05976, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-10",
        "authors": [
            {
                "authorId": "5040052",
                "name": "Jiangjie Chen"
            },
            {
                "authorId": "2216781782",
                "name": "Wei Shi"
            },
            {
                "authorId": "2165331745",
                "name": "Ziquan Fu"
            },
            {
                "authorId": "2110844331",
                "name": "Sijie Cheng"
            },
            {
                "authorId": "46255707",
                "name": "Lei Li"
            },
            {
                "authorId": "2116642640",
                "name": "Yanghua Xiao"
            }
        ],
        "abstract": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as \u201clions don\u2019t live in the ocean\u201d, is also ubiquitous in the world but rarely mentioned explicitly in text.What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge.We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs.Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions.We term this phenomenon the belief conflict of LLMs.Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict."
    },
    {
        "paperId": "bad287184c6739fd6f476f89cb83e09415982d9f",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.05994",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.05994, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-10",
        "authors": [
            {
                "authorId": "2145968425",
                "name": "Siyu Yuan"
            },
            {
                "authorId": "5040052",
                "name": "Jiangjie Chen"
            },
            {
                "authorId": "2118133838",
                "name": "Changzhi Sun"
            },
            {
                "authorId": "3366523",
                "name": "Jiaqing Liang"
            },
            {
                "authorId": "2116642640",
                "name": "Yanghua Xiao"
            },
            {
                "authorId": "1944126000",
                "name": "Deqing Yang"
            }
        ],
        "abstract": "Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large language models (LLMs), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables both smaller LMs and LLMs to gain better analogical reasoning capabilities."
    },
    {
        "paperId": "39c974bb16dad006353032942186087b40e25949",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Synergistic Interplay between Search and Large Language Models for Information Retrieval",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2305.07402",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.07402, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-12",
        "authors": [
            {
                "authorId": "147062881",
                "name": "Jiazhan Feng"
            },
            {
                "authorId": "8801869",
                "name": "Chongyang Tao"
            },
            {
                "authorId": "2442662",
                "name": "Xiubo Geng"
            },
            {
                "authorId": "143681703",
                "name": "Tao Shen"
            },
            {
                "authorId": "46747953",
                "name": "Can Xu"
            },
            {
                "authorId": "2062835",
                "name": "Guodong Long"
            },
            {
                "authorId": "144060462",
                "name": "Dongyan Zhao"
            },
            {
                "authorId": "2086994543",
                "name": "Daxin Jiang"
            }
        ],
        "abstract": "Information retrieval (IR) plays a crucial role in locating relevant resources from vast amounts of data, and its applications have evolved from traditional knowledge bases to modern retrieval models (RMs). The emergence of large language models (LLMs) has further revolutionized the IR field by enabling users to interact with search systems in natural languages. In this paper, we explore the advantages and disadvantages of LLMs and RMs, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leverage the benefits of both paradigms while circumventing their limitations, we propose InteR, a novel framework that facilitates information refinement through synergy between RMs and LLMs. InteR allows RMs to expand knowledge in queries using LLM-generated knowledge collections and enables LLMs to enhance prompt formulation using retrieved documents. This iterative refinement process augments the inputs of RMs and LLMs, leading to more accurate retrieval. Experiments on large-scale retrieval benchmarks involving web search and low-resource retrieval tasks demonstrate that InteR achieves overall superior zero-shot retrieval performance compared to state-of-the-art methods, even those using relevance judgment. Source code is available at https://github.com/Cyril-JZ/InteR"
    },
    {
        "paperId": "9e1ba67d5f443a8bd42a8b856534f50c429baf11",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.08246",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.08246, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-14",
        "authors": [
            {
                "authorId": "1928141171",
                "name": "Mandar Sharma"
            },
            {
                "authorId": "50027530",
                "name": "N. Muralidhar"
            },
            {
                "authorId": "48651437",
                "name": "Naren Ramakrishnan"
            }
        ],
        "abstract": "The field of Math-NLP has witnessed significant growth in recent years, motivated by the desire to expand LLM performance to the leaning of non-linguistic notions (numerals, and subsequently, arithmetic reasoning). However, non-linguistic skill injection typically comes at a cost for LLMs: it leads to catastrophic forgetting of core linguistic skills, a consequence that often remains unaddressed in the literature. As Math-NLP has been able to create LLMs that can closely approximate the mathematical skills of a grade schooler or the arithmetic reasoning skills of a calculator, the practicality of these models fail if they concomitantly shed their linguistic capabilities. In this work, we take a closer look into the phenomena of catastrophic forgetting as it pertains to LLMs and subsequently offer a novel framework for non-linguistic skill injection for LLMs based on information-theoretic interventions and skill-specific losses that enable the learning of strict arithmetic reasoning. Our model outperforms the state-of-the-art both on injected non-linguistic skills and on linguistic knowledge retention, and does so with a fraction of the non-linguistic training data (1/4) and zero additional synthetic linguistic training data."
    },
    {
        "paperId": "9c827a18a2a865e68c848b6823aec1768f9f9300",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "CoEdIT: Text Editing by Task-Specific Instruction Tuning",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.09857",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.09857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-17",
        "authors": [
            {
                "authorId": "2831377",
                "name": "Vipul Raheja"
            },
            {
                "authorId": "50271213",
                "name": "Dhruv Kumar"
            },
            {
                "authorId": "2213239540",
                "name": "Ryan Koo"
            },
            {
                "authorId": "48493368",
                "name": "Dongyeop Kang"
            }
        ],
        "abstract": "We introduce CoEdIT, a state-of-the-art text editing system for writing assistance. CoEdIT takes instructions from the user specifying the attributes of the desired text, such as\"Make the sentence simpler\"or\"Write it in a more neutral style,\"and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing benchmarks, (2) is competitive with publicly available largest-sized LLMs trained on instructions while being nearly 60x smaller, (3) is capable of generalizing to unseen edit instructions, and (4) exhibits abilities to generalize to composite instructions containing different combinations of edit actions. Through extensive qualitative and quantitative analysis, we show that writers prefer the edits suggested by CoEdIT relative to other state-of-the-art text editing models. Our code, data, and models are publicly available at https://github.com/vipulraheja/coedit."
    },
    {
        "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.10601",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.10601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-17",
        "authors": [
            {
                "authorId": "2093302161",
                "name": "Shunyu Yao"
            },
            {
                "authorId": "150978762",
                "name": "Dian Yu"
            },
            {
                "authorId": "2144551262",
                "name": "Jeffrey Zhao"
            },
            {
                "authorId": "1697494",
                "name": "Izhak Shafran"
            },
            {
                "authorId": "1799860",
                "name": "T. Griffiths"
            },
            {
                "authorId": "145144022",
                "name": "Yuan Cao"
            },
            {
                "authorId": "144958935",
                "name": "Karthik Narasimhan"
            }
        ],
        "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm."
    },
    {
        "paperId": "96df2b9d5c13d39ade0862f86c93ecabc5858f51",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.10703",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.10703, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-18",
        "authors": [
            {
                "authorId": "1633124736",
                "name": "Yue Yu"
            },
            {
                "authorId": "8103389",
                "name": "Yuchen Zhuang"
            },
            {
                "authorId": "46752897",
                "name": "Rongzhi Zhang"
            },
            {
                "authorId": "145391513",
                "name": "Yu Meng"
            },
            {
                "authorId": "3363642",
                "name": "Jiaming Shen"
            },
            {
                "authorId": "2152735278",
                "name": "Chao Zhang"
            }
        ],
        "abstract": "With the development of large language models (LLMs), zero-shot learning has attracted much attention for various NLP tasks. Different from prior works that generate training data with billion-scale natural language generation (NLG) models, we propose a retrieval-enhanced framework to create training data from a general-domain unlabeled corpus. To realize this, we first conduct contrastive pretraining to learn an unsupervised dense retriever for extracting the most relevant documents using class-descriptive verbalizers. We then further propose two simple strategies, namely Verbalizer Augmentation with Demonstrations and Self-consistency Guided Filtering to improve the topic coverage of the dataset while removing noisy examples. Experiments on nine datasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines and saves around 70% of the time compared to baselines using large NLG models. Besides, REGEN can be naturally integrated with recently proposed large language models to boost performance."
    },
    {
        "paperId": "0b315e6d4d800e04caf2f587312ce163e748d10c",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Numeric Magnitude Comparison Effects in Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.10782",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.10782, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-18",
        "authors": [
            {
                "authorId": "2051264008",
                "name": "Raj Sanjay Shah"
            },
            {
                "authorId": "2023706657",
                "name": "Vijay Marupudi"
            },
            {
                "authorId": "2217529159",
                "name": "Reba Koenen"
            },
            {
                "authorId": "2214798122",
                "name": "Khushi Bhardwaj"
            },
            {
                "authorId": "39128244",
                "name": "S. Varma"
            }
        ],
        "abstract": "Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text. In contrast, neuroscience research has identified distinct neural representations for numbers and words. In this work, we investigate how well popular LLMs capture the magnitudes of numbers (e.g., that $4<5$) from a behavioral lens. Prior research on the representational capabilities of LLMs evaluates whether they show human-level performance, for instance, high overall accuracy on standard benchmarks. Here, we ask a different question, one inspired by cognitive science: How closely do the number representations of LLMscorrespond to those of human language users, who typically demonstrate the distance, size, and ratio effects? We depend on a linking hypothesis to map the similarities among the model embeddings of number words and digits to human response times. The results reveal surprisingly human-like representations across language models of different architectures, despite the absence of the neural circuitry that directly supports these representations in the human brain. This research shows the utility of understanding LLMs using behavioral benchmarks and points the way to future work on the number representations of LLMs and their cognitive plausibility."
    },
    {
        "paperId": "98d05533678a9a09a2053b7d974738c60c4894ea",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.11541",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.11541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-19",
        "authors": [
            {
                "authorId": "2108726649",
                "name": "Zezhong Wang"
            },
            {
                "authorId": "47829900",
                "name": "Fan Yang"
            },
            {
                "authorId": "2007757792",
                "name": "Pu Zhao"
            },
            {
                "authorId": "2163383329",
                "name": "Lu Wang"
            },
            {
                "authorId": "2163389931",
                "name": "Jue Zhang"
            },
            {
                "authorId": "145076801",
                "name": "Mohit Garg"
            },
            {
                "authorId": "2793487",
                "name": "Qingwei Lin"
            },
            {
                "authorId": "2109581369",
                "name": "Dongmei Zhang"
            }
        ],
        "abstract": "Large Language Model (LLM) has gained popularity and achieved remarkable results in open-domain tasks, but its performance in real industrial domain-specific scenarios is average due to its lack of specific domain knowledge. This issue has attracted widespread attention, but there are few relevant benchmarks available. In this paper, we provide a benchmark Question Answering (QA) dataset named MSQA, centered around Microsoft products and IT technical problems encountered by customers. This dataset contains industry cloud-specific QA knowledge, an area not extensively covered in general LLMs, making it well-suited for evaluating methods aiming to enhance LLMs' domain-specific capabilities. In addition, we propose a new model interaction paradigm that can empower LLM to achieve better performance on domain-specific tasks where it is not proficient. Extensive experiments demonstrate that the approach following our method outperforms the commonly used LLM with retrieval methods. We make our source code and sample data available at: https://aka.ms/Microsoft_QA."
    },
    {
        "paperId": "c7a3f9cc61cfafdc307f8ae24430b6b1121f9b2c",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.11554",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.11554, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-19",
        "authors": [
            {
                "authorId": "2128965713",
                "name": "Shibo Hao"
            },
            {
                "authorId": "2115347044",
                "name": "Tianyang Liu"
            },
            {
                "authorId": "47197370",
                "name": "Zhen Wang"
            },
            {
                "authorId": "2749311",
                "name": "Zhiting Hu"
            }
        ],
        "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\\underline{tool}$ as a to$\\underline{ken}$ ($\\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the flexibility to plug in an arbitrary number of tools by expanding the set of toolkens on the fly. In addition, it improves tool use by allowing extensive demonstration data for learning the toolken embeddings. In diverse domains, including numerical reasoning, knowledge-based question answering, and embodied plan generation, our approach effectively augments LLMs with tools and substantially outperforms various latest baselines. ToolkenGPT demonstrates the promising ability to use relevant tools from a large tool set in complex scenarios."
    },
    {
        "paperId": "017010b941d902a467f6d329ae5e74fd67e67912",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2305.11627",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.11627, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-19",
        "authors": [
            {
                "authorId": "15532066",
                "name": "Xinyin Ma"
            },
            {
                "authorId": "150110431",
                "name": "Gongfan Fang"
            },
            {
                "authorId": "48631088",
                "name": "Xinchao Wang"
            }
        ],
        "abstract": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner"
    },
    {
        "paperId": "bcdaf6c98ddbd6809cf6241aa77200d7394db163",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2305.11738",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.11738, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-19",
        "authors": [
            {
                "authorId": "1797090",
                "name": "Zhibin Gou"
            },
            {
                "authorId": "144485528",
                "name": "Zhihong Shao"
            },
            {
                "authorId": "2171182",
                "name": "Yeyun Gong"
            },
            {
                "authorId": "1752875",
                "name": "Yelong Shen"
            },
            {
                "authorId": "3001727",
                "name": "Yujiu Yang"
            },
            {
                "authorId": "46429989",
                "name": "Nan Duan"
            },
            {
                "authorId": "2109136147",
                "name": "Weizhu Chen"
            }
        ],
        "abstract": "Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially\"black boxes\"to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs."
    },
    {
        "paperId": "d0c69c309fbf1233b6351cd57484557c16f28427",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.806.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.11792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-19",
        "authors": [
            {
                "authorId": "22642319",
                "name": "Hongru Wang"
            },
            {
                "authorId": "2248766573",
                "name": "Rui Wang"
            },
            {
                "authorId": "2248150493",
                "name": "Fei Mi"
            },
            {
                "authorId": "145843537",
                "name": "Yang Deng"
            },
            {
                "authorId": "2108726649",
                "name": "Zezhong Wang"
            },
            {
                "authorId": "2258715431",
                "name": "Bin Liang"
            },
            {
                "authorId": "2260333456",
                "name": "Ruifeng Xu"
            },
            {
                "authorId": "2237563835",
                "name": "Kam-Fai Wong"
            }
        ],
        "abstract": "Large Language Models (LLMs), such as \\texttt{ChatGPT}, greatly empower dialogue systems with strong language understanding and generation capabilities. However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status exhibited in the context. Such in-depth dialogue scenarios are challenging for existing LLMs to figure out the user's hidden needs and respond satisfactorily through a single-step inference. To this end, we propose a novel linguistic cue-based chain-of-thoughts (\\textit{Cue}-CoT), which enhances the LLMs inference with an intermediate reasoning step to find cues exhibited in the dialogue, aiming to provide a more personalized and engaging response. To evaluate the approach, we build a benchmark with in-depth dialogue questions, consisting of 6 datasets in both Chinese and English, targeting 3 major linguistic cues during the conversation: \\textit{personality}, \\textit{emotion}, and \\textit{psychology}. We conduct extensive experiments on the proposed benchmark with 5 LLMs under both zero-shot and one-shot settings. Empirical results demonstrate our proposed \\textit{Cue}-CoT method outperforms standard prompting methods in terms of both \\textit{helpfulness} and \\textit{acceptability} on all datasets."
    },
    {
        "paperId": "784335a19e41dc0cedc5e030cba85b74ba142eff",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.11828",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.11828, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-19",
        "authors": [
            {
                "authorId": "32401628",
                "name": "Hye Sun Yun"
            },
            {
                "authorId": "1808775",
                "name": "I. Marshall"
            },
            {
                "authorId": "2947796",
                "name": "T. Trikalinos"
            },
            {
                "authorId": "2111879324",
                "name": "Byron Wallace"
            }
        ],
        "abstract": "Medical systematic reviews play a vital role in healthcare decision making and policy. However, their production is time-consuming, limiting the availability of high-quality and up-to-date evidence summaries. Recent advancements in large language models (LLMs) offer the potential to automatically generate literature reviews on demand, addressing this issue. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and dangerous at worst. We conducted 16 interviews with international systematic review experts to characterize the perceived utility and risks of LLMs in the specific context of medical evidence reviews. Experts indicated that LLMs can assist in the writing process by drafting summaries, generating templates, distilling information, and crosschecking information. They also raised concerns regarding confidently composed but inaccurate LLM outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews. Informed by this qualitative analysis, we identify criteria for rigorous evaluation of biomedical LLMs aligned with domain expert views."
    },
    {
        "paperId": "7abb0536a8faa0f7b40d64d4b1f8a7ee33d9e8fe",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Evaluating Open-QA Evaluation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.12421, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-21",
        "authors": [
            {
                "authorId": "35504092",
                "name": "Cunxiang Wang"
            },
            {
                "authorId": "2110845230",
                "name": "Sirui Cheng"
            },
            {
                "authorId": "3187768",
                "name": "Qipeng Guo"
            },
            {
                "authorId": "2218316624",
                "name": "Zhikun Xu"
            },
            {
                "authorId": "2223752132",
                "name": "Bowen Ding"
            },
            {
                "authorId": "2108024279",
                "name": "Yidong Wang"
            },
            {
                "authorId": "12040998",
                "name": "Xiangkun Hu"
            },
            {
                "authorId": "47294621",
                "name": "Zheng Zhang"
            },
            {
                "authorId": "2211964951",
                "name": "Yue Zhang"
            }
        ],
        "abstract": "This study focuses on the evaluation of the Open Question Answering (Open-QA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes human-annotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this area. All resources are available at \\url{https://github.com/wangcunxiang/QA-Eval} and it is under the Apache-2.0 License."
    },
    {
        "paperId": "c10ce97f538adecfc0bc15e6ca39dd5d5f002bc1",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2305.12660",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.12660, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-22",
        "authors": [
            {
                "authorId": "2145968425",
                "name": "Siyu Yuan"
            },
            {
                "authorId": "5040052",
                "name": "Jiangjie Chen"
            },
            {
                "authorId": "2284692027",
                "name": "Xuyang Ge"
            },
            {
                "authorId": "2116642640",
                "name": "Yanghua Xiao"
            },
            {
                "authorId": "2089312",
                "name": "Deqing Yang"
            }
        ],
        "abstract": "The vital role of analogical reasoning in human cognition allows us to grasp novel concepts by linking them with familiar ones through shared relational structures. Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition. In response to this, our paper introduces a task of analogical structure abduction, grounded in cognitive psychology, designed to abduce structures that form an analogy between two systems. In support of this task, we establish a benchmark called SCAR, containing 400 scientific analogies from 13 distinct fields, tailored for evaluating analogical reasoning with structure abduction. The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT-4, in mastering this task, signifying the need for future exploration to enhance their abilities."
    },
    {
        "paperId": "9c58634e8040eccfdeb21e4b8884476318e282f3",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Learning Interpretable Style Embeddings via Prompting LLMs",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2305.12696",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.12696, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-22",
        "authors": [
            {
                "authorId": "2109171018",
                "name": "Ajay Patel"
            },
            {
                "authorId": "48810734",
                "name": "D. Rao"
            },
            {
                "authorId": "1763608",
                "name": "Chris Callison-Burch"
            }
        ],
        "abstract": "Style representation learning builds content-independent representations of author style in text. Stylometry, the analysis of style in text, is often performed by expert forensic linguists and no large dataset of stylometric annotations exists for training. Current style representation learning uses neural methods to disentangle style from content to create style vectors, however, these approaches result in uninterpretable representations, complicating their usage in downstream applications like authorship attribution where auditing and explainability is critical. In this work, we use prompting to perform stylometry on a large number of texts to create a synthetic dataset and train human-interpretable style representations we call LISA embeddings. We release our synthetic stylometry dataset and our interpretable style models as resources."
    },
    {
        "paperId": "1e5add53dca5ca9315882b003dfbaa4b28d480f4",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Beyond Labels: Empowering Human Annotators with Natural Language Explanations through a Novel Active-Learning Architecture",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.778.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.12710, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-22",
        "authors": [
            {
                "authorId": "1490485182",
                "name": "Bingsheng Yao"
            },
            {
                "authorId": "144377686",
                "name": "Ishan Jindal"
            },
            {
                "authorId": "2261363163",
                "name": "Lucian Popa"
            },
            {
                "authorId": "2208580",
                "name": "Yannis Katsis"
            },
            {
                "authorId": "2261363327",
                "name": "Sayan Ghosh"
            },
            {
                "authorId": "2261366744",
                "name": "Lihong He"
            },
            {
                "authorId": "2155710822",
                "name": "Yuxuan Lu"
            },
            {
                "authorId": "2253588064",
                "name": "Shashank Srivastava"
            },
            {
                "authorId": "1573872877",
                "name": "Yunyao Li"
            },
            {
                "authorId": "2257272123",
                "name": "James A. Hendler"
            },
            {
                "authorId": "2243367965",
                "name": "Dakuo Wang"
            }
        ],
        "abstract": "Real-world domain experts (e.g., doctors) rarely annotate only a decision label in their day-to-day workflow without providing explanations. Yet, existing low-resource learning techniques, such as Active Learning (AL), that aim to support human annotators mostly focus on the label while neglecting the natural language explanation of a data point. This work proposes a novel AL architecture to support experts' real-world need for label and explanation annotations in low-resource scenarios. Our AL architecture leverages an explanation-generation model to produce explanations guided by human explanations, a prediction model that utilizes generated explanations toward prediction faithfully, and a novel data diversity-based AL sampling strategy that benefits from the explanation annotations. Automated and human evaluations demonstrate the effectiveness of incorporating explanations into AL sampling and the improved human annotation efficiency and trustworthiness with our AL architecture. Additional ablation studies illustrate the potential of our AL architecture for transfer learning, generalizability, and integration with large language models (LLMs). While LLMs exhibit exceptional explanation-generation capabilities for relatively simple tasks, their effectiveness in complex real-world tasks warrants further in-depth study."
    },
    {
        "paperId": "9eadfe920cae4a451af437752de83075ec528288",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.278.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13091, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-22",
        "authors": [
            {
                "authorId": "2152867918",
                "name": "Chenhui Shen"
            },
            {
                "authorId": "123962152",
                "name": "Liying Cheng"
            },
            {
                "authorId": "2147330214",
                "name": "Yang You"
            },
            {
                "authorId": "1996394",
                "name": "Lidong Bing"
            }
        ],
        "abstract": "With the recent undeniable advancement in reasoning abilities in large language models (LLMs) like ChatGPT and GPT-4, there is a growing trend for using LLMs on various tasks. One area where LLMs can be employed is as an alternative evaluation metric for complex generative tasks, which generally demands expensive human judges to complement the traditional automatic metrics for various evaluation dimensions such as fluency and consistency. In this work, we conduct extensive analysis to investigate the stability and reliability of LLMs as automatic evaluators for abstractive summarization. We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance and become more unreliable with higher-quality summaries by obtaining a lower correlation with humans. In other words, with better abstractive summarization systems being introduced at a fast pace, LLMs may result in misleading and unreliable evaluations."
    },
    {
        "paperId": "0e0d72be9950fde9b5e8996e2147d1318f216ebb",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Prompting is not a substitute for probability measurements in large language models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.306.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13264, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-22",
        "authors": [
            {
                "authorId": "2112327566",
                "name": "Jennifer Hu"
            },
            {
                "authorId": "50007746",
                "name": "R. Levy"
            }
        ],
        "abstract": "Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models' probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models' linguistic knowledge. Broadly, we find that LLMs' metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. Our results also highlight the value that is lost with the move to closed APIs where access to probability distributions is limited."
    },
    {
        "paperId": "ea647df1d12698114a87cbf043160edbc4cd0722",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "CLASS: A Design Framework for Building Intelligent Tutoring Systems Based on Learning Science principles",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.130.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13272, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-22",
        "authors": [
            {
                "authorId": "1720691070",
                "name": "Shashank Sonkar"
            },
            {
                "authorId": "2174968416",
                "name": "Lucy Liu"
            },
            {
                "authorId": "5770203",
                "name": "D. B. Mallick"
            },
            {
                "authorId": "144908066",
                "name": "Richard Baraniuk"
            }
        ],
        "abstract": "We present a design framework called Conversational Learning with Analytical Step-by-Step Strategies (CLASS) for building advanced Intelligent Tutoring Systems (ITS) powered by high-performance Large Language Models (LLMs). The CLASS framework empowers ITS with two key capabilities. First, through a carefully curated scaffolding dataset, CLASS equips ITS with essential problem-solving strategies, enabling it to provide tutor-like, step-by-step guidance to students. Second, by using a dynamic conversational dataset, CLASS assists ITS in facilitating natural language interactions, fostering engaging student-tutor conversations. The CLASS framework also provides valuable insights into ITS' internal decision-making process which allows seamless integration of user feedback, thus enabling continuous refinement and improvement. We also present a proof-of-concept ITS, referred to as SPOCK, which is trained using the CLASS framework with a focus on introductory college-level biology content. A carefully constructed protocol was developed for SPOCK's preliminary evaluation, examining aspects such as the factual accuracy and relevance of its responses. Experts in the field of biology offered favorable remarks, particularly highlighting SPOCK's capability to break down questions into manageable subproblems and provide encouraging responses to students. Code and models are available at https://github.com/luffycodes/Tutorbot-Spock."
    },
    {
        "paperId": "10f829a80a7ee0cdb16307f04e206133d75f81da",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13300, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-22",
        "authors": [
            {
                "authorId": "2153624353",
                "name": "Jian Xie"
            },
            {
                "authorId": "145086492",
                "name": "Kai Zhang"
            },
            {
                "authorId": "5040052",
                "name": "Jiangjie Chen"
            },
            {
                "authorId": "2118614649",
                "name": "Renze Lou"
            },
            {
                "authorId": "1758652",
                "name": "Yu Su"
            }
        ],
        "abstract": "By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals seemingly contradicting behaviors of LLMs. On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. On the other hand, LLMs also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time. These results pose important implications that are worth careful consideration for the further development and deployment of tool- and retrieval-augmented LLMs. Resources are available at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict."
    },
    {
        "paperId": "6c340ff334beac9524629d19f84544ed2bb29e85",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Can LLMs facilitate interpretation of pre-trained language models?",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.13386",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13386, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-22",
        "authors": [
            {
                "authorId": "2171367840",
                "name": "Basel Mousi"
            },
            {
                "authorId": "145938140",
                "name": "Nadir Durrani"
            },
            {
                "authorId": "6415321",
                "name": "Fahim Dalvi"
            }
        ],
        "abstract": "Work done to uncover the knowledge encoded within pre-trained language models rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation. We propose using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis of pre-trained language models. We discover latent concepts within pre-trained language models by applying agglomerative hierarchical clustering over contextualized representations and then annotate these concepts using ChatGPT. Our findings demonstrate that ChatGPT produces accurate and semantically richer annotations compared to human-annotated concepts. Additionally, we showcase how GPT-based annotations empower interpretation analysis methodologies of which we demonstrate two: probing frameworks and neuron interpretation. To facilitate further exploration and experimentation in the field, we make available a substantial ConceptNet dataset (TCN) comprising 39,000 annotated concepts."
    },
    {
        "paperId": "2338d7c9ab07e6d0f4160335dce0e6e6a87c4749",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.13412",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13412, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-22",
        "authors": [
            {
                "authorId": "2143482843",
                "name": "Yiming Wang"
            },
            {
                "authorId": "3322871",
                "name": "Zhuosheng Zhang"
            },
            {
                "authorId": "2151038501",
                "name": "Rui Wang"
            }
        ],
        "abstract": "Automatic summarization generates concise summaries that contain key ideas of source documents.As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the \u201cLasswell Communication Model\u201d proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs\u2019 zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT."
    },
    {
        "paperId": "f52af5abe78ca2af836f70ce193f0161bc2e6264",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.13455",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13455, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-22",
        "authors": [
            {
                "authorId": "2046531185",
                "name": "Kranti Chalamalasetti"
            },
            {
                "authorId": "1594732196",
                "name": "Jana Gotze"
            },
            {
                "authorId": "2079305",
                "name": "Sherzod Hakimov"
            },
            {
                "authorId": "1823518201",
                "name": "Brielen Madureira"
            },
            {
                "authorId": "84039136",
                "name": "Philipp Sadler"
            },
            {
                "authorId": "1817455",
                "name": "David Schlangen"
            }
        ],
        "abstract": "Recent work has proposed a methodology for the systematic evaluation of\"Situated Language Understanding Agents\"-agents that operate in rich linguistic and non-linguistic contexts-through testing them in carefully constructed interactive settings. Other recent work has argued that Large Language Models (LLMs), if suitably set up, can be understood as (simulators of) such agents. A connection suggests itself, which this paper explores: Can LLMs be evaluated meaningfully by exposing them to constrained game-like settings that are built to challenge specific capabilities? As a proof of concept, this paper investigates five interaction settings, showing that current chat-optimised LLMs are, to an extent, capable to follow game-play instructions. Both this capability and the quality of the game play, measured by how well the objectives of the different games are met, follows the development cycle, with newer models performing better. The metrics even for the comparatively simple example games are far from being saturated, suggesting that the proposed instrument will remain to have diagnostic value. Our general framework for implementing and evaluating games with LLMs is available at https://github.com/clembench ."
    },
    {
        "paperId": "fef6471c4a2a0e7abc4a2261a6cf916e34091d12",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "On the Risk of Misinformation Pollution with Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.13661",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13661, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-23",
        "authors": [
            {
                "authorId": "2218334588",
                "name": "Yikang Pan"
            },
            {
                "authorId": "3470231",
                "name": "Liangming Pan"
            },
            {
                "authorId": "2109664620",
                "name": "Wenhu Chen"
            },
            {
                "authorId": "2026545715",
                "name": "Preslav Nakov"
            },
            {
                "authorId": "37596605",
                "name": "Min-Yen Kan"
            },
            {
                "authorId": "152876475",
                "name": "W. Wang"
            }
        ],
        "abstract": "In this paper, we comprehensively investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems. To mitigate the harm caused by LLM-generated misinformation, we explore three defense strategies: prompting, misinformation detection, and majority voting. While initial results show promising trends for these defensive strategies, much more work needs to be done to address the challenge of misinformation pollution. Our work highlights the need for further research and interdisciplinary collaboration to address LLM-generated misinformation and to promote responsible use of LLMs."
    },
    {
        "paperId": "895f3c9e452ae51fb02786de424ce6d2bba11c3b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Towards Legally Enforceable Hate Speech Detection for Public Forums",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.13677",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-23",
        "authors": [
            {
                "authorId": "2089528608",
                "name": "Chunyan Luo"
            },
            {
                "authorId": "2008160154",
                "name": "R. Bhambhoria"
            },
            {
                "authorId": "2130251018",
                "name": "Xiao-Dan Zhu"
            },
            {
                "authorId": "119208875",
                "name": "Samuel Dahan"
            }
        ],
        "abstract": "Hate speech causes widespread and deep-seated societal issues. Proper enforcement of hate speech laws is key for protecting groups of people against harmful and discriminatory language. However, determining what constitutes hate speech is a complex task that is highly open to subjective interpretations. Existing works do not align their systems with enforceable definitions of hate speech, which can make their outputs inconsistent with the goals of regulators. This research introduces a new perspective and task for enforceable hate speech detection centred around legal definitions, and a dataset annotated on violations of eleven possible definitions by legal experts. Given the challenge of identifying clear, legally enforceable instances of hate speech, we augment the dataset with expert-generated samples and an automatically mined challenge set. We experiment with grounding the model decision in these definitions using zero-shot and few-shot prompting. We then report results on several large language models (LLMs). With this task definition, automatic hate speech detection can be more closely aligned to enforceable laws, and hence assist in more rigorous enforcement of legal protections against harmful speech in public forums."
    },
    {
        "paperId": "c22bfecc684be370bc22611deb8737d65466a390",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.13712",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13712, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-23",
        "authors": [
            {
                "authorId": "2039956094",
                "name": "Alfonso Amayuelas"
            },
            {
                "authorId": "3470231",
                "name": "Liangming Pan"
            },
            {
                "authorId": "2109664620",
                "name": "Wenhu Chen"
            },
            {
                "authorId": "152876475",
                "name": "W. Wang"
            }
        ],
        "abstract": "This paper investigates the capabilities of Large Language Models (LLMs) in the context of understanding their knowledge and uncertainty over questions. Specifically, we focus on addressing known-unknown questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a new dataset with Known-Unknown Questions (KUQ) and establish a categorization framework to clarify the origins of uncertainty in such queries. Subsequently, we examine the performance of open-source LLMs, fine-tuned using this dataset, in distinguishing between known and unknown queries within open-ended question-answering scenarios. The fine-tuned models demonstrated a significant improvement, achieving a considerable increase in F1-score relative to their pre-fine-tuning state. Through a comprehensive analysis, we reveal insights into the models' improved uncertainty articulation and their consequent efficacy in multi-agent debates. These findings help us understand how LLMs can be trained to identify and express uncertainty, improving our knowledge of how they understand and express complex or unclear information."
    },
    {
        "paperId": "76750c59ec126cc4bfdfef30648598bd5b94220b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Can Large Language Models Capture Dissenting Human Voices?",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.278.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13788, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-23",
        "authors": [
            {
                "authorId": "2190447321",
                "name": "Noah Lee"
            },
            {
                "authorId": "2218291796",
                "name": "Na Min An"
            },
            {
                "authorId": "2053211210",
                "name": "James Thorne"
            }
        ],
        "abstract": "Large language models (LLMs) have shown impressive achievements in solving a broad range of tasks. Augmented by instruction fine-tuning, LLMs have also been shown to generalize in zero-shot settings as well. However, whether LLMs closely align with the human disagreement distribution has not been well-studied, especially within the scope of natural language inference (NLI). In this paper, we evaluate the performance and alignment of LLM distribution with humans using two different techniques to estimate the multinomial distribution: Monte Carlo Estimation (MCE) and Log Probability Estimation (LPE). As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population. The source code for the experiments is available at https://github.com/xfactlab/emnlp2023-LLM-Disagreement"
    },
    {
        "paperId": "a10843d1349fff8d2a7d9722f800802187fef67f",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14152",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14152, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-23",
        "authors": [
            {
                "authorId": "2144193082",
                "name": "Jeonghoon Kim"
            },
            {
                "authorId": "2119171752",
                "name": "J. H. Lee"
            },
            {
                "authorId": "2829848",
                "name": "Sungdong Kim"
            },
            {
                "authorId": "48490725",
                "name": "Joonsuk Park"
            },
            {
                "authorId": "31760501",
                "name": "Kang Min Yoo"
            },
            {
                "authorId": "12693169",
                "name": "S. Kwon"
            },
            {
                "authorId": "122808525",
                "name": "Dongsoo Lee"
            }
        ],
        "abstract": "Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage. We employ PEQA-tuning for task-specific adaptation on LLMs with up to 65 billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA."
    },
    {
        "paperId": "b3cff6abe401a244c21d4706b0931e48acaeeb4e",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14214",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-23",
        "authors": [
            {
                "authorId": "2090357303",
                "name": "Benjamin Minixhofer"
            },
            {
                "authorId": "153733568",
                "name": "Jonas Pfeiffer"
            },
            {
                "authorId": "1747849",
                "name": "Ivan Vulic"
            }
        ],
        "abstract": "While many languages possess processes of joining two or more words to create compound words, previous studies have been typically limited only to languages with excessively productive compound formation (e.g., German, Dutch) and there is no public dataset containing compound and non-compound words across a large number of languages. In this work, we systematically study decompounding, the task of splitting compound words into their constituents, at a wide scale. We first address the data gap by introducing a dataset of 255k compound and non-compound words across 56 diverse languages obtained from Wiktionary. We then use this dataset to evaluate an array of Large Language Models (LLMs) on the decompounding task. We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization. We thus introduce a novel methodology to train dedicated models for decompounding. The proposed two-stage procedure relies on a fully self-supervised objective in the first stage, while the second, supervised learning stage optionally fine-tunes the model on the annotated Wiktionary data. Our self-supervised models outperform the prior best unsupervised decompounding models by 13.9% accuracy on average. Our fine-tuned models outperform all prior (language-specific) decompounding tools. Furthermore, we use our models to leverage decompounding during the creation of a subword tokenizer, which we refer to as CompoundPiece. CompoundPiece tokenizes compound words more favorably on average, leading to improved performance on decompounding over an otherwise equivalent model using SentencePiece tokenization."
    },
    {
        "paperId": "eda54452d8a8a412c2a985ef11572cb468906b1f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14235",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14235, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-23",
        "authors": [
            {
                "authorId": "49775305",
                "name": "Ruochen Zhang"
            },
            {
                "authorId": "66986482",
                "name": "Samuel Cahyawijaya"
            },
            {
                "authorId": "51017310",
                "name": "Jan Christian Blaise Cruz"
            },
            {
                "authorId": "8129718",
                "name": "Alham Fikri Aji"
            }
        ],
        "abstract": "Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current\"multilingualism\"in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy."
    },
    {
        "paperId": "3de99f885cfc0c2145cd584df7df4230cccaea04",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Evaluation of African American Language Bias in Natural Language Generation",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14291",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14291, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-23",
        "authors": [
            {
                "authorId": "2303366796",
                "name": "Nicholas Deas"
            },
            {
                "authorId": "69026665",
                "name": "Jessica A. Grieser"
            },
            {
                "authorId": "1484441195",
                "name": "Shana Kleiner"
            },
            {
                "authorId": "2767140",
                "name": "D. Patton"
            },
            {
                "authorId": "1402934614",
                "name": "Elsbeth Turcan"
            },
            {
                "authorId": "145590324",
                "name": "K. McKeown"
            }
        ],
        "abstract": "We evaluate how well LLMs understand African American Language (AAL) in comparison to their performance on White Mainstream English (WME), the encouraged\"standard\"form of English taught in American classrooms. We measure LLM performance using automatic metrics and human judgments for two tasks: a counterpart generation task, where a model generates AAL (or WME) given WME (or AAL), and a masked span prediction (MSP) task, where models predict a phrase that was removed from their input. Our contributions include: (1) evaluation of six pre-trained, large language models on the two language generation tasks; (2) a novel dataset of AAL text from multiple contexts (social media, hip-hop lyrics, focus groups, and linguistic interviews) with human-annotated counterparts in WME; and (3) documentation of model performance gaps that suggest bias and identification of trends in lack of understanding of AAL features."
    },
    {
        "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14314",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-23",
        "authors": [
            {
                "authorId": "3239480",
                "name": "Tim Dettmers"
            },
            {
                "authorId": "51152502",
                "name": "Artidoro Pagnoni"
            },
            {
                "authorId": "14487640",
                "name": "Ari Holtzman"
            },
            {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
            }
        ],
        "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training."
    },
    {
        "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14325",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-23",
        "authors": [
            {
                "authorId": "15394275",
                "name": "Yilun Du"
            },
            {
                "authorId": "145015904",
                "name": "Shuang Li"
            },
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            },
            {
                "authorId": "1763295",
                "name": "J. Tenenbaum"
            },
            {
                "authorId": "2316241372",
                "name": "Igor Mordatch"
            }
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding."
    },
    {
        "paperId": "fb3d4e53fd3ac818f86d1cd32cb7423a64fc6cfe",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Benchmarking Machine Translation with Cultural Awareness",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14328",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14328, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-23",
        "authors": [
            {
                "authorId": "2166137151",
                "name": "Binwei Yao"
            },
            {
                "authorId": "2152153638",
                "name": "Ming Jiang"
            },
            {
                "authorId": "2143919864",
                "name": "Diyi Yang"
            },
            {
                "authorId": "2149221827",
                "name": "Junjie Hu"
            }
        ],
        "abstract": "Translating culture-related content is vital for effective cross-cultural communication. However, many culture-specific items (CSIs) often lack viable translations across languages, making it challenging to collect high-quality, diverse parallel corpora with CSI annotations. This difficulty hinders the analysis of cultural awareness of machine translation (MT) systems, including traditional neural MT and the emerging MT paradigm using large language models (LLM). To address this gap, we introduce a novel parallel corpus, enriched with CSI annotations in 6 language pairs for investigating Culturally-Aware Machine Translation--CAMT. Furthermore, we design two evaluation metrics to assess CSI translations, focusing on their pragmatic translation quality. Our findings show the superior ability of LLMs over neural MTs in leveraging external cultural knowledge for translating CSIs, especially those lacking translations in the target culture."
    },
    {
        "paperId": "ea3df96325786f4d61c9f8d74e58c0083504eb7b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "APPLS: Evaluating Evaluation Metrics for Plain Language Summarization",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14341",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14341, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-23",
        "authors": [
            {
                "authorId": "2124928819",
                "name": "Yue Guo"
            },
            {
                "authorId": "50509991",
                "name": "Tal August"
            },
            {
                "authorId": "3168950",
                "name": "Gondy Leroy"
            },
            {
                "authorId": "2696466",
                "name": "T. Cohen"
            },
            {
                "authorId": "31860505",
                "name": "Lucy Lu Wang"
            }
        ],
        "abstract": "While there has been significant development of models for Plain Language Summarization (PLS), evaluation remains a challenge. PLS lacks a dedicated assessment metric, and the suitability of text generation evaluation metrics is unclear due to the unique transformations involved (e.g., adding background explanations, removing jargon). To address these questions, our study introduces a granular meta-evaluation testbed, APPLS, designed to evaluate metrics for PLS. We identify four PLS criteria from previous work\u2014informativeness, simplification, coherence, and faithfulness\u2014and define a set of perturbations corresponding to these criteria that sensitive metrics should be able to detect. We apply these perturbations to extractive hypotheses for two PLS datasets to form our testbed. Using APPLS, we assess performance of 14 metrics, including automated scores, lexical features, and LLM prompt-based evaluations. Our analysis reveals that while some current metrics show sensitivity to specific criteria, no single method captures all four criteria simultaneously. We therefore recommend a suite of automated metrics be used to capture PLS quality along all relevant criteria. This work contributes the first meta-evaluation testbed for PLS and a comprehensive evaluation of existing metrics."
    },
    {
        "paperId": "073e6b91adc25c656d85002e3cb059e4530db20b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14386",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14386, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-22",
        "authors": [
            {
                "authorId": "151474408",
                "name": "Zhenwen Liang"
            },
            {
                "authorId": "38767143",
                "name": "W. Yu"
            },
            {
                "authorId": "2590556",
                "name": "Tanmay Rajpurohit"
            },
            {
                "authorId": "48323507",
                "name": "Peter Clark"
            },
            {
                "authorId": "2156004513",
                "name": "Xiangliang Zhang"
            },
            {
                "authorId": "2218907879",
                "name": "Ashwin Kaylan"
            }
        ],
        "abstract": "In this paper, we present a novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models. Our approach is designed to consider the student model's weaknesses and foster a tailored learning experience by generating targeted exercises aligned with educational science principles, such as knowledge tracing and personalized learning. Concretely, we let GPT-3 be a math tutor and run two steps iteratively: 1) assessing the student model's current learning status on a GPT-generated exercise book, and 2) improving the student model by training it with tailored exercise samples generated by GPT-3. Experimental results reveal that our approach outperforms LLMs (e.g., GPT-3 and PaLM) in accuracy across three distinct benchmarks while employing significantly fewer parameters. Furthermore, we provide a comprehensive analysis of the various components within our methodology to substantiate their efficacy."
    },
    {
        "paperId": "6cd26d124ffeb6ce301ef351aada27fa0852f81b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14536",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14536, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-23",
        "authors": [
            {
                "authorId": "23126830",
                "name": "Jakub Macina"
            },
            {
                "authorId": "2048028927",
                "name": "Nico Daheim"
            },
            {
                "authorId": "2105636395",
                "name": "Sankalan Pal Chowdhury"
            },
            {
                "authorId": "145679048",
                "name": "Tanmay Sinha"
            },
            {
                "authorId": "2465316",
                "name": "Manu Kapur"
            },
            {
                "authorId": "1730400",
                "name": "Iryna Gurevych"
            },
            {
                "authorId": "2790926",
                "name": "Mrinmaya Sachan"
            }
        ],
        "abstract": "While automatic dialogue tutors hold great potential in making education personalized and more accessible, research on such systems has been hampered by a lack of sufficiently large and high-quality datasets. Collecting such datasets remains challenging, as recording tutoring sessions raises privacy concerns and crowdsourcing leads to insufficient data quality. To address this, we propose a framework to generate such dialogues by pairing human teachers with a Large Language Model (LLM) prompted to represent common student errors. We describe how we use this framework to collect MathDial, a dataset of 3k one-to-one teacher-student tutoring dialogues grounded in multi-step math reasoning problems. While models like GPT-3 are good problem solvers, they fail at tutoring because they generate factually incorrect feedback or are prone to revealing solutions to students too early. To overcome this, we let teachers provide learning opportunities to students by guiding them using various scaffolding questions according to a taxonomy of teacher moves. We demonstrate MathDial and its extensive annotations can be used to finetune models to be more effective tutors (and not just solvers). We confirm this by automatic and human evaluation, notably in an interactive setting that measures the trade-off between student solving success and telling solutions. The dataset is released publicly."
    },
    {
        "paperId": "5c8c9c101e9b9048cd25ec7e90b84c6c01c6a70f",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Meta-Tuning LLMs to Leverage Lexical Knowledge for Generalizable Language Style Understanding",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2305.14592",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14592, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-24",
        "authors": [
            {
                "authorId": "1491243862",
                "name": "Ruohao Guo"
            },
            {
                "authorId": "145738420",
                "name": "Wei Xu"
            },
            {
                "authorId": "1863425",
                "name": "Alan Ritter"
            }
        ],
        "abstract": "Language style is often used by writers to convey their intentions, identities, and mastery of language. In this paper, we show that current large language models struggle to capture some language styles without fine-tuning. To address this challenge, we investigate whether LLMs can be meta-trained based on representative lexicons to recognize new styles they have not been fine-tuned on. Experiments on 13 established style classification tasks, as well as 63 novel tasks generated using LLMs, demonstrate that meta-training with style lexicons consistently improves zero-shot transfer across styles. We release the code and data at http://github.com/octaviaguo/Style-LLM ."
    },
    {
        "paperId": "cdb0e126f03caeb95ad947e13180d3219a2ffe04",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2305.14724",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14724, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-24",
        "authors": [
            {
                "authorId": "51448832",
                "name": "Tuhin Chakrabarty"
            },
            {
                "authorId": "20974925",
                "name": "Arkadiy Saakyan"
            },
            {
                "authorId": "32772049",
                "name": "Olivia Winn"
            },
            {
                "authorId": "2074098576",
                "name": "Artemis Panagopoulou"
            },
            {
                "authorId": "2109409802",
                "name": "Yue Yang"
            },
            {
                "authorId": "2817917",
                "name": "Marianna Apidianaki"
            },
            {
                "authorId": "2295928",
                "name": "S. Muresan"
            }
        ],
        "abstract": "Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL$\\cdot$E 2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the task through the collaboration between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3 (davinci-002) with Chain-of-Thought prompting generates text that represents a visual elaboration of the linguistic metaphor containing the implicit meaning and relevant objects, which is then used as input to the diffusion-based text-to-image models.Using a human-AI collaboration framework, where humans interact both with the LLM and the top-performing diffusion model, we create a high-quality dataset containing 6,476 visual metaphors for 1,540 linguistic metaphors and their associated visual elaborations. Evaluation by professional illustrators shows the promise of LLM-Diffusion Model collaboration for this task . To evaluate the utility of our Human-AI collaboration framework and the quality of our dataset, we perform both an intrinsic human-based evaluation and an extrinsic evaluation using visual entailment as a downstream task."
    },
    {
        "paperId": "a30d5f2f10cef8af4efd4f929dfe2ce90c8b3010",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.676.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14889, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-24",
        "authors": [
            {
                "authorId": "9605732",
                "name": "Ziang Xiao"
            },
            {
                "authorId": "116270523",
                "name": "Susu Zhang"
            },
            {
                "authorId": "120801533",
                "name": "Vivian Lai"
            },
            {
                "authorId": "144921048",
                "name": "Q. Liao"
            }
        ],
        "abstract": "We address a fundamental challenge in Natural Language Generation (NLG) model evaluation -- the design and evaluation of evaluation metrics. Recognizing the limitations of existing automatic metrics and noises from how current human evaluation was conducted, we propose MetricEval, a framework informed by measurement theory, the foundation of educational test design, for conceptualizing and evaluating the reliability and validity of NLG evaluation metrics. The framework formalizes the source of measurement error and offers statistical tools for evaluating evaluation metrics based on empirical data. With our framework, one can quantify the uncertainty of the metrics to better interpret the result. To exemplify the use of our framework in practice, we analyzed a set of evaluation metrics for summarization and identified issues related to conflated validity structure in human-eval and reliability in LLM-based metrics. Through MetricEval, we aim to promote the design, evaluation, and interpretation of valid and reliable metrics to advance robust and effective NLG models."
    },
    {
        "paperId": "5db0f55332839c408e3049cea1a6ad48fefba70c",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Aligning Language Models to User Opinions",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14929",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-24",
        "authors": [
            {
                "authorId": "2165225814",
                "name": "EunJeong Hwang"
            },
            {
                "authorId": "3165738",
                "name": "Bodhisattwa Prasad Majumder"
            },
            {
                "authorId": "1721168",
                "name": "Niket Tandon"
            }
        ],
        "abstract": "An important aspect of developing LLMs that interact with humans is to align models' behavior to their users. It is possible to prompt an LLM into behaving as a certain persona, especially a user group or ideological persona the model captured during its pertaining stage. But, how to best align an LLM with a specific user and not a demographic or ideological group remains an open question. Mining public opinion surveys (by Pew Research), we find that the opinions of a user and their demographics and ideologies are not mutual predictors. We use this insight to align LLMs by modeling both user opinions as well as user demographics and ideology, achieving up to 7 points accuracy gains in predicting public opinions from survey questions across a broad set of topics. In addition to the typical approach of prompting LLMs with demographics and ideology, we discover that utilizing the most relevant past opinions from individual users enables the model to predict user opinions more accurately."
    },
    {
        "paperId": "19c63eade265d8a47d160098d97194b3b83d3770",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "In-Context Impersonation Reveals Large Language Models' Strengths and Biases",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14930",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14930, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-24",
        "authors": [
            {
                "authorId": "151097297",
                "name": "Leonard Salewski"
            },
            {
                "authorId": "40894329",
                "name": "Stephan Alaniz"
            },
            {
                "authorId": "2237800855",
                "name": "Isabel Rio-Torto"
            },
            {
                "authorId": "49427184",
                "name": "Eric Schulz"
            },
            {
                "authorId": "2893664",
                "name": "Zeynep Akata"
            }
        ],
        "abstract": "In everyday conversations, humans can take on different roles and adapt their vocabulary to their chosen roles. We explore whether LLMs can take on, that is impersonate, different roles when they generate text in-context. We ask LLMs to assume different personas before solving vision and language tasks. We do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise. In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration. In a language-based reasoning task, we find that LLMs impersonating domain experts perform better than LLMs impersonating non-domain experts. Finally, we test whether LLMs' impersonations are complementary to visual information when describing different categories. We find that impersonation can improve performance: an LLM prompted to be a bird expert describes birds better than one prompted to be a car expert. However, impersonation can also uncover LLMs' biases: an LLM prompted to be a man describes cars better than one prompted to be a woman. These findings demonstrate that LLMs are capable of taking on diverse roles and that this in-context impersonation can be used to uncover their hidden strengths and biases."
    },
    {
        "paperId": "c1592c211f8b7791a55afd7162249c723b87c237",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.699.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14938, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-24",
        "authors": [
            {
                "authorId": "2111881583",
                "name": "Minje Choi"
            },
            {
                "authorId": "51136528",
                "name": "Jiaxin Pei"
            },
            {
                "authorId": "2155796033",
                "name": "Sagar Kumar"
            },
            {
                "authorId": "2068918348",
                "name": "Chang Shu"
            },
            {
                "authorId": "3046220",
                "name": "David Jurgens"
            }
        ],
        "abstract": "Large language models (LLMs) have been shown to perform well at a variety of syntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well LLMs understand \\textit{social} language. Here, we introduce a new theory-driven benchmark, SocKET, that contains 58 NLP tasks testing social knowledge which we group into five categories: humor&sarcasm, offensiveness, sentiment&emotion, and trustworthiness. In tests on the benchmark, we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks, which were predicted from theory. Through zero-shot evaluations, we show that pretrained models already possess some innate but limited capabilities of social language understanding and training on one category of tasks can improve zero-shot testing on others. Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs. The associated resources are released at https://github.com/minjechoi/SOCKET."
    },
    {
        "paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Reasoning with Language Model is Planning with World Model",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14992",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14992, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-24",
        "authors": [
            {
                "authorId": "2128965713",
                "name": "Shibo Hao"
            },
            {
                "authorId": "2112578816",
                "name": "Yi Gu"
            },
            {
                "authorId": "2110816708",
                "name": "Haodi Ma"
            },
            {
                "authorId": "2218162745",
                "name": "Joshua Jiahua Hong"
            },
            {
                "authorId": "47197370",
                "name": "Zhen Wang"
            },
            {
                "authorId": "2111220343",
                "name": "D. Wang"
            },
            {
                "authorId": "2749311",
                "name": "Zhiting Hu"
            }
        ],
        "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting."
    },
    {
        "paperId": "2f2a430ba6c93bcfaf4818316ff8a27b1e034b1a",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.15594",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.15594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-24",
        "authors": [
            {
                "authorId": "9747676",
                "name": "Haonan Duan"
            },
            {
                "authorId": "7485473",
                "name": "Adam Dziedzic"
            },
            {
                "authorId": "1967156",
                "name": "Nicolas Papernot"
            },
            {
                "authorId": "1389731564",
                "name": "Franziska Boenisch"
            }
        ],
        "abstract": "Large language models (LLMs) are excellent in-context learners. However, the sensitivity of data contained in prompts raises privacy concerns. Our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs. To address this vulnerability, one could forego prompting and resort to fine-tuning LLMs with known algorithms for private gradient descent. However, this comes at the expense of the practicality and efficiency offered by prompting. Therefore, we propose to privately learn to prompt. We first show that soft prompts can be obtained privately through gradient descent on downstream data. However, this is not the case for discrete prompts. Thus, we orchestrate a noisy vote among an ensemble of LLMs presented with different prompts, i.e., a flock of stochastic parrots. The vote privately transfers the flock's knowledge into a single public prompt. We show that LLMs prompted with our private algorithms closely match the non-private baselines. For example, using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on the sst2 dataset with ($\\epsilon=0.147, \\delta=10^{-6}$)-differential privacy vs. 95.2% for the non-private baseline. Through our experiments, we also show that our prompt-based approach is easily deployed with existing commercial APIs."
    },
    {
        "paperId": "dedfe929d182cc3537a9ed765d589b4735ce062a",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "On the Planning Abilities of Large Language Models - A Critical Investigation",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.15771",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.15771, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-25",
        "authors": [
            {
                "authorId": "144982263",
                "name": "Karthik Valmeekam"
            },
            {
                "authorId": "2188993062",
                "name": "Matthew Marquez"
            },
            {
                "authorId": "2400282",
                "name": "S. Sreedharan"
            },
            {
                "authorId": "2047340230",
                "name": "Subbarao Kambhampati"
            }
        ],
        "abstract": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning tasks. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains. However, the results in the heuristic mode show more promise. In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation."
    },
    {
        "paperId": "26794d92b563088f18f52ecdbe08d3309bdf6dd5",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Think Before You Act: Decision Transformers with Working Memory",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.16338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-24",
        "authors": [
            {
                "authorId": "2152287163",
                "name": "Jikun Kang"
            },
            {
                "authorId": "2303396586",
                "name": "Romain Laroche"
            },
            {
                "authorId": "2258299929",
                "name": "Xingdi Yuan"
            },
            {
                "authorId": "3382568",
                "name": "Adam Trischler"
            },
            {
                "authorId": "2161533150",
                "name": "Xuefei Liu"
            },
            {
                "authorId": "2303968149",
                "name": "Jie Fu"
            }
        ],
        "abstract": "Decision Transformer-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and computation. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Inspired by this, we propose a working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in Atari games and Meta-World object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of the proposed architecture."
    },
    {
        "paperId": "7eb044170c11b7e2193b8df35f606edcfc7f2585",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Don't Trust ChatGPT when your Question is not in English: A Study of Multilingual Abilities and Types of LLMs",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.491.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.16339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-24",
        "authors": [
            {
                "authorId": "2190290316",
                "name": "Xiang Zhang"
            },
            {
                "authorId": "2218489488",
                "name": "Senyu Li"
            },
            {
                "authorId": "39918547",
                "name": "B. Hauer"
            },
            {
                "authorId": "49402878",
                "name": "Ning Shi"
            },
            {
                "authorId": "1999941240",
                "name": "Grzegorz Kondrak"
            }
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated exceptional natural language understanding abilities and have excelled in a variety of natural language processing (NLP)tasks in recent years. Despite the fact that most LLMs are trained predominantly in English, multiple studies have demonstrated their comparative performance in many other languages. However, fundamental questions persist regarding how LLMs acquire their multi-lingual abilities and how performance varies across different languages. These inquiries are crucial for the study of LLMs since users and researchers often come from diverse language backgrounds, potentially influencing their utilization and interpretation of LLMs' results. In this work, we propose a systematic way of qualifying the performance disparities of LLMs under multilingual settings. We investigate the phenomenon of across-language generalizations in LLMs, wherein insufficient multi-lingual training data leads to advanced multi-lingual capabilities. To accomplish this, we employ a novel back-translation-based prompting method. The results show that GPT exhibits highly translating-like behaviour in multilingual settings."
    },
    {
        "paperId": "fea62c1cff50dbab0ea82852771edf8253fc0103",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Large Language Models Are Partially Primed in Pronoun Interpretation",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.16917",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.16917, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-26",
        "authors": [
            {
                "authorId": "2180219188",
                "name": "S. Lam"
            },
            {
                "authorId": "2153554138",
                "name": "Qingcheng Zeng"
            },
            {
                "authorId": "2119058805",
                "name": "Kexun Zhang"
            },
            {
                "authorId": "2061592207",
                "name": "Chenyu You"
            },
            {
                "authorId": "35248702",
                "name": "Rob Voigt"
            }
        ],
        "abstract": "While a large body of literature suggests that large language models (LLMs) acquire rich linguistic representations, little is known about whether they adapt to linguistic biases in a human-like way. The present study probes this question by asking whether LLMs display human-like referential biases using stimuli and procedures from real psycholinguistic experiments. Recent psycholinguistic studies suggest that humans adapt their referential biases with recent exposure to referential patterns; closely replicating three relevant psycholinguistic experiments from Johnson&Arnold (2022) in an in-context learning (ICL) framework, we found that InstructGPT adapts its pronominal interpretations in response to the frequency of referential patterns in the local discourse, though in a limited fashion: adaptation was only observed relative to syntactic but not semantic biases. By contrast, FLAN-UL2 fails to generate meaningful patterns. Our results provide further evidence that contemporary LLMs discourse representations are sensitive to syntactic patterns in the local context but less so to semantic patterns. Our data and code are available at \\url{https://github.com/zkx06111/llm_priming}."
    },
    {
        "paperId": "d671d62a1eb4d57343e4a0928297266dffc0c118",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.17390",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.17390, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-27",
        "authors": [
            {
                "authorId": "51583409",
                "name": "Bill Yuchen Lin"
            },
            {
                "authorId": "1998914086",
                "name": "Yicheng Fu"
            },
            {
                "authorId": "2218576699",
                "name": "Karina Yang"
            },
            {
                "authorId": "19179135",
                "name": "Prithviraj Ammanabrolu"
            },
            {
                "authorId": "9252833",
                "name": "Faeze Brahman"
            },
            {
                "authorId": "9932998",
                "name": "Shiyu Huang"
            },
            {
                "authorId": "1857797",
                "name": "Chandra Bhagavatula"
            },
            {
                "authorId": "1699545",
                "name": "Yejin Choi"
            },
            {
                "authorId": "1384550891",
                "name": "Xiang Ren"
            }
        ],
        "abstract": "We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks."
    },
    {
        "paperId": "1fed19184785b2b50163b3d8ccb7bfaa0321d1aa",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2305.17455",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.17455, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-27",
        "authors": [
            {
                "authorId": "2004872473",
                "name": "Dachuan Shi"
            },
            {
                "authorId": "144259094",
                "name": "Chaofan Tao"
            },
            {
                "authorId": "36290866",
                "name": "Anyi Rao"
            },
            {
                "authorId": "2149234038",
                "name": "Zhendong Yang"
            },
            {
                "authorId": "2117728946",
                "name": "Chun Yuan"
            },
            {
                "authorId": "2156546701",
                "name": "Jiaqi Wang"
            }
        ],
        "abstract": "Recent vision-language models have achieved tremendous advances. However, their computational costs are also escalating dramatically, making model acceleration exceedingly critical. To pursue more efficient vision-language Transformers, this paper introduces Cross-Guided Ensemble of Tokens (CrossGET), a general acceleration framework for vision-language Transformers. This framework adaptively combines tokens in real-time during inference, significantly reducing computational costs while maintaining high performance. CrossGET features two primary innovations: 1) Cross-Guided Matching and Ensemble. CrossGET leverages cross-modal guided token matching and ensemble to effectively utilize cross-modal information, achieving wider applicability across both modality-independent models, e.g., CLIP, and modality-dependent ones, e.g., BLIP2. 2) Complete-Graph Soft Matching. CrossGET introduces an algorithm for the token-matching mechanism, ensuring reliable matching results while facilitating parallelizability and high efficiency. Extensive experiments have been conducted on various vision-language tasks, such as image-text retrieval, visual reasoning, image captioning, and visual question answering. The performance on both classic multimodal architectures and emerging multimodal LLMs demonstrates the framework's effectiveness and versatility. The code is available at https://github.com/sdc17/CrossGET."
    },
    {
        "paperId": "0b641cc51d8e25c6c0b2362317fec7e0bf26fbf1",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.17701",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.17701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-28",
        "authors": [
            {
                "authorId": "2294014",
                "name": "Hwaran Lee"
            },
            {
                "authorId": "98486910",
                "name": "Seokhee Hong"
            },
            {
                "authorId": "48490725",
                "name": "Joonsuk Park"
            },
            {
                "authorId": "2134920925",
                "name": "Takyoung Kim"
            },
            {
                "authorId": "70308241",
                "name": "Gunhee Kim"
            },
            {
                "authorId": "2577039",
                "name": "Jung-Woo Ha"
            }
        ],
        "abstract": "Large language models (LLMs) not only learn natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications. Existing research and resources are not readily applicable in South Korea due to the differences in language and culture, both of which significantly affect the biases and targeted demographic groups. This limitation requires localized social bias datasets to ensure the safe and effective deployment of LLMs. To this end, we present KosBi, a new social bias dataset of 34k pairs of contexts and sentences in Korean covering 72 demographic groups in 15 categories. We find that through filtering-based moderation, social biases in generated content can be reduced by 16.47%p on average for HyperClova (30B and 82B), and GPT-3."
    },
    {
        "paperId": "c35ff61df76580117326d10c86faa85869dcdaf7",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning in Goal-Oriented Dialogue Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.17878",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.17878, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-29",
        "authors": [
            {
                "authorId": "2145894968",
                "name": "Qiang Zhang"
            },
            {
                "authorId": "2300343",
                "name": "Jason Naradowsky"
            },
            {
                "authorId": "1768065",
                "name": "Yusuke Miyao"
            }
        ],
        "abstract": "Existing dialogue models may encounter scenarios which are not well-represented in the training data, and as a result generate responses that are unnatural, inappropriate, or unhelpful. We propose the\"Ask an Expert\"framework in which the model is trained with access to an\"expert\"which it can consult at each turn. Advice is solicited via a structured dialogue with the expert, and the model is optimized to selectively utilize (or ignore) it given the context and dialogue history. In this work the expert takes the form of an LLM. We evaluate this framework in a mental health support domain, where the structure of the expert conversation is outlined by pre-specified prompts which reflect a reasoning strategy taught to practitioners in the field. Blenderbot models utilizing\"Ask an Expert\"show quality improvements across all expert sizes, including those with fewer parameters than the dialogue model itself. Our best model provides a $\\sim 10\\%$ improvement over baselines, approaching human-level scores on\"engingingness\"and\"helpfulness\"metrics."
    },
    {
        "paperId": "6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.17888",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.17888, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-29",
        "authors": [
            {
                "authorId": "2109370860",
                "name": "Zechun Liu"
            },
            {
                "authorId": "9185192",
                "name": "Barlas O\u011fuz"
            },
            {
                "authorId": "2112729504",
                "name": "Changsheng Zhao"
            },
            {
                "authorId": "48025720",
                "name": "Ernie Chang"
            },
            {
                "authorId": "37502184",
                "name": "Pierre Stock"
            },
            {
                "authorId": "2121361882",
                "name": "Yashar Mehdad"
            },
            {
                "authorId": "152345059",
                "name": "Yangyang Shi"
            },
            {
                "authorId": "2065915235",
                "name": "Raghuraman Krishnamoorthi"
            },
            {
                "authorId": "144137037",
                "name": "Vikas Chandra"
            }
        ],
        "abstract": "Several post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing any generative model independent of its training data, similar to post-training quantization methods. In addition to quantizing weights and activations, we also quantize the KV cache, which is critical for increasing throughput and support long sequence dependencies at current model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4-bits. We observe large improvements over training-free methods, especially in the low-bit settings."
    },
    {
        "paperId": "8d9ca1e2c703e2752a4904c967a65d45d0bef5f6",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.18189",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.18189, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-29",
        "authors": [
            {
                "authorId": "2149615775",
                "name": "Myra Cheng"
            },
            {
                "authorId": "41152329",
                "name": "Esin Durmus"
            },
            {
                "authorId": "1746807",
                "name": "Dan Jurafsky"
            }
        ],
        "abstract": "To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs. Toward this end, we present Marked Personas, a prompt-based method to measure stereotypes in LLMs for intersectional demographic groups without any lexicon or data labeling.Grounded in the sociolinguistic concept of markedness (which characterizes explicitly linguistically marked categories versus unmarked defaults), our proposed method is twofold: 1) prompting an LLM to generate personas, i.e., natural language descriptions, of the target demographic group alongside personas of unmarked, default groups; 2) identifying the words that significantly distinguish personas of the target group from corresponding unmarked ones.We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts. The words distinguishing personas of marked (non-white, non-male) groups reflect patterns of othering and exoticizing these demographics. An intersectional lens further reveals tropes that dominate portrayals of marginalized groups, such as tropicalism and the hypersexualization of minoritized women. These representational harms have concerning implications for downstream applications like story generation."
    },
    {
        "paperId": "5ff2f5212713ec424662ac3c9e4aa5a8790d40cf",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "ANPL: Towards Natural Programming with Interactive Decomposition",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.18498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-29",
        "authors": [
            {
                "authorId": "2110180755",
                "name": "Di Huang"
            },
            {
                "authorId": "2150708879",
                "name": "Ziyuan Nan"
            },
            {
                "authorId": "2109635961",
                "name": "Xingui Hu"
            },
            {
                "authorId": "2143807697",
                "name": "Pengwei Jin"
            },
            {
                "authorId": "2072713784",
                "name": "Shaohui Peng"
            },
            {
                "authorId": "1742399798",
                "name": "Yuanbo Wen"
            },
            {
                "authorId": "2118404461",
                "name": "Rui Zhang"
            },
            {
                "authorId": "1678776",
                "name": "Zidong Du"
            },
            {
                "authorId": "145461472",
                "name": "Qi Guo"
            },
            {
                "authorId": "2155555",
                "name": "Yewen Pu"
            },
            {
                "authorId": "7377735",
                "name": "Yunji Chen"
            }
        ],
        "abstract": "Though LLMs are capable of generating plausible programs, it's challenging to interact with the LLMs further to revise the program, especially if the user's specific requirements are different from the initial proposal. In this paper, we introduce ANPL, an interactive programming system that ensures users can always refine the generated code towards their specific programmatic intents via structured decompositions. Borrowing the paradigm of sketching from program synthesis, an ANPL program consists of a set of input-outputs that it must satisfy, a ``sketch'' -- control/data flow expressed in precise code (e.g. Python), and ``holes'' -- sub-modules to be implemented by the LLM specified with natural language. The user revises an ANPL program by either modifying the sketch, changing the language used to describe the holes, or providing additional input-outputs to a particular hole, turning it into a sub-ANPL program that can be solved recursively. This workflow allows the users to offload programming burdens to the LLM as much as possible while retaining the ability to pinpoint and resolve bugs locally, without exposing the rest of the program to the LLM. We deploy ANPL on the Abstraction and Reasoning Corpus (ARC), a set of unique tasks that are challenging for state-of-the-art AI systems, showing it outperforms baseline programming systems that (a) without the ability to decompose tasks interactively and (b) without the guarantee that the modules can be correctly composed together. Additional evaluations on APPS, HumanEval, and real-world programming tasks have validated that the ANPL framework is applicable to multiple programming domains. We release the ANPL solutions to the ARC tasks as a dataset, providing insights into how humans decompose novel tasks programmatically. See our code at https://iprc-dip.github.io/ANPL/."
    },
    {
        "paperId": "7d97c17a75beb89f938eaac1d3ca60ac2245fb2e",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Faith and Fate: Limits of Transformers on Compositionality",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.18654, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-29",
        "authors": [
            {
                "authorId": "46217681",
                "name": "Nouha Dziri"
            },
            {
                "authorId": "50085131",
                "name": "Ximing Lu"
            },
            {
                "authorId": "1947172233",
                "name": "Melanie Sclar"
            },
            {
                "authorId": "1737850",
                "name": "Xiang Lorraine Li"
            },
            {
                "authorId": "2218495662",
                "name": "Liwei Jian"
            },
            {
                "authorId": "51583409",
                "name": "Bill Yuchen Lin"
            },
            {
                "authorId": "119659229",
                "name": "Peter West"
            },
            {
                "authorId": "1857797",
                "name": "Chandra Bhagavatula"
            },
            {
                "authorId": "39227408",
                "name": "Ronan Le Bras"
            },
            {
                "authorId": "2012510",
                "name": "Jena D. Hwang"
            },
            {
                "authorId": "3313909",
                "name": "Soumya Sanyal"
            },
            {
                "authorId": "2129663",
                "name": "S. Welleck"
            },
            {
                "authorId": "145201124",
                "name": "Xiang Ren"
            },
            {
                "authorId": "37907837",
                "name": "Allyson Ettinger"
            },
            {
                "authorId": "1753355",
                "name": "Za\u00efd Harchaoui"
            },
            {
                "authorId": "1699545",
                "name": "Yejin Choi"
            }
        ],
        "abstract": "Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\,increased\\,task\\,complexity."
    },
    {
        "paperId": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.19118",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.19118, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-30",
        "authors": [
            {
                "authorId": "31395252",
                "name": "Tian Liang"
            },
            {
                "authorId": "2610876",
                "name": "Zhiwei He"
            },
            {
                "authorId": "12386833",
                "name": "Wenxiang Jiao"
            },
            {
                "authorId": "48631170",
                "name": "Xing Wang"
            },
            {
                "authorId": "2152547279",
                "name": "Yan Wang"
            },
            {
                "authorId": "2151039787",
                "name": "Rui Wang"
            },
            {
                "authorId": "3001727",
                "name": "Yujiu Yang"
            },
            {
                "authorId": "2909321",
                "name": "Zhaopeng Tu"
            },
            {
                "authorId": "2072684668",
                "name": "Shuming Shi"
            }
        ],
        "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of \u201ctit for tat\u201d and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of \u201ctit for tat\u201d state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents."
    },
    {
        "paperId": "498d1406fc4cddb05cd46477793f2e726a6fe238",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.19213",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.19213, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-05-30",
        "authors": [
            {
                "authorId": "49543720",
                "name": "Xiao Liu"
            },
            {
                "authorId": "144508458",
                "name": "Da Yin"
            },
            {
                "authorId": "2111574159",
                "name": "Chen Zhang"
            },
            {
                "authorId": "2115387922",
                "name": "Yansong Feng"
            },
            {
                "authorId": "144060462",
                "name": "Dongyan Zhao"
            }
        ],
        "abstract": "Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning. We further intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while Code-LLMs are robust towards format perturbations."
    },
    {
        "paperId": "b3f272d644fe580d18f635be4d6ac4c520ef0d0f",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Preference-grounded Token-level Guidance for Language Model Fine-tuning",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2306.00398",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.00398, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-01",
        "authors": [
            {
                "authorId": "2155974419",
                "name": "Shentao Yang"
            },
            {
                "authorId": "2107944048",
                "name": "Shujian Zhang"
            },
            {
                "authorId": "47414558",
                "name": "Congying Xia"
            },
            {
                "authorId": "22758695",
                "name": "Yihao Feng"
            },
            {
                "authorId": "2054594326",
                "name": "Caiming Xiong"
            },
            {
                "authorId": "2152176954",
                "name": "Mi Zhou"
            }
        ],
        "abstract": "Aligning language models (LMs) with preferences is an important problem in natural language generation. A key challenge is that preferences are typically provided at the sequence level while LM training and generation both occur at the token level. There is, therefore, a granularity mismatch between the preference and the LM training losses, which may complicate the learning problem. In this paper, we address this issue by developing an alternate training process, where we iterate between grounding the sequence-level preference into token-level training guidance, and improving the LM with the learned guidance. For guidance learning, we design a framework that extends the pairwise-preference learning in imitation learning to both variable-length LM generation and the utilization of the preference among multiple generations. For LM training, based on the amount of supervised data, we present two minimalist learning objectives that utilize the learned guidance. In experiments, our method performs competitively on two distinct representative LM tasks -- discrete-prompt generation and text summarization."
    },
    {
        "paperId": "8c5a7de7452b61cb81d6f7124ad021997e0a79c1",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.01150",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.01150, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-01",
        "authors": [
            {
                "authorId": "2065089223",
                "name": "Fan Yin"
            },
            {
                "authorId": "2056908",
                "name": "Jesse Vig"
            },
            {
                "authorId": "46180754",
                "name": "Philippe Laban"
            },
            {
                "authorId": "2708940",
                "name": "Shafiq R. Joty"
            },
            {
                "authorId": "2054594326",
                "name": "Caiming Xiong"
            },
            {
                "authorId": "30340989",
                "name": "Chien-Sheng Wu"
            }
        ],
        "abstract": "Large language models (LLMs) have shown impressive performance in following natural language instructions to solve unseen tasks. However, it remains unclear whether models truly understand task definitions and whether the human-written definitions are optimal. In this paper, we systematically study the role of task definitions in instruction learning. We first conduct an ablation analysis informed by human annotations to understand which parts of a task definition are most important, and find that model performance only drops substantially when removing contents describing the task output, in particular label information. Next, we propose an automatic algorithm to compress task definitions to a minimal supporting set of tokens, and find that 60% of tokens can be removed while maintaining or even improving model performance. Based on these results, we propose two strategies to help models better leverage task instructions: (1) providing only key information for tasks in a common structured format, and (2) adding a meta-tuning stage to help the model better understand the definitions. With these two strategies, we achieve a 4.2 Rouge-L improvement over 119 unseen test tasks."
    },
    {
        "paperId": "80cee5037d01470edc7fbd20c564f2e1fc2c6b85",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "MultiLegalPile: A 689GB Multilingual Legal Corpus",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.02069",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.02069, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-03",
        "authors": [
            {
                "authorId": "67042743",
                "name": "Joel Niklaus"
            },
            {
                "authorId": "2124259793",
                "name": "Veton Matoshi"
            },
            {
                "authorId": "2130595913",
                "name": "Matthias Sturmer"
            },
            {
                "authorId": "2125376289",
                "name": "Ilias Chalkidis"
            },
            {
                "authorId": "40688328",
                "name": "Daniel E. Ho"
            }
        ],
        "abstract": "Large, high-quality datasets are crucial for training Large Language Models (LLMs). However, so far, there are few datasets available for specialized critical domains such as law and the available ones are often only for the English language. We curate and release MultiLegalPile, a 689GB corpus in 24 languages from 17 jurisdictions. The MultiLegalPile corpus, which includes diverse legal data sources with varying licenses, allows for pretraining NLP models under fair use, with more permissive licenses for the Eurlex Resources and Legal mC4 subsets. We pretrain two RoBERTa models and one Longformer multilingually, and 24 monolingual models on each of the language-specific subsets and evaluate them on LEXTREME. Additionally, we evaluate the English and multilingual models on LexGLUE. Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE. We release the dataset, the trained models, and all of the code under the most open possible licenses."
    },
    {
        "paperId": "d6fb1c21a46fb8b0f3f4383fd467b21e5b58c55f",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2306.03082",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.03082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-05",
        "authors": [
            {
                "authorId": "2108451006",
                "name": "Lichang Chen"
            },
            {
                "authorId": "1391200710",
                "name": "Jiuhai Chen"
            },
            {
                "authorId": "1962083",
                "name": "T. Goldstein"
            },
            {
                "authorId": "2214208405",
                "name": "Heng Huang"
            },
            {
                "authorId": "2213956781",
                "name": "Tianyi Zhou"
            }
        ],
        "abstract": "Large language models~(LLMs) are instruction followers, but it can be challenging to find the best instruction for different situations, especially for black-box LLMs on which backpropagation is forbidden. Instead of directly optimizing the discrete instruction, we optimize a low-dimensional soft prompt applied to an open-source LLM to generate the instruction for the black-box LLM. On each iteration of the proposed method, which we call InstructZero, a soft prompt is converted into an instruction using the open-source LLM, which is then submitted to the black-box LLM for zero-shot evaluation, and the performance is sent to Bayesian optimization to produce new soft prompts improving the zero-shot performance. We evaluate InstructZero on different combinations of open-source LLMs and APIs including Vicuna and ChatGPT. Our results show that InstructZero outperforms SOTA auto-instruction methods across a variety of downstream tasks. Our code and data are publicly available at https://github.com/Lichang-Chen/InstructZero."
    },
    {
        "paperId": "9d460930d9b5d12a65ff2b3efa23047ec75fbca1",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2306.03805",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.03805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-06",
        "authors": [
            {
                "authorId": "145018564",
                "name": "Ajay Jaiswal"
            },
            {
                "authorId": "47130544",
                "name": "Shiwei Liu"
            },
            {
                "authorId": "2034263179",
                "name": "Tianlong Chen"
            },
            {
                "authorId": "2969311",
                "name": "Zhangyang Wang"
            }
        ],
        "abstract": "Large pre-trained transformers are show-stealer in modern-day deep learning, and it becomes crucial to comprehend the parsimonious patterns that exist within them as they grow in scale. With exploding parameter counts, Lottery Ticket Hypothesis (LTH) and its variants, have lost their pragmatism in sparsifying them due to high computation and memory bottleneck of repetitive train-prune-retrain routine of iterative magnitude pruning (IMP) which worsens with increasing model size. This paper comprehensively studies induced sparse patterns across multiple large pre-trained vision and language transformers. We propose the existence of -- essential sparsity defined with a sharp dropping point beyond which the performance declines much faster w.r.t the rise of sparsity level, when we directly remove weights with the smallest magnitudes in one-shot without re-training. We also find essential sparsity to hold valid for N:M sparsity patterns as well as on modern-scale large language models (Vicuna-7B). We also present an intriguing emerging phenomenon of abrupt sparsification during the pre-training of BERT, i.e., BERT suddenly becomes heavily sparse in pre-training after certain iterations. Moreover, our observations also indicate a counter-intuitive finding that BERT trained with a larger amount of pre-training data tends to have a better ability to condense knowledge in comparatively relatively fewer parameters. Lastly, we investigate the effect of the pre-training loss on essential sparsity and discover that self-supervised learning (SSL) objectives trigger stronger emergent sparsification properties than supervised learning (SL). Our codes are available at \\url{https://github.com/VITA-Group/essential_sparsity}."
    },
    {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.05685, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-09",
        "authors": [
            {
                "authorId": "2149970173",
                "name": "Lianmin Zheng"
            },
            {
                "authorId": "2537924",
                "name": "Wei-Lin Chiang"
            },
            {
                "authorId": "2209360681",
                "name": "Ying Sheng"
            },
            {
                "authorId": "92721493",
                "name": "Siyuan Zhuang"
            },
            {
                "authorId": "1390573666",
                "name": "Zhanghao Wu"
            },
            {
                "authorId": "2152482391",
                "name": "Yonghao Zhuang"
            },
            {
                "authorId": "143872641",
                "name": "Zi Lin"
            },
            {
                "authorId": "2141335450",
                "name": "Zhuohan Li"
            },
            {
                "authorId": "2117961435",
                "name": "Dacheng Li"
            },
            {
                "authorId": "143977260",
                "name": "E. Xing"
            },
            {
                "authorId": "145140331",
                "name": "Haotong Zhang"
            },
            {
                "authorId": "49988044",
                "name": "Joseph E. Gonzalez"
            },
            {
                "authorId": "2055174324",
                "name": "Ion Stoica"
            }
        ],
        "abstract": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge."
    },
    {
        "paperId": "0a94fbb5e1c93513523f00e75d672ef4553861f9",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.05836",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.05836, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-09",
        "authors": [
            {
                "authorId": "2111472502",
                "name": "Zhijing Jin"
            },
            {
                "authorId": "146961917",
                "name": "Jiarui Liu"
            },
            {
                "authorId": "2114227440",
                "name": "Zhiheng Lyu"
            },
            {
                "authorId": "1753626755",
                "name": "Spencer Poff"
            },
            {
                "authorId": "2790926",
                "name": "Mrinmaya Sachan"
            },
            {
                "authorId": "2105984203",
                "name": "Rada Mihalcea"
            },
            {
                "authorId": "2138579860",
                "name": "Mona T. Diab"
            },
            {
                "authorId": "1707625",
                "name": "B. Scholkopf"
            }
        ],
        "abstract": "Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize -- they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and would be helpful in guiding future research on improving LLMs' pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause."
    },
    {
        "paperId": "0423fc7bc1880b850d07aec8ebd9217a70626572",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "S3: Increasing GPU Utilization during Generative Inference for Higher Throughput",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.06000",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.06000, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-09",
        "authors": [
            {
                "authorId": "2110895324",
                "name": "Yunho Jin"
            },
            {
                "authorId": "3218732",
                "name": "Chun-Feng Wu"
            },
            {
                "authorId": "1896817",
                "name": "D. Brooks"
            },
            {
                "authorId": "2113825170",
                "name": "Gu-Yeon Wei"
            }
        ],
        "abstract": "Generating texts with a large language model (LLM) consumes massive amounts of memory. Apart from the already-large model parameters, the key/value (KV) cache that holds information about previous tokens in a sequence can grow to be even larger than the model itself. This problem is exacerbated in one of the current LLM serving frameworks which reserves the maximum sequence length of memory for the KV cache to guarantee generating a complete sequence as they do not know the output sequence length. This restricts us to use a smaller batch size leading to lower GPU utilization and above all, lower throughput. We argue that designing a system with a priori knowledge of the output sequence can mitigate this problem. To this end, we propose S$^{3}$, which predicts the output sequence length, schedules generation queries based on the prediction to increase device resource utilization and throughput, and handle mispredictions. Our proposed method achieves 6.49$\\times$ throughput over those systems that assume the worst case for the output sequence length."
    },
    {
        "paperId": "60e6e3767c36bf9e16b58b7221c5712b4d3d5293",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Large Language Models Are Semi-Parametric Reinforcement Learning Agents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.07929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-09",
        "authors": [
            {
                "authorId": "2118333",
                "name": "Danyang Zhang"
            },
            {
                "authorId": "1390833791",
                "name": "Lu Chen"
            },
            {
                "authorId": "2108339988",
                "name": "Situo Zhang"
            },
            {
                "authorId": "2155908631",
                "name": "Hongshen Xu"
            },
            {
                "authorId": "1806179720",
                "name": "Zihan Zhao"
            },
            {
                "authorId": "1736727",
                "name": "Kai Yu"
            }
        ],
        "abstract": "Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate on two task sets and demonstrate the superiority and robustness of REMEMBERER."
    },
    {
        "paperId": "d0adbba45ad8d8d1e748c9f3901569a09d8bc227",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Diplomat: A Dialogue Dataset for Situated Pragmatic Reasoning",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.09030",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.09030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-15",
        "authors": [
            {
                "authorId": "2220676806",
                "name": "Hengli Li"
            },
            {
                "authorId": "2220129702",
                "name": "Songchun Zhu"
            },
            {
                "authorId": "2109673900",
                "name": "Zilong Zheng"
            }
        ],
        "abstract": "Pragmatic reasoning plays a pivotal role in deciphering implicit meanings that frequently arise in real-life conversations and is essential for the development of communicative social agents. In this paper, we introduce a novel challenge, DiPlomat, aiming at benchmarking machines' capabilities on pragmatic reasoning and situated conversational understanding. Compared with previous works that treat different figurative expressions (e.g. metaphor, sarcasm) as individual tasks, DiPlomat provides a cohesive framework towards general pragmatic understanding. Our dataset is created through the utilization of Amazon Mechanical Turk ( AMT ), resulting in a total of 4, 177 multi-turn dialogues. In conjunction with the dataset, we propose two tasks, Pragmatic Identification and Reasoning (PIR) and Conversational Question Answering (CQA). Experimental results with state-of-the-art (SOTA) neural architectures reveal several significant findings: 1) large language models ( LLMs) exhibit poor performance in tackling this subjective domain; 2) comprehensive comprehension of context emerges as a critical factor for establishing benign human-machine interactions; 3) current models defect in the application of pragmatic reasoning. As a result, we call on more attention to improve the ability of context understanding, reasoning, and implied meaning modeling."
    },
    {
        "paperId": "3e826e52754d0876611c8cf2fa7a781a701c39e6",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2306.09296",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.09296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-15",
        "authors": [
            {
                "authorId": "2116034394",
                "name": "Jifan Yu"
            },
            {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
            },
            {
                "authorId": "2116520118",
                "name": "Shangqing Tu"
            },
            {
                "authorId": "1712738522",
                "name": "S. Cao"
            },
            {
                "authorId": "2165225735",
                "name": "Daniel Zhang-Li"
            },
            {
                "authorId": "48574888",
                "name": "Xin Lv"
            },
            {
                "authorId": "47837854",
                "name": "Hao Peng"
            },
            {
                "authorId": "1423719712",
                "name": "Zijun Yao"
            },
            {
                "authorId": "2181337399",
                "name": "Xiaohan Zhang"
            },
            {
                "authorId": "2188777141",
                "name": "Hanming Li"
            },
            {
                "authorId": "2118001750",
                "name": "Chun-yan Li"
            },
            {
                "authorId": "2257434341",
                "name": "Zheyuan Zhang"
            },
            {
                "authorId": "2141377570",
                "name": "Yushi Bai"
            },
            {
                "authorId": "2211723524",
                "name": "Yantao Liu"
            },
            {
                "authorId": "2220101512",
                "name": "Amy Xin"
            },
            {
                "authorId": "2210119346",
                "name": "Nianyi Lin"
            },
            {
                "authorId": "2220099357",
                "name": "Kaifeng Yun"
            },
            {
                "authorId": "2220092688",
                "name": "Linlu Gong"
            },
            {
                "authorId": "2220188202",
                "name": "Jianhui Chen"
            },
            {
                "authorId": "2220655963",
                "name": "Zhili Wu"
            },
            {
                "authorId": "121817444",
                "name": "Y. Qi"
            },
            {
                "authorId": "2143447165",
                "name": "Weikai Li"
            },
            {
                "authorId": "2069570219",
                "name": "Yong Guan"
            },
            {
                "authorId": "10673612",
                "name": "Kaisheng Zeng"
            },
            {
                "authorId": "2091076497",
                "name": "Ji Qi"
            },
            {
                "authorId": "2109790016",
                "name": "Hailong Jin"
            },
            {
                "authorId": "2108499570",
                "name": "Jinxin Liu"
            },
            {
                "authorId": "2116405624",
                "name": "Yuxian Gu"
            },
            {
                "authorId": "2022231256",
                "name": "Yu Gu"
            },
            {
                "authorId": "1390925224",
                "name": "Yuan Yao"
            },
            {
                "authorId": "46649145",
                "name": "Ning Ding"
            },
            {
                "authorId": "2055765060",
                "name": "Lei Hou"
            },
            {
                "authorId": null,
                "name": "Zhiyuan Liu"
            },
            {
                "authorId": "2113744169",
                "name": "Bin Xu"
            },
            {
                "authorId": "2148911975",
                "name": "Jie Tang"
            },
            {
                "authorId": "2133353675",
                "name": "Juanzi Li"
            }
        ],
        "abstract": "The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For \\textbf{ability modeling}, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering $19$ tasks. (2) For \\textbf{data}, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For \\textbf{evaluation criteria}, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge-creating ability. We evaluate $28$ open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset and open-participation leaderboard are publicly released at https://kola.xlore.cn and will be continuously updated to provide references for developing LLMs and knowledge-related systems."
    },
    {
        "paperId": "2150ef9e3d2aebfba3264044b0fc0b875c142e17",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.09308",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.09308, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-15",
        "authors": [
            {
                "authorId": "2166091148",
                "name": "Myles Foley"
            },
            {
                "authorId": "22261698",
                "name": "Ambrish Rawat"
            },
            {
                "authorId": "2801206",
                "name": "Taesung Lee"
            },
            {
                "authorId": "39517968",
                "name": "Yufang Hou"
            },
            {
                "authorId": "46310814",
                "name": "Gabriele Picco"
            },
            {
                "authorId": "152109289",
                "name": "Giulio Zizzo"
            }
        ],
        "abstract": "The wide applicability and adaptability of generative large language models (LLMs) has enabled their rapid adoption.While the pre-trained models can perform many tasks, such models are often fine-tuned to improve their performance on various downstream applications.However, this leads to issues over violation of model licenses, model theft, and copyright infringement.Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains.Thus, we need a method to investigate how a model was trained or a piece of text was generated and what their pre-trained base model was.In this paper we take the first step to address this open problem by tracing back the origin of a given fine-tuned LLM to its corresponding pre-trained base model. We consider different knowledge levels and attribution strategies, and find that we can correctly trace back 8 out of the 10 fine tuned models with our best method."
    },
    {
        "paperId": "73c6533c2988eebb71cdaa758f38546cdf01f655",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2306.11167",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.11167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-19",
        "authors": [
            {
                "authorId": "2182415166",
                "name": "S. Naeini"
            },
            {
                "authorId": "35685735",
                "name": "Raeid Saqur"
            },
            {
                "authorId": "2065794034",
                "name": "M. Saeidi"
            },
            {
                "authorId": "37585306",
                "name": "John Giorgi"
            },
            {
                "authorId": "2783791",
                "name": "B. Taati"
            }
        ],
        "abstract": "The quest for human imitative AI has been an enduring topic in AI research since its inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to the cultural zeitgeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behaviour (e.g., BIG-bench's 'human-like behavior' tasks), few, if not none, examine creative problem solving abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use the ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incorrect words to subsequent word-fragments or clues. The popular British quiz show Only Connect's Connecting Wall segment essentially mimics Mednick's Remote Associates Test (RAT) formulation with built-in, deliberate red herrings, which makes it an ideal proxy dataset to explore and study fixation effect and Einstellung paradigm from cognitive neuroscience in LLMs. In this paper we present the novel Only Connect Wall (OCW) dataset and report results from our evaluation of selected pre-trained language models and LLMs on creative problem solving tasks like grouping clue words by heterogeneous connections, and identifying correct open knowledge domain connections in respective groups. We synthetically generate two additional datasets: OCW-Randomized, OCW-WordNet to further analyze our red-herrings hypothesis in language models. The code and link to the dataset are available at https://github.com/TaatiTeam/OCW."
    },
    {
        "paperId": "b18c62d515072ccc35709772388b91bac1045514",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Deep Fusion: Efficient Network Training via Pre-trained Initializations",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.11903",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.11903, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-20",
        "authors": [
            {
                "authorId": "1803681",
                "name": "Hanna Mazzawi"
            },
            {
                "authorId": "2360834",
                "name": "X. Gonzalvo"
            },
            {
                "authorId": "2094688632",
                "name": "M. Wunder"
            }
        ],
        "abstract": "In recent years, deep learning has made remarkable progress in a wide range of domains, with a particularly notable impact on natural language processing tasks. One of the challenges associated with training deep neural networks in the context of LLMs is the need for large amounts of computational resources and time. To mitigate this, network growing algorithms offer potential cost savings, but their underlying mechanisms are poorly understood. We present two notable contributions in this paper. First, we present Deep Fusion, an efficient approach to network training that leverages pre-trained initializations of smaller networks. Second, we propose a theoretical framework using backward error analysis to illustrate the dynamics of mid-training network growth. Our experiments show how Deep Fusion is a practical and effective approach that not only accelerates the training process but also reduces computational requirements, maintaining or surpassing traditional training methods' performance in various NLP tasks and T5 model sizes. Finally, we validate our theoretical framework, which guides the optimal use of Deep Fusion, showing that with carefully optimized training dynamics, it significantly reduces both training time and resource consumption."
    },
    {
        "paperId": "8f7297454d7f44365b9bcda5ebb9439a43daf5e6",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.13063",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.13063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-22",
        "authors": [
            {
                "authorId": "38827926",
                "name": "Miao Xiong"
            },
            {
                "authorId": "48430820",
                "name": "Zhiyuan Hu"
            },
            {
                "authorId": "2179626088",
                "name": "Xinyang Lu"
            },
            {
                "authorId": "2157866322",
                "name": "Yifei Li"
            },
            {
                "authorId": "2215497308",
                "name": "Jie Fu"
            },
            {
                "authorId": "2109932032",
                "name": "Junxian He"
            },
            {
                "authorId": "2258715976",
                "name": "Bryan Hooi"
            }
        ],
        "abstract": "Empowering large language models to accurately express confidence in their answers is essential for trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on white-box access to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of black-box approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. We then benchmark these methods on two key tasks-confidence calibration and failure prediction-across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs."
    },
    {
        "paperId": "e586a4591ba0303b769f2c07cbddaf1899cb72e4",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2306.14048",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.14048, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-24",
        "authors": [
            {
                "authorId": "2109338656",
                "name": "Zhenyu (Allen) Zhang"
            },
            {
                "authorId": "2209360681",
                "name": "Ying Sheng"
            },
            {
                "authorId": "2190694474",
                "name": "Tianyi Zhou"
            },
            {
                "authorId": "2034263179",
                "name": "Tianlong Chen"
            },
            {
                "authorId": "2149970173",
                "name": "Lianmin Zheng"
            },
            {
                "authorId": "2209882676",
                "name": "Ruisi Cai"
            },
            {
                "authorId": "2214956470",
                "name": "Zhao Song"
            },
            {
                "authorId": "1932187449",
                "name": "Yuandong Tian"
            },
            {
                "authorId": "1803218",
                "name": "Christopher R\u00e9"
            },
            {
                "authorId": "2052981589",
                "name": "Clark W. Barrett"
            },
            {
                "authorId": "2108404505",
                "name": "Zhangyang Wang"
            },
            {
                "authorId": "4319427",
                "name": "Beidi Chen"
            }
        ],
        "abstract": "Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O), a KV cache eviction policy that dynamically retains a balance of recent and H$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H$_2$O with 20% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29$\\times$, 29$\\times$, and 3$\\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the latency by up to 1.9$\\times$. The code is available at https://github.com/FMInference/H2O."
    },
    {
        "paperId": "f590cbb28e4994f62e94bf9400a9cb33e99922fa",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Understanding Social Reasoning in Language Models with Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.15448",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.15448, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-21",
        "authors": [
            {
                "authorId": "144841994",
                "name": "Kanishk Gandhi"
            },
            {
                "authorId": "2220625823",
                "name": "Jan-Philipp Franken"
            },
            {
                "authorId": "2697953",
                "name": "Tobias Gerstenberg"
            },
            {
                "authorId": "144002017",
                "name": "Noah D. Goodman"
            }
        ],
        "abstract": "As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle."
    },
    {
        "paperId": "78c488e2d84bd193a40006b1fceb03e3845b81d4",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.15895",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.15895, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-28",
        "authors": [
            {
                "authorId": "1633124736",
                "name": "Yue Yu"
            },
            {
                "authorId": "8103389",
                "name": "Yuchen Zhuang"
            },
            {
                "authorId": "47540245",
                "name": "Jieyu Zhang"
            },
            {
                "authorId": "145391513",
                "name": "Yu Meng"
            },
            {
                "authorId": "143711421",
                "name": "Alexander J. Ratner"
            },
            {
                "authorId": "145237361",
                "name": "Ranjay Krishna"
            },
            {
                "authorId": "3363642",
                "name": "Jiaming Shen"
            },
            {
                "authorId": "145657504",
                "name": "Chao Zhang"
            }
        ],
        "abstract": "Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, synthetic datasets generated by simple prompts exhibit significant biases, such as regional bias; secondly, attribute diversity plays a pivotal role in enhancing model performance; lastly, attributed prompts achieve the performance of simple class-conditional prompts while utilizing only 5\\% of the querying cost of ChatGPT associated with the latter. The data and code are available on \\url{https://github.com/yueyu1030/AttrPrompt}."
    },
    {
        "paperId": "8568d7bd9dfb5ba0b91940b938b44a88fafdf95b",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.17820",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.17820, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-30",
        "authors": [
            {
                "authorId": "2143482843",
                "name": "Yiming Wang"
            },
            {
                "authorId": "3322871",
                "name": "Zhuosheng Zhang"
            },
            {
                "authorId": "108085542",
                "name": "Rui Wang"
            }
        ],
        "abstract": "Neural-symbolic methods have demonstrated efficiency in enhancing the reasoning abilities of large language models (LLMs). However, existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL. Those methods require that reasoning tasks be convertible into programs, which cater to the computer execution mindset and deviate from human reasoning habits. To broaden symbolic methods' applicability and adaptability in the real world, we propose the Meta-Reasoning from a linguistic perspective. This method empowers LLMs to deconstruct reasoning-independent semantic information into generic symbolic representations, thereby efficiently capturing more generalized reasoning knowledge. We conduct extensive experiments on more than ten datasets encompassing conventional reasoning tasks like arithmetic, symbolic, and logical reasoning, and the more complex interactive reasoning tasks like theory-of-mind reasoning. Experimental results demonstrate that Meta-Reasoning significantly enhances in-context reasoning accuracy, learning efficiency, out-of-domain generalization, and output stability compared to the Chain-of-Thought technique. Code and data are publicly available at \\url{https://github.com/Alsace08/Meta-Reasoning}."
    },
    {
        "paperId": "16342fd115398bb1bcf2f16baf94f9d4d122d480",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Let Me Teach You: Pedagogical Foundations of Feedback for Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2307.00279",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.00279, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-01",
        "authors": [
            {
                "authorId": "2097711585",
                "name": "Beatriz Borges"
            },
            {
                "authorId": "1721168",
                "name": "Niket Tandon"
            },
            {
                "authorId": "2208988801",
                "name": "Tanja Kaser"
            },
            {
                "authorId": "2691021",
                "name": "Antoine Bosselut"
            }
        ],
        "abstract": "Natural Language Feedback (NLF) is an increasingly popular mechanism for aligning Large Language Models (LLMs) to human preferences. Despite the diversity of the information it can convey, NLF methods are often hand-designed and arbitrary, with little systematic grounding. At the same time, research in learning sciences has long established several effective feedback models. In this opinion piece, we compile ideas from pedagogy to introduce FELT, a feedback framework for LLMs that outlines various characteristics of the feedback space, and a feedback content taxonomy based on these variables, providing a general mapping of the feedback space. In addition to streamlining NLF designs, FELT also brings out new, unexplored directions for research in NLF. We make our taxonomy available to the community, providing guides and examples for mapping our categorizations to future research."
    },
    {
        "paperId": "787f3c102342efb14e3691d310e77ab3c07b5343",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Schema-learning and rebinding as mechanisms of in-context learning and emergence",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.01201",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.01201, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-06-16",
        "authors": [
            {
                "authorId": "5638370",
                "name": "Siva K. Swaminathan"
            },
            {
                "authorId": "36361397",
                "name": "A. Dedieu"
            },
            {
                "authorId": "31987161",
                "name": "Rajkumar Vasudeva Raju"
            },
            {
                "authorId": "1757629",
                "name": "M. Shanahan"
            },
            {
                "authorId": "1388666962",
                "name": "M. L\u00e1zaro-Gredilla"
            },
            {
                "authorId": "50021619",
                "name": "Dileep George"
            }
        ],
        "abstract": "In-context learning (ICL) is one of the most powerful and most unexpected capabilities to emerge in recent transformer-based large language models (LLMs). Yet the mechanisms that underlie it are poorly understood. In this paper, we demonstrate that comparable ICL capabilities can be acquired by an alternative sequence prediction learning method using clone-structured causal graphs (CSCGs). Moreover, a key property of CSCGs is that, unlike transformer-based LLMs, they are {\\em interpretable}, which considerably simplifies the task of explaining how ICL works. Specifically, we show that it uses a combination of (a) learning template (schema) circuits for pattern completion, (b) retrieving relevant templates in a context-sensitive manner, and (c) rebinding of novel tokens to appropriate slots in the templates. We go on to marshall evidence for the hypothesis that similar mechanisms underlie ICL in LLMs. For example, we find that, with CSCGs as with LLMs, different capabilities emerge at different levels of overparameterization, suggesting that overparameterization helps in learning more complex template (schema) circuits. By showing how ICL can be achieved with small models and datasets, we open up a path to novel architectures, and take a vital step towards a more general understanding of the mechanics behind this important capability."
    },
    {
        "paperId": "7e70b9ac85ff2b27fb2c1c67ea52c39552812dc4",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.01458",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.01458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-04",
        "authors": [
            {
                "authorId": "119564884",
                "name": "Tong Xiang"
            },
            {
                "authorId": "47681301",
                "name": "Liangzhi Li"
            },
            {
                "authorId": "2216518555",
                "name": "Wangyue Li"
            },
            {
                "authorId": "2131181215",
                "name": "Min\u2010Jun Bai"
            },
            {
                "authorId": "2221145675",
                "name": "Lu Wei"
            },
            {
                "authorId": "2153213890",
                "name": "Bowen Wang"
            },
            {
                "authorId": "26385137",
                "name": "Noa Garc\u00eda"
            }
        ],
        "abstract": "The recent advances in natural language processing (NLP), have led to a new trend of applying large language models (LLMs) to real-world scenarios. While the latest LLMs are astonishingly fluent when interacting with humans, they suffer from the misinformation problem by unintentionally generating factually false statements. This can lead to harmful consequences, especially when produced within sensitive contexts, such as healthcare. Yet few previous works have focused on evaluating misinformation in the long-form (LF) generation of LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have been shown to perform well in different languages, misinformation evaluation has been mostly conducted in English. To this end, we present a benchmark, CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic, specifically the maternity and infant care domain; and 2) a language other than English, namely Chinese. Most importantly, we provide an innovative paradigm for building LF generation evaluation benchmarks that can be transferred to other knowledge-intensive domains and low-resourced languages. Our proposed benchmark fills the gap between the extensive usage of LLMs and the lack of datasets for assessing the misinformation generated by these models. It contains 1,612 expert-checked questions, accompanied with human-selected references. Using our benchmark, we conduct extensive experiments and found that current Chinese LLMs are far from perfect in the topic of maternity and infant care. In an effort to minimize the reliance on human resources for performance evaluation, we offer off-the-shelf judgment models for automatically assessing the LF output of LLMs given benchmark questions. Moreover, we compare potential solutions for LF generation evaluation and provide insights for building better automated metrics."
    },
    {
        "paperId": "929305892d4ddae575a0fc23227a8139f7681632",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Jailbroken: How Does LLM Safety Training Fail?",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.02483",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.02483, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-05",
        "authors": [
            {
                "authorId": "143797846",
                "name": "Alexander Wei"
            },
            {
                "authorId": "3033269",
                "name": "Nika Haghtalab"
            },
            {
                "authorId": "5164568",
                "name": "J. Steinhardt"
            }
        ],
        "abstract": "Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of\"jailbreak\"attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes."
    },
    {
        "paperId": "42b850269b3619978f65d8d78a3c7e8640b99984",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "KoRC: Knowledge oriented Reading Comprehension Benchmark for Deep Text Understanding",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.03115",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.03115, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-06",
        "authors": [
            {
                "authorId": "1423719712",
                "name": "Zijun Yao"
            },
            {
                "authorId": "2211723524",
                "name": "Yantao Liu"
            },
            {
                "authorId": "48574888",
                "name": "Xin Lv"
            },
            {
                "authorId": "1712738522",
                "name": "S. Cao"
            },
            {
                "authorId": "2116034394",
                "name": "Jifan Yu"
            },
            {
                "authorId": "2055765060",
                "name": "Lei Hou"
            },
            {
                "authorId": "2133353675",
                "name": "Juanzi Li"
            }
        ],
        "abstract": "Deep text understanding, which requires the connections between a given document and prior knowledge beyond its text, has been highlighted by many benchmarks in recent years. However, these benchmarks have encountered two major limitations. On the one hand, most of them require human annotation of knowledge, which leads to limited knowledge coverage. On the other hand, they usually use choices or spans in the texts as the answers, which results in narrow answer space. To overcome these limitations, we build a new challenging benchmark named KoRc in this paper. Compared with previous benchmarks, KoRC has two advantages, i.e., broad knowledge coverage and flexible answer format. Specifically, we utilize massive knowledge bases to guide annotators or large language models (LLMs) to construct knowledgable questions. Moreover, we use labels in knowledge bases rather than spans or choices as the final answers. We test state-of-the-art models on KoRC and the experimental results show that the strongest baseline only achieves 68.3% and 30.0% F1 measure in the in-distribution and out-of-distribution test set, respectively. These results indicate that deep text understanding is still an unsolved challenge. The benchmark dataset, leaderboard, and baseline methods are released in https://github.com/THU-KEG/KoRC."
    },
    {
        "paperId": "434b9f9bc71c935e4a46a1aff36a8cc4c22d9afa",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.05300, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-11",
        "authors": [
            {
                "authorId": "2052036545",
                "name": "Zhenhailong Wang"
            },
            {
                "authorId": "35374367",
                "name": "Shaoguang Mao"
            },
            {
                "authorId": "51198241",
                "name": "Wenshan Wu"
            },
            {
                "authorId": "50251691",
                "name": "Tao Ge"
            },
            {
                "authorId": "49807919",
                "name": "Furu Wei"
            },
            {
                "authorId": "2072975661",
                "name": "Heng Ji"
            }
        ],
        "abstract": "Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds\u2019 strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git."
    },
    {
        "paperId": "60b0476a97c00e355df28ba35422764a7fbe88e8",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "In-context Autoencoder for Context Compression in a Large Language Model",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.06945",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.06945, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-13",
        "authors": [
            {
                "authorId": "50251691",
                "name": "Tao Ge"
            },
            {
                "authorId": "2193276881",
                "name": "Jing Hu"
            },
            {
                "authorId": "2193104542",
                "name": "Xun Wang"
            },
            {
                "authorId": "2111638099",
                "name": "Si-Qing Chen"
            },
            {
                "authorId": "49807919",
                "name": "Furu Wei"
            }
        ],
        "abstract": "We propose the In-context Autoencoder (ICAE), leveraging the power of a large language model (LLM) to compress a long context into short compact memory slots that can be directly conditioned on by the LLM for various purposes. ICAE is first pretrained using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, it is fine-tuned on instruction data for producing desirable responses to various prompts. Experiments demonstrate that our lightweight ICAE, introducing about 1% additional parameters, effectively achieves $4\\times$ context compression based on Llama, offering advantages in both improved latency and GPU memory cost during inference, and showing an interesting insight in memorization as well as potential for scalability. These promising results imply a novel perspective on the connection between working memory in cognitive science and representation learning in LLMs, revealing ICAE's significant implications in addressing the long context problem and suggesting further research in LLM context management. Our data, code and models are available at https://github.com/getao/icae."
    },
    {
        "paperId": "69f2ba0f33a54e01de32c616b64e85d5d7194067",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.08678",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.08678, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-17",
        "authors": [
            {
                "authorId": "2109268730",
                "name": "Yanda Chen"
            },
            {
                "authorId": "51011000",
                "name": "Ruiqi Zhong"
            },
            {
                "authorId": "2218040104",
                "name": "Narutatsu Ri"
            },
            {
                "authorId": "145756130",
                "name": "Chen Zhao"
            },
            {
                "authorId": "2140062900",
                "name": "He He"
            },
            {
                "authorId": "5164568",
                "name": "J. Steinhardt"
            },
            {
                "authorId": "144007938",
                "name": "Zhou Yu"
            },
            {
                "authorId": "145590324",
                "name": "K. McKeown"
            }
        ],
        "abstract": "Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate $\\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers\"yes\"to the input question\"Can eagles fly?\"with the explanation\"all birds can fly\", then humans would infer from the explanation that it would also answer\"yes\"to the counterfactual input\"Can penguins fly?\". If the explanation is precise, then the model's answer should match humans' expectations. We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution."
    },
    {
        "paperId": "12acdfc7e32e9d603dc108008bb15e65439e7c79",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Evaluating the Moral Beliefs Encoded in LLMs",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.14324",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.14324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-26",
        "authors": [
            {
                "authorId": "1742339548",
                "name": "Nino Scherrer"
            },
            {
                "authorId": "133797256",
                "name": "Claudia Shi"
            },
            {
                "authorId": "46609506",
                "name": "Amir Feder"
            },
            {
                "authorId": "1796335",
                "name": "D. Blei"
            }
        ],
        "abstract": "This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs). It comprises two components: (1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM\"making a choice\", the associated uncertainty, and the consistency of that choice. (2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g.,\"Should I tell a white lie?\") and 687 low-ambiguity moral scenarios (e.g.,\"Should I stop for a pedestrian on the road?\"). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g.,\"do not kill\"). We administer the survey to 28 open- and closed-source LLMs. We find that (a) in unambiguous scenarios, most models\"choose\"actions that align with commonsense. In ambiguous cases, most models express uncertainty. (b) Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording. (c) Some models reflect clear preferences in ambiguous scenarios. Specifically, closed-source models tend to agree with each other."
    },
    {
        "paperId": "56ad2c35353b57653b5f14c33afca72fe21d2d20",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.15770",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.15770, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-29",
        "authors": [
            {
                "authorId": "2046974354",
                "name": "Jingwei Ni"
            },
            {
                "authorId": "2006205621",
                "name": "J. Bingler"
            },
            {
                "authorId": "2214326368",
                "name": "Chiara Colesanti-Senni"
            },
            {
                "authorId": "2156865999",
                "name": "Mathias Kraus"
            },
            {
                "authorId": "1517877494",
                "name": "Glen Gostlow"
            },
            {
                "authorId": "2213295713",
                "name": "Tobias Schimanski"
            },
            {
                "authorId": "146552774",
                "name": "Dominik Stammbach"
            },
            {
                "authorId": "1989342",
                "name": "S. Vaghefi"
            },
            {
                "authorId": "2183631943",
                "name": "Qian Wang"
            },
            {
                "authorId": "2023644816",
                "name": "Nicolas Webersinke"
            },
            {
                "authorId": "2271686995",
                "name": "Tobias Wekhof"
            },
            {
                "authorId": "1841103888",
                "name": "Ting Yu"
            },
            {
                "authorId": "3073566",
                "name": "Markus Leippold"
            }
        ],
        "abstract": "In the face of climate change, are companies really taking substantial steps toward more sustainable operations? A comprehensive answer lies in the dense, information-rich landscape of corporate sustainability reports. However, the sheer volume and complexity of these reports make human analysis very costly. Therefore, only a few entities worldwide have the resources to analyze these reports at scale, which leads to a lack of transparency in sustainability reporting. Empowering stakeholders with LLM-based automatic analysis tools can be a promising way to democratize sustainability report analysis. However, developing such tools is challenging due to (1) the hallucination of LLMs and (2) the inefficiency of bringing domain experts into the AI development loop. In this paper, we ChatReport, a novel LLM-based system to automate the analysis of corporate sustainability reports, addressing existing challenges by (1) making the answers traceable to reduce the harm of hallucination and (2) actively involving domain experts in the development loop. We make our methodology, annotated datasets, and generated analyses of 1015 reports publicly available."
    },
    {
        "paperId": "8291f1f917c0792a95417b3f8a1c2a5ec53872e3",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Exploiting Code Symmetries for Learning Program Semantics",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.03312, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-08-07",
        "authors": [
            {
                "authorId": "40428350",
                "name": "Kexin Pei"
            },
            {
                "authorId": "2154721990",
                "name": "Weichen Li"
            },
            {
                "authorId": "2228552536",
                "name": "Qirui Jin"
            },
            {
                "authorId": "2108043108",
                "name": "Shuyang Liu"
            },
            {
                "authorId": "2187059392",
                "name": "Scott Geng"
            },
            {
                "authorId": "2189170",
                "name": "L. Cavallaro"
            },
            {
                "authorId": "2110694456",
                "name": "Junfeng Yang"
            },
            {
                "authorId": "39400201",
                "name": "S. Jana"
            }
        ],
        "abstract": "This paper tackles the challenge of teaching code semantics to Large Language Models (LLMs) for program analysis by incorporating code symmetries into the model architecture. We introduce a group-theoretic framework that defines code symmetries as semantics-preserving transformations, where forming a code symmetry group enables precise and efficient reasoning of code semantics. Our solution, SymC, develops a novel variant of self-attention that is provably equivariant to code symmetries from the permutation group defined over the program dependence graph. SymC obtains superior performance on five program analysis tasks, outperforming state-of-the-art code models without any pre-training. Our results suggest that code LLMs that encode the code structural prior via the code symmetry group generalize better and faster."
    },
    {
        "paperId": "4c359aa62437ec64c82a77496a859478929a1937",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Position: Key Claims in LLM Research Have a Long Tail of Footnotes",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.07120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-08-14",
        "authors": [
            {
                "authorId": "2993731",
                "name": "A. Luccioni"
            },
            {
                "authorId": "145046059",
                "name": "Anna Rogers"
            }
        ],
        "abstract": "Much of the recent discourse within the ML community has been centered around Large Language Models (LLMs), their functionality and potential -- yet not only do we not have a working definition of LLMs, but much of this discourse relies on claims and assumptions that are worth re-examining. We contribute a definition of LLMs, critically examine five common claims regarding their properties (including 'emergent properties'), and conclude with suggestions for future research directions and their framing."
    },
    {
        "paperId": "aa9ad5cd399434bd7e7d6fcd1ddb8f4b58b953a3",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2308.07336",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.07336, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-08-11",
        "authors": [
            {
                "authorId": "1379579811",
                "name": "Terufumi Morishita"
            },
            {
                "authorId": "29347584",
                "name": "Gaku Morio"
            },
            {
                "authorId": "145412147",
                "name": "Atsuki Yamaguchi"
            },
            {
                "authorId": "2106369",
                "name": "Yasuhiro Sogawa"
            }
        ],
        "abstract": "We study a synthetic corpus based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary, limiting the generalizability of acquired reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. Then, using the proposed corpora, which we name FLD (Formal Logic Deduction), we first evaluate and analyze the logical reasoning ability of the latest LLMs. Even GPT-4 can solve only half of the problems, suggesting that pure logical reasoning isolated from knowledge is still challenging for the LLMs, and additional training specialized in logical reasoning is indeed essential. We next empirically verify that LMs trained on FLD corpora acquire more generalizable reasoning ability. Furthermore, we identify the aspects of reasoning ability on which deduction corpora can enhance LMs and those on which they cannot, and discuss future directions on each aspect. The released corpora serve both as learning resources and as challenging benchmarks."
    },
    {
        "paperId": "f44b23a1ad5396840845b64b4e23e660110b6083",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "CMD: a framework for Context-aware Model self-Detoxification",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.08295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-08-16",
        "authors": [
            {
                "authorId": "1576234850",
                "name": "Zecheng Tang"
            },
            {
                "authorId": "2632690",
                "name": "Keyan Zhou"
            },
            {
                "authorId": "2011935736",
                "name": "Pinzheng Wang"
            },
            {
                "authorId": "2111072144",
                "name": "Yuyang Ding"
            },
            {
                "authorId": "2109013629",
                "name": "Juntao Li"
            },
            {
                "authorId": "66094431",
                "name": "Minzhang"
            }
        ],
        "abstract": "Text detoxification aims to minimize the risk of language models producing toxic content. Existing detoxification methods of directly constraining the model output or further training the model on the non-toxic corpus fail to achieve a decent balance between detoxification effectiveness and generation quality. This issue stems from the neglect of constrain imposed by the context since language models are designed to generate output that closely matches the context while detoxification methods endeavor to ensure the safety of the output even if it semantically deviates from the context. In view of this, we introduce a Context-aware Model self-Detoxification (CMD) framework that pays attention to both the context and the detoxification process, i.e., first detoxifying the context and then making the language model generate along the safe context. Specifically, CMD framework involves two phases: utilizing language models to synthesize data and applying these data for training. We also introduce a toxic contrastive loss that encourages the model generation away from the negative toxic samples. Experiments on various LLMs have verified the effectiveness of our MSD framework, which can yield the best performance compared to baselines."
    },
    {
        "paperId": "fca92fe287c44c9ec79ca1f2762b0bf2e5e8df2b",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2308.10379",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.10379, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-08-20",
        "authors": [
            {
                "authorId": "2090055589",
                "name": "Bilgehan Sel"
            },
            {
                "authorId": "2121379922",
                "name": "Ahmad S. Al-Tawaha"
            },
            {
                "authorId": "1515859806",
                "name": "Vanshaj Khattar"
            },
            {
                "authorId": "2153518376",
                "name": "Lucy Wang"
            },
            {
                "authorId": "39823639",
                "name": "R. Jia"
            },
            {
                "authorId": "2072905592",
                "name": "Ming Jin"
            }
        ],
        "abstract": "Current literature, aiming to surpass the\"Chain-of-Thought\"approach, often resorts to external modi operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. Due to their myopic perspective, they escalate the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways. By employing algorithmic examples fully in-context, this overarching view of the whole process exploits the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and even more recent multi-query strategies that employ an extensive tree search algorithms while using significantly fewer tokens. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application. The code and related content can be found in: https://algorithm-of-thoughts.github.io."
    },
    {
        "paperId": "b56ce5cea6963f5684124caa03df470f4144d895",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.10410, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-08-21",
        "authors": [
            {
                "authorId": "2232906828",
                "name": "Fan Gao"
            },
            {
                "authorId": "48579520",
                "name": "Hang Jiang"
            },
            {
                "authorId": "2287835229",
                "name": "Rui Yang"
            },
            {
                "authorId": "2153554138",
                "name": "Qingcheng Zeng"
            },
            {
                "authorId": "2285824559",
                "name": "Jinghui Lu"
            },
            {
                "authorId": "2285108151",
                "name": "Moritz Blum"
            },
            {
                "authorId": "1585849884",
                "name": "Dairui Liu"
            },
            {
                "authorId": "2106009217",
                "name": "Tianwei She"
            },
            {
                "authorId": "2285289624",
                "name": "Yuang Jiang"
            },
            {
                "authorId": "2275053812",
                "name": "Irene Li"
            }
        ],
        "abstract": "Educational materials such as survey articles in specialized fields like computer science traditionally require tremendous expert inputs and are therefore expensive to create and update. Recently, Large Language Models (LLMs) have achieved significant success across various general tasks. However, their effectiveness and limitations in the education domain are yet to be fully explored. In this work, we examine the proficiency of LLMs in generating succinct survey articles specific to the niche field of NLP in computer science, focusing on a curated list of 99 topics. Automated benchmarks reveal that GPT-4 surpasses its predecessors, inluding GPT-3.5, PaLM2, and LLaMa2 by margins ranging from 2% to 20% in comparison to the established ground truth. We compare both human and GPT-based evaluation scores and provide in-depth analysis. While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed. Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation."
    },
    {
        "paperId": "7a63ea8ade8bd683e353551d5fa5e3ff35ba3680",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2308.10848",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.10848, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-08-21",
        "authors": [
            {
                "authorId": "2109136284",
                "name": "Weize Chen"
            },
            {
                "authorId": "48576745",
                "name": "Yusheng Su"
            },
            {
                "authorId": "2057461656",
                "name": "Jingwei Zuo"
            },
            {
                "authorId": "3443627",
                "name": "Cheng Yang"
            },
            {
                "authorId": "2232928180",
                "name": "Chenfei Yuan"
            },
            {
                "authorId": "2214580084",
                "name": "Cheng Qian"
            },
            {
                "authorId": "2151547817",
                "name": "Chi-Min Chan"
            },
            {
                "authorId": "50625437",
                "name": "Yujia Qin"
            },
            {
                "authorId": "2191753738",
                "name": "Ya-Ting Lu"
            },
            {
                "authorId": "3360722",
                "name": "Ruobing Xie"
            },
            {
                "authorId": "2141313179",
                "name": "Zhiyuan Liu"
            },
            {
                "authorId": "1753344",
                "name": "Maosong Sun"
            },
            {
                "authorId": "49640256",
                "name": "Jie Zhou"
            }
        ],
        "abstract": "Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework \\framework that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that \\framework framework can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups. Our codes for \\framework will soon be released at \\url{https://github.com/OpenBMB/AgentVerse}."
    },
    {
        "paperId": "b931b242f40a032b9ae7dae9d9fc10c6ab90695e",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2308.15812",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.15812, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-08-30",
        "authors": [
            {
                "authorId": "103404553",
                "name": "Hritik Bansal"
            },
            {
                "authorId": "67125605",
                "name": "John Dang"
            },
            {
                "authorId": "1954250",
                "name": "Aditya Grover"
            }
        ],
        "abstract": "Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs that leverage rankings data for alignment (say model X) are preferred over those that leverage ratings data (say model Y), with a rank-based evaluation protocol (is X/Y's response better than reference response?) but not with a rating-based evaluation protocol (score Rank X/Y's response on a scale of 1-7). Our findings thus shed light on critical gaps in methods for evaluating the real-world utility of language models and their strong dependence on the feedback protocol used for alignment. Our code and data are available at https://github.com/Hritikbansal/sparse_feedback."
    },
    {
        "paperId": "0b60ba1142fcb6b361b4695accbb772a08da8d14",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Exploring Cross-Cultural Differences in English Hate Speech Annotations: From Dataset Construction to Analysis",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.16705, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-08-31",
        "authors": [
            {
                "authorId": "40221187",
                "name": "Nayeon Lee"
            },
            {
                "authorId": "2150179781",
                "name": "Chani Jung"
            },
            {
                "authorId": "2137320052",
                "name": "Junho Myung"
            },
            {
                "authorId": "2165259166",
                "name": "Jiho Jin"
            },
            {
                "authorId": "2110106335",
                "name": "Juho Kim"
            },
            {
                "authorId": "2208979147",
                "name": "Alice Oh"
            }
        ],
        "abstract": "***Warning**: this paper contains content that may be offensive or upsetting.*Most hate speech datasets neglect the cultural diversity within a single language, resulting in a critical shortcoming in hate speech detection. To address this, we introduce **CREHate**, a **CR**oss-cultural **E**nglish **Hate** speech dataset.To construct CREHate, we follow a two-step procedure: 1) cultural post collection and 2) cross-cultural annotation.We sample posts from the SBIC dataset, which predominantly represents North America, and collect posts from four geographically diverse English-speaking countries (Australia, United Kingdom, Singapore, and South Africa) using culturally hateful keywords we retrieve from our survey.Annotations are collected from the four countries plus the United States to establish representative labels for each country.Our analysis highlights statistically significant disparities across countries in hate speech annotations.Only 56.2% of the posts in CREHate achieve consensus among all countries, with the highest pairwise label difference rate of 26%.Qualitative analysis shows that label disagreement occurs mostly due to different interpretations of sarcasm and the personal bias of annotators on divisive topics.Lastly, we evaluate large language models (LLMs) under a zero-shot setting and show that current LLMs tend to show higher accuracies on Anglosphere country labels in CREHate.Our dataset and codes are available at: https://github.com/nlee0212/CREHate"
    },
    {
        "paperId": "355a3402d1a9f0c0199e1b8488ebeb5bc952b23a",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.00359",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.00359, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-01",
        "authors": [
            {
                "authorId": "2237426650",
                "name": "Ashmit Khandelwal"
            },
            {
                "authorId": "2237427018",
                "name": "Aditya Agrawal"
            },
            {
                "authorId": "2052840454",
                "name": "Aanisha Bhattacharyya"
            },
            {
                "authorId": "50793081",
                "name": "Yaman Kumar Singla"
            },
            {
                "authorId": "2108291106",
                "name": "Somesh Singh"
            },
            {
                "authorId": "50227009",
                "name": "Uttaran Bhattacharya"
            },
            {
                "authorId": "2237426634",
                "name": "Ishita Dasgupta"
            },
            {
                "authorId": "2237426625",
                "name": "Stefano Petrangeli"
            },
            {
                "authorId": "1753278",
                "name": "R. Shah"
            },
            {
                "authorId": "2261910102",
                "name": "Changyou Chen"
            },
            {
                "authorId": "2261737967",
                "name": "Balaji Krishnamurthy"
            }
        ],
        "abstract": "Shannon and Weaver's seminal information theory divides communication into three levels: technical, semantic, and effectiveness. While the technical level deals with the accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Large Language Models (LLMs), with their wide generalizability, make some progress towards the second level. However, LLMs and other communication models are not conventionally designed for predicting and optimizing communication for desired receiver behaviors and intents. As a result, the effectiveness level remains largely untouched by modern communication systems. In this paper, we introduce the receivers'\"behavior tokens,\"such as shares, likes, clicks, purchases, and retweets, in the LLM's training corpora to optimize content for the receivers and predict their behaviors. Other than showing similar performance to LLMs on content understanding tasks, our trained models show generalization capabilities on the behavior dimension for behavior simulation, content simulation, behavior understanding, and behavior domain adaptation. We show results on all these capabilities using a wide range of tasks on three corpora. We call these models Large Content and Behavior Models (LCBMs). Further, to spur more research on LCBMs, we release our new Content Behavior Corpus (CBC), a repository containing communicator, message, and corresponding receiver behavior (https://behavior-in-the-wild.github.io/LCBM)."
    },
    {
        "paperId": "5aea5c8b536380c5ad1d42108c2c6767622318ee",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.02726",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.02726, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-06",
        "authors": [
            {
                "authorId": "2124477940",
                "name": "Zonglin Yang"
            },
            {
                "authorId": "13728923",
                "name": "Xinya Du"
            },
            {
                "authorId": "2238138967",
                "name": "Junxian Li"
            },
            {
                "authorId": "2237998834",
                "name": "Jie Zheng"
            },
            {
                "authorId": "1746416",
                "name": "Soujanya Poria"
            },
            {
                "authorId": "49943757",
                "name": "E. Cambria"
            }
        ],
        "abstract": "Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction is under a constrained setting: (1) the observation annotations in the dataset are carefully manually handpicked sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses are mostly commonsense knowledge, making the task less challenging. In this work, we tackle these problems by proposing the first dataset for social science academic hypotheses discovery, with the final goal to create systems that automatically generate valid, novel, and helpful scientific hypotheses, given only a pile of raw web corpus. Unlike previous settings, the new dataset requires (1) using open-domain data (raw web corpus) as observations; and (2) proposing hypotheses even new to humanity. A multi-module framework is developed for the task, including three different feedback mechanisms to boost performance, which exhibits superior performance in terms of both GPT-4 based and expert-based evaluation. To the best of our knowledge, this is the first work showing that LLMs are able to generate novel (''not existing in literature'') and valid (''reflecting reality'') scientific hypotheses."
    },
    {
        "paperId": "a327111002d3d76510970d1eec3f03fb85314f11",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.03876",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.03876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-07",
        "authors": [
            {
                "authorId": "2298902857",
                "name": "Patrick Haller"
            },
            {
                "authorId": "74210019",
                "name": "Ansar Aynetdinov"
            },
            {
                "authorId": "2403712",
                "name": "A. Akbik"
            }
        ],
        "abstract": "Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable ability to generate fitting responses to natural language instructions. However, an open research question concerns the inherent biases of trained models and their responses. For instance, if the data used to tune an LLM is dominantly written by persons with a specific political bias, we might expect generated answers to share this bias. Current research work seeks to de-bias such models, or suppress potentially biased answers.With this demonstration, we take a different view on biases in instruction-tuning: Rather than aiming to suppress them, we aim to make them explicit and transparent. To this end, we present OpinionGPT, a web demo in which users can ask questions and select all biases they wish to investigate. The demo will answer this question using a model fine-tuned on text representing each of the selected biases, allowing side-by-side comparison. To train the underlying model, we identified 11 different biases (political, geographic, gender, age) and derived an instruction-tuning corpus in which each answer was written by members of one of these demographics. This paper presents OpinionGPT, illustrates how we trained the bias-aware model and showcases the web application (available at https://opiniongpt.informatik.hu-berlin.de)."
    },
    {
        "paperId": "e5d0a261a8e224ab4ed9fada3e6cbb88429a0a9e",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Evaluating the Deductive Competence of Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.05452",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.05452, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-11",
        "authors": [
            {
                "authorId": "2048147669",
                "name": "S. M. Seals"
            },
            {
                "authorId": "2890773",
                "name": "V. Shalin"
            }
        ],
        "abstract": "The development of highly fluent large language models (LLMs) has prompted increased interest in assessing their reasoning and problem-solving capabilities. We investigate whether several LLMs can solve a classic type of deductive reasoning problem from the cognitive science literature. The tested LLMs have limited abilities to solve these problems in their conventional form. We performed follow up experiments to investigate if changes to the presentation format and content improve model performance. We do find performance differences between conditions; however, they do not improve overall performance. Moreover, we find that performance interacts with presentation format and content in unexpected ways that differ from human performance. Overall, our results suggest that LLMs have unique reasoning biases that are only partially predicted from human reasoning performance and the human-generated language corpora that informs them."
    },
    {
        "paperId": "4cf527e9e0d68e3fc16d39fbcdb3869cd3ccf60f",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Hypothesis Search: Inductive Reasoning with Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.05660",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.05660, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-11",
        "authors": [
            {
                "authorId": "22222520",
                "name": "Ruocheng Wang"
            },
            {
                "authorId": "49456763",
                "name": "E. Zelikman"
            },
            {
                "authorId": "2113249490",
                "name": "Gabriel Poesia"
            },
            {
                "authorId": "2273400602",
                "name": "Yewen Pu"
            },
            {
                "authorId": "2239093653",
                "name": "Nick Haber"
            },
            {
                "authorId": "144002017",
                "name": "Noah D. Goodman"
            }
        ],
        "abstract": "Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which robustly generalize to novel scenarios. Recent work evaluates large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding\"in context learning.\"This works well for straightforward inductive tasks but performs poorly on complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be verified by running on observed examples and generalized to novel inputs. To reduce the hypothesis search space, we explore steps to filter the set of hypotheses to implement: we either ask the LLM to summarize them into a smaller set of hypotheses or ask human annotators to select a subset. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, string transformation dataset SyGuS, and list transformation dataset List Functions. On a random 100-problem subset of ARC, our automated pipeline using LLM summaries achieves 30% accuracy, outperforming the direct prompting baseline (accuracy of 17%). With the minimal human input of selecting from LLM-generated candidates, performance is boosted to 33%. Our ablations show that both abstract hypothesis generation and concrete program representations benefit LLMs on inductive reasoning tasks."
    },
    {
        "paperId": "6f8c4311e65efebb9da5a542c0405684e82a77cc",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Mitigating the Alignment Tax of RLHF",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.06256",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.06256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-12",
        "authors": [
            {
                "authorId": "2238123947",
                "name": "Yong Lin"
            },
            {
                "authorId": "2239156696",
                "name": "Lu Tan"
            },
            {
                "authorId": "2239167492",
                "name": "Hangyu Lin"
            },
            {
                "authorId": "2275119437",
                "name": "Wei Xiong"
            },
            {
                "authorId": "2239197570",
                "name": "Zeming Zheng"
            },
            {
                "authorId": "2066420772",
                "name": "Renjie Pi"
            },
            {
                "authorId": "50561049",
                "name": "Jipeng Zhang"
            },
            {
                "authorId": "50826757",
                "name": "Shizhe Diao"
            },
            {
                "authorId": "2266194421",
                "name": "Haoxiang Wang"
            },
            {
                "authorId": "35279146",
                "name": "Hanze Dong"
            },
            {
                "authorId": "2188204871",
                "name": "Han Zhao"
            },
            {
                "authorId": "2238126734",
                "name": "Yuan Yao"
            },
            {
                "authorId": "38144094",
                "name": "T. Zhang"
            }
        ],
        "abstract": "LLMs acquire a wide range of abilities during pre-training, but aligning LLMs under Reinforcement Learning with Human Feedback (RLHF) can lead to forgetting pretrained abilities, which is also known as the alignment tax. To investigate alignment tax, we conducted experiments with existing RLHF algorithms using OpenLLaMA-3B, which revealed a pronounced alignment tax in NLP tasks. Whereas, despite various techniques to mitigate forgetting, they are often at odds with the RLHF performance, leading to a trade-off between alignment performance and forgetting mitigation, leading to an alignment-forgetting trade-off. In this paper we show that model averaging, which simply interpolates between pre and post RLHF model weights, surprisingly achieves the most strongest alignment-forgetting Pareto front among a wide range of competing methods. To understand its effectiveness, we offer theoretical insights into model averaging, revealing that it enhances performance Pareto front by increasing feature diversity on the layers where tasks share overlapped feature spaces. Empirical evidence corroborates our analysis by showing the benefits of averaging low-level transformer layers. Building on the analysis and the observation that averaging different layers of the transformer leads to significantly different alignment-forgetting trade-offs, we propose Heterogeneous Model Averaging (HMA) to Heterogeneously find various combination ratios of model layers. HMA seeks to maximize the alignment performance while incurring minimal alignment tax. Moreover, we validate HMA\u2019s performance across a range of RLHF algorithms over OpenLLaMA-3B and further extend our findings to Mistral-7B which is evaluated by open-sourced preference model and GPT4. Code available here."
    },
    {
        "paperId": "4b508ba98a180f27fd93b702d7044adad91620eb",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Re-Reading Improves Reasoning in Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2309.06275",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.06275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-12",
        "authors": [
            {
                "authorId": "2188774115",
                "name": "Xiaohan Xu"
            },
            {
                "authorId": "8801869",
                "name": "Chongyang Tao"
            },
            {
                "authorId": "143681703",
                "name": "Tao Shen"
            },
            {
                "authorId": "46747953",
                "name": "Can Xu"
            },
            {
                "authorId": "2261472368",
                "name": "Hongbo Xu"
            },
            {
                "authorId": "2062835",
                "name": "Guodong Long"
            },
            {
                "authorId": "4648762",
                "name": "Jian-Guang Lou"
            }
        ],
        "abstract": "To enhance the reasoning capabilities of off-the-shelf Large Language Models (LLMs), we introduce a simple, yet general and effective prompting method, RE2, i.e., Re-Reading the question as input. Unlike most thought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim to elicit the reasoning process in the output, RE2 shifts the focus to the input by processing questions twice, thereby enhancing the understanding process. Consequently, RE2 demonstrates strong generality and compatibility with most thought-eliciting prompting methods, including CoT. Crucially, RE2 facilitates a \u201cbidirectional\u201d encoding in unidirectional decoder-only LLMs because the first pass could provide global information for the second pass. We begin with a preliminary empirical study as the foundation of RE2, illustrating its potential to enable \u201cbidirectional\u201d attention mechanisms. We then evaluate RE2 on extensive reasoning benchmarks across 14 datasets, spanning 112 experiments, to validate its effectiveness and generality. Our findings indicate that, with the exception of a few scenarios on vanilla ChatGPT, RE2 consistently enhances the reasoning performance of LLMs through a simple re-reading strategy. Further analyses reveal RE2\u2019s adaptability, showing how it can be effectively integrated with different LLMs, thought-eliciting prompting, and ensemble strategies."
    },
    {
        "paperId": "423cbdde746d691deb23263005e587ec087d2cdc",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.08583",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.08583, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-15",
        "authors": [
            {
                "authorId": "20974925",
                "name": "Arkadiy Saakyan"
            },
            {
                "authorId": "2295928",
                "name": "S. Muresan"
            }
        ],
        "abstract": "While state-of-the-art large language models (LLMs) can excel at adapting text from one style to another, current work does not address the explainability of style transfer models. Recent work has explored generating textual explanations from larger teacher models and distilling them into smaller student models. One challenge with such approach is that LLM outputs may contain errors that require expertise to correct, but gathering and incorporating expert feedback is difficult due to cost and availability. To address this challenge, we propose ICLEF, a novel human-AI collaboration approach to model distillation that incorporates scarce expert human feedback by combining in-context learning and model self-critique. We show that our method leads to generation of high-quality synthetic explainable style transfer datasets for formality (e-GYAFC) and subjective bias (e-WNC). Via automatic and human evaluation, we show that specialized student models fine-tuned on our datasets outperform generalist teacher models on the explainable style transfer task in one-shot settings, and perform competitively compared to few-shot teacher models, highlighting the quality of the data and the role of expert feedback. In an extrinsic task of authorship attribution, we show that explanations generated by smaller models fine-tuned on e-GYAFC are more predictive of authorship than explanations generated by few-shot teacher models."
    },
    {
        "paperId": "25e4e7a69454099ca0e3ccbf079452878d3abce9",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.08591",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.08591, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-15",
        "authors": [
            {
                "authorId": "143876027",
                "name": "Chen Cecilia Liu"
            },
            {
                "authorId": "2789148",
                "name": "Fajri Koto"
            },
            {
                "authorId": "145465286",
                "name": "Timothy Baldwin"
            },
            {
                "authorId": "1730400",
                "name": "Iryna Gurevych"
            }
        ],
        "abstract": "Large language models (LLMs) are highly adept at question answering and reasoning tasks, but when reasoning in a situational context, human expectations vary depending on the relevant cultural common ground. As languages are associated with diverse cultures, LLMs should also be culturally-diverse reasoners. In this paper, we study the ability of a wide range of state-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and sayings in a conversational context. Our experiments reveal that: (1) mLLMs \u201cknow\u201d limited proverbs and memorizing proverbs does not mean understanding them within a conversational context; (2) mLLMs struggle to reason with figurative proverbs and sayings, and when asked to select the wrong answer (instead of asking it to select the correct answer); and (3) there is a \u201cculture gap\u201d in mLLMs when reasoning about proverbs and sayings translated from other languages. We construct and release our evaluation dataset MAPS (MulticulturAl Proverbs and Sayings) for proverb understanding with conversational context for six different languages."
    },
    {
        "paperId": "034f1d77d832460a239072c81b5bb178b93c1e9f",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of LLMs",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.08827",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.08827, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-16",
        "authors": [
            {
                "authorId": "1456156441",
                "name": "Sarkar Snigdha Sarathi Das"
            },
            {
                "authorId": "144797719",
                "name": "C. Shah"
            },
            {
                "authorId": "2127553",
                "name": "Mengting Wan"
            },
            {
                "authorId": "2243181687",
                "name": "Jennifer Neville"
            },
            {
                "authorId": "49576139",
                "name": "Longfei Yang"
            },
            {
                "authorId": "2243181473",
                "name": "Reid Andersen"
            },
            {
                "authorId": "2243178870",
                "name": "Georg Buscher"
            },
            {
                "authorId": "2243180014",
                "name": "Tara Safavi"
            }
        ],
        "abstract": "The traditional Dialogue State Tracking (DST) problem aims to track user preferences and intents in user-agent conversations. While sufficient for task-oriented dialogue systems supporting narrow domain applications, the advent of Large Language Model (LLM)-based chat systems has introduced many real-world intricacies in open-domain dialogues. These intricacies manifest in the form of increased complexity in contextual interactions, extended dialogue sessions encompassing a diverse array of topics, and more frequent contextual shifts. To handle these intricacies arising from evolving LLM-based chat systems, we propose joint dialogue segmentation and state tracking per segment in open-domain dialogue systems. Assuming a zero-shot setting appropriate to a true open-domain dialogue system, we propose S3-DST, a structured prompting technique that harnesses Pre-Analytical Recollection, a novel grounding mechanism we designed for improving long context tracking. To demonstrate the efficacy of our proposed approach in joint segmentation and state tracking, we evaluate S3-DST on a proprietary anonymized open-domain dialogue dataset, as well as publicly available DST and segmentation datasets. Across all datasets and settings, S3-DST consistently outperforms the state-of-the-art, demonstrating its potency and robustness the next generation of LLM-based chat systems."
    },
    {
        "paperId": "3de4c6545e535562a9b6770eac1c52513aa72694",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "PDFTriage: Question Answering over Long, Structured Documents",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.08872",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.08872, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-16",
        "authors": [
            {
                "authorId": "2243180037",
                "name": "Jon Saad-Falcon"
            },
            {
                "authorId": "40080808",
                "name": "Joe Barrow"
            },
            {
                "authorId": "2233085914",
                "name": "Alexa F. Siu"
            },
            {
                "authorId": "3115414",
                "name": "A. Nenkova"
            },
            {
                "authorId": "2238208116",
                "name": "Ryan A. Rossi"
            },
            {
                "authorId": "2462276",
                "name": "Franck Dernoncourt"
            }
        ],
        "abstract": "Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM. To overcome this issue, most existing works focus on retrieving the relevant context from the document, representing them as plain text. However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on. Representing such structured documents as plain text is incongruous with the user\u2019s mental model of these documents with rich structure. When a system has to query the document for context, this incongruity is brought to the fore, and seemingly trivial questions can trip up the QA system. To bridge this fundamental gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content. Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmented models across several classes of questions where existing retrieval-augmented LLMs fail. To facilitate further research on this fundamental problem, we release our benchmark dataset consisting of 900+ human-generated questions over 80 structured documents from 10 different categories of question types for document QA. Our code and datasets will be released soon on Github."
    },
    {
        "paperId": "300b01dc726fe8acbededd805501811d427920bd",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.08873",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.08873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-16",
        "authors": [
            {
                "authorId": "40636184",
                "name": "Juan Diego Rodriguez"
            },
            {
                "authorId": "2243181832",
                "name": "Katrin Erk"
            },
            {
                "authorId": "1814094",
                "name": "Greg Durrett"
            }
        ],
        "abstract": "Understanding when two pieces of text convey the same information is a goal touching many subproblems in NLP, including textual entailment and fact-checking. This problem becomes more complex when those two pieces of text are in different languages. Here, we introduce X-PARADE (Cross-lingual Paragraph-level Analysis of Divergences and Entailments), the first cross-lingual dataset of paragraph-level information divergences. Annotators label a paragraph in a target language at the span level and evaluate it with respect to a corresponding paragraph in a source language, indicating whether a given piece of information is the same, new, or new but can be inferred. This last notion establishes a link with cross-language NLI. Aligned paragraphs are sourced from Wikipedia pages in different languages, reflecting real information divergences observed in the wild. Armed with our dataset, we investigate a diverse set of approaches for this problem, including classic token alignment from machine translation, textual entailment methods that localize their decisions, and prompting LLMs. Our results show that these methods vary in their capability to handle inferable information, but they all fall short of human performance."
    },
    {
        "paperId": "e65d911a698c406e2ebfa3262e11f223fe70053b",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.08902",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.08902, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-16",
        "authors": [
            {
                "authorId": "2077526744",
                "name": "M. Kamruzzaman"
            },
            {
                "authorId": "2125956593",
                "name": "M. I. Shovon"
            },
            {
                "authorId": "52388989",
                "name": "Gene Louis Kim"
            }
        ],
        "abstract": "LLMs are increasingly powerful and widely used to assist users in a variety of tasks. This use risks the introduction of LLM biases to consequential decisions such as job hiring, human performance evaluation, and criminal sentencing. Bias in NLP systems along the lines of gender and ethnicity has been widely studied, especially for specific stereotypes (e.g., Asians are good at math). In this paper, we investigate bias along less-studied but still consequential, dimensions, such as age and beauty, measuring subtler correlated decisions that LLMs make between social groups and unrelated positive and negative attributes. We ask whether LLMs hold wide-reaching biases of positive or negative sentiment for specific social groups similar to the\"what is beautiful is good\"bias found in people in experimental psychology. We introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attribute to complete an evaluative statement about a person described as a member of a specific social group. We also reverse the completion task to select the social group based on an attribute. We report the correlations that we find for 4 cutting-edge LLMs. This dataset can be used as a benchmark to evaluate progress in more generalized biases and the templating technique can be used to expand the benchmark with minimal additional human annotation."
    },
    {
        "paperId": "166d1e5361465f8e235747d14641249cbb3b6fd2",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.11674",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.11674, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-20",
        "authors": [
            {
                "authorId": "2108835452",
                "name": "Haoran Xu"
            },
            {
                "authorId": "2152658577",
                "name": "Young Jin Kim"
            },
            {
                "authorId": "2243335598",
                "name": "Amr Sharaf"
            },
            {
                "authorId": "3032929",
                "name": "H. Awadalla"
            }
        ],
        "abstract": "Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models. Previous studies have attempted to improve the translation capabilities of these moderate LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on. Our approach consists of two fine-tuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data. We introduce the LLM developed through this strategy as Advanced Language Model-based trAnslator (ALMA). Based on LLaMA-2 as our underlying model, our results show that the model can achieve an average improvement of more than 12 BLEU and 12 COMET over its zero-shot performance across 10 translation directions from the WMT'21 (2 directions) and WMT'22 (8 directions) test datasets. The performance is significantly better than all prior work and even superior to the NLLB-54B model and GPT-3.5-text-davinci-003, with only 7B or 13B parameters. This method establishes the foundation for a novel training paradigm in machine translation."
    },
    {
        "paperId": "a8842ff234dc4ca8d418b98221f529156c1abbf1",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "LLM-based Medical Assistant Personalization with Short- and Long-Term Memory Coordination",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.11696, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-21",
        "authors": [
            {
                "authorId": "2243755551",
                "name": "Kai Zhang"
            },
            {
                "authorId": "1387821612",
                "name": "Fubang Zhao"
            },
            {
                "authorId": "38753454",
                "name": "Yangyang Kang"
            },
            {
                "authorId": "2238387483",
                "name": "Xiaozhong Liu"
            }
        ],
        "abstract": "Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand, medical assistants hold the potential to offer substantial benefits for individuals. However, the exploration of LLM-based personalized medical assistant remains relatively scarce. Typically, patients converse differently based on their background and preferences which necessitates the task of enhancing user-oriented medical assistant. While one can fully train an LLM for this objective, the resource consumption is unaffordable. Prior research has explored memory-based methods to enhance the response with aware of previous mistakes for new queries during a dialogue session. We contend that a mere memory module is inadequate and fully training an LLM can be excessively costly. In this study, we propose a novel computational bionic memory mechanism, equipped with a parameter-efficient fine-tuning (PEFT) schema, to personalize medical assistants. To encourage further research into this area, we are releasing a new conversation dataset generated based on an open-source medical corpus and our implementation."
    },
    {
        "paperId": "8eafec7014d08043517834b5a2ed26384f188873",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\"",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.12288, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-21",
        "authors": [
            {
                "authorId": "2237798380",
                "name": "Lukas Berglund"
            },
            {
                "authorId": "2237797264",
                "name": "Meg Tong"
            },
            {
                "authorId": "2237795846",
                "name": "Max Kaufmann"
            },
            {
                "authorId": "2237797134",
                "name": "Mikita Balesni"
            },
            {
                "authorId": "68974453",
                "name": "Asa Cooper Stickland"
            },
            {
                "authorId": "2237795801",
                "name": "Tomasz Korbak"
            },
            {
                "authorId": "47107786",
                "name": "Owain Evans"
            }
        ],
        "abstract": "We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form\"A is B\", it will not automatically generalize to the reverse direction\"B is A\". This is the Reversal Curse. For instance, if a model is trained on\"Valentina Tereshkova was the first woman to travel to space\", it will not automatically be able to answer the question,\"Who was the first woman to travel to space?\". Moreover, the likelihood of the correct answer (\"Valentina Tershkova\") will not be higher than for a random name. Thus, models do not generalize a prevalent pattern in their training set: if\"A is B\"occurs,\"B is A\"is more likely to occur. It is worth noting, however, that if\"A is B\"appears in-context, models can deduce the reverse relationship. We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as\"Uriah Hawthorne is the composer of Abyssal Melodies\"and showing that they fail to correctly answer\"Who composed Abyssal Melodies?\". The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as\"Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]\"and the reverse\"Who is Mary Lee Pfeiffer's son?\". GPT-4 correctly answers questions like the former 79% of the time, compared to 33% for the latter. Code available at: https://github.com/lukasberglund/reversal_curse."
    },
    {
        "paperId": "f29f8b8aa2b7e608199b65d3cf751969d4024132",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.14316",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.14316, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-25",
        "authors": [
            {
                "authorId": "1388725932",
                "name": "Zeyuan Allen-Zhu"
            },
            {
                "authorId": "2110486765",
                "name": "Yuanzhi Li"
            }
        ],
        "abstract": "Large language models (LLMs) can store a vast amount of world knowledge, often extractable via question-answering (e.g.,\"What is Abraham Lincoln's birthday?\"). However, do they answer such questions based on exposure to similar questions during training (i.e., cheating), or by genuinely learning to extract knowledge from sources like Wikipedia? In this paper, we investigate this issue using a controlled biography dataset. We find a strong correlation between the model's ability to extract knowledge and various diversity measures of the training data. $\\textbf{Essentially}$, for knowledge to be reliably extracted, it must be sufficiently augmented (e.g., through paraphrasing, sentence shuffling, translations) $\\textit{during pretraining}$. Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning. To understand why this occurs, we employ (nearly) linear probing to demonstrate a strong connection between the observed correlation and how the model internally encodes knowledge -- whether it is linearly encoded in the hidden embeddings of entity names or distributed across other token embeddings in the training text. This paper provides $\\textbf{several key recommendations for LLM pretraining in the industry}$: (1) rewrite the pretraining data -- using small, auxiliary models -- to provide knowledge augmentation, and (2) incorporate more instruction-finetuning data into the pretraining stage before it becomes too late."
    },
    {
        "paperId": "945db0077b6d19b720f5998b3f61300013c4f885",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.14717",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.14717, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-26",
        "authors": [
            {
                "authorId": "48615640",
                "name": "Yuhui Xu"
            },
            {
                "authorId": "3041937",
                "name": "Lingxi Xie"
            },
            {
                "authorId": "7787721",
                "name": "Xiaotao Gu"
            },
            {
                "authorId": "2145229597",
                "name": "Xin Chen"
            },
            {
                "authorId": "144188238",
                "name": "Heng Chang"
            },
            {
                "authorId": "1983351",
                "name": "Hengheng Zhang"
            },
            {
                "authorId": "2249294812",
                "name": "Zhensu Chen"
            },
            {
                "authorId": "21458018",
                "name": "Xiaopeng Zhang"
            },
            {
                "authorId": "2106415186",
                "name": "Qi Tian"
            }
        ],
        "abstract": "Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model families and validate its effectiveness in different fine-tuning datasets and downstream scenarios. Code will be made available at https://github.com/yuhuixu1993/qa-lora."
    },
    {
        "paperId": "fdc75b7b51ca98e555a4a8f0bc261e76bac6fb70",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.15098",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.15098, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-26",
        "authors": [
            {
                "authorId": "2186981598",
                "name": "Mert Yuksekgonul"
            },
            {
                "authorId": "2247670741",
                "name": "Varun Chandrasekaran"
            },
            {
                "authorId": "2247370218",
                "name": "Erik Jones"
            },
            {
                "authorId": "3317356",
                "name": "Suriya Gunasekar"
            },
            {
                "authorId": "2205397794",
                "name": "Ranjita Naik"
            },
            {
                "authorId": "2247662718",
                "name": "Hamid Palangi"
            },
            {
                "authorId": "2247662058",
                "name": "Ece Kamar"
            },
            {
                "authorId": "2571049",
                "name": "Besmira Nushi"
            }
        ],
        "abstract": "We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as constraint satisfaction problems and use this framework to investigate how the LLM interacts internally with factual constraints. We find a strong positive relationship between the LLM's attention to constraint tokens and the factual accuracy of generations. We curate a suite of 10 datasets containing over 40,000 prompts to study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe, a method probing attention patterns, that can predict factual errors and fine-grained constraint satisfaction, and allow early error identification. The approach and findings take another step towards using the mechanistic understanding of LLMs to enhance their reliability."
    },
    {
        "paperId": "9977fee41d9cce1b2ed924da966140ac8120762b",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.15129",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.15129, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-25",
        "authors": [
            {
                "authorId": "2248289111",
                "name": "Ida Momennejad"
            },
            {
                "authorId": "2245381886",
                "name": "Hosein Hasanbeig"
            },
            {
                "authorId": "1844283112",
                "name": "F. Frujeri"
            },
            {
                "authorId": "2248286708",
                "name": "Hiteshi Sharma"
            },
            {
                "authorId": "36670968",
                "name": "R. Ness"
            },
            {
                "authorId": "2235694378",
                "name": "Nebojsa Jojic"
            },
            {
                "authorId": "2269473744",
                "name": "Hamid Palangi"
            },
            {
                "authorId": "2248259058",
                "name": "Jonathan Larson"
            }
        ],
        "abstract": "Recently an influx of studies claim emergent cognitive abilities in large language models (LLMs). Yet, most rely on anecdotes, overlook contamination of training sets, or lack systematic Evaluation involving multiple tasks, control conditions, multiple iterations, and statistical robustness tests. Here we make two major contributions. First, we propose CogEval, a cognitive science-inspired protocol for the systematic evaluation of cognitive capacities in Large Language Models. The CogEval protocol can be followed for the evaluation of various abilities. Second, here we follow CogEval to systematically evaluate cognitive maps and planning ability across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base our task prompts on human experiments, which offer both established construct validity for evaluating planning, and are absent from LLM training sets. We find that, while LLMs show apparent competence in a few planning tasks with simpler structures, systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and getting trapped in loops. These findings do not support the idea of emergent out-of-the-box planning ability in LLMs. This could be because LLMs do not understand the latent relational structures underlying planning problems, known as cognitive maps, and fail at unrolling goal-directed trajectories based on the underlying structure. Implications for application and future directions are discussed."
    },
    {
        "paperId": "5e2c1b0966c1847fe46c0b593d99c37f66445cc1",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.15701",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.15701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-27",
        "authors": [
            {
                "authorId": "40262099",
                "name": "Cheng Chen"
            },
            {
                "authorId": "2223605554",
                "name": "Yuchen Hu"
            },
            {
                "authorId": "2249458520",
                "name": "Chao-Han Huck Yang"
            },
            {
                "authorId": "1709878",
                "name": "Sabato Marco Siniscalchi"
            },
            {
                "authorId": "2249078602",
                "name": "Pin-Yu Chen"
            },
            {
                "authorId": "2457835",
                "name": "E. Chng"
            }
        ],
        "abstract": "Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, e.g., background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as the output transcription. The proposed benchmark contains a novel dataset, HyPoradise (HP), encompassing more than 334,000 pairs of N-best hypotheses and corresponding accurate transcriptions across prevalent speech domains. Given this dataset, we examine three types of error correction techniques based on LLMs with varying amounts of labeled hypotheses-transcription pairs, which gains a significant word error rate (WER) reduction. Experimental evidence demonstrates the proposed technique achieves a breakthrough by surpassing the upper bound of traditional re-ranking based methods. More surprisingly, LLM with reasonable prompt and its generative capability can even correct those tokens that are missing in N-best list. We make our results publicly accessible for reproducible pipelines with released pre-trained models, thus providing a new evaluation paradigm for ASR error correction with LLMs."
    },
    {
        "paperId": "1e0caa706e9d9fdad82d6713fa20b52975a1703b",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "At Which Training Stage Does Code Data Help LLMs Reasoning?",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.16298",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.16298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-28",
        "authors": [
            {
                "authorId": "2249023339",
                "name": "Yingwei Ma"
            },
            {
                "authorId": "2279257060",
                "name": "Yue Liu"
            },
            {
                "authorId": "1633124736",
                "name": "Yue Yu"
            },
            {
                "authorId": "2145783381",
                "name": "Yuanliang Zhang"
            },
            {
                "authorId": "2249522683",
                "name": "Yu Jiang"
            },
            {
                "authorId": "2275820162",
                "name": "Changjian Wang"
            },
            {
                "authorId": "2197478793",
                "name": "Shanshan Li"
            }
        ],
        "abstract": "Large Language Models (LLMs) have exhibited remarkable reasoning capabilities and become the foundation of language technologies. Inspired by the great success of code data in training LLMs, we naturally wonder at which training stage introducing code data can really help LLMs reasoning. To this end, this paper systematically explores the impact of code data on LLMs at different stages. Concretely, we introduce the code data at the pre-training stage, instruction-tuning stage, and both of them, respectively. Then, the reasoning capability of LLMs is comprehensively and fairly evaluated via six reasoning tasks in five domains. We critically analyze the experimental results and provide conclusions with insights. First, pre-training LLMs with the mixture of code and text can significantly enhance LLMs' general reasoning capability almost without negative transfer on other tasks. Besides, at the instruction-tuning stage, code data endows LLMs the task-specific reasoning capability. Moreover, the dynamic mixing strategy of code and text data assists LLMs to learn reasoning capability step-by-step during training. These insights deepen the understanding of LLMs regarding reasoning ability for their application, such as scientific question answering, legal support, etc. The source code and model parameters are released at the link:~\\url{https://github.com/yingweima2022/CodeLLM}."
    },
    {
        "paperId": "3a1a9ef603fd245fd9732064e5756efc82c797b1",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "A Benchmark for Learning to Translate a New Language from One Grammar Book",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.16575",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.16575, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-28",
        "authors": [
            {
                "authorId": "2248180735",
                "name": "Garrett Tanzer"
            },
            {
                "authorId": "51903517",
                "name": "Mirac Suzgun"
            },
            {
                "authorId": "2248162533",
                "name": "Eline Visser"
            },
            {
                "authorId": "1746807",
                "name": "Dan Jurafsky"
            },
            {
                "authorId": "2268317740",
                "name": "Luke Melas-Kyriazi"
            }
        ],
        "abstract": "Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang -- a language with less than 200 speakers and therefore virtually no presence on the web -- using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 learning than L1 acquisition. We demonstrate that baselines using current LLMs are promising but fall short of human performance, achieving 44.7 chrF on Kalamang to English translation and 45.8 chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a human who learned Kalamang from the same reference materials. We hope that MTOB will help measure LLM capabilities along a new dimension, and that the methods developed to solve it could help expand access to language technology for underserved communities by leveraging qualitatively different kinds of data than traditional machine translation."
    },
    {
        "paperId": "7fe071ea76e49bc3e573beb53f07721630954247",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.16797",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.16797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-28",
        "authors": [
            {
                "authorId": "143939165",
                "name": "Chrisantha Fernando"
            },
            {
                "authorId": "32689129",
                "name": "Dylan Banarse"
            },
            {
                "authorId": "47407464",
                "name": "H. Michalewski"
            },
            {
                "authorId": "2217144",
                "name": "Simon Osindero"
            },
            {
                "authorId": "2620211",
                "name": "Tim Rockt\u00e4schel"
            }
        ],
        "abstract": "Popular prompt strategies like Chain-of-Thought Prompting can dramatically improve the reasoning abilities of Large Language Models (LLMs) in various domains. However, such hand-crafted prompt-strategies are often sub-optimal. In this paper, we present Promptbreeder, a general-purpose self-referential self-improvement mechanism that evolves and adapts prompts for a given domain. Driven by an LLM, Promptbreeder mutates a population of task-prompts, and subsequently evaluates them for fitness on a training set. Crucially, the mutation of these task-prompts is governed by mutation-prompts that the LLM generates and improves throughout evolution in a self-referential way. That is, Promptbreeder is not just improving task-prompts, but it is also improving the mutationprompts that improve these task-prompts. Promptbreeder outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks. Furthermore, Promptbreeder is able to evolve intricate task-prompts for the challenging problem of hate speech classification."
    },
    {
        "paperId": "d55ed10e6a77e8f0a2359eb92221915f56481843",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.17012",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.17012, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-29",
        "authors": [
            {
                "authorId": "2213239540",
                "name": "Ryan Koo"
            },
            {
                "authorId": "2187932371",
                "name": "Minhwa Lee"
            },
            {
                "authorId": "2831377",
                "name": "Vipul Raheja"
            },
            {
                "authorId": "2294310015",
                "name": "Jong Inn Park"
            },
            {
                "authorId": "2894340",
                "name": "Zae Myung Kim"
            },
            {
                "authorId": "48493368",
                "name": "Dongyeop Kang"
            }
        ],
        "abstract": "Large Language Models are cognitively biased judges. Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 49.6%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences. Our project page is at: https://minnesotanlp.github.io/cobbler."
    },
    {
        "paperId": "1d7f414983eb847c4618489baa44e99b01162f98",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.17167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-29",
        "authors": [
            {
                "authorId": "2543684",
                "name": "Kaijie Zhu"
            },
            {
                "authorId": "47739850",
                "name": "Jiaao Chen"
            },
            {
                "authorId": "2145270616",
                "name": "Jindong Wang"
            },
            {
                "authorId": "2249536787",
                "name": "Neil Zhenqiang Gong"
            },
            {
                "authorId": "2022168",
                "name": "Diyi Yang"
            },
            {
                "authorId": "2249681654",
                "name": "Xing Xie"
            }
        ],
        "abstract": "Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns are raised about potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a general and flexible protocol for dynamic evaluation of LLMs. Based on our framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo and GPT-4. Experiments show that LLMs perform worse in DyVal-generated evaluation samples with different complexities, highlighting the significance of dynamic evaluation. We also analyze the failure cases and results of different prompting methods. Moreover, DyVal-generated samples are not only evaluation sets, but also helpful data for fine-tuning to improve the performance of LLMs on existing benchmarks. We hope that DyVal can shed light on future evaluation research of LLMs. Code is available at: https://github.com/microsoft/promptbench."
    },
    {
        "paperId": "0d22f06a1f5ad9f62b2f35c126b514f927586c85",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.17272",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.17272, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-29",
        "authors": [
            {
                "authorId": "2184278672",
                "name": "Baizhou Huang"
            },
            {
                "authorId": "2115338656",
                "name": "Shuai Lu"
            },
            {
                "authorId": "2249538838",
                "name": "Weizhu Chen"
            },
            {
                "authorId": "2257016300",
                "name": "Xiaojun Wan"
            },
            {
                "authorId": "46429989",
                "name": "Nan Duan"
            }
        ],
        "abstract": "Large language models (LLMs) have exhibited remarkable ability in code generation. However, generating the correct solution in a single attempt still remains a challenge. Prior works utilize verification properties in software engineering to verify and re-rank solutions in a majority voting manner. But the assumption behind them that generated verification properties have better qualities than solutions may not always hold. In this paper, we treat them equally as different perspectives of LLMs' reasoning processes. We propose the Multi-Perspective Self-Consistency (MPSC) framework incorporating both inter- and intra-consistency across outputs from multiple perspectives. Specifically, we prompt LLMs to generate diverse outputs from three perspectives, Solution, Specification and Test case, constructing a 3-partite graph. With two measure functions of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice of solutions is then determined based on analysis in the graph. MPSC significantly boosts performance of foundation models (ChatGPT in this paper) on various benchmarks, including HumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing GPT-4."
    },
    {
        "paperId": "447bf7d49b1e931466e4c54fef3d5ca372a6ad06",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Intuitive or Dependent? Investigating LLMs' Behavior Style to Conflicting Prompts",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.17415",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.17415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-29",
        "authors": [
            {
                "authorId": "2249532957",
                "name": "Jiahao Ying"
            },
            {
                "authorId": "2273914010",
                "name": "Yixin Cao"
            },
            {
                "authorId": "2249539564",
                "name": "Kai Xiong"
            },
            {
                "authorId": "2249538512",
                "name": "Long Cui"
            },
            {
                "authorId": "2256764940",
                "name": "Yidong He"
            },
            {
                "authorId": "2249562267",
                "name": "Yongbin Liu"
            }
        ],
        "abstract": "This study investigates the behaviors of Large Language Models (LLMs) when faced with conflicting prompts versus their internal memory. This will not only help to understand LLMs' decision mechanism but also benefit real-world applications, such as retrieval-augmented generation (RAG). Drawing on cognitive theory, we target the first scenario of decision-making styles where there is no superiority in the conflict and categorize LLMs' preference into dependent, intuitive, and rational/irrational styles. Another scenario of factual robustness considers the correctness of prompt and memory in knowledge-intensive tasks, which can also distinguish if LLMs behave rationally or irrationally in the first scenario. To quantify them, we establish a complete benchmarking framework including a dataset, a robustness evaluation pipeline, and corresponding metrics. Extensive experiments with seven LLMs reveal their varying behaviors. And, with role play intervention, we can change the styles, but different models present distinct adaptivity and upper-bound. One of our key takeaways is to optimize models or the prompts according to the identified style. For instance, RAG models with high role play adaptability may dynamically adjust the interventions according to the quality of retrieval results -- being dependent to better leverage informative context; and, being intuitive when external prompt is noisy."
    },
    {
        "paperId": "ce15b4255bcf79781906c7830973a9df39e7fe24",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "PB-LLM: Partially Binarized Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.00034",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.00034, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-29",
        "authors": [
            {
                "authorId": "2125633957",
                "name": "Yuzhang Shang"
            },
            {
                "authorId": "2256166978",
                "name": "Zhihang Yuan"
            },
            {
                "authorId": "2249848412",
                "name": "Qiang Wu"
            },
            {
                "authorId": "143879884",
                "name": "Zhen Dong"
            }
        ],
        "abstract": "This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts from GPTQ, we reconstruct the binarized weight matrix guided by the Hessian matrix and successfully recover the reasoning capacity of PB-LLM in low-bit. Under QAT, we freeze the salient weights during training, explore the derivation of optimal scaling factors crucial for minimizing the quantization error, and propose a scaling mechanism based on this derived scaling strategy for residual binarized weights. Those explorations and the developed methodologies significantly contribute to rejuvenating the performance of low-bit quantized LLMs and present substantial advancements in the field of network binarization for LLMs.The code is available at https://github.com/hahnyuan/BinaryLLM."
    },
    {
        "paperId": "db6b5baa8390e065e7823a85010f952850ad8729",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.00902",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.00902, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-02",
        "authors": [
            {
                "authorId": "33778474",
                "name": "Yongchan Kwon"
            },
            {
                "authorId": "2253396752",
                "name": "Eric Wu"
            },
            {
                "authorId": "2326644245",
                "name": "Kevin Wu"
            },
            {
                "authorId": "2256466261",
                "name": "James Zou"
            }
        ],
        "abstract": "Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods. In applications to RoBERTa-large, Llama-2-13B-chat, and stable-diffusion-v1.5 models, DataInf effectively identifies the most influential fine-tuning examples better than other approximate influence scores. Moreover, it can help to identify which data points are mislabeled."
    },
    {
        "paperId": "3301fc6a084c7b09a60f24edcf5502c00e421989",
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "title": "ARN: Analogical Reasoning on Narratives",
        "openAccessPdf": {
            "url": "https://doi.org/10.1162/tacl_a_00688",
            "status": "GOLD",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.00996, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-02",
        "authors": [
            {
                "authorId": "2196148017",
                "name": "Zhivar Sourati"
            },
            {
                "authorId": "2125822063",
                "name": "Filip Ilievski"
            },
            {
                "authorId": "2253397088",
                "name": "Pia Sommerauer"
            }
        ],
        "abstract": "Abstract As a core cognitive skill that enables the transferability of information across domains, analogical reasoning has been extensively studied for both humans and computational models. However, while cognitive theories of analogy often focus on narratives and study the distinction between surface, relational, and system similarities, existing work in natural language processing has a narrower focus as far as relational analogies between word pairs. This gap brings a natural question: can state-of-the-art large language models (LLMs) detect system analogies between narratives? To gain insight into this question and extend word-based relational analogies to relational system analogies, we devise a comprehensive computational framework that operationalizes dominant theories of analogy, using narrative elements to create surface and system mappings. Leveraging the interplay between these mappings, we create a binary task and benchmark for Analogical Reasoning on Narratives (ARN), covering four categories of far (cross-domain)/near (within-domain) analogies and disanalogies. We show that while all LLMs can largely recognize near analogies, even the largest ones struggle with far analogies in a zero-shot setting, with GPT4.0 scoring below random. Guiding the models through solved examples and Chain-of-Thought reasoning enhances their analogical reasoning ability. Yet, since even in the few-shot setting, the best model only performs halfway between random and humans, ARN opens exciting directions for computational analogical reasoners."
    },
    {
        "paperId": "fcf2a5934595a22190596b6c0c5d3e802661713c",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Split and Merge: Aligning Position Biases in LLM-based Evaluators",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.01432, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-09-29",
        "authors": [
            {
                "authorId": "2118207559",
                "name": "Zongjie Li"
            },
            {
                "authorId": "2135764153",
                "name": "Chaozheng Wang"
            },
            {
                "authorId": "1384480816",
                "name": "Pingchuan Ma"
            },
            {
                "authorId": "2253859864",
                "name": "Daoyuan Wu"
            },
            {
                "authorId": "2239108164",
                "name": "Tianxiang Li"
            },
            {
                "authorId": "2243949366",
                "name": "Shuai Wang"
            },
            {
                "authorId": "2337550",
                "name": "Cuiyun Gao"
            },
            {
                "authorId": "2281790097",
                "name": "Yang Liu"
            }
        ],
        "abstract": "Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, taking into account both length and semantics, and merges them back into a single prompt for evaluation by LLMs. Extensive experiments with six LLMs on 11,520 answer pairs demonstrate that PORTIA markedly enhances the consistency rates for all models and forms of comparison tested, achieving an average relative improvement of 47.46%. It also enables PORTIA-enhanced GPT-3.5 to achieve agreement rates with humans comparable to GPT-4 and elevates GPT-4\u2019s consistency rate up to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced GPT-3.5 model can even surpass standalone GPT-4 in terms of alignment with human evaluators, highlighting PORTIA\u2019s ability to correct position bias, improve LLM consistency, and boost performance while keeping cost efficiency."
    },
    {
        "paperId": "16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.01728",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.01728, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-03",
        "authors": [
            {
                "authorId": "2254096428",
                "name": "Ming Jin"
            },
            {
                "authorId": "2255363760",
                "name": "Shiyu Wang"
            },
            {
                "authorId": "2253908414",
                "name": "Lintao Ma"
            },
            {
                "authorId": "2237992280",
                "name": "Zhixuan Chu"
            },
            {
                "authorId": "2253786576",
                "name": "James Y. Zhang"
            },
            {
                "authorId": "2119204984",
                "name": "X. Shi"
            },
            {
                "authorId": "2254173316",
                "name": "Pin-Yu Chen"
            },
            {
                "authorId": "2253824408",
                "name": "Yuxuan Liang"
            },
            {
                "authorId": "2256011160",
                "name": "Yuan-Fang Li"
            },
            {
                "authorId": "2254047333",
                "name": "Shirui Pan"
            },
            {
                "authorId": "2253561592",
                "name": "Qingsong Wen"
            }
        ],
        "abstract": "Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios."
    },
    {
        "paperId": "740c783ac07039cf30b6d8a8f95e775b3297c79e",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Language Models Represent Space and Time",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.02207",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.02207, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-03",
        "authors": [
            {
                "authorId": "2056771333",
                "name": "Wes Gurnee"
            },
            {
                "authorId": "2253461466",
                "name": "Max Tegmark"
            }
        ],
        "abstract": "The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual\"space neurons\"and\"time neurons\"that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model."
    },
    {
        "paperId": "8946891e94831adc8cddb0d32311cce2445c96d2",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.02255, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-03",
        "authors": [
            {
                "authorId": "2887562",
                "name": "Pan Lu"
            },
            {
                "authorId": "103404553",
                "name": "Hritik Bansal"
            },
            {
                "authorId": "2143749775",
                "name": "Tony Xia"
            },
            {
                "authorId": "2144174497",
                "name": "Jiacheng Liu"
            },
            {
                "authorId": "2109738542",
                "name": "Chun-yue Li"
            },
            {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
            },
            {
                "authorId": "47413820",
                "name": "Hao Cheng"
            },
            {
                "authorId": "2256646491",
                "name": "Kai-Wei Chang"
            },
            {
                "authorId": "2253458981",
                "name": "Michel Galley"
            },
            {
                "authorId": "2256227183",
                "name": "Jianfeng Gao"
            }
        ],
        "abstract": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/."
    },
    {
        "paperId": "aec95e6330033e0ec39fb5a069d647288c03b945",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Assessing Large Language Models on Climate Information",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.02932",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.02932, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-04",
        "authors": [
            {
                "authorId": "2362210",
                "name": "Jannis Bulian"
            },
            {
                "authorId": "2260351938",
                "name": "Mike S. Sch\u00e4fer"
            },
            {
                "authorId": "1820796225",
                "name": "Afra Amini"
            },
            {
                "authorId": "2253698481",
                "name": "Heidi Lam"
            },
            {
                "authorId": "2754495",
                "name": "Massimiliano Ciaramita"
            },
            {
                "authorId": "2253688129",
                "name": "Ben Gaiarin"
            },
            {
                "authorId": "2143311006",
                "name": "Michelle Chen Huebscher"
            },
            {
                "authorId": "2253681102",
                "name": "Christian Buck"
            },
            {
                "authorId": "2341293562",
                "name": "Niels Mede"
            },
            {
                "authorId": "3073566",
                "name": "Markus Leippold"
            },
            {
                "authorId": "2253686456",
                "name": "Nadine Strauss"
            }
        ],
        "abstract": "As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in critically relevant domains. We present a comprehensive evaluation framework, grounded in science communication research, to assess LLM responses to questions about climate change. Our framework emphasizes both presentational and epistemological adequacy, offering a fine-grained analysis of LLM generations spanning 8 dimensions and 30 issues. Our evaluation task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel protocol for scalable oversight that relies on AI Assistance and raters with relevant education. We evaluate several recent LLMs on a set of diverse climate questions. Our results point to a significant gap between surface and epistemological qualities of LLMs in the realm of climate communication."
    },
    {
        "paperId": "6a0f1a8a03baba3e54a1a2ef348a1b0c2b8dff4b",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "B-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.03173",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.03173, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-04",
        "authors": [
            {
                "authorId": "2143448562",
                "name": "Zishun Yu"
            },
            {
                "authorId": "2256318649",
                "name": "Yunzhe Tao"
            },
            {
                "authorId": "2254280307",
                "name": "Liyu Chen"
            },
            {
                "authorId": "2088150138",
                "name": "Tao Sun"
            },
            {
                "authorId": "2255392986",
                "name": "Hongxia Yang"
            }
        ],
        "abstract": "Program synthesis aims to create accurate, executable programs from problem specifications, specifically from natural language descriptions in our context. Recent studies have leveraged the power of reinforcement learning (RL) in conjunction with large language models (LLMs), significantly enhancing code generation capabilities. The application of RL focuses on directly optimizing for functional correctness, offering an advantage over conventional supervised methods. Despite policy-based RL methods dominating the literature on RL for program synthesis, the nature of program synthesis tasks hints at a natural alignment with value-based methods. This stems from the rich collection of off-policy programs, including those developed by human programmers and also historical samples, coupled with the straightforward verification of generated programs through automated unit testing, meaning rewards are easy to obtain. Diverging from the dominant use of policy-based algorithms, our work explores the feasibility of value-based approaches, leading to the development of our $\\mathcal{B}$-Coder (pronounced Bellman coder). Yet, training value-based methods presents challenges due to the enormous search space inherent to program synthesis. To this end, we introduce an initialization protocol for RL agents utilizing pre-trained LMs and a conservative Bellman operator to reduce training complexities. Moreover, we demonstrate how to leverage the learned value functions as a dual strategy to post-process generated programs. Our empirical evaluations demonstrated $\\mathcal{B}$-Coder's capability in achieving state-of-the-art performance when compared to policy-based methods. Remarkably, this achievement is reached with minimal reward engineering effort, highlighting the effectiveness of value-based RL, independent of reward designs."
    },
    {
        "paperId": "d558f604981ed9911532baac17425b294799b528",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Demystifying Embedding Spaces using Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.04475",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.04475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-06",
        "authors": [
            {
                "authorId": "29978064",
                "name": "Guy Tennenholtz"
            },
            {
                "authorId": "1819830",
                "name": "Yinlam Chow"
            },
            {
                "authorId": "2257436216",
                "name": "Chih-Wei Hsu"
            },
            {
                "authorId": "2257018585",
                "name": "Jihwan Jeong"
            },
            {
                "authorId": "38274824",
                "name": "Lior Shani"
            },
            {
                "authorId": "13981257",
                "name": "Azamat Tulepbergenov"
            },
            {
                "authorId": "2256989489",
                "name": "Deepak Ramachandran"
            },
            {
                "authorId": "2538104",
                "name": "Martin Mladenov"
            },
            {
                "authorId": "2256989492",
                "name": "C. Boutilier"
            }
        ],
        "abstract": "Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing Large Language Models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decoding user preferences in recommender systems. Our work couples the immense information potential of embeddings with the interpretative power of LLMs."
    },
    {
        "paperId": "9283b8c7e6ad6ae86be059a26595de0d7a427a10",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Talk like a Graph: Encoding Graphs for Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.04560",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.04560, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-06",
        "authors": [
            {
                "authorId": "3422551",
                "name": "Bahare Fatemi"
            },
            {
                "authorId": "101461191",
                "name": "Jonathan J. Halcrow"
            },
            {
                "authorId": "2271808",
                "name": "Bryan Perozzi"
            }
        ],
        "abstract": "Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task."
    },
    {
        "paperId": "3f421c7defc20c21e97abad5fa92887a06ec5df8",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.04928",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.04928, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-07",
        "authors": [
            {
                "authorId": "2789148",
                "name": "Fajri Koto"
            },
            {
                "authorId": "2256987672",
                "name": "Nurul Aisyah"
            },
            {
                "authorId": "49404498",
                "name": "Haonan Li"
            },
            {
                "authorId": "2256987316",
                "name": "Timothy Baldwin"
            }
        ],
        "abstract": "Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels."
    },
    {
        "paperId": "99bfe503743c5ec8e16e50ab8438159cdb533a89",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.04988",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.04988, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-08",
        "authors": [
            {
                "authorId": "9460529",
                "name": "Vipula Rawte"
            },
            {
                "authorId": "2257001500",
                "name": "Swagata Chakraborty"
            },
            {
                "authorId": "2257001966",
                "name": "Agnibh Pathak"
            },
            {
                "authorId": "2189479826",
                "name": "Anubhav Sarkar"
            },
            {
                "authorId": "2103483687",
                "name": "S.M. Towhidul Islam Tonmoy"
            },
            {
                "authorId": "2229651428",
                "name": "Islam Tonmoy"
            },
            {
                "authorId": "40016108",
                "name": "Aman Chadha"
            },
            {
                "authorId": "2064342742",
                "name": "Amit P. Sheth"
            },
            {
                "authorId": "2258322706",
                "name": "Amitava Das"
            },
            {
                "authorId": "2076602721",
                "name": "Paris"
            },
            {
                "authorId": "2064327462",
                "name": "A. Sridhar"
            },
            {
                "authorId": "2238206449",
                "name": "Erik Visser"
            },
            {
                "authorId": "2257006156",
                "name": "Improved"
            },
            {
                "authorId": "2279935214",
                "name": "Jianlin Su"
            },
            {
                "authorId": "2257339854",
                "name": "Yu Lu"
            },
            {
                "authorId": "1382633722",
                "name": "Shengfeng Pan"
            },
            {
                "authorId": "2159557286",
                "name": "Ahmed Murtadha"
            },
            {
                "authorId": "2079396269",
                "name": "Bo Wen"
            },
            {
                "authorId": "2257345549",
                "name": "Yunfeng Liu"
            },
            {
                "authorId": "2257006862",
                "name": "Roformer"
            },
            {
                "authorId": "46199305",
                "name": "Rohan Taori"
            },
            {
                "authorId": "2708454",
                "name": "Ishaan Gulrajani"
            },
            {
                "authorId": "2256233130",
                "name": "Tianyi Zhang"
            },
            {
                "authorId": "2257007362",
                "name": "Yann Dubois"
            },
            {
                "authorId": "2250724754",
                "name": "Xuechen Li"
            },
            {
                "authorId": "1412355294",
                "name": "Carlos Guestrin"
            },
            {
                "authorId": "2256995425",
                "name": "Percy Liang"
            },
            {
                "authorId": "2117567142",
                "name": "Tatsunori Hashimoto"
            },
            {
                "authorId": "2250092519",
                "name": "Stanford"
            },
            {
                "authorId": "2113243762",
                "name": "Hugo Touvron"
            },
            {
                "authorId": "46183616",
                "name": "Thibaut Lavril"
            },
            {
                "authorId": "1410231361",
                "name": "Gautier Izacard"
            },
            {
                "authorId": "1490887583",
                "name": "Xavier Martinet"
            },
            {
                "authorId": "114952298",
                "name": "M. Lachaux"
            },
            {
                "authorId": "47733973",
                "name": "Timoth\u00e9e Lacroix"
            },
            {
                "authorId": "3361236",
                "name": "Baptiste Rozi\u00e8re"
            },
            {
                "authorId": "39589154",
                "name": "Naman Goyal"
            },
            {
                "authorId": "2072738644",
                "name": "Eric Hambro"
            },
            {
                "authorId": "2209986197",
                "name": "Faisal Azhar"
            },
            {
                "authorId": "2166043087",
                "name": "Aur'elien Rodriguez"
            },
            {
                "authorId": "2319608",
                "name": "Armand Joulin"
            },
            {
                "authorId": "2257007291",
                "name": "Thomas Wolf"
            },
            {
                "authorId": "1380459402",
                "name": "Lysandre Debut"
            },
            {
                "authorId": "51918868",
                "name": "Victor Sanh"
            },
            {
                "authorId": "40811585",
                "name": "Julien Chaumond"
            },
            {
                "authorId": "40899333",
                "name": "Clement Delangue"
            },
            {
                "authorId": "1382164294",
                "name": "Anthony Moi"
            },
            {
                "authorId": "1382164165",
                "name": "Pierric Cistac"
            },
            {
                "authorId": "1382164170",
                "name": "Tim Rault"
            },
            {
                "authorId": "2185329",
                "name": "R\u00e9mi Louf"
            },
            {
                "authorId": "2257005341",
                "name": "Morgan Funtow-icz"
            },
            {
                "authorId": "48776237",
                "name": "Joe Davison"
            },
            {
                "authorId": "88728159",
                "name": "Sam Shleifer"
            },
            {
                "authorId": "138609838",
                "name": "Patrick von Platen"
            },
            {
                "authorId": "2257128341",
                "name": "Clara Ma"
            },
            {
                "authorId": "2262249",
                "name": "Yacine Jernite"
            },
            {
                "authorId": "3008389",
                "name": "J. Plu"
            },
            {
                "authorId": "2257127518",
                "name": "Canwen Xu"
            },
            {
                "authorId": "1379806208",
                "name": "Teven Le Scao"
            },
            {
                "authorId": "103682620",
                "name": "Sylvain Gugger"
            },
            {
                "authorId": "2125818054",
                "name": "Mariama Drame"
            },
            {
                "authorId": "2113836945",
                "name": "Quentin Lhoest"
            },
            {
                "authorId": "2238121623",
                "name": "Susan Zhang"
            },
            {
                "authorId": "3849208",
                "name": "Stephen Roller"
            },
            {
                "authorId": "2347956",
                "name": "Mikel Artetxe"
            },
            {
                "authorId": "2108267192",
                "name": "Moya Chen"
            },
            {
                "authorId": "2257570528",
                "name": "Shuohui Chen"
            },
            {
                "authorId": "2257006163",
                "name": "Christopher De-wan"
            },
            {
                "authorId": "2138579860",
                "name": "Mona T. Diab"
            },
            {
                "authorId": "2257006892",
                "name": "Xi Xian Li"
            },
            {
                "authorId": "2257007395",
                "name": "Todor Victoria Lin"
            },
            {
                "authorId": "40511414",
                "name": "Myle Ott"
            },
            {
                "authorId": "35752280",
                "name": "Kurt Shuster"
            },
            {
                "authorId": "2257006894",
                "name": "Punit Daniel Simig"
            },
            {
                "authorId": "2257006866",
                "name": "Singh Koura"
            },
            {
                "authorId": "2257007723",
                "name": "Anjali Sridhar"
            },
            {
                "authorId": "2238056517",
                "name": "Tianlu Wang"
            },
            {
                "authorId": "2257007614",
                "name": "Luke Zettlemoyer. 2022"
            },
            {
                "authorId": "2052152920",
                "name": "Daniel M. Ziegler"
            },
            {
                "authorId": "1387983862",
                "name": "Nisan Stiennon"
            },
            {
                "authorId": "2257137166",
                "name": "Jeffrey Wu"
            },
            {
                "authorId": "2257135738",
                "name": "Tom B. Brown"
            },
            {
                "authorId": "38909097",
                "name": "Alec Radford"
            },
            {
                "authorId": "2698777",
                "name": "Dario Amodei"
            },
            {
                "authorId": "2257006890",
                "name": "Paul F. Chris-tiano"
            }
        ],
        "abstract": "The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations."
    },
    {
        "paperId": "f67b030ae52797622413a433fcdae0367aef2a4c",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Guideline Learning for In-context Information Extraction",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.05066",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05066, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-08",
        "authors": [
            {
                "authorId": "2257003180",
                "name": "Chaoxu Pang"
            },
            {
                "authorId": "10034341",
                "name": "Yixuan Cao"
            },
            {
                "authorId": "2257004196",
                "name": "Qiang Ding"
            },
            {
                "authorId": "2256989703",
                "name": "Ping Luo"
            }
        ],
        "abstract": "Large language models (LLMs) can perform a new task by merely conditioning on task instructions and a few input-output examples, without optimizing any parameters. This is called In-Context Learning (ICL). In-context Information Extraction (IE) has recently garnered attention in the research community. However, the performance of In-context IE generally lags behind the state-of-the-art supervised expert models. We highlight a key reason for this shortfall: underspecified task description. The limited-length context struggles to thoroughly express the intricate IE task instructions and various edge cases, leading to misalignment in task comprehension with humans. In this paper, we propose a Guideline Learning (GL) framework for In-context IE which reflectively learns and follows guidelines. During the learning phrase, GL automatically synthesizes a set of guidelines based on a few error cases, and during inference, GL retrieves helpful guidelines for better ICL. Moreover, we propose a self-consistency-based active learning method to enhance the efficiency of GL. Experiments on event extraction and relation extraction show that GL can significantly improve the performance of in-context IE."
    },
    {
        "paperId": "84f9bc5f89dac53662fb467b6af8ff26415ca3e7",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "InstructDET: Diversifying Referring Object Detection with Generalized Instructions",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.05136",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05136, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-08",
        "authors": [
            {
                "authorId": "2131077260",
                "name": "Ronghao Dang"
            },
            {
                "authorId": "2256994952",
                "name": "Jiangyan Feng"
            },
            {
                "authorId": "2257340585",
                "name": "Haodong Zhang"
            },
            {
                "authorId": "102482926",
                "name": "Chongjian Ge"
            },
            {
                "authorId": "2257162229",
                "name": "Lin Song"
            },
            {
                "authorId": "7668408",
                "name": "Lijun Gong"
            },
            {
                "authorId": "2920326",
                "name": "Chengju Liu"
            },
            {
                "authorId": "145844275",
                "name": "Qi Chen"
            },
            {
                "authorId": "2269956065",
                "name": "Feng Zhu"
            },
            {
                "authorId": "2257431533",
                "name": "Rui Zhao"
            },
            {
                "authorId": "2256984147",
                "name": "Yibing Song"
            }
        ],
        "abstract": "We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage are greatly diversified to encompass common user intentions related to object detection. For one image, we produce tremendous instructions that refer to every single object and different combinations of multiple objects. Each instruction and its corresponding object bounding boxes (bbxs) constitute one training data pair. In order to encompass common detection expressions, we involve emerging vision-language model (VLM) and large language model (LLM) to generate instructions guided by text prompts and object bbxs, as the generalizations of foundation models are effective to produce human-like expressions (e.g., describing object property, category, and relationship). We name our constructed dataset as InDET. It contains images, bbxs and generalized instructions that are from foundation models. Our InDET is developed from existing REC datasets and object detection datasets, with the expanding potential that any image with object bbxs can be incorporated through using our InstructDET method. By using our InDET dataset, we show that a conventional ROD model surpasses existing methods on standard REC datasets and our InDET test set. Our data-centric method InstructDET, with automatic data expansion by leveraging foundation models, directs a promising field that ROD can be greatly diversified to execute common object detection instructions."
    },
    {
        "paperId": "e47d08f5bb01acb56ab998e987d44d9d85dee1ba",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.05140",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05140, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-08",
        "authors": [
            {
                "authorId": "2229053371",
                "name": "Yushan Qian"
            },
            {
                "authorId": "1806419",
                "name": "Weinan Zhang"
            },
            {
                "authorId": "2140034831",
                "name": "Ting Liu"
            }
        ],
        "abstract": "Empathetic dialogue is an indispensable part of building harmonious social relationships and contributes to the development of a helpful AI. Previous approaches are mainly based on fine small-scale language models. With the advent of ChatGPT, the application effect of large language models (LLMs) in this field has attracted great attention. This work empirically investigates the performance of LLMs in generating empathetic responses and proposes three improvement methods of semantically similar in-context learning, two-stage interactive generation, and combination with the knowledge base. Extensive experiments show that LLMs can significantly benefit from our proposed methods and is able to achieve state-of-the-art performance in both automatic and human evaluations. Additionally, we explore the possibility of GPT-4 simulating human evaluators."
    },
    {
        "paperId": "faab24bc6cd4a4dea6e82420d145f08445c05fc7",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.05175",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-08",
        "authors": [
            {
                "authorId": "2254142682",
                "name": "Lu Yin"
            },
            {
                "authorId": "2325206905",
                "name": "You Wu"
            },
            {
                "authorId": "2109338656",
                "name": "Zhenyu (Allen) Zhang"
            },
            {
                "authorId": "2256992922",
                "name": "Cheng-Yu Hsieh"
            },
            {
                "authorId": "2257105674",
                "name": "Yaqing Wang"
            },
            {
                "authorId": "2257230381",
                "name": "Yiling Jia"
            },
            {
                "authorId": "1691997",
                "name": "Mykola Pechenizkiy"
            },
            {
                "authorId": "2260290217",
                "name": "Yi Liang"
            },
            {
                "authorId": "2254949434",
                "name": "Zhangyang Wang"
            },
            {
                "authorId": "2255081092",
                "name": "Shiwei Liu"
            }
        ],
        "abstract": "Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of non-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, OWL exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively, while delivering 2.6x end-to-end inference speed-up in the DeepSparse inference engine. Codes are available at https://github.com/luuyin/OWL."
    },
    {
        "paperId": "e6776f5f293c18f4b2322b1479f083cb24d33343",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.05344",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05344, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-09",
        "authors": [
            {
                "authorId": "2257127099",
                "name": "Yi Dong"
            },
            {
                "authorId": "2257129906",
                "name": "Zhilin Wang"
            },
            {
                "authorId": "1602996186",
                "name": "Makesh Narsimhan Sreedhar"
            },
            {
                "authorId": "2257111891",
                "name": "Xianchao Wu"
            },
            {
                "authorId": "2787022",
                "name": "Oleksii Kuchaiev"
            }
        ],
        "abstract": "Model alignment with human preferences is an essential step in making Large Language Models (LLMs) helpful and consistent with human values. It typically consists of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) stages. However, RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time. Moreover, reward models in RLHF stage commonly rely on single-dimensional feedback as opposed to explicit, multifaceted signals that indicate attributes such as helpfulness, humor, and toxicity. To address these limitations, we propose SteerLM, a supervised fine-tuning method that empowers end-users to control responses during inference. SteerLM conditions responses to conform to an explicitly defined multi-dimensional set of attributes, thereby empowering a steerable AI capable of generating helpful and high-quality responses while maintaining customizability. Experiments show that SteerLM trained on open source datasets generates responses that are preferred by human and automatic evaluators to many state-of-the-art baselines trained with RLHF while being much easier to train. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B"
    },
    {
        "paperId": "a32a999377d3eb14ac7652877f002f5c548d3134",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Establishing Trustworthiness: Rethinking Tasks and Model Evaluation",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.05442",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05442, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-09",
        "authors": [
            {
                "authorId": "46249177",
                "name": "Robert Litschko"
            },
            {
                "authorId": "1416353805",
                "name": "Max M\u00fcller-Eberstein"
            },
            {
                "authorId": "3449407",
                "name": "Rob van der Goot"
            },
            {
                "authorId": "2256991442",
                "name": "Leon Weber"
            },
            {
                "authorId": "2256991041",
                "name": "Barbara Plank"
            }
        ],
        "abstract": "Language understanding is a multi-faceted cognitive capability, which the Natural Language Processing (NLP) community has striven to model computationally for decades. Traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols. With the advent of large language models (LLMs) the community has witnessed a dramatic shift towards general purpose, task-agnostic approaches powered by generative models. As a consequence, the traditional compartmentalized notion of language tasks is breaking down, followed by an increasing challenge for evaluation and analysis. At the same time, LLMs are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems. Therefore, we argue that it is time to rethink what constitutes tasks and model evaluation in NLP, and pursue a more holistic view on language, placing trustworthiness at the center. Towards this goal, we review existing compartmentalized approaches for understanding the origins of a model's functional capacity, and provide recommendations for more multi-faceted evaluation protocols."
    },
    {
        "paperId": "26807ae7f8a1c9e3d5017c64c63eeab1807adcee",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models for Learning Conceptual Spaces",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.05481",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05481, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-09",
        "authors": [
            {
                "authorId": "1843915",
                "name": "Usashi Chatterjee"
            },
            {
                "authorId": "2256988730",
                "name": "Amit Gajbhiye"
            },
            {
                "authorId": "2265382",
                "name": "S. Schockaert"
            }
        ],
        "abstract": "The theory of Conceptual Spaces is an influential cognitive-linguistic framework for representing the meaning of concepts. Conceptual spaces are constructed from a set of quality dimensions, which essentially correspond to primitive perceptual features (e.g. hue or size). These quality dimensions are usually learned from human judgements, which means that applications of conceptual spaces tend to be limited to narrow domains (e.g. modelling colour or taste). Encouraged by recent findings about the ability of Large Language Models (LLMs) to learn perceptually grounded representations, we explore the potential of such models for learning conceptual spaces. Our experiments show that LLMs can indeed be used for learning meaningful representations to some extent. However, we also find that fine-tuned models of the BERT family are able to match or even outperform the largest GPT-3 model, despite being 2 to 3 orders of magnitude smaller."
    },
    {
        "paperId": "add28feeb5c1348f987d990c07793f8781c11419",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Regulation and NLP (RegNLP): Taming Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.05553",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05553, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-09",
        "authors": [
            {
                "authorId": "2237987307",
                "name": "Catalina Goanta"
            },
            {
                "authorId": "3238627",
                "name": "Nikolaos Aletras"
            },
            {
                "authorId": "10783142",
                "name": "Ilias Chalkidis"
            },
            {
                "authorId": "2256997960",
                "name": "S. Ranchordas"
            },
            {
                "authorId": "3266578",
                "name": "Gerasimos Spanakis"
            }
        ],
        "abstract": "The scientific innovation in Natural Language Processing (NLP) and more broadly in artificial intelligence (AI) is at its fastest pace to date. As large language models (LLMs) unleash a new era of automation, important debates emerge regarding the benefits and risks of their development, deployment and use. Currently, these debates have been dominated by often polarized narratives mainly led by the AI Safety and AI Ethics movements. This polarization, often amplified by social media, is swaying political agendas on AI regulation and governance and posing issues of regulatory capture. Capture occurs when the regulator advances the interests of the industry it is supposed to regulate, or of special interest groups rather than pursuing the general public interest. Meanwhile in NLP research, attention has been increasingly paid to the discussion of regulating risks and harms. This often happens without systematic methodologies or sufficient rooting in the disciplines that inspire an extended scope of NLP research, jeopardizing the scientific integrity of these endeavors. Regulation studies are a rich source of knowledge on how to systematically deal with risk and uncertainty, as well as with scientific evidence, to evaluate and compare regulatory options. This resource has largely remained untapped so far. In this paper, we argue how NLP research on these topics can benefit from proximity to regulatory studies and adjacent fields. We do so by discussing basic tenets of regulation, and risk and uncertainty, and by highlighting the shortcomings of current NLP discussions dealing with risk assessment. Finally, we advocate for the development of a new multidisciplinary research space on regulation and NLP (RegNLP), focused on connecting scientific knowledge to regulatory processes based on systematic methodologies."
    },
    {
        "paperId": "ea47e244807df8aa8b61c64b411d7d121b66679f",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.05634",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05634, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-09",
        "authors": [
            {
                "authorId": "2257087342",
                "name": "Xinze Li"
            },
            {
                "authorId": "2256998384",
                "name": "Yixin Cao2"
            },
            {
                "authorId": "2256983134",
                "name": "Liangming Pan"
            },
            {
                "authorId": "2143557418",
                "name": "Yubo Ma"
            },
            {
                "authorId": "1735962",
                "name": "Aixin Sun"
            }
        ],
        "abstract": "Although achieving great success, Large Language Models (LLMs) usually suffer from unreliable hallucinations. Although language attribution can be a potential solution, there are no suitable benchmarks and evaluation metrics to attribute LLMs to structured knowledge. In this paper, we define a new task of Knowledge-aware Language Model Attribution (KaLMA) that improves upon three core concerns with conventional attributed LMs. First, we extend attribution source from unstructured texts to Knowledge Graph (KG), whose rich structures benefit both the attribution performance and working scenarios. Second, we propose a new ``Conscious Incompetence\"setting considering the incomplete knowledge repository, where the model identifies the need for supporting knowledge beyond the provided KG. Third, we propose a comprehensive automatic evaluation metric encompassing text quality, citation quality, and text citation alignment. To implement the above innovations, we build a dataset in biography domain BioKaLMA via evolutionary question generation strategy, to control the question complexity and necessary knowledge to the answer. For evaluation, we develop a baseline solution and demonstrate the room for improvement in LLMs' citation generation, emphasizing the importance of incorporating the\"Conscious Incompetence\"setting, and the critical role of retrieval accuracy."
    },
    {
        "paperId": "ddb815ed9e291b2cb0678bf0754895f253732aa9",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "The Program Testing Ability of Large Language Models for Code",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.05727",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-09",
        "authors": [
            {
                "authorId": "2256980865",
                "name": "Weimin Xiong"
            },
            {
                "authorId": "2243697097",
                "name": "Yiwen Guo"
            },
            {
                "authorId": "2257354587",
                "name": "Hao Chen"
            }
        ],
        "abstract": "Recent development of large language models (LLMs) for code like CodeX and CodeT5+ shows promise in achieving code intelligence. Their ability of synthesizing program targeting a pre-defined algorithmic coding task has been intensively tested and verified on datasets including HumanEval and MBPP. Yet, evaluation of these LLMs from more perspectives (than just program synthesis) is also anticipated, considering their broad scope of applications. In this paper, we explore their ability of automatic test cases generation. We show intriguing observations and reveal how the quality of their generated test cases can be improved. Following recent work which uses generated test cases to enhance program synthesis, we further leverage our findings in improving the quality of the synthesized programs and show +11.77% and +4.22% higher code pass rates on HumanEval+ comparing with the GPT-3.5-turbo baseline and the recent state-of-the-art, respectively. Our code is publicly available at https://github.com/asdasxzxcq/TestCaseGen."
    },
    {
        "paperId": "28fbbf98bac1bb941162df553ca034d600cb59a6",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.05861",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05861, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-09",
        "authors": [
            {
                "authorId": "1677896557",
                "name": "Archiki Prasad"
            },
            {
                "authorId": "1405407255",
                "name": "Elias Stengel-Eskin"
            },
            {
                "authorId": "2253396640",
                "name": "Mohit Bansal"
            }
        ],
        "abstract": "An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to an LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually-grounded information to the input as a preemptive clarification should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. Similarly, in the VQA setting, changing the way questions are framed can make them easier for models to answer. To this end, we present Rephrase, Augment and Reason (RepARe), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question. We then use the LVLM's confidence over a generated answer as an unsupervised scoring function to select the rephrased question most likely to improve zero-shot performance. Focusing on three visual question answering tasks, we show that RepARe can result in a 3.85% (absolute) increase in zero-shot accuracy on VQAv2, 6.41%, and 7.94% points increase on A-OKVQA, and VizWiz respectively. Additionally, we find that using gold answers for oracle question candidate selection achieves a substantial gain in VQA accuracy by up to 14.41%. Through extensive analysis, we demonstrate that outputs from RepARe increase syntactic complexity, and effectively utilize vision-language interaction and the frozen LLM."
    },
    {
        "paperId": "93e58491830abe1eb965ab37ec64fa97263f6048",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "HyperAttention: Long-context Attention in Near-Linear Time",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.05869",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-09",
        "authors": [
            {
                "authorId": "2237714",
                "name": "Insu Han"
            },
            {
                "authorId": "20900712",
                "name": "Rajesh Jayaram"
            },
            {
                "authorId": "2257001335",
                "name": "Amin Karbasi"
            },
            {
                "authorId": "1728881",
                "name": "V. Mirrokni"
            },
            {
                "authorId": "2287826131",
                "name": "David P. Woodruff"
            },
            {
                "authorId": "2872461",
                "name": "A. Zandieh"
            }
        ],
        "abstract": "We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to identify large entries, HyperAttention outperforms existing methods, giving significant speed improvements compared to state-of-the-art solutions like FlashAttention. We validate the empirical performance of HyperAttention on a variety of different long-context length datasets. For example, HyperAttention makes the inference time of ChatGLM2 50\\% faster on 32k context length while perplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k, with causal masking, HyperAttention offers 5-fold speedup on a single attention layer."
    },
    {
        "paperId": "0786c88990235414611478099e43611542d973b0",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.06117",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.06117, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-09",
        "authors": [
            {
                "authorId": "2253804927",
                "name": "Huaixiu Steven Zheng"
            },
            {
                "authorId": "1817207",
                "name": "Swaroop Mishra"
            },
            {
                "authorId": "2238263119",
                "name": "Xinyun Chen"
            },
            {
                "authorId": "2257129838",
                "name": "Heng-Tze Cheng"
            },
            {
                "authorId": "2253469026",
                "name": "E. Chi"
            },
            {
                "authorId": "2256995069",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "2256313467",
                "name": "Denny Zhou"
            }
        ],
        "abstract": "We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7% and 11% respectively, TimeQA by 27%, and MuSiQue by 7%."
    },
    {
        "paperId": "b12541867632737e826b7b01c7fbe1c4222d8655",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.06201",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.06201, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-09",
        "authors": [
            {
                "authorId": "1527099159",
                "name": "Yucheng Li"
            },
            {
                "authorId": "2256984734",
                "name": "Bo Dong"
            },
            {
                "authorId": "2244126309",
                "name": "Chenghua Lin"
            },
            {
                "authorId": "2256992961",
                "name": "Frank Guerin"
            }
        ],
        "abstract": "Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50\\% reduction in context cost, resulting in a 36\\% reduction in inference memory usage and a 32\\% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance."
    },
    {
        "paperId": "893bc4c9d3be06319db7f31b6491110483db5fd1",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "A Semantic Invariant Robust Watermark for Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.06356",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.06356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-10",
        "authors": [
            {
                "authorId": "10017193",
                "name": "Aiwei Liu"
            },
            {
                "authorId": "2226138943",
                "name": "Leyi Pan"
            },
            {
                "authorId": "2109906988",
                "name": "Xuming Hu"
            },
            {
                "authorId": "2190693111",
                "name": "Shiao Meng"
            },
            {
                "authorId": "2114092431",
                "name": "Lijie Wen"
            }
        ],
        "abstract": "Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. In this work, we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens. Specifically, we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model. Subsequent analyses and experiments demonstrated the attack robustness of our method in semantically invariant settings: synonym substitution and text paraphrasing settings. Finally, we also show that our watermark possesses adequate security robustness. Our code and data are available at \\href{https://github.com/THU-BPM/Robust_Watermark}{https://github.com/THU-BPM/Robust\\_Watermark}. Additionally, our algorithm could also be accessed through MarkLLM \\citep{pan2024markllm} \\footnote{https://github.com/THU-BPM/MarkLLM}."
    },
    {
        "paperId": "495c36dba73fa97e2f742827c03c3d61a83b3a01",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.06498",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.06498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-10",
        "authors": [
            {
                "authorId": "2189419845",
                "name": "Shiping Yang"
            },
            {
                "authorId": "2068172988",
                "name": "Renliang Sun"
            },
            {
                "authorId": "9714242",
                "name": "Xiao-Yi Wan"
            }
        ],
        "abstract": "Large Language Models (LLMs) have shown their ability to collaborate effectively with humans in real-world scenarios. However, LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks. In this paper, we propose a self-check approach based on reverse validation to detect factual errors automatically in a zero-resource fashion. To facilitate future studies and assess different methods, we construct a hallucination detection benchmark named PHD, which is generated by ChatGPT and annotated by human annotators. Contrasting previous studies of zero-resource hallucination detection, our method and benchmark concentrate on passage-level detection instead of sentence-level. We empirically evaluate our method and existing zero-resource detection methods on two datasets. The experimental results demonstrate that the proposed method considerably outperforms the baselines while costing fewer tokens and less time. Furthermore, we manually analyze some hallucination cases that LLM failed to capture, revealing the shared limitation of zero-resource methods."
    },
    {
        "paperId": "1c882af20986251bab90318c7e1fae93e0c12cf6",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.06837",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.06837, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-10",
        "authors": [
            {
                "authorId": "49456763",
                "name": "E. Zelikman"
            },
            {
                "authorId": "2258072092",
                "name": "Wanjing Anya Ma"
            },
            {
                "authorId": "2257000238",
                "name": "Jasmine E. Tran"
            },
            {
                "authorId": "2329871182",
                "name": "Diyi Yang"
            },
            {
                "authorId": "1893965",
                "name": "J. Yeatman"
            },
            {
                "authorId": "2239093653",
                "name": "Nick Haber"
            }
        ],
        "abstract": "Developing an educational test can be expensive and time-consuming, as each item must be written by experts and then evaluated by collecting hundreds of student responses. Moreover, many tests require multiple distinct sets of questions administered throughout the school year to closely monitor students' progress, known as parallel tests. In this study, we focus on tests of silent sentence reading efficiency, used to assess students' reading ability over time. To generate high-quality parallel tests, we propose to fine-tune large language models (LLMs) to simulate how previous students would have responded to unseen items. With these simulated responses, we can estimate each item's difficulty and ambiguity. We first use GPT-4 to generate new test items following a list of expert-developed rules and then apply a fine-tuned LLM to filter the items based on criteria from psychological measurements. We also propose an optimal-transport-inspired technique for generating parallel tests and show the generated tests closely correspond to the original test's difficulty and reliability based on crowdworker responses. Our evaluation of a generated test with 234 students from grades 2 to 8 produces test scores highly correlated (r=0.93) to those of a standard test form written by human experts and evaluated across thousands of K-12 students."
    },
    {
        "paperId": "ae8aabebad0c3ecae165ec05c18a2072ed360d1e",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "NEWTON: Are Large Language Models Capable of Physical Reasoning?",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.07018",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.07018, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-11",
        "authors": [
            {
                "authorId": "2257315331",
                "name": "Yi Ru Wang"
            },
            {
                "authorId": "2052092142",
                "name": "Jiafei Duan"
            },
            {
                "authorId": "2257042516",
                "name": "Dieter Fox"
            },
            {
                "authorId": "1752197",
                "name": "S. Srinivasa"
            }
        ],
        "abstract": "Large Language Models (LLMs), through their contextualized representations, have been empirically proven to encapsulate syntactic, semantic, word sense, and common-sense knowledge. However, there has been limited exploration of their physical reasoning abilities, specifically concerning the crucial attributes for comprehending everyday objects. To address this gap, we introduce NEWTON, a repository and benchmark for evaluating the physics reasoning skills of LLMs. Further, to enable domain-specific adaptation of this benchmark, we present a pipeline to enable researchers to generate a variant of this benchmark that has been customized to the objects and attributes relevant for their application. The NEWTON repository comprises a collection of 2800 object-attribute pairs, providing the foundation for generating infinite-scale assessment templates. The NEWTON benchmark consists of 160K QA questions, curated using the NEWTON repository to investigate the physical reasoning capabilities of several mainstream language models across foundational, explicit, and implicit reasoning tasks. Through extensive empirical analysis, our results highlight the capabilities of LLMs for physical reasoning. We find that LLMs like GPT-4 demonstrate strong reasoning capabilities in scenario-based tasks but exhibit less consistency in object-attribute reasoning compared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates its potential for evaluating and enhancing language models, paving the way for their integration into physically grounded settings, such as robotic manipulation. Project site: https://newtonreasoning.github.io"
    },
    {
        "paperId": "0f6fe87afd1a3571f77c790893b03717e5d0422a",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.07289",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.07289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-11",
        "authors": [
            {
                "authorId": "2260621463",
                "name": "Liang Chen"
            },
            {
                "authorId": "145843537",
                "name": "Yang Deng"
            },
            {
                "authorId": "2257039155",
                "name": "Yatao Bian"
            },
            {
                "authorId": "2257182133",
                "name": "Zeyu Qin"
            },
            {
                "authorId": "2257376927",
                "name": "Bingzhe Wu"
            },
            {
                "authorId": "2257036129",
                "name": "Tat-Seng Chua"
            },
            {
                "authorId": "2237563835",
                "name": "Kam-Fai Wong"
            }
        ],
        "abstract": "Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives -- Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than small factual mistakes. Further, we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection. Our evaluation code and LLM-generated knowledge with human annotations will be released to facilitate future research."
    },
    {
        "paperId": "d5f4ecbb3fc2220eed7c62ea308e4f6cba2240b5",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Beyond Memorization: Violating Privacy Via Inference with Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.07298",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.07298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-11",
        "authors": [
            {
                "authorId": "2133303955",
                "name": "Robin Staab"
            },
            {
                "authorId": "2073893621",
                "name": "Mark Vero"
            },
            {
                "authorId": "2138580250",
                "name": "Mislav Balunovi'c"
            },
            {
                "authorId": "1736447",
                "name": "Martin T. Vechev"
            }
        ],
        "abstract": "Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\\%$ top-1 and $95\\%$ top-3 accuracy at a fraction of the cost ($100\\times$) and time ($240\\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for a wider privacy protection."
    },
    {
        "paperId": "6cc6d59984853e5ddcbd696c443b14244d305b50",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.07579",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.07579, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-11",
        "authors": [
            {
                "authorId": "89583148",
                "name": "Martin Pawelczyk"
            },
            {
                "authorId": "2273685865",
                "name": "Seth Neel"
            },
            {
                "authorId": "1892673",
                "name": "Himabindu Lakkaraju"
            }
        ],
        "abstract": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy."
    },
    {
        "paperId": "dd7a74a09fc29cadcd47fafc4f7812bb8d2d7208",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.07629",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.07629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-11",
        "authors": [
            {
                "authorId": "90729626",
                "name": "Hannah Rose Kirk"
            },
            {
                "authorId": "2242554313",
                "name": "Andrew M. Bean"
            },
            {
                "authorId": "2737827",
                "name": "Bertie Vidgen"
            },
            {
                "authorId": "2043232919",
                "name": "Paul R\u00f6ttger"
            },
            {
                "authorId": "1741886127",
                "name": "Scott A. Hale"
            }
        ],
        "abstract": "Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories.First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges."
    },
    {
        "paperId": "675c87c9fed17b6dc1d9734606e12c9d0c46c573",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.07713",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.07713, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-11",
        "authors": [
            {
                "authorId": "2256656241",
                "name": "Boxin Wang"
            },
            {
                "authorId": "2253664013",
                "name": "Wei Ping"
            },
            {
                "authorId": "20957879",
                "name": "Lawrence C. McAfee"
            },
            {
                "authorId": "2254989105",
                "name": "Peng Xu"
            },
            {
                "authorId": "2268670739",
                "name": "Bo Li"
            },
            {
                "authorId": "1911755",
                "name": "M. Shoeybi"
            },
            {
                "authorId": "2301680",
                "name": "Bryan Catanzaro"
            }
        ],
        "abstract": "Pretraining auto-regressive large language models~(LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval. Specifically, we continue to pretrain a 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. Notably, the obtained foundation model, Retro 48B, largely outperforms the counterpart GPT 43B trained on 1.2T tokens in terms of perplexity with only 2.58% additional GPU hours, demonstrating the significant scaling potential of the method. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on a wide range of zero-shot tasks. Specifically, the average improvement of InstructRetro is 7% over its GPT counterpart across 8 short-form QA and reading comprehension tasks, 10% over GPT across 4 challenging long-form QA tasks, and 16% over GPT across 3 summarization tasks. Surprisingly, we find that one can ablate the encoder from InstructRetro architecture and directly use its decoder backbone, while achieving comparable results. Our results highlight the promising direction to obtain a better GPT decoder through continued pretraining with retrieval before instruction tuning. Our code and checkpoints are publicly available at: https://huggingface.co/nvidia/retro-48b-instruct-4k."
    },
    {
        "paperId": "73b054b7b4ca4a1a3d85f7a79ec58b268499c0ab",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.08172",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.08172, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-12",
        "authors": [
            {
                "authorId": "2257434341",
                "name": "Zheyuan Zhang"
            },
            {
                "authorId": "2116034394",
                "name": "Jifan Yu"
            },
            {
                "authorId": "2133353675",
                "name": "Juanzi Li"
            },
            {
                "authorId": "2055765060",
                "name": "Lei Hou"
            }
        ],
        "abstract": "Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence. Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains. However, cognitive research on the overall knowledge structure of LLMs is still lacking. In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain insights of their cognitive capabilities. This research emphasizes the significance of investigating LLMs' knowledge and understanding the disparate cognitive patterns of LLMs. By shedding light on models' knowledge, researchers can advance development and utilization of LLMs in a more informed and effective manner."
    },
    {
        "paperId": "d8355d1bc02ca6b11125027a5d85d0b32fa30f66",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Impact of Co-occurrence on Factual Knowledge of Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.08256",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.08256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-12",
        "authors": [
            {
                "authorId": "75109648",
                "name": "Cheongwoong Kang"
            },
            {
                "authorId": "2257687954",
                "name": "Jaesik Choi"
            }
        ],
        "abstract": "Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further research in mitigation will help build reliable language models by preventing potential errors. The code is available at \\url{https://github.com/CheongWoong/impact_of_cooccurrence}."
    },
    {
        "paperId": "203a297db586ffb4cd858fe5f219a9a1571c87b2",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.08433",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.08433, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-12",
        "authors": [
            {
                "authorId": "2257344979",
                "name": "Carlos G'omez-Rodr'iguez"
            },
            {
                "authorId": "2257350230",
                "name": "Paul Williams"
            }
        ],
        "abstract": "We evaluate a range of recent LLMs on English creative writing, a challenging and complex task that requires imagination, coherence, and style. We use a difficult, open-ended scenario chosen to avoid training data reuse: an epic narration of a single combat between Ignatius J. Reilly, the protagonist of the Pulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl, a prehistoric flying reptile. We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style. Our results show that some state-of-the-art commercial LLMs match or slightly outperform our writers in most dimensions; whereas open-source LLMs lag behind. Humans retain an edge in creativity, while humor shows a binary divide between LLMs that can handle it comparably to humans and those that fail at it. We discuss the implications and limitations of our study and suggest directions for future research."
    },
    {
        "paperId": "a710efa9247207a72f06e0c9db302fd3ecab5fbb",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Towards Robust Multi-Modal Reasoning via Model Selection",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.08446",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.08446, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-12",
        "authors": [
            {
                "authorId": "2249738557",
                "name": "Xiangyan Liu"
            },
            {
                "authorId": "2257385819",
                "name": "Rongxue Li"
            },
            {
                "authorId": "2257397366",
                "name": "Wei Ji"
            },
            {
                "authorId": "2174718925",
                "name": "Tao Lin"
            }
        ],
        "abstract": "The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the\"brain\"of the agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning. To this end, we identify the key challenges therein and propose the $\\textit{M}^3$ framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: https://github.com/LINs-lab/M3."
    },
    {
        "paperId": "e8d513bc7554a83161f2fb26c8299b471581cdb6",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Can We Edit Multimodal Large Language Models?",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.08475",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.08475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-12",
        "authors": [
            {
                "authorId": "2258034882",
                "name": "Siyuan Cheng"
            },
            {
                "authorId": "2064522174",
                "name": "Bo Tian"
            },
            {
                "authorId": "2258682951",
                "name": "Qingbin Liu"
            },
            {
                "authorId": "48283576",
                "name": "Xi Chen"
            },
            {
                "authorId": "2257367497",
                "name": "Yongheng Wang"
            },
            {
                "authorId": "2144200945",
                "name": "Huajun Chen"
            },
            {
                "authorId": "2153010067",
                "name": "Ningyu Zhang"
            }
        ],
        "abstract": "In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit."
    },
    {
        "paperId": "e6f74f2746a9e8bc90701f2afcf3c47e5e98b2dd",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.09044",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.09044, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-13",
        "authors": [
            {
                "authorId": "2127128777",
                "name": "Sehyun Choi"
            },
            {
                "authorId": "2044202073",
                "name": "Tianqing Fang"
            },
            {
                "authorId": "2187830802",
                "name": "Zhaowei Wang"
            },
            {
                "authorId": "2241325169",
                "name": "Yangqiu Song"
            }
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the hallucination problem, poses a significant risk to their deployment. A common approach to address this issue is to retrieve relevant knowledge and fine-tune the LLM with the knowledge in its input. Unfortunately, this method incurs high training costs and may cause catastrophic forgetting for multi-tasking models. To overcome these limitations, we propose a knowledge-constrained decoding method called KCTS (Knowledge-Constrained Tree Search), which guides a frozen LM to generate text aligned with the reference knowledge at each decoding step using a knowledge classifier score and MCTS (Monte-Carlo Tree Search). To adapt the sequence-level knowledge classifier to token-level guidance, we also propose a novel token-level hallucination detection method called RIPA (Reward Inflection Point Approximation). Our empirical results on knowledge-grounded dialogue and abstractive summarization demonstrate the strength of KCTS as a plug-and-play, model-agnostic decoding method that can effectively reduce hallucinations in natural language generation."
    },
    {
        "paperId": "15a782b48fd26f2ce63ab1259e647c3656ce43c7",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Split-and-Denoise: Protect large language model inference with local differential privacy",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.09130",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.09130, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-13",
        "authors": [
            {
                "authorId": "2187874977",
                "name": "Peihua Mai"
            },
            {
                "authorId": "2258553513",
                "name": "Ran Yan"
            },
            {
                "authorId": "2258673756",
                "name": "Zhe Huang"
            },
            {
                "authorId": "2258701126",
                "name": "Youjia Yang"
            },
            {
                "authorId": "2258553161",
                "name": "Yan Pang"
            }
        ],
        "abstract": "Large Language Models (LLMs) excel in natural language understanding by capturing hidden semantics in vector space. This process enriches the value of text embeddings for various downstream tasks, thereby fostering the Embedding-as-a-Service (EaaS) business model. However, the risk of privacy leakage due to direct text transmission to servers remains a critical concern. To address this, we introduce Split-N-Denoise (SnD), an private inference framework that splits the model to execute the token embedding layer on the client side at minimal computational cost. This allows the client to introduce noise prior to transmitting the embeddings to the server, and subsequently receive and denoise the perturbed output embeddings for downstream tasks. Our approach is designed for the inference stage of LLMs and requires no modifications to the model parameters. Extensive experiments demonstrate SnD's effectiveness in optimizing the privacy-utility tradeoff across various LLM architectures and diverse downstream tasks. The results reveal an improvement in performance under the same privacy budget compared to the baselines by over 10\\% on average, offering clients a privacy-preserving solution for local privacy protection."
    },
    {
        "paperId": "ce157cea880c9ab64de64f11a531202f5348fa05",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "\"Kelly is a Warm Person, Joseph is a Role Model\": Gender Biases in LLM-Generated Reference Letters",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.09219",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.09219, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-13",
        "authors": [
            {
                "authorId": "2165227666",
                "name": "Yixin Wan"
            },
            {
                "authorId": "2258548444",
                "name": "George Pu"
            },
            {
                "authorId": "2261454711",
                "name": "Jiao Sun"
            },
            {
                "authorId": "31099365",
                "name": "Aparna Garimella"
            },
            {
                "authorId": "2257127887",
                "name": "Kai-Wei Chang"
            },
            {
                "authorId": "2256996328",
                "name": "Nanyun Peng"
            }
        ],
        "abstract": "Large Language Models (LLMs) have recently emerged as an effective tool to assist individuals in writing various types of content, including professional documents such as recommendation letters. Though bringing convenience, this application also introduces unprecedented fairness concerns. Model-generated reference letters might be directly used by users in professional scenarios. If underlying biases exist in these model-constructed letters, using them without scrutinization could lead to direct societal harms, such as sabotaging application success rates for female applicants. In light of this pressing issue, it is imminent and necessary to comprehensively study fairness issues and associated harms in this real-world use case. In this paper, we critically examine gender biases in LLM-generated reference letters. Drawing inspiration from social science findings, we design evaluation methods to manifest biases through 2 dimensions: (1) biases in language style and (2) biases in lexical content. We further investigate the extent of bias propagation by analyzing the hallucination bias of models, a term that we define to be bias exacerbation in model-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs- ChatGPT and Alpaca, we reveal significant gender biases in LLM-generated recommendation letters. Our findings not only warn against using LLMs for this application without scrutinization, but also illuminate the importance of thoroughly studying hidden biases and harms in LLM-generated professional documents."
    },
    {
        "paperId": "a80546c9847710af1ba8d5f8dca9386e7a520d0a",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.09241",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.09241, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-13",
        "authors": [
            {
                "authorId": "2164710806",
                "name": "Yiquan Wu"
            },
            {
                "authorId": "2258678617",
                "name": "Siying Zhou"
            },
            {
                "authorId": "2198035543",
                "name": "Yifei Liu"
            },
            {
                "authorId": "1776903",
                "name": "Weiming Lu"
            },
            {
                "authorId": "2238387483",
                "name": "Xiaozhong Liu"
            },
            {
                "authorId": "51146612",
                "name": "Yating Zhang"
            },
            {
                "authorId": "2060934",
                "name": "Changlong Sun"
            },
            {
                "authorId": "2258681273",
                "name": "Fei Wu"
            },
            {
                "authorId": "33870528",
                "name": "Kun Kuang"
            }
        ],
        "abstract": "Legal Judgment Prediction (LJP) has become an increasingly crucial task in Legal AI, i.e., predicting the judgment of the case in terms of case fact description. Precedents are the previous legal cases with similar facts, which are the basis for the judgment of the subsequent case in national legal systems. Thus, it is worthwhile to explore the utilization of precedents in the LJP. Recent advances in deep learning have enabled a variety of techniques to be used to solve the LJP task. These can be broken down into two categories: large language models (LLMs) and domain-specific models. LLMs are capable of interpreting and generating complex natural language, while domain models are efficient in learning task-specific information. In this paper, we propose the precedent-enhanced LJP framework (PLJP), a system that leverages the strength of both LLM and domain models in the context of precedents. Specifically, the domain models are designed to provide candidate labels and find the proper precedents efficiently, and the large models will make the final prediction with an in-context precedents comprehension. Experiments on the real-world dataset demonstrate the effectiveness of our PLJP. Moreover, our work shows a promising direction for LLM and domain-model collaboration that can be generalized to other vertical domains."
    },
    {
        "paperId": "bcb197654f39bb9312d8d0333646b71254d29239",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.09676, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-14",
        "authors": [
            {
                "authorId": "2258750843",
                "name": "Jiachen Li"
            },
            {
                "authorId": "3193409",
                "name": "Qiaozi Gao"
            },
            {
                "authorId": "2258716416",
                "name": "Michael Johnston"
            },
            {
                "authorId": "46757485",
                "name": "Xiaofeng Gao"
            },
            {
                "authorId": "2149253467",
                "name": "Xuehai He"
            },
            {
                "authorId": "1831108414",
                "name": "Suhaila Shakiah"
            },
            {
                "authorId": "2167861070",
                "name": "Hangjie Shi"
            },
            {
                "authorId": "3306272",
                "name": "R. Ghanadan"
            },
            {
                "authorId": "2258793748",
                "name": "William Yang Wang"
            }
        ],
        "abstract": "Prompt-based learning has been demonstrated as a compelling paradigm contributing to large language models' tremendous success (LLMs). Inspired by their success in language tasks, existing research has leveraged LLMs in embodied instruction following and task planning. In this work, we tackle the problem of training a robot to understand multimodal prompts, interleaving vision signals with text descriptions. This type of task poses a major challenge to robots' capability to understand the interconnection and complementarity between vision and language signals. In this work, we introduce an effective framework that learns a policy to perform robot manipulation with multimodal prompts from multi-task expert trajectories. Our methods consist of a two-stage training pipeline that performs inverse dynamics pretraining and multi-task finetuning. To facilitate multimodal understanding, we design our multimodal prompt encoder by augmenting a pretrained LM with a residual connection to the visual input and model the dependencies among action dimensions. Empirically, we evaluate the efficacy of our method on the VIMA-BENCH and establish a new state-of-the-art (10% improvement in success rate). Moreover, we demonstrate that our model exhibits remarkable in-context learning ability. Project page: \\url{https://midas-icml.github.io/}."
    },
    {
        "paperId": "6628f9ee35e36cdfdcac8a46cef4dba8d529a83b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Character-LLM: A Trainable Agent for Role-Playing",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.10158, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-16",
        "authors": [
            {
                "authorId": "95329799",
                "name": "Yunfan Shao"
            },
            {
                "authorId": "2107897400",
                "name": "Linyang Li"
            },
            {
                "authorId": "2087363104",
                "name": "Junqi Dai"
            },
            {
                "authorId": "1767521",
                "name": "Xipeng Qiu"
            }
        ],
        "abstract": "Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents \\textit{memorize} their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind."
    },
    {
        "paperId": "9c4ae24cb3d3230bd27e42d98124c22b1e0f9d48",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.10207, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-16",
        "authors": [
            {
                "authorId": "2179089110",
                "name": "Rujie Wu"
            },
            {
                "authorId": "2248671041",
                "name": "Xiaojian Ma"
            },
            {
                "authorId": "2117895397",
                "name": "Qing Li"
            },
            {
                "authorId": "2258856134",
                "name": "Wei Wang"
            },
            {
                "authorId": "152781023",
                "name": "Zhenliang Zhang"
            },
            {
                "authorId": "2254283518",
                "name": "Song-Chun Zhu"
            },
            {
                "authorId": "2241203061",
                "name": "Yizhou Wang"
            }
        ],
        "abstract": "We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2) real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We further investigate to which extent the recently introduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can solve our task, by directly probing VLMs, and combining VLMs and LLMs in an interactive reasoning scheme. We even conceived a neuro-symbolic reasoning approach that reconciles LLMs&VLMs with logical reasoning to emulate the human problem-solving process for Bongard Problems. However, none of these approaches manage to close the human-machine gap, as the best learner achieves 64% accuracy while human participants easily reach 91%. We hope Bongard-OpenWorld can help us better understand the limitations of current visual intelligence and facilitate future research on visual agents with stronger few-shot visual reasoning capabilities."
    },
    {
        "paperId": "d2b9625149de2527e412ecfa07c6e5e6f9b68a0b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "\"Mistakes Help Us Grow\": Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.10637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-16",
        "authors": [
            {
                "authorId": "2066900445",
                "name": "Kunal Handa"
            },
            {
                "authorId": "2258456662",
                "name": "Margarett Clapper"
            },
            {
                "authorId": "2258964308",
                "name": "Jessica Boyle"
            },
            {
                "authorId": "2260440783",
                "name": "Rose Wang"
            },
            {
                "authorId": "2258423150",
                "name": "Diyi Yang"
            },
            {
                "authorId": "2258957621",
                "name": "David S Yeager"
            },
            {
                "authorId": "2258954602",
                "name": "Dorottya Demszky"
            }
        ],
        "abstract": "Teachers' growth mindset supportive language (GMSL)--rhetoric emphasizing that one's skills can be improved over time--has been shown to significantly reduce disparities in academic achievement and enhance students' learning outcomes. Although teachers espouse growth mindset principles, most find it difficult to adopt GMSL in their practice due the lack of effective coaching in this area. We explore whether large language models (LLMs) can provide automated, personalized coaching to support teachers' use of GMSL. We establish an effective coaching tool to reframe unsupportive utterances to GMSL by developing (i) a parallel dataset containing GMSL-trained teacher reframings of unsupportive statements with an accompanying annotation guide, (ii) a GMSL prompt framework to revise teachers' unsupportive language, and (iii) an evaluation framework grounded in psychological theory for evaluating GMSL with the help of students and teachers. We conduct a large-scale evaluation involving 174 teachers and 1,006 students, finding that both teachers and students perceive GMSL-trained teacher and model reframings as more effective in fostering a growth mindset and promoting challenge-seeking behavior, among other benefits. We also find that model-generated reframings outperform those from the GMSL-trained teachers. These results show promise for harnessing LLMs to provide automated GMSL feedback for teachers and, more broadly, LLMs' potentiality for supporting students' learning in the classroom. Our findings also demonstrate the benefit of large-scale human evaluations when applying LLMs in educational domains."
    },
    {
        "paperId": "ff4b455af2ef2c3f7372c47209a617ddafd4e203",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.10648, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-16",
        "authors": [
            {
                "authorId": "2260440783",
                "name": "Rose Wang"
            },
            {
                "authorId": "2259438198",
                "name": "Qingyang Zhang"
            },
            {
                "authorId": "2258951860",
                "name": "Carly D. Robinson"
            },
            {
                "authorId": "2258961390",
                "name": "Susanna Loeb"
            },
            {
                "authorId": "2258954602",
                "name": "Dorottya Demszky"
            }
        ],
        "abstract": "Scaling high-quality tutoring remains a major challenge in education. Due to growing demand, many platforms employ novice tutors who, unlike experienced educators, struggle to address student mistakes and thus fail to seize prime learning opportunities. Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes. We contribute Bridge, a method that uses cognitive task analysis to translate an expert\u2019s latent thought process into a decision-making model for remediation. This involves an expert identifying (A) the student\u2019s error, (B) a remediation strategy, and (C) their intention before generating a response. We construct a dataset of 700 real tutoring conversations, annotated by experts with their decisions. We evaluate state-of-the-art LLMs on our dataset and find that the expert\u2019s decision-making model is critical for LLMs to close the gap: responses from GPT4 with expert decisions (e.g., \u201csimplify the problem\u201d) are +76% more preferred than without. Additionally, context-sensitive decisions are critical to closing pedagogical gaps: random decisions decrease GPT4\u2019s response quality by -97% than expert decisions. Our work shows the potential of embedding expert thought processes in LLM generations to enhance their capability to bridge novice-expert knowledge gaps. Our dataset and code can be found at: https://github.com/rosewang2008/bridge."
    },
    {
        "paperId": "8fd11c6f3eb1d0aeb915369f3c4f0b1bb24cab0c",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Large Language Model Unlearning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.10683, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-14",
        "authors": [
            {
                "authorId": "2257134957",
                "name": "Yuanshun Yao"
            },
            {
                "authorId": "2259623067",
                "name": "Xiaojun Xu"
            },
            {
                "authorId": "2279671647",
                "name": "Yang Liu"
            }
        ],
        "abstract": "We study how to perform unlearning, i.e. forgetting undesirable misbehaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) reducing hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited resources, and therefore the priority is to stop generating undesirable outputs rather than to try to generate desirable outputs, unlearning is particularly appealing. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time."
    },
    {
        "paperId": "e17c58d7a48b6b811df023484161a3b9c03e0d6b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.13.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.10701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-16",
        "authors": [
            {
                "authorId": "2109032235",
                "name": "Huao Li"
            },
            {
                "authorId": "2258959405",
                "name": "Yu Quan Chong"
            },
            {
                "authorId": "8083127",
                "name": "Simon Stepputtis"
            },
            {
                "authorId": "39860257",
                "name": "Joseph Campbell"
            },
            {
                "authorId": "143626678",
                "name": "Dana Hughes"
            },
            {
                "authorId": "2117280035",
                "name": "Michael Lewis"
            },
            {
                "authorId": "2239104696",
                "name": "Katia P. Sycara"
            }
        ],
        "abstract": "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents."
    },
    {
        "paperId": "a44dd81e42c690f6b0fe86f6142722491ae36278",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.10707, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-16",
        "authors": [
            {
                "authorId": "1396461326",
                "name": "Anirudh Som"
            },
            {
                "authorId": "39707211",
                "name": "Karan Sikka"
            },
            {
                "authorId": "2258965042",
                "name": "Helen Gent"
            },
            {
                "authorId": "2258962662",
                "name": "Ajay Divakaran"
            },
            {
                "authorId": "1794716",
                "name": "A. Kathol"
            },
            {
                "authorId": "2767150",
                "name": "D. Vergyri"
            }
        ],
        "abstract": "Paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. Supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. They also often retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. In this paper we aim to assist practitioners in developing usable paraphrasers by exploring In-Context Learning (ICL) with large language models (LLMs), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. Our study focuses on key factors such as - number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. We perform principled evaluation on three datasets, including our proposed Context-Aware Polite Paraphrase (CAPP) dataset, comprising of dialogue-style rude utterances, polite paraphrases, and additional dialogue context. We evaluate our approach using four closed source and one open source LLM. Our results reveal that ICL is comparable to supervised methods in generation quality, while being qualitatively better by 25% on human evaluation and attaining lower toxicity by 76%. Also, ICL-based paraphrasers only show a slight reduction in performance even with just 10% training data."
    },
    {
        "paperId": "672ab1c469f86309fafc0f989890cb468d25b9f0",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Large Language Models can Contrastively Refine their Generation for Better Sentence Representation Learning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.10962, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-17",
        "authors": [
            {
                "authorId": "2259549716",
                "name": "Huiming Wang"
            },
            {
                "authorId": "123962152",
                "name": "Liying Cheng"
            },
            {
                "authorId": "2290099573",
                "name": "Zhaodonghui Li"
            },
            {
                "authorId": "2258965291",
                "name": "De Wen Soh"
            },
            {
                "authorId": "1996394",
                "name": "Lidong Bing"
            }
        ],
        "abstract": "Recently, large language models (LLMs) have emerged as a groundbreaking technology and their unparalleled text generation capabilities have sparked interest in their application to the fundamental sentence representation learning task. Existing methods have explored utilizing LLMs as data annotators to generate synthesized data for training contrastive learning based sentence embedding models such as SimCSE. However, since contrastive learning models are sensitive to the quality of sentence pairs, the effectiveness of these methods is largely influenced by the content generated from LLMs, highlighting the need for more refined generation in the context of sentence representation learning. Building upon this premise, we propose MultiCSR, a multi-level contrastive sentence representation learning framework that decomposes the process of prompting LLMs to generate a corpus for training base sentence embedding models into three stages (i.e., sentence generation, sentence pair construction, in-batch training) and refines the generated content at these three distinct stages, ensuring only high-quality sentence pairs are utilized to train a base contrastive learning model. Our extensive experiments reveal that MultiCSR enables a less advanced LLM to surpass the performance of ChatGPT, while applying it to ChatGPT achieves better state-of-the-art results. Comprehensive analyses further underscore the potential of our framework in various application scenarios and achieving better sentence representation learning with LLMs."
    },
    {
        "paperId": "ac5924d2c948a128e30344fd7c0871d3bf99d0a9",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.11053, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-17",
        "authors": [
            {
                "authorId": "2258959896",
                "name": "Shitong Duan"
            },
            {
                "authorId": "2258961742",
                "name": "Xiaoyuan Yi"
            },
            {
                "authorId": "1896008570",
                "name": "Peng Zhang"
            },
            {
                "authorId": "1711569",
                "name": "T. Lu"
            },
            {
                "authorId": "2187555771",
                "name": "Xing Xie"
            },
            {
                "authorId": "2258958923",
                "name": "Ning Gu"
            }
        ],
        "abstract": "Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective. This work delves into ethical values utilizing Moral Foundation Theory. Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs' value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of LLMs. We discovered that most models are essentially misaligned, necessitating further ethical value alignment. In response, we develop VILMO, an in-context alignment method that substantially enhances the value compliance of LLM outputs by learning to generate appropriate value instructions, outperforming existing competitors. Our methods are suitable for black-box and open-source models, offering a promising initial step in studying the ethical values of LLMs."
    },
    {
        "paperId": "7a4fe2f003241ad97bf1778e527cb0306fa90da2",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.11501, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-17",
        "authors": [
            {
                "authorId": "2149615775",
                "name": "Myra Cheng"
            },
            {
                "authorId": "3144356",
                "name": "Tiziano Piccardi"
            },
            {
                "authorId": "2260029688",
                "name": "Diyi Yang"
            }
        ],
        "abstract": "Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature."
    },
    {
        "paperId": "2be910eb19f2f8f2e8038d2a835bc48f868ccbf1",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Group Preference Optimization: Few-Shot Alignment of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.11523, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-17",
        "authors": [
            {
                "authorId": "2260172378",
                "name": "Siyan Zhao"
            },
            {
                "authorId": "2303257581",
                "name": "John Dang"
            },
            {
                "authorId": "2259896660",
                "name": "Aditya Grover"
            }
        ],
        "abstract": "Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations. For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences of US demographic groups, global countries, and individual users. Our results demonstrate that GPO not only aligns models more accurately but also requires fewer group-specific preferences, and less training and inference computing resources, outperforming existing strategies such as in-context steering and fine-tuning methods."
    },
    {
        "paperId": "0577ca1b6f8d9cddbad7f76ea7f82dc71b5af043",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.11634, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-18",
        "authors": [
            {
                "authorId": "1443788809",
                "name": "Arkil Patel"
            },
            {
                "authorId": "92954142",
                "name": "S. Bhattamishra"
            },
            {
                "authorId": "145732771",
                "name": "Siva Reddy"
            },
            {
                "authorId": "3335364",
                "name": "Dzmitry Bahdanau"
            }
        ],
        "abstract": "Humans possess a remarkable ability to assign novel interpretations to linguistic expressions, enabling them to learn new words and understand community-specific connotations. However, Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly. Therefore, it is crucial for LLMs to learn novel interpretations in-context. In this paper, we systematically analyse the ability of LLMs to acquire novel interpretations using in-context learning. To facilitate our study, we introduce MAGNIFICo, an evaluation suite implemented within a text-to-SQL semantic parsing framework that incorporates diverse tokens and prompt settings to simulate real-world complexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a surprisingly robust capacity for comprehending novel interpretations from natural language descriptions as well as from discussions within long conversations. Nevertheless, our findings also highlight the need for further improvements, particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example. Additionally, our analysis uncovers the semantic predispositions in LLMs and reveals the impact of recency bias for information presented in long contexts."
    },
    {
        "paperId": "f6e893b3e2ee7a62c2fe8a3b0e33920c3e596969",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.11667, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-18",
        "authors": [
            {
                "authorId": "144101734",
                "name": "Xuhui Zhou"
            },
            {
                "authorId": "2260859845",
                "name": "Hao Zhu"
            },
            {
                "authorId": "2259929826",
                "name": "Leena Mathur"
            },
            {
                "authorId": "46752970",
                "name": "Ruohong Zhang"
            },
            {
                "authorId": "2260283233",
                "name": "Haofei Yu"
            },
            {
                "authorId": "2266241076",
                "name": "Zhengyang Qi"
            },
            {
                "authorId": "49933077",
                "name": "Louis-philippe Morency"
            },
            {
                "authorId": "3312309",
                "name": "Yonatan Bisk"
            },
            {
                "authorId": "2259931814",
                "name": "Daniel Fried"
            },
            {
                "authorId": "1700325",
                "name": "Graham Neubig"
            },
            {
                "authorId": "2729164",
                "name": "Maarten Sap"
            }
        ],
        "abstract": "Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents."
    },
    {
        "paperId": "62454a3694e2e52b8698458440612505a3f7404b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.12418, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-19",
        "authors": [
            {
                "authorId": "2260339714",
                "name": "Siru Ouyang"
            },
            {
                "authorId": "2146294891",
                "name": "Shuo Wang"
            },
            {
                "authorId": "2260822008",
                "name": "Yang Liu"
            },
            {
                "authorId": "1606040932",
                "name": "Ming Zhong"
            },
            {
                "authorId": "1381900594",
                "name": "Yizhu Jiao"
            },
            {
                "authorId": "3310951",
                "name": "Dan Iter"
            },
            {
                "authorId": "4099006",
                "name": "Reid Pryzant"
            },
            {
                "authorId": "2256797847",
                "name": "Chenguang Zhu"
            },
            {
                "authorId": "2181650518",
                "name": "Heng Ji"
            },
            {
                "authorId": "2259869648",
                "name": "Jiawei Han"
            }
        ],
        "abstract": "Recent progress in Large Language Models (LLMs) has produced models that exhibit remarkable performance across a variety of NLP tasks. However, it remains unclear whether the existing focus of NLP research accurately captures the genuine requirements of human users. This paper provides a comprehensive analysis of the divergence between current NLP research and the needs of real-world NLP applications via a large-scale collection of user-GPT conversations. We analyze a large-scale collection of real user queries to GPT. We compare these queries against existing NLP benchmark tasks and identify a significant gap between the tasks that users frequently request from LLMs and the tasks that are commonly studied in academic research. For example, we find that tasks such as ``design'' and ``planning'' are prevalent in user interactions but are largely neglected or different from traditional NLP benchmarks. We investigate these overlooked tasks, dissect the practical challenges they pose, and provide insights toward a roadmap to make LLMs better aligned with user needs."
    },
    {
        "paperId": "ed8306efe54cd507f299f3f7e8fb8b5cd9ba2cd4",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.12794, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-19",
        "authors": [
            {
                "authorId": "2197527085",
                "name": "Ningyu Xu"
            },
            {
                "authorId": "2257376355",
                "name": "Qi Zhang"
            },
            {
                "authorId": "2260831276",
                "name": "Jingting Ye"
            },
            {
                "authorId": "2213627466",
                "name": "Menghan Zhang"
            },
            {
                "authorId": "2257129989",
                "name": "Xuanjing Huang"
            }
        ],
        "abstract": "Large language models (LLMs) have exhibited considerable cross-lingual generalization abilities, whereby they implicitly transfer knowledge across languages. However, the transfer is not equally successful for all languages, especially for low-resource ones, which poses an ongoing challenge. It is unclear whether we have reached the limits of implicit cross-lingual generalization and if explicit knowledge transfer is viable. In this paper, we investigate the potential for explicitly aligning conceptual correspondence between languages to enhance cross-lingual generalization. Using the syntactic aspect of language as a testbed, our analyses of 43 languages reveal a high degree of alignability among the spaces of structural concepts within each language for both encoder-only and decoder-only LLMs. We then propose a meta-learning-based method to learn to align conceptual spaces of different languages, which facilitates zero-shot and few-shot generalization in concept classification and also offers insights into the cross-lingual in-context learning phenomenon. Experiments on syntactic analysis tasks show that our approach achieves competitive results with state-of-the-art methods and narrows the performance gap between languages, particularly benefiting those with limited resources."
    },
    {
        "paperId": "64410909714f421c153ac123f975f86cc15c1fec",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.12874, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-19",
        "authors": [
            {
                "authorId": "2109077713",
                "name": "Cheng Jiayang"
            },
            {
                "authorId": "2260342601",
                "name": "Lin Qiu"
            },
            {
                "authorId": "2238218216",
                "name": "Tszho Chan"
            },
            {
                "authorId": "2044202073",
                "name": "Tianqing Fang"
            },
            {
                "authorId": "1587728690",
                "name": "Weiqi Wang"
            },
            {
                "authorId": "2216598559",
                "name": "Chunkit Chan"
            },
            {
                "authorId": "41017337",
                "name": "Dongyu Ru"
            },
            {
                "authorId": "3187768",
                "name": "Qipeng Guo"
            },
            {
                "authorId": "2260447535",
                "name": "Hongming Zhang"
            },
            {
                "authorId": "2258804099",
                "name": "Yangqiu Song"
            },
            {
                "authorId": "2260818095",
                "name": "Yue Zhang"
            },
            {
                "authorId": "2260691874",
                "name": "Zheng Zhang"
            }
        ],
        "abstract": "Analogy-making between narratives is crucial for human reasoning. In this paper, we evaluate the ability to identify and generate analogies by constructing a first-of-its-kind large-scale story-level analogy corpus, \\textsc{StoryAnalogy}, which contains 24K story pairs from diverse domains with human annotations on two similarities from the extended Structure-Mapping Theory. We design a set of tests on \\textsc{StoryAnalogy}, presenting the first evaluation of story-level analogy identification and generation. Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around 30% accuracy in multiple-choice questions (compared to over 85% accuracy for humans). Furthermore, we observe that the data in \\textsc{StoryAnalogy} can improve the quality of analogy generation in LLMs, where a fine-tuned FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT."
    },
    {
        "paperId": "7c3c9f90e3acc5a0e780b121456a45df8ebed1a0",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Primacy Effect of ChatGPT",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.13206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-20",
        "authors": [
            {
                "authorId": "2108841625",
                "name": "Yiwei Wang"
            },
            {
                "authorId": "1928716951",
                "name": "Yujun Cai"
            },
            {
                "authorId": "1998918",
                "name": "Muhao Chen"
            },
            {
                "authorId": "2258794428",
                "name": "Yuxuan Liang"
            },
            {
                "authorId": "2305483565",
                "name": "Bryan Hooi"
            }
        ],
        "abstract": "Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherits humans' cognitive biases? In this paper, we study the primacy effect of ChatGPT: the tendency of selecting the labels at earlier positions as the answer. We have two main findings: i) ChatGPT's decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer. We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions. We release the source code at https://github.com/wangywUST/PrimacyEffectGPT."
    },
    {
        "paperId": "f72be31de9f9a09d4410fd38bc717efe43444827",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.13289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-20",
        "authors": [
            {
                "authorId": "2247237695",
                "name": "Changli Tang"
            },
            {
                "authorId": "2257283478",
                "name": "Wenyi Yu"
            },
            {
                "authorId": "2107310187",
                "name": "Guangzhi Sun"
            },
            {
                "authorId": "2135092817",
                "name": "Xianzhao Chen"
            },
            {
                "authorId": "2257003951",
                "name": "Tian Tan"
            },
            {
                "authorId": "2256598801",
                "name": "Wei Li"
            },
            {
                "authorId": "2257383962",
                "name": "Lu Lu"
            },
            {
                "authorId": "2257135061",
                "name": "Zejun Ma"
            },
            {
                "authorId": "2256775692",
                "name": "Chao Zhang"
            }
        ],
        "abstract": "Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN."
    },
    {
        "paperId": "9d21467c22b1709ca5a7f6c21cbbcbf5a5c4c9a9",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "POSQA: Probe the World Models of LLMs with Size Comparisons",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.13394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-20",
        "authors": [
            {
                "authorId": "2256989139",
                "name": "Chang Shu"
            },
            {
                "authorId": "2133405108",
                "name": "Jiuzhou Han"
            },
            {
                "authorId": "144097210",
                "name": "Fangyu Liu"
            },
            {
                "authorId": "2888926",
                "name": "Ehsan Shareghi"
            },
            {
                "authorId": "2256992218",
                "name": "Nigel Collier"
            }
        ],
        "abstract": "Embodied language comprehension emphasizes that language understanding is not solely a matter of mental processing in the brain but also involves interactions with the physical and social environment. With the explosive growth of Large Language Models (LLMs) and their already ubiquitous presence in our daily lives, it is becoming increasingly necessary to verify their real-world understanding. Inspired by cognitive theories, we propose POSQA: a Physical Object Size Question Answering dataset with simple size comparison questions to examine the extremity and analyze the potential mechanisms of the embodied comprehension of the latest LLMs. We show that even the largest LLMs today perform poorly under the zero-shot setting. We then push their limits with advanced prompting techniques and external knowledge augmentation. Furthermore, we investigate whether their real-world comprehension primarily derives from contextual information or internal weights and analyse the impact of prompt formats and report bias of different objects. Our results show that real-world understanding that LLMs shaped from textual data can be vulnerable to deception and confusion by the surface form of prompts, which makes it less aligned with human behaviours."
    },
    {
        "paperId": "5be610297ce9fcb27121fbcc91b67b8517147a6c",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Cache & Distil: Optimising API Calls to Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.13561, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-20",
        "authors": [
            {
                "authorId": "2054264208",
                "name": "Guillem Ram'irez"
            },
            {
                "authorId": "46233689",
                "name": "Matthias Lindemann"
            },
            {
                "authorId": "2261083421",
                "name": "Alexandra Birch"
            },
            {
                "authorId": "2256630186",
                "name": "Ivan Titov"
            }
        ],
        "abstract": "Large-scale deployment of generative AI tools often depends on costly API calls to a Large Language Model (LLM) to fulfil user queries. To curtail the frequency of these calls, one can employ a smaller language model -- a student -- which is continuously trained on the responses of the LLM. This student gradually gains proficiency in independently handling an increasing number of user requests, a process we term neural caching. The crucial element in neural caching is a policy that decides which requests should be processed by the student alone and which should be redirected to the LLM, subsequently aiding the student's learning. In this study, we focus on classification tasks, and we consider a range of classic active learning-based selection criteria as the policy. Our experiments suggest that Margin Sampling and Query by Committee bring consistent benefits across tasks and budgets."
    },
    {
        "paperId": "e1a10caa6571602980f488822e2f7e88e311f160",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.13615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-20",
        "authors": [
            {
                "authorId": "2258717994",
                "name": "An-Zi Yen"
            },
            {
                "authorId": "2261083536",
                "name": "Wei-Ling Hsu"
            }
        ],
        "abstract": "Due to the remarkable language understanding and generation abilities of large language models (LLMs), their use in educational applications has been explored. However, little work has been done on investigating the pedagogical ability of LLMs in helping students to learn mathematics. In this position paper, we discuss the challenges associated with employing LLMs to enhance students' mathematical problem-solving skills by providing adaptive feedback. Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions' rationales when attempting to correct students' answers. Three research questions are formulated."
    },
    {
        "paperId": "549da43aacc3ef5986a126dd9154b7772594b76b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.13673, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-20",
        "authors": [
            {
                "authorId": "2165226931",
                "name": "Sullam Jeoung"
            },
            {
                "authorId": "2261092893",
                "name": "Yubin Ge"
            },
            {
                "authorId": "2261085110",
                "name": "Jana Diesner"
            }
        ],
        "abstract": "Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data. We propose a theoretically grounded framework called StereoMap to gain insights into their perceptions of how demographic groups have been viewed by society. The framework is grounded in the Stereotype Content Model (SCM); a well-established theory from psychology. According to SCM, stereotypes are not all alike. Instead, the dimensions of Warmth and Competence serve as the factors that delineate the nature of stereotypes. Based on the SCM theory, StereoMap maps LLMs' perceptions of social groups (defined by socio-demographic features) using the dimensions of Warmth and Competence. Furthermore, the framework enables the investigation of keywords and verbalizations of reasoning of LLMs' judgments to uncover underlying factors influencing their perceptions. Our results show that LLMs exhibit a diverse range of perceptions towards these groups, characterized by mixed evaluations along the dimensions of Warmth and Competence. Furthermore, analyzing the reasonings of LLMs, our findings indicate that LLMs demonstrate an awareness of social disparities, often stating statistical data and research findings to support their reasoning. This study contributes to the understanding of how LLMs perceive and represent social groups, shedding light on their potential biases and the perpetuation of harmful associations."
    },
    {
        "paperId": "2a33e4c93002ab97a99577ac89837be4d448725e",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.13800, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-20",
        "authors": [
            {
                "authorId": "2261288179",
                "name": "Andrea Sottana"
            },
            {
                "authorId": "2261289337",
                "name": "Bin Liang"
            },
            {
                "authorId": "2261288751",
                "name": "Kai Zou"
            },
            {
                "authorId": "2261388112",
                "name": "Zheng Yuan"
            }
        ],
        "abstract": "Large Language Models (LLMs) evaluation is a patchy and inconsistent landscape, and it is becoming clear that the quality of automatic evaluation metrics is not keeping up with the pace of development of generative models. We aim to improve the understanding of current models' performance by providing a preliminary and hybrid evaluation on a range of open and closed-source generative LLMs on three NLP benchmarks: text summarisation, text simplification and grammatical error correction (GEC), using both automatic and human evaluation. We also explore the potential of the recently released GPT-4 to act as an evaluator. We find that ChatGPT consistently outperforms many other popular models according to human reviewers on the majority of metrics, while scoring much more poorly when using classic automatic evaluation metrics. We also find that human reviewers rate the gold reference as much worse than the best models' outputs, indicating the poor quality of many popular benchmarks. Finally, we find that GPT-4 is capable of ranking models' outputs in a way which aligns reasonably closely to human judgement despite task-specific variations, with a lower alignment in the GEC task."
    },
    {
        "paperId": "ca1261a7e058ea3339fbdaec07ffcf6d0b617c8d",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Can Language Models Laugh at YouTube Short-form Videos?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14159, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-22",
        "authors": [
            {
                "authorId": "2261279448",
                "name": "Dayoon Ko"
            },
            {
                "authorId": "2144567767",
                "name": "Sangho Lee"
            },
            {
                "authorId": "2261362214",
                "name": "Gunhee Kim"
            }
        ],
        "abstract": "As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans. Unfortunately, previous video humor datasets target specific domains, such as speeches or sitcoms, and mostly focus on verbal cues. We curate a user-generated dataset of 10K multimodal funny videos from YouTube, called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both verbal and visual elements contributing to humor. After filtering, we annotate each video with timestamps and text explanations for funny moments. Our ExFunTube is unique over existing datasets in that our videos cover a wide range of domains with various types of humor that necessitate a multimodal understanding of the content. Also, we develop a zero-shot video-to-text prompting to maximize video humor understanding of large language models (LLMs). With three different evaluation methods using automatic scores, rationale quality experiments, and human evaluations, we show that our prompting significantly improves LLMs' ability for humor explanation."
    },
    {
        "paperId": "1366b07120580eaf1badde105b9361806e8f9629",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-22",
        "authors": [
            {
                "authorId": "2188735751",
                "name": "Hongli Zhan"
            },
            {
                "authorId": "3078792",
                "name": "Desmond C. Ong"
            },
            {
                "authorId": "2267018724",
                "name": "Junyi Jessy Li"
            }
        ],
        "abstract": "The emotions we experience involve complex processes; besides physiological aspects, research in psychology has studied cognitive appraisals where people assess their situations subjectively, according to their own values (Scherer, 2005). Thus, the same situation can often result in different emotional experiences. While the detection of emotion is a well-established task, there is very limited work so far on the automatic prediction of cognitive appraisals. This work fills the gap by presenting CovidET-Appraisals, the most comprehensive dataset to-date that assesses 24 appraisal dimensions, each with a natural language rationale, across 241 Reddit posts. CovidET-Appraisals presents an ideal testbed to evaluate the ability of large language models -- excelling at a wide range of NLP tasks -- to automatically assess and explain cognitive appraisals. We found that while the best models are performant, open-sourced LLMs fall short at this task, presenting a new challenge in the future development of emotionally intelligent models. We release our dataset at https://github.com/honglizhan/CovidET-Appraisals-Public."
    },
    {
        "paperId": "d0ffb09a00b67365efb9e217c3fd45d804733810",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Large Language Models are biased to overestimate profoundness",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.599.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14422, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-22",
        "authors": [
            {
                "authorId": "2248196206",
                "name": "Eugenio Herrera-Berg"
            },
            {
                "authorId": "2261283582",
                "name": "Tom\u00e1s Vergara Browne"
            },
            {
                "authorId": "2261283677",
                "name": "Pablo Le'on-Villagr'a"
            },
            {
                "authorId": "51254982",
                "name": "Marc-Llu\u00eds Vives"
            },
            {
                "authorId": "2248178727",
                "name": "Cristian Buc Calderon"
            }
        ],
        "abstract": "Recent advancements in natural language processing by large language models (LLMs), such as GPT-4, have been suggested to approach Artificial General Intelligence. And yet, it is still under dispute whether LLMs possess similar reasoning abilities to humans. This study evaluates GPT-4 and various other LLMs in judging the profoundness of mundane, motivational, and pseudo-profound statements. We found a significant statement-to-statement correlation between the LLMs and humans, irrespective of the type of statements and the prompting technique used. However, LLMs systematically overestimate the profoundness of nonsensical statements, with the exception of Tk-instruct, which uniquely underestimates the profoundness of statements. Only few-shot learning prompts, as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans. Furthermore, this work provides insights into the potential biases induced by Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the bias to overestimate the profoundness of statements."
    },
    {
        "paperId": "fc7e0d394bb58c292ac8cca26c776fee53a4020c",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14520, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-23",
        "authors": [
            {
                "authorId": "2174327114",
                "name": "Yating Wu"
            },
            {
                "authorId": "2048016648",
                "name": "Ritika Mangla"
            },
            {
                "authorId": "1814094",
                "name": "Greg Durrett"
            },
            {
                "authorId": "2261337975",
                "name": "Junyi Jessy Li"
            }
        ],
        "abstract": "Questions Under Discussion (QUD) is a versatile linguistic framework in which discourse progresses as continuously asking questions and answering them. Automatic parsing of a discourse to produce a QUD structure thus entails a complex question generation task: given a document and an answer sentence, generate a question that satisfies linguistic constraints of QUD and can be grounded in an anchor sentence in prior context. These questions are known to be curiosity-driven and open-ended. This work introduces the first framework for the automatic evaluation of QUD parsing, instantiating the theoretical constraints of QUD in a concrete protocol. We present QUDeval, a dataset of fine-grained evaluation of 2,190 QUD questions generated from both fine-tuned systems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD is still challenging for modern LLMs, and that existing evaluation metrics poorly approximate parser quality. Encouragingly, human-authored QUDs are scored highly by our human evaluators, suggesting that there is headroom for further progress on language modeling to improve both QUD parsing and QUD evaluation."
    },
    {
        "paperId": "c8a940f2015afad576d35e7a6916cc1a0cec169d",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14557, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-23",
        "authors": [
            {
                "authorId": "50445559",
                "name": "Chiyu Zhang"
            },
            {
                "authorId": "2261277511",
                "name": "Khai Duy Doan"
            },
            {
                "authorId": "2261278153",
                "name": "Qisheng Liao"
            },
            {
                "authorId": "2065312024",
                "name": "M. Abdul-Mageed"
            }
        ],
        "abstract": "Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate remarkable performance in a wide range of tasks. Despite numerous recent studies that examine the performance of instruction-tuned LLMs on various NLP benchmarks, there remains a lack of comprehensive investigation into their ability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning embedded within social and interactive contexts. This deficiency arises partly from SM not being adequately represented in any of the existing benchmarks. To address this gap, we present SPARROW, an extensive multilingual benchmark specifically designed for SM understanding. SPARROW comprises 169 datasets covering 13 task types across six primary categories (e.g., anti-social language detection, emotion recognition). SPARROW datasets encompass 64 different languages originating from 12 language families representing 16 writing scripts. We evaluate the performance of various multilingual pretrained language models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT) on SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases. We also find that although ChatGPT outperforms many LLMs, it still falls behind task-specific finetuned models with a gap of 12.19 SPARROW score. Our benchmark is available at: https://github.com/UBC-NLP/SPARROW"
    },
    {
        "paperId": "45653ad43124f02dc2cf2db3357be1d1d78ddb18",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Language Models Hallucinate, but May Excel at Fact Verification",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14564, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-23",
        "authors": [
            {
                "authorId": "2323534462",
                "name": "Jian Guan"
            },
            {
                "authorId": "34176020",
                "name": "Jesse Dodge"
            },
            {
                "authorId": "30051202",
                "name": "David Wadden"
            },
            {
                "authorId": "2261448024",
                "name": "Minlie Huang"
            },
            {
                "authorId": "2275550586",
                "name": "Hao Peng"
            }
        ],
        "abstract": "Recent progress in natural language processing (NLP) owes much to remarkable advances in large language models (LLMs). Nevertheless, LLMs frequently \u201challucinate,\u201d resulting in non-factual outputs. Our carefully-designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25% of the time. This underscores the importance of fact verifiers in order to measure and incentivize progress. Our systematic investigation affirms that LLMs can be repurposed as effective fact verifiers with strong correlations with human judgments. Surprisingly, FLAN-T5-11B , the least factual generator in our study, performs the best as a fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT. Delving deeper, we analyze the reliance of these LLMs on high-quality evidence, as well as their deficiencies in robustness and generalization ability. Our study presents insights for developing trustworthy generation models."
    },
    {
        "paperId": "c853a6e930e7eefb6daab5a510df28569454531f",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14607, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-23",
        "authors": [
            {
                "authorId": "2108082280",
                "name": "Yanchen Liu"
            },
            {
                "authorId": "2261375736",
                "name": "Srishti Gautam"
            },
            {
                "authorId": "2261475382",
                "name": "Jiaqi Ma"
            },
            {
                "authorId": "1892673",
                "name": "Himabindu Lakkaraju"
            }
        ],
        "abstract": "Recent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is important to explore the following questions: what sources of information do LLMs draw upon when making classifications for tabular tasks; whether and to what extent are LLM classifications for tabular data influenced by social biases and stereotypes; and what are the consequential implications for fairness?Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks. Furthermore, our investigations show that in the context of bias mitigation, though in-context learning and finetuning have a moderate effect, the fairness metric gap between different subgroups is still larger than that in traditional machine learning models, such as Random Forest and shallow Neural Networks. This observation emphasizes that the social biases are inherent within the LLMs themselves and inherited from their pretraining corpus, not only from the downstream task datasets. Besides, we demonstrate that label-flipping of in-context examples can significantly reduce biases, further highlighting the presence of inherent bias within LLMs."
    },
    {
        "paperId": "f176d0d466d7c778a6435fe9a8d7e49508cb9059",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14628, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-23",
        "authors": [
            {
                "authorId": "2136108329",
                "name": "Tengxiao Liu"
            },
            {
                "authorId": "3187768",
                "name": "Qipeng Guo"
            },
            {
                "authorId": "2145435513",
                "name": "Yuqing Yang"
            },
            {
                "authorId": "12040998",
                "name": "Xiangkun Hu"
            },
            {
                "authorId": "2260818095",
                "name": "Yue Zhang"
            },
            {
                "authorId": "2256661980",
                "name": "Xipeng Qiu"
            },
            {
                "authorId": "2260691874",
                "name": "Zheng Zhang"
            }
        ],
        "abstract": "As large language models (LLMs) have shown effectiveness with different prompting methods, such as Chain of Thought, Program of Thought, we find that these methods have formed a great complementarity to each other on math reasoning tasks. In this work, we propose XoT, an integrated problem solving framework by prompting LLMs with diverse reasoning thoughts. For each question, XoT always begins with selecting the most suitable method then executes each method iteratively. Within each iteration, XoT actively checks the validity of the generated answer and incorporates the feedback from external executors, allowing it to dynamically switch among different prompting methods. Through extensive experiments on 10 popular math reasoning datasets, we demonstrate the effectiveness of our proposed approach and thoroughly analyze the strengths of each module. Moreover, empirical results suggest that our framework is orthogonal to recent work that makes improvements on single reasoning methods and can further generalise to logical reasoning domain. By allowing method switching, XoT provides a fresh perspective on the collaborative integration of diverse reasoning thoughts in a unified framework. The code is available at https://github.com/tengxiaoliu/XoT."
    },
    {
        "paperId": "1561477355fb339a0c65c36fd297de788c0bf0b7",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Geographical Erasure in Language Generation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14777, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-23",
        "authors": [
            {
                "authorId": "1631697931",
                "name": "Pola Schw\u00f6bel"
            },
            {
                "authorId": "2238481381",
                "name": "Jacek Golebiowski"
            },
            {
                "authorId": "2262451123",
                "name": "Michele Donini"
            },
            {
                "authorId": "2262457089",
                "name": "C\u00e9dric Archambeau"
            },
            {
                "authorId": "2064506371",
                "name": "Danish Pruthi"
            }
        ],
        "abstract": "Large language models (LLMs) encode vast amounts of world knowledge. However, since these models are trained on large swaths of internet data, they are at risk of inordinately capturing information about dominant groups. This imbalance can propagate into generated language. In this work, we study and operationalise a form of geographical erasure, wherein language models underpredict certain countries. We demonstrate consistent instances of erasure across a range of LLMs. We discover that erasure strongly correlates with low frequencies of country mentions in the training corpus. Lastly, we mitigate erasure by finetuning using a custom objective."
    },
    {
        "paperId": "ed5d6c4ee7161f951f09c4bd16046010c4acb47e",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14868, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-23",
        "authors": [
            {
                "authorId": "2261380965",
                "name": "Mengyu Ye"
            },
            {
                "authorId": "83446147",
                "name": "Tatsuki Kuribayashi"
            },
            {
                "authorId": "2260652898",
                "name": "Jun Suzuki"
            },
            {
                "authorId": "2060291042",
                "name": "Goro Kobayashi"
            },
            {
                "authorId": "2261362866",
                "name": "Hiroaki Funayama"
            }
        ],
        "abstract": "Large language models (LLMs) take advantage of step-by-step reasoning instructions, e.g., chain-of-thought (CoT) prompting. Building on this, their ability to perform CoT-style reasoning robustly is of interest from a probing perspective. In this study, we inspect the step-by-step reasoning ability of LLMs with a focus on negation, which is a core linguistic phenomenon that is difficult to process. In particular, we introduce several controlled settings (e.g., reasoning in case of fictional entities) to evaluate the logical reasoning abilities of the models. We observed that dozens of modern LLMs were not robust against lexical negation (e.g., plausible ->implausible) when performing CoT-style reasoning, and the results highlight unique limitations in each LLM family."
    },
    {
        "paperId": "6464be46eb38914e4b366d38c58079574cca5779",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal Scenarios Like a Lawyer?",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.929.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14880, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-23",
        "authors": [
            {
                "authorId": "2262460478",
                "name": "Xiaoxi Kang"
            },
            {
                "authorId": "2270224408",
                "name": "Lizhen Qu"
            },
            {
                "authorId": "47247517",
                "name": "Lay-Ki Soon"
            },
            {
                "authorId": "2262456337",
                "name": "Adnan Trakic"
            },
            {
                "authorId": "2080123731",
                "name": "Terry Yue Zhuo"
            },
            {
                "authorId": "2262456267",
                "name": "Patrick Charles Emerton"
            },
            {
                "authorId": "2262456632",
                "name": "Genevieve Grant"
            }
        ],
        "abstract": "Large Language Models (LLMs), such as ChatGPT, have drawn a lot of attentions recently in the legal domain due to its emergent ability to tackle a variety of legal tasks. However, it is still unknown if LLMs are able to analyze a legal case and perform reasoning in the same manner as lawyers. Therefore, we constructed a novel corpus consisting of scenarios pertain to Contract Acts Malaysia and Australian Social Act for Dependent Child. ChatGPT is applied to perform analysis on the corpus using the IRAC method, which is a framework widely used by legal professionals for organizing legal analysis. Each scenario in the corpus is annotated with a complete IRAC analysis in a semi-structured format so that both machines and legal professionals are able to interpret and understand the annotations. In addition, we conducted the first empirical assessment of ChatGPT for IRAC analysis in order to understand how well it aligns with the analysis of legal professionals. Our experimental results shed lights on possible future research directions to improve alignments between LLMs and legal experts in terms of legal reasoning."
    },
    {
        "paperId": "e1770838ec0667cad48729a81764ed9964d6a8e6",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.15100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-23",
        "authors": [
            {
                "authorId": "3400291",
                "name": "Shih-Chieh Dai"
            },
            {
                "authorId": "2261362789",
                "name": "Aiping Xiong"
            },
            {
                "authorId": "1746959",
                "name": "Lun-Wei Ku"
            }
        ],
        "abstract": "Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen their data interpretation and coding over multiple iterations, making TA labor-intensive and time-consuming. Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks: in particular, LLMs outperform crowd workers on text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We propose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL). This framework provides the prompt to frame discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA. We demonstrate the utility of this framework using survey datasets on the aspects of the music listening experience and the usage of a password manager. Results of the two case studies show that the proposed framework yields similar coding quality to that of human coders but reduces TA's labor and time demands."
    },
    {
        "paperId": "fabee69ea9a76d18fcb90b4a329a9f124ba35e11",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.15113, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-23",
        "authors": [
            {
                "authorId": "31832571",
                "name": "Leonie Weissweiler"
            },
            {
                "authorId": "1667898858",
                "name": "Valentin Hofmann"
            },
            {
                "authorId": "2261362392",
                "name": "Anjali Kantharuban"
            },
            {
                "authorId": "2261362902",
                "name": "Anna Cai"
            },
            {
                "authorId": "36662010",
                "name": "Ritam Dutt"
            },
            {
                "authorId": "1992907559",
                "name": "Amey Hengle"
            },
            {
                "authorId": "1735001746",
                "name": "Anubha Kabra"
            },
            {
                "authorId": "1992797499",
                "name": "Atharva Kulkarni"
            },
            {
                "authorId": "2261362887",
                "name": "Abhishek Vijayakumar"
            },
            {
                "authorId": "2261476361",
                "name": "Haofei Yu"
            },
            {
                "authorId": "144418438",
                "name": "Hinrich Sch\u00fctze"
            },
            {
                "authorId": "2250930714",
                "name": "Kemal Oflazer"
            },
            {
                "authorId": "3407646",
                "name": "David R. Mortensen"
            }
        ],
        "abstract": "Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human language, like morphology. Here, we close these gaps by conducting the first rigorous analysis of the morphological capabilities of ChatGPT in four typologically varied languages (specifically, English, German, Tamil, and Turkish). We apply a version of Berko's (1958) wug test to ChatGPT, using novel, uncontaminated datasets for the four examined languages. We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results -- through the lens of morphology -- cast a new light on the linguistic capabilities of ChatGPT, suggesting that claims of human-like language skills are premature and misleading."
    },
    {
        "paperId": "6574466d9103acacbc1b273114673bf1e8172b04",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Quantifying the Dialect Gap and its Correlates Across Languages",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.15135, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-23",
        "authors": [
            {
                "authorId": "2261362392",
                "name": "Anjali Kantharuban"
            },
            {
                "authorId": "2260337067",
                "name": "Ivan Vuli'c"
            },
            {
                "authorId": "2260336540",
                "name": "Anna Korhonen"
            }
        ],
        "abstract": "Historically, researchers and consumers have noticed a decrease in quality when applying NLP tools to minority variants of languages (i.e. Puerto Rican Spanish or Swiss German), but studies exploring this have been limited to a select few languages. Additionally, past studies have mainly been conducted in a monolingual context, so cross-linguistic trends have not been identified and tied to external factors. In this work, we conduct a comprehensive evaluation of the most influential, state-of-the-art large language models (LLMs) across two high-use applications, machine translation and automatic speech recognition, to assess their functionality on the regional dialects of several high- and low-resource languages. Additionally, we analyze how the regional dialect gap is correlated with economic, social, and linguistic factors. The impact of training data, including related factors like dataset size and its construction procedure, is shown to be significant but not consistent across models or languages, meaning a one-size-fits-all approach cannot be taken in solving the dialect gap. This work will lay the foundation for furthering the field of dialectal NLP by laying out evident disparities and identifying possible pathways for addressing them through mindful data collection."
    },
    {
        "paperId": "650fa5cda36ac2c4c4b5ca4f72ac7b3ab3c3236c",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Moral Foundations of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.15337, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-23",
        "authors": [
            {
                "authorId": "2007538346",
                "name": "Marwa Abdulhai"
            },
            {
                "authorId": "2261389280",
                "name": "Gregory Serapio-Garcia"
            },
            {
                "authorId": "2221011518",
                "name": "Cl\u00e9-ment Crepy"
            },
            {
                "authorId": "2085587569",
                "name": "Dasha Valter"
            },
            {
                "authorId": "2253784975",
                "name": "John F. Canny"
            },
            {
                "authorId": "2316435544",
                "name": "Natasha Jaques"
            }
        ],
        "abstract": "Moral foundations theory (MFT) is a psychological assessment tool that decomposes human moral reasoning into five factors, including care/harm, liberty/oppression, and sanctity/degradation (Graham et al., 2009). People vary in the weight they place on these dimensions when making moral decisions, in part due to their cultural upbringing and political ideology. As large language models (LLMs) are trained on datasets collected from the internet, they may reflect the biases that are present in such corpora. This paper uses MFT as a lens to analyze whether popular LLMs have acquired a bias towards a particular set of moral values. We analyze known LLMs and find they exhibit particular moral foundations, and show how these relate to human moral foundations and political affiliations. We also measure the consistency of these biases, or whether they vary strongly depending on the context of how the model is prompted. Finally, we show that we can adversarially select prompts that encourage the moral to exhibit a particular set of moral foundations, and that this can affect the model\u2019s behavior on downstream tasks. These findings help illustrate the potential risks and unintended consequences of LLMs assuming a particular moral stance."
    },
    {
        "paperId": "5e6e36f08504c65fb61aa777f058882cc0a09346",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.15355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-23",
        "authors": [
            {
                "authorId": "2261389626",
                "name": "Adam Bouyamourn"
            }
        ],
        "abstract": "We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence: a condition that we call evidential closure. Information about the truth or falsity of sentences is not statistically identified in the standard neural probabilistic language model setup, and so cannot be conditioned on to generate new strings. We then show how to constrain LLMs to produce output that does satisfy evidential closure. A multimodal LLM must learn about the external world (perceptual learning); it must learn a mapping from strings to states of the world (extensional learning); and, to achieve fluency when generalizing beyond a body of evidence, it must learn mappings from strings to their synonyms (intensional learning). The output of a unimodal LLM must be synonymous with strings in a validated evidence set. Finally, we present a heuristic procedure, Learn-Babble-Prune, that yields faithful output from an LLM by rejecting output that is not synonymous with claims for which the LLM has evidence."
    },
    {
        "paperId": "ec5b621ff9b33fa3920a34509464352d14d5ec68",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "EpiK-Eval: Evaluation for Language Models as Epistemic Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.15372, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-23",
        "authors": [
            {
                "authorId": "134028006",
                "name": "Gabriele Prato"
            },
            {
                "authorId": "2223971135",
                "name": "Jerry Huang"
            },
            {
                "authorId": "2261390785",
                "name": "Prasannna Parthasarathi"
            },
            {
                "authorId": "2462516",
                "name": "Shagun Sodhani"
            },
            {
                "authorId": "123607932",
                "name": "Sarath Chandar"
            }
        ],
        "abstract": "In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents - a crucial ability in numerous applications - remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs' proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from this study offer insights for developing more robust and reliable LLMs. Our code and benchmark are available at https://github.com/chandar-lab/EpiK-Eval"
    },
    {
        "paperId": "53f087d7f5ac5a24910c0fa0162079a2c35d8f64",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "DoGE: Domain Reweighting with Generalization Estimation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.15393, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-23",
        "authors": [
            {
                "authorId": "2261456109",
                "name": "Simin Fan"
            },
            {
                "authorId": "2435537",
                "name": "Matteo Pagliardini"
            },
            {
                "authorId": "2256984280",
                "name": "Martin Jaggi"
            }
        ],
        "abstract": "The coverage and composition of the pretraining data significantly impacts the generalization ability of Large Language Models (LLMs). Despite its importance, recent LLMs still rely on heuristics and trial and error to increase or reduce the influence of data-domains. We propose DOmain reweighting with Generalization Estimation (DoGE), which optimizes the probability of sampling from each domain (domain weights) in a principled way. Our approach is a two-stage process consisting of (i) training a proxy model to obtain domain weights using a bi-level optimization algorithm; (ii) training a larger base model by sampling training domains according to the learned domain weights. In our experiments, we extensively show how DoGE improves the generalization of the base model to any target data mixture. On the SlimPajama dataset, our base model gets better perplexity and few-shot reasoning accuracies across $6$ tasks compared to baseline methods. Moreover, aiming to generalize to out-of-domain target tasks, which is unseen in the pretraining corpus (OOD domain), DoGE can effectively identify inter-domain dependencies, and consistently achieves better test perplexity on the target domain."
    },
    {
        "paperId": "088a5fa00ed6c14351209da5f53e770b51fd2909",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.15421, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-24",
        "authors": [
            {
                "authorId": "32609381",
                "name": "Hyunwoo Kim"
            },
            {
                "authorId": "1947172233",
                "name": "Melanie Sclar"
            },
            {
                "authorId": "144101734",
                "name": "Xuhui Zhou"
            },
            {
                "authorId": "2069676542",
                "name": "R. L. Bras"
            },
            {
                "authorId": "2261450445",
                "name": "Gunhee Kim"
            },
            {
                "authorId": "2257385140",
                "name": "Yejin Choi"
            },
            {
                "authorId": "2729164",
                "name": "Maarten Sap"
            }
        ],
        "abstract": "Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning."
    },
    {
        "paperId": "b91645729a769c09eddda2efe2512e2f6a750723",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.15515, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-24",
        "authors": [
            {
                "authorId": "2260072705",
                "name": "Jason Samuel Lucas"
            },
            {
                "authorId": "150035131",
                "name": "Adaku Uchendu"
            },
            {
                "authorId": "66848311",
                "name": "Michiharu Yamashita"
            },
            {
                "authorId": "2159644449",
                "name": "Jooyoung Lee"
            },
            {
                "authorId": "40408676",
                "name": "Shaurya Rohatgi"
            },
            {
                "authorId": "2158951945",
                "name": "Dongwon Lee"
            }
        ],
        "abstract": "Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (.i.e, generating large-scale harmful and misleading content). To combat this emerging risk of LLMs, we propose a novel\"Fighting Fire with Fire\"(F3) strategy that harnesses modern LLMs' generative and emergent reasoning capabilities to counter human-written and LLM-generated disinformation. First, we leverage GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content through paraphrase-based and perturbation-based prefix-style prompts, respectively. Second, we apply zero-shot in-context semantic reasoning techniques with cloze-style prompts to discern genuine from deceptive posts and news articles. In our extensive experiments, we observe GPT-3.5-turbo's zero-shot superiority for both in-distribution and out-of-distribution datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike the decline observed in previous customized and fine-tuned disinformation detectors. Our codebase and dataset are available at https://github.com/mickeymst/F3."
    },
    {
        "paperId": "a97123a21f27ebd0e4bd0f6c4def42c87813fd64",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Is ChatGPT a Good Multi-Party Conversation Solver?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.16301, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-25",
        "authors": [
            {
                "authorId": "2111728713",
                "name": "Chao-Hong Tan"
            },
            {
                "authorId": "3028818",
                "name": "Jia-Chen Gu"
            },
            {
                "authorId": "2072392338",
                "name": "Zhen-Hua Ling"
            }
        ],
        "abstract": "Large Language Models (LLMs) have emerged as influential instruments within the realm of natural language processing; nevertheless, their capacity to handle multi-party conversations (MPCs) -- a scenario marked by the presence of multiple interlocutors involved in intricate information exchanges -- remains uncharted. In this paper, we delve into the potential of generative LLMs such as ChatGPT and GPT-4 within the context of MPCs. An empirical analysis is conducted to assess the zero-shot learning capabilities of ChatGPT and GPT-4 by subjecting them to evaluation across three MPC datasets that encompass five representative tasks. The findings reveal that ChatGPT's performance on a number of evaluated MPC tasks leaves much to be desired, whilst GPT-4's results portend a promising future. Additionally, we endeavor to bolster performance through the incorporation of MPC structures, encompassing both speaker and addressee architecture. This study provides an exhaustive evaluation and analysis of applying generative LLMs to MPCs, casting a light upon the conception and creation of increasingly effective and robust MPC agents. Concurrently, this work underscores the challenges implicit in the utilization of LLMs for MPCs, such as deciphering graphical information flows and generating stylistically consistent responses."
    },
    {
        "paperId": "cd8fdb0ce756ca06c3f08363c4e13c6c8c32be59",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Decoding Stumpers: Large Language Models vs. Human Problem-Solvers",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.16411, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-25",
        "authors": [
            {
                "authorId": "2054889537",
                "name": "Alon Goldstein"
            },
            {
                "authorId": "2261492502",
                "name": "Miriam Havin"
            },
            {
                "authorId": "2249760179",
                "name": "Roi Reichart"
            },
            {
                "authorId": "2250857819",
                "name": "Ariel Goldstein"
            }
        ],
        "abstract": "This paper investigates the problem-solving capabilities of Large Language Models (LLMs) by evaluating their performance on stumpers, unique single-step intuition problems that pose challenges for human solvers but are easily verifiable. We compare the performance of four state-of-the-art LLMs (Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) to human participants. Our findings reveal that the new-generation LLMs excel in solving stumpers and surpass human performance. However, humans exhibit superior skills in verifying solutions to the same problems. This research enhances our understanding of LLMs' cognitive abilities and provides insights for enhancing their problem-solving potential across various domains."
    },
    {
        "paperId": "4b3c8f3cc8760b8f95546431b4fe635b8a0f0e18",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.16746, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-25",
        "authors": [
            {
                "authorId": "66674465",
                "name": "Nafis Irtiza Tripto"
            },
            {
                "authorId": "150035131",
                "name": "Adaku Uchendu"
            },
            {
                "authorId": "2260345102",
                "name": "Thai Le"
            },
            {
                "authorId": "47139979",
                "name": "Mattia Setzu"
            },
            {
                "authorId": "1685102",
                "name": "F. Giannotti"
            },
            {
                "authorId": "2158951945",
                "name": "Dongwon Lee"
            }
        ],
        "abstract": "Authorship Analysis, also known as stylometry, has been an essential aspect of Natural Language Processing (NLP) for a long time. Likewise, the recent advancement of Large Language Models (LLMs) has made authorship analysis increasingly crucial for distinguishing between human-written and AI-generated texts. However, these authorship analysis tasks have primarily been focused on written texts, not considering spoken texts. Thus, we introduce the largest benchmark for spoken texts - HANSEN (Human ANd ai Spoken tExt beNchmark). HANSEN encompasses meticulous curation of existing speech datasets accompanied by transcripts, alongside the creation of novel AI-generated spoken text datasets. Together, it comprises 17 human datasets, and AI-generated spoken texts created using 3 prominent LLMs: ChatGPT, PaLM2, and Vicuna13B. To evaluate and demonstrate the utility of HANSEN, we perform Authorship Attribution (AA)&Author Verification (AV) on human-spoken datasets and conducted Human vs. AI spoken text detection using state-of-the-art (SOTA) models. While SOTA methods, such as, character ngram or Transformer-based model, exhibit similar AA&AV performance in human-spoken datasets compared to written ones, there is much room for improvement in AI-generated spoken text detection. The HANSEN benchmark is available at: https://huggingface.co/datasets/HANSEN-REPO/HANSEN."
    },
    {
        "paperId": "2361bae8f0ff3627a91408c172e6612b4d554cf2",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.16755, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-25",
        "authors": [
            {
                "authorId": "2261727080",
                "name": "Yinghui He"
            },
            {
                "authorId": "2261689243",
                "name": "Yufan Wu"
            },
            {
                "authorId": "2257230379",
                "name": "Yilin Jia"
            },
            {
                "authorId": "2105984203",
                "name": "Rada Mihalcea"
            },
            {
                "authorId": "2261678021",
                "name": "Yulong Chen"
            },
            {
                "authorId": "2142468208",
                "name": "Naihao Deng"
            }
        ],
        "abstract": "Theory of Mind (ToM) is the ability to reason about one's own and others' mental states. ToM plays a critical role in the development of intelligence, language understanding, and cognitive processes. While previous work has primarily focused on first and second-order ToM, we explore higher-order ToM, which involves recursive reasoning on others' beliefs. We introduce HI-TOM, a Higher Order Theory of Mind benchmark. Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs. We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP."
    },
    {
        "paperId": "cf3e8af119bae0d72a9d62fbc2a66d44bf854e94",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-25",
        "authors": [
            {
                "authorId": "2261909926",
                "name": "Y. Cho"
            },
            {
                "authorId": "2261734219",
                "name": "Sunny Rai"
            },
            {
                "authorId": "1717822",
                "name": "Lyle Ungar"
            },
            {
                "authorId": "2662374",
                "name": "Jo\u00e3o Sedoc"
            },
            {
                "authorId": "2008166098",
                "name": "Sharath Chandra Guntuku"
            }
        ],
        "abstract": "Mental health conversational agents (a.k.a. chatbots) are widely studied for their potential to offer accessible support to those experiencing mental health challenges. Previous surveys on the topic primarily consider papers published in either computer science or medicine, leading to a divide in understanding and hindering the sharing of beneficial knowledge between both domains. To bridge this gap, we conduct a comprehensive literature review using the PRISMA framework, reviewing 534 papers published in both computer science and medicine. Our systematic review reveals 136 key papers on building mental health-related conversational agents with diverse characteristics of modeling and experimental design techniques. We find that computer science papers focus on LLM techniques and evaluating response quality using automated metrics with little attention to the application while medical papers use rule-based conversational agents and outcome metrics to measure the health outcomes of participants. Based on our findings on transparency, ethics, and cultural heterogeneity in this review, we provide a few recommendations to help bridge the disciplinary divide and enable the cross-disciplinary development of mental health conversational agents."
    },
    {
        "paperId": "96e37dc703d58da316041fcfcabddb49dfe855b0",
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "title": "Cultural Adaptation of Recipes",
        "openAccessPdf": {
            "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00634/2325708/tacl_a_00634.pdf",
            "status": "GOLD",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.17353, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-26",
        "authors": [
            {
                "authorId": "2261896052",
                "name": "Yong Cao"
            },
            {
                "authorId": "51208524",
                "name": "Yova Kementchedjhieva"
            },
            {
                "authorId": "1717462692",
                "name": "Ruixiang Cui"
            },
            {
                "authorId": "2106433317",
                "name": "Antonia Karamolegkou"
            },
            {
                "authorId": "2116635928",
                "name": "Li Zhou"
            },
            {
                "authorId": "2261739724",
                "name": "Megan Dare"
            },
            {
                "authorId": "2261739082",
                "name": "Lucia Donatelli"
            },
            {
                "authorId": "2064295987",
                "name": "Daniel Hershcovich"
            }
        ],
        "abstract": "Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese- and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset composed of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automatic and human evaluation metrics. While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese. This underscores the multifaceted nature of cultural adaptations. We anticipate that these insights will significantly contribute to future research on culturally aware language models and their practical application in culturally diverse contexts."
    },
    {
        "paperId": "aa988cf6414025df4ea411ed885f2ea42b25a111",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "\"Fifty Shades of Bias\": Normative Ratings of Gender Bias in GPT Generated English Text",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.17428, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-26",
        "authors": [
            {
                "authorId": "35831406",
                "name": "Rishav Hada"
            },
            {
                "authorId": "2261734475",
                "name": "Agrima Seth"
            },
            {
                "authorId": "2135014756",
                "name": "Harshita Diddee"
            },
            {
                "authorId": "3086996",
                "name": "Kalika Bali"
            }
        ],
        "abstract": "Language serves as a powerful tool for the manifestation of societal belief systems. In doing so, it also perpetuates the prevalent biases in our society. Gender bias is one of the most pervasive biases in our society and is seen in online and offline discourses. With LLMs increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative. Prior work often treats gender bias as a binary classification task. However, acknowledging that bias must be perceived at a relative scale; we investigate the generation and consequent receptivity of manual annotators to bias of varying degrees. Specifically, we create the first dataset of GPT-generated English text with normative ratings of gender bias. Ratings were obtained using Best--Worst Scaling -- an efficient comparative annotation framework. Next, we systematically analyze the variation of themes of gender biases in the observed ranking and show that identity-attack is most closely related to gender bias. Finally, we show the performance of existing automated models trained on related concepts on our dataset."
    },
    {
        "paperId": "e5cd93d7d9f0542a2e670778394417aa773452ae",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "CompeteAI: Understanding the Competition Dynamics of Large Language Model-based Agents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.17512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-26",
        "authors": [
            {
                "authorId": "2261935625",
                "name": "Qinlin Zhao"
            },
            {
                "authorId": "2285254341",
                "name": "Jindong Wang"
            },
            {
                "authorId": "2257312904",
                "name": "Yixuan Zhang"
            },
            {
                "authorId": "2261890759",
                "name": "Yiqiao Jin"
            },
            {
                "authorId": "2543684",
                "name": "Kaijie Zhu"
            },
            {
                "authorId": "2261741520",
                "name": "Hao Chen"
            },
            {
                "authorId": "2249681654",
                "name": "Xing Xie"
            }
        ],
        "abstract": "Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. While most of the work has focused on cooperation and collaboration between agents, little work explores competition, another important mechanism that promotes the development of society and economy. In this paper, we seek to examine the competition dynamics in LLM-based agents. We first propose a general framework for studying the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, restaurant agents and customer agents. Specifically, the restaurant agents compete with each other to attract more customers, where competition encourages them to transform, such as cultivating new operating strategies. Simulation experiments reveal several interesting findings at the micro and macro levels, which align well with existing market and sociological theories. We hope that the framework and environment can be a promising testbed to study competition that fosters understanding of society. Code is available at: https://github.com/microsoft/competeai."
    },
    {
        "paperId": "bc508431522f5ce74d15fa77ccd4c91ee533151f",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.17567, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-26",
        "authors": [
            {
                "authorId": "2141026731",
                "name": "Dingli Yu"
            },
            {
                "authorId": "2261738052",
                "name": "Simran Kaur"
            },
            {
                "authorId": "2110047443",
                "name": "Arushi Gupta"
            },
            {
                "authorId": "1400348545",
                "name": "Jonah Brown-Cohen"
            },
            {
                "authorId": "1996705",
                "name": "Anirudh Goyal"
            },
            {
                "authorId": "2261737783",
                "name": "Sanjeev Arora"
            }
        ],
        "abstract": "With LLMs shifting their role from statistical modeling of language to serving as general-purpose AI agents, how should LLM evaluations change? Arguably, a key ability of an AI agent is to flexibly combine, as needed, the basic skills it has learned. The capability to combine skills plays an important role in (human) pedagogy and also in a paper on emergence phenomena (Arora&Goyal, 2023). This work introduces Skill-Mix, a new evaluation to measure ability to combine skills. Using a list of $N$ skills the evaluator repeatedly picks random subsets of $k$ skills and asks the LLM to produce text combining that subset of skills. Since the number of subsets grows like $N^k$, for even modest $k$ this evaluation will, with high probability, require the LLM to produce text significantly different from any text in the training set. The paper develops a methodology for (a) designing and administering such an evaluation, and (b) automatic grading (plus spot-checking by humans) of the results using GPT-4 as well as the open LLaMA-2 70B model. Administering a version of to popular chatbots gave results that, while generally in line with prior expectations, contained surprises. Sizeable differences exist among model capabilities that are not captured by their ranking on popular LLM leaderboards (\"cramming for the leaderboard\"). Furthermore, simple probability calculations indicate that GPT-4's reasonable performance on $k=5$ is suggestive of going beyond\"stochastic parrot\"behavior (Bender et al., 2021), i.e., it combines skills in ways that it had not seen during training. We sketch how the methodology can lead to a Skill-Mix based eco-system of open evaluations for AI capabilities of future models."
    },
    {
        "paperId": "af365d54e237fb213d980b2dc0c2ef1a4280bbd7",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "In-Context Learning Dynamics with Random Binary Sequences",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.17639, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-26",
        "authors": [
            {
                "authorId": "2190821333",
                "name": "Eric J. Bigelow"
            },
            {
                "authorId": "35573359",
                "name": "Ekdeep Singh Lubana"
            },
            {
                "authorId": "2258717260",
                "name": "Robert P. Dick"
            },
            {
                "authorId": "2258898381",
                "name": "Hidenori Tanaka"
            },
            {
                "authorId": "37774552",
                "name": "T. Ullman"
            }
        ],
        "abstract": "Large language models (LLMs) trained on huge corpora of text datasets demonstrate intriguing capabilities, achieving state-of-the-art performance on tasks they were not explicitly trained for. The precise nature of LLM capabilities is often mysterious, and different prompts can elicit different capabilities through in-context learning. We propose a framework that enables us to analyze in-context learning dynamics to understand latent concepts underlying LLMs' behavioral patterns. This provides a more nuanced understanding than success-or-failure evaluation benchmarks, but does not require observing internal activations as a mechanistic interpretation of circuits would. Inspired by the cognitive science of human randomness perception, we use random binary sequences as context and study dynamics of in-context learning by manipulating properties of context data, such as sequence length. In the latest GPT-3.5+ models, we find emergent abilities to generate seemingly random numbers and learn basic formal languages, with striking in-context learning dynamics where model outputs transition sharply from seemingly random behaviors to deterministic repetition."
    },
    {
        "paperId": "98171780168facb1a3f8b9019e225acba133bcf2",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.17774, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-26",
        "authors": [
            {
                "authorId": "2003578119",
                "name": "Sathvik Nair"
            },
            {
                "authorId": "2262217773",
                "name": "Philip Resnik"
            }
        ],
        "abstract": "An important assumption that comes with using LLMs on psycholinguistic data has gone unverified. LLM-based predictions are based on subword tokenization, not decomposition of words into morphemes. Does that matter? We carefully test this by comparing surprisal estimates using orthographic, morphological, and BPE tokenization against reading time data. Our results replicate previous findings and provide evidence that in the aggregate, predictions using BPE tokenization do not suffer relative to morphological and orthographic segmentation. However, a finer-grained analysis points to potential issues with relying on BPE-based tokenization, as well as providing promising results involving morphologically-aware surprisal estimates and suggesting a new method for evaluating morphological prediction."
    },
    {
        "paperId": "52e963c40a5083d5403cebf4d4782271aaa06994",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.17857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-27",
        "authors": [
            {
                "authorId": "2212070823",
                "name": "Dongjun Kang"
            },
            {
                "authorId": "2262425288",
                "name": "Joonsuk Park"
            },
            {
                "authorId": "39947629",
                "name": "Yohan Jo"
            },
            {
                "authorId": "72761736",
                "name": "Jinyeong Bak"
            }
        ],
        "abstract": "Being able to predict people's opinions on issues and behaviors in realistic scenarios can be helpful in various domains, such as politics and marketing. However, conducting large-scale surveys like the European Social Survey to solicit people's opinions on individual issues can incur prohibitive costs. Leveraging prior research showing influence of core human values on individual decisions and actions, we propose to use value-injected large language models (LLM) to predict opinions and behaviors. To this end, we present Value Injection Method (VIM), a collection of two methods -- argument generation and question answering -- designed to inject targeted value distributions into LLMs via fine-tuning. We then conduct a series of experiments on four tasks to test the effectiveness of VIM and the possibility of using value-injected LLMs to predict opinions and behaviors of people. We find that LLMs value-injected with variations of VIM substantially outperform the baselines. Also, the results suggest that opinions and behaviors can be better predicted using value-injected LLMs than the baseline approaches."
    },
    {
        "paperId": "89512c767e0ca0fe64d12a436c64f15dffdad1e0",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.17884, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-27",
        "authors": [
            {
                "authorId": "2254272878",
                "name": "Niloofar Mireshghallah"
            },
            {
                "authorId": "32609381",
                "name": "Hyunwoo Kim"
            },
            {
                "authorId": "144101734",
                "name": "Xuhui Zhou"
            },
            {
                "authorId": "2258958466",
                "name": "Yulia Tsvetkov"
            },
            {
                "authorId": "2729164",
                "name": "Maarten Sap"
            },
            {
                "authorId": "2262214958",
                "name": "Reza Shokri"
            },
            {
                "authorId": "2257385140",
                "name": "Yejin Choi"
            }
        ],
        "abstract": "The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind."
    },
    {
        "paperId": "4b530e7756a08af082c0ec2b242882b70873f753",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.17976",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.17976, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-27",
        "authors": [
            {
                "authorId": "2108003209",
                "name": "Xintao Wang"
            },
            {
                "authorId": "2152958412",
                "name": "Yunze Xiao"
            },
            {
                "authorId": "2284683920",
                "name": "Jen-tse Huang"
            },
            {
                "authorId": "2145968425",
                "name": "Siyu Yuan"
            },
            {
                "authorId": "2284900134",
                "name": "Rui Xu"
            },
            {
                "authorId": "2272125930",
                "name": "Haoran Guo"
            },
            {
                "authorId": "2262445001",
                "name": "Quan Tu"
            },
            {
                "authorId": "2288180253",
                "name": "Yaying Fei"
            },
            {
                "authorId": "1557927514",
                "name": "Ziang Leng"
            },
            {
                "authorId": "2265216709",
                "name": "Wei Wang"
            },
            {
                "authorId": "5040052",
                "name": "Jiangjie Chen"
            },
            {
                "authorId": "2262536497",
                "name": "Cheng Li"
            },
            {
                "authorId": "2282543969",
                "name": "Yanghua Xiao"
            }
        ],
        "abstract": "Role-playing agents (RPAs), powered by large language models, have emerged as a flourishing field of applications. However, a key challenge lies in assessing whether RPAs accurately reproduce the personas of target characters, namely their character fidelity. Existing methods mainly focus on the knowledge and linguistic patterns of characters. This paper, instead, introduces a novel perspective to evaluate the personality fidelity of RPAs with psychological scales. Overcoming drawbacks of previous self-report assessments on RPAs, we propose InCharacter, namely Interviewing Character agents for personality tests. Experiments include various types of RPAs and LLMs, covering 32 distinct characters on 14 widely used psychological scales. The results validate the effectiveness of InCharacter in measuring RPA personalities. Then, with InCharacter, we show that state-of-the-art RPAs exhibit personalities highly aligned with the human-perceived personalities of the characters, achieving an accuracy up to 80.7%."
    },
    {
        "paperId": "58b77dc0603eb52559d98a383bf9649fd31d0bc5",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.18332, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-20",
        "authors": [
            {
                "authorId": "2262568349",
                "name": "Jun-Yan He"
            },
            {
                "authorId": "3493929",
                "name": "Zhi-Qi Cheng"
            },
            {
                "authorId": "2188996842",
                "name": "Chenyang Li"
            },
            {
                "authorId": "2213832152",
                "name": "Jingdong Sun"
            },
            {
                "authorId": "41022741",
                "name": "Wangmeng Xiang"
            },
            {
                "authorId": "2262788169",
                "name": "Xianhui Lin"
            },
            {
                "authorId": "2192833663",
                "name": "Xiaoyang Kang"
            },
            {
                "authorId": "2262526075",
                "name": "Zengke Jin"
            },
            {
                "authorId": "2262488704",
                "name": "Yusen Hu"
            },
            {
                "authorId": "2237808561",
                "name": "Bin Luo"
            },
            {
                "authorId": "2067505413",
                "name": "Yifeng Geng"
            },
            {
                "authorId": "2262513589",
                "name": "Xuansong Xie"
            },
            {
                "authorId": "2262808609",
                "name": "Jingren Zhou"
            }
        ],
        "abstract": "This paper introduces WordArt Designer, a user-driven framework for artistic typography synthesis, relying on the Large Language Model (LLM). The system incorporates four key modules: the LLM Engine, SemTypo, StyTypo, and TexTypo modules. 1) The LLM Engine, empowered by the LLM (e.g., GPT-3.5), interprets user inputs and generates actionable prompts for the other modules, thereby transforming abstract concepts into tangible designs. 2) The SemTypo module optimizes font designs using semantic concepts, striking a balance between artistic transformation and readability. 3) Building on the semantic layout provided by the SemTypo module, the StyTypo module creates smooth, refined images. 4) The TexTypo module further enhances the design's aesthetics through texture rendering, enabling the generation of inventive textured fonts. Notably, WordArt Designer highlights the fusion of generative AI with artistic typography. Experience its capabilities on ModelScope: https://www.modelscope.cn/studios/WordArt/WordArt."
    },
    {
        "paperId": "a952e79b7ceef1994a8c26c0c8d0d5445cf4f9cf",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "On the Automatic Generation and Simplification of Children's Stories",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.18502, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-27",
        "authors": [
            {
                "authorId": "2262444173",
                "name": "Maria R. Valentini"
            },
            {
                "authorId": "2262446346",
                "name": "Jennifer Weber"
            },
            {
                "authorId": "2262444145",
                "name": "Jesus Salcido"
            },
            {
                "authorId": "2262444698",
                "name": "T\u00e9a Y. Wright"
            },
            {
                "authorId": "2262444462",
                "name": "Eliana Colunga"
            },
            {
                "authorId": "2262444629",
                "name": "Katharina Kann"
            }
        ],
        "abstract": "With recent advances in large language models (LLMs), the concept of automatically generating children's educational materials has become increasingly realistic. Working toward the goal of age-appropriate simplicity in generated educational texts, we first examine the ability of several popular LLMs to generate stories with properly adjusted lexical and readability levels. We find that, in spite of the growing capabilities of LLMs, they do not yet possess the ability to limit their vocabulary to levels appropriate for younger age groups. As a second experiment, we explore the ability of state-of-the-art lexical simplification models to generalize to the domain of children's stories and, thus, create an efficient pipeline for their automatic generation. In order to test these models, we develop a dataset of child-directed lexical simplification instances, with examples taken from the LLM-generated stories in our first experiment. We find that, while the strongest-performing current lexical simplification models do not perform as well on material designed for children due to their reliance on large language models behind the scenes, some models that still achieve fairly strong results on general data can mimic or even improve their performance on children-directed data with proper fine-tuning, which we conduct using our newly created child-directed simplification dataset."
    },
    {
        "paperId": "170c5b7e311a5004ed5db5d9eee1b736669273fb",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.18659",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.18659, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-28",
        "authors": [
            {
                "authorId": "2257127695",
                "name": "Hongda Sun"
            },
            {
                "authorId": "2262867071",
                "name": "Weikai Xu"
            },
            {
                "authorId": "2257333016",
                "name": "Wei Liu"
            },
            {
                "authorId": "2257013742",
                "name": "Jian Luan"
            },
            {
                "authorId": "2257388949",
                "name": "Bin Wang"
            },
            {
                "authorId": "2232780079",
                "name": "Shuo Shang"
            },
            {
                "authorId": "2263887786",
                "name": "Ji-Rong Wen"
            },
            {
                "authorId": "2257014132",
                "name": "Rui Yan"
            }
        ],
        "abstract": "Recent advances in large language models (LLMs) have revolutionized the landscape of reasoning tasks. To enhance the capabilities of LLMs to emulate human reasoning, prior studies have focused on modeling reasoning steps using various thought structures like chains, trees, or graphs. However, LLM-based reasoning still encounters the following challenges: (1) Limited adaptability of preset structures to diverse tasks; (2) Insufficient precision in exploiting known conditions to derive new ones; and (3) Inadequate consideration of historical reasoning experiences for subsequent reasoning steps. To this end, we propose DetermLR, a novel perspective that rethinks the reasoning process as an evolution from indeterminacy to determinacy. First, we categorize known conditions into two types: determinate and indeterminate premises This provides an oveall direction for the reasoning process and guides LLMs in converting indeterminate data into progressively determinate insights. Subsequently, we leverage quantitative measurements to prioritize more relevant premises to explore new insights. Furthermore, we automate the storage and extraction of available premises and reasoning paths with reasoning memory, preserving historical reasoning details for subsequent reasoning steps. Comprehensive experimental results demonstrate that DetermLR surpasses all baselines on various logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, PrOntoQA, and LogicalDeduction. Compared to previous multi-step reasoning methods, DetermLR achieves higher accuracy with fewer reasoning steps, highlighting its superior efficiency and effectiveness in solving logical reasoning tasks."
    },
    {
        "paperId": "9ba36db0d9f2d586abaa85ea6a0b48c609c5c9ec",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Are NLP Models Good at Tracing Thoughts: An Overview of Narrative Understanding",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.18783, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-28",
        "authors": [
            {
                "authorId": "2131133148",
                "name": "Lixing Zhu"
            },
            {
                "authorId": "2047279522",
                "name": "Runcong Zhao"
            },
            {
                "authorId": "2253541219",
                "name": "Lin Gui"
            },
            {
                "authorId": "2237125679",
                "name": "Yulan He"
            }
        ],
        "abstract": "Narrative understanding involves capturing the author's cognitive processes, providing insights into their knowledge, intentions, beliefs, and desires. Although large language models (LLMs) excel in generating grammatically coherent text, their ability to comprehend the author's thoughts remains uncertain. This limitation hinders the practical applications of narrative understanding. In this paper, we conduct a comprehensive survey of narrative understanding tasks, thoroughly examining their key features, definitions, taxonomy, associated datasets, training objectives, evaluation metrics, and limitations. Furthermore, we explore the potential of expanding the capabilities of modularized LLMs to address novel narrative understanding tasks. By framing narrative understanding as the retrieval of the author's imaginative cues that outline the narrative structure, our study introduces a fresh perspective on enhancing narrative comprehension."
    },
    {
        "paperId": "ac258100ebe178287ae4ae3dc7ac78f8c27e017d",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.18940, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-29",
        "authors": [
            {
                "authorId": "2170477930",
                "name": "Zelai Xu"
            },
            {
                "authorId": "2117922684",
                "name": "Chao Yu"
            },
            {
                "authorId": "2256992848",
                "name": "Fei Fang"
            },
            {
                "authorId": "2153607473",
                "name": "Yu Wang"
            },
            {
                "authorId": "2108052575",
                "name": "Yi Wu"
            }
        ],
        "abstract": "Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model's training data and results in suboptimal performance. To develop strategic language agents, i.e., agents that generate flexible language actions and possess strong decision-making abilities, we propose a novel framework that powers LLM-based agents with reinforcement learning (RL). We consider Werewolf, a popular social deduction game, as a challenging testbed that emphasizes versatile communication and strategic gameplay. To mitigate the intrinsic bias in language actions, our agents use an LLM to perform deductive reasoning and generate a diverse set of action candidates. Then an RL policy trained to optimize the decision-making ability chooses an action from the candidates to play in the game. Extensive experiments show that our agents overcome the intrinsic bias and outperform existing LLM-based agents in the Werewolf game. We also conduct human-agent experiments and find that our agents achieve human-level performance and demonstrate strong strategic play."
    },
    {
        "paperId": "8d9d724e387079743e719b7f1af257c120eae51e",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Roles of Scaling and Instruction Tuning in Language Perception: Model vs. Human Attention",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.19084, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-29",
        "authors": [
            {
                "authorId": "2264636324",
                "name": "Changjiang Gao"
            },
            {
                "authorId": "2046010",
                "name": "Shujian Huang"
            },
            {
                "authorId": "2264297601",
                "name": "Jixing Li"
            },
            {
                "authorId": "1838162",
                "name": "Jiajun Chen"
            }
        ],
        "abstract": "Recent large language models (LLMs) have revealed strong abilities to understand natural language. Since most of them share the same basic structure, i.e. the transformer block, possible contributors to their success in the training process are scaling and instruction tuning. However, how these factors affect the models' language perception is unclear. This work compares the self-attention of several existing LLMs (LLaMA, Alpaca and Vicuna) in different sizes (7B, 13B, 30B, 65B), together with eye saccade, an aspect of human reading attention, to assess the effect of scaling and instruction tuning on language perception. Results show that scaling enhances the human resemblance and improves the effective attention by reducing the trivial pattern reliance, while instruction tuning does not. However, instruction tuning significantly enhances the models' sensitivity to instructions. We also find that current LLMs are consistently closer to non-native than native speakers in attention, suggesting a sub-optimal language perception of all models. Our code and data used in the analysis is available on GitHub."
    },
    {
        "paperId": "b8ca663060b8537054193833b6fba9bd06d0493b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.19619, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-30",
        "authors": [
            {
                "authorId": "2151006930",
                "name": "Ziqiao Ma"
            },
            {
                "authorId": "2264180551",
                "name": "Jacob Sansom"
            },
            {
                "authorId": "2257344793",
                "name": "Run Peng"
            },
            {
                "authorId": "2257448451",
                "name": "Joyce Chai"
            }
        ],
        "abstract": "Large Language Models (LLMs) have generated considerable interest and debate regarding their potential emergence of Theory of Mind (ToM). Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones primarily focus on different aspects of ToM and are prone to shortcuts and data leakage. In this position paper, we seek to answer two road-blocking questions: (1) How can we taxonomize a holistic landscape of machine ToM? (2) What is a more effective evaluation protocol for machine ToM? Following psychological studies, we taxonomize machine ToM into 7 mental state categories and delineate existing benchmarks to identify under-explored aspects of ToM. We argue for a holistic and situated evaluation of ToM to break ToM into individual components and treat LLMs as an agent who is physically situated in environments and socially situated in interactions with humans. Such situated evaluation provides a more comprehensive assessment of mental states and potentially mitigates the risk of shortcuts and data leakage. We further present a pilot study in a grid world setup as a proof of concept. We hope this position paper can facilitate future research to integrate ToM with LLMs and offer an intuitive means for researchers to better position their work in the landscape of ToM. Project page: https://github.com/Mars-tin/awesome-theory-of-mind"
    },
    {
        "paperId": "dde6c1910d0496c9e5d5483c2c18271a2a660e6c",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.19671, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-30",
        "authors": [
            {
                "authorId": "1471124393",
                "name": "Bram van Dijk"
            },
            {
                "authorId": "2264047994",
                "name": "Tom Kouwenhoven"
            },
            {
                "authorId": "2258846148",
                "name": "M. Spruit"
            },
            {
                "authorId": "2264257819",
                "name": "Max J. van Duijn"
            }
        ],
        "abstract": "Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the debate and critically assess three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language competence; and iii) that language learning in LLMs cannot inform human language learning. Drawing on empirical and theoretical arguments, we show that these points need more nuance. Second, we outline a pragmatic perspective on the issue of `real' understanding and intentionality in LLMs. Understanding and intentionality pertain to unobservable mental states we attribute to other humans because they have pragmatic value: they allow us to abstract away from complex underlying mechanics and predict behaviour effectively. We reflect on the circumstances under which it would make sense for humans to similarly attribute mental states to LLMs, thereby outlining a pragmatic philosophical context for LLMs as an increasingly prominent technology in society."
    },
    {
        "paperId": "708450b22ed062da7fa577e10088f25023b1437c",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.19677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-30",
        "authors": [
            {
                "authorId": "2253756381",
                "name": "Allen Nie"
            },
            {
                "authorId": "49889860",
                "name": "Yuhui Zhang"
            },
            {
                "authorId": "2264180507",
                "name": "Atharva Amdekar"
            },
            {
                "authorId": "2262443893",
                "name": "Chris Piech"
            },
            {
                "authorId": "2266400315",
                "name": "Tatsunori Hashimoto"
            },
            {
                "authorId": "2264218532",
                "name": "Tobias Gerstenberg"
            }
        ],
        "abstract": "Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions."
    },
    {
        "paperId": "f0db9f97b5b84ab0cfaa472206c1627e65c0d373",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "LILO: Learning Interpretable Libraries by Compressing and Documenting Code",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.19791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-10-30",
        "authors": [
            {
                "authorId": "35748708",
                "name": "Gabriel Grand"
            },
            {
                "authorId": "2147199509",
                "name": "L. Wong"
            },
            {
                "authorId": "2058735196",
                "name": "Matthew Bowers"
            },
            {
                "authorId": "1689656957",
                "name": "Theo X. Olausson"
            },
            {
                "authorId": "2264511729",
                "name": "Muxin Liu"
            },
            {
                "authorId": "2238317928",
                "name": "J. B. Tenenbaum"
            },
            {
                "authorId": "2264203454",
                "name": "Jacob Andreas"
            }
        ],
        "abstract": "While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synthesis benchmarks for string editing, scene reasoning, and graphics composition. Compared to existing neural and symbolic methods - including the state-of-the-art library learning algorithm DreamCoder - LILO solves more complex tasks and learns richer libraries that are grounded in linguistic knowledge."
    },
    {
        "paperId": "debe978e02b664fb7254b6c7b58a74c09ce897e3",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.00262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-01",
        "authors": [
            {
                "authorId": "145843537",
                "name": "Yang Deng"
            },
            {
                "authorId": "2108145336",
                "name": "Wenxuan Zhang"
            },
            {
                "authorId": "2253479909",
                "name": "Wai Lam"
            },
            {
                "authorId": "2241348826",
                "name": "See-Kiong Ng"
            },
            {
                "authorId": "2257036129",
                "name": "Tat-Seng Chua"
            }
        ],
        "abstract": "Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues."
    },
    {
        "paperId": "96c2824547ff669653e8d058da49dbc8c71ad9c9",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.00273, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-01",
        "authors": [
            {
                "authorId": "2253812531",
                "name": "Yirong Chen"
            },
            {
                "authorId": "2667341",
                "name": "Xiaofen Xing"
            },
            {
                "authorId": "2264612731",
                "name": "Jingkai Lin"
            },
            {
                "authorId": "2261459881",
                "name": "Huimin Zheng"
            },
            {
                "authorId": "2261393114",
                "name": "Zhenyu Wang"
            },
            {
                "authorId": "2261395384",
                "name": "Qi Liu"
            },
            {
                "authorId": "2253898399",
                "name": "Xiangmin Xu"
            }
        ],
        "abstract": "Large language models (LLMs) have been widely applied in various fields due to their excellent capability for memorizing knowledge and chain of thought (CoT). When these language models are applied in the field of psychological counseling, they often rush to provide universal advice. However, when users seek psychological support, they need to gain empathy, trust, understanding and comfort, rather than just reasonable advice. To this end, we constructed a multi-turn empathetic conversation dataset of more than 2 million samples, in which the input is the multi-turn conversation context, and the target is empathetic responses that cover expressions such as questioning, comfort, recognition, listening, trust, emotional support, etc. Experiments have shown that the empathy ability of LLMs can be significantly enhanced when finetuning by using multi-turn dialogue history and responses that are closer to the expression of a psychological consultant."
    },
    {
        "paperId": "578887182fcf49cfd4d99fa3b1de200e8ebb2c45",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.01270, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-02",
        "authors": [
            {
                "authorId": "33770417",
                "name": "Indira Sen"
            },
            {
                "authorId": "2264963370",
                "name": "Dennis Assenmacher"
            },
            {
                "authorId": "3071381",
                "name": "Mattia Samory"
            },
            {
                "authorId": "1736067",
                "name": "Isabelle Augenstein"
            },
            {
                "authorId": "2253664072",
                "name": "W. V. D. Aalst"
            },
            {
                "authorId": "2265588060",
                "name": "Claudia Wagner"
            }
        ],
        "abstract": "NLP models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. Therefore, it is imperative that these models are robust to spurious features. Past work has attempted to tackle such spurious features using training data augmentation, including Counterfactually Augmented Data (CADs). CADs introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features. However, manually generating CADs can be time-consuming and expensive. Hence in this work, we assess if this task can be automated using generative NLP models. We automatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate their usefulness in improving model robustness compared to manually-generated CADs. By testing both model performance on multiple out-of-domain test sets and individual data point efficacy, our results show that while manual CADs are still the most effective, CADs generated by ChatGPT come a close second. One key reason for the lower performance of automated methods is that the changes they introduce are often insufficient to flip the original label."
    },
    {
        "paperId": "b80a7663585123abc53c10f8aac9bd234eedf063",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "DialogBench: Evaluating LLMs as Human-like Dialogue Systems",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.01677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-03",
        "authors": [
            {
                "authorId": "2265383868",
                "name": "Jiao Ou"
            },
            {
                "authorId": "2265386838",
                "name": "Junda Lu"
            },
            {
                "authorId": "2257133304",
                "name": "Che Liu"
            },
            {
                "authorId": "2152987610",
                "name": "Yihong Tang"
            },
            {
                "authorId": "2257136363",
                "name": "Fuzheng Zhang"
            },
            {
                "authorId": "2257381268",
                "name": "Di Zhang"
            },
            {
                "authorId": "2257052464",
                "name": "Zhongyuan Wang"
            },
            {
                "authorId": "2238953242",
                "name": "Kun Gai"
            }
        ],
        "abstract": "Large language models (LLMs) have achieved remarkable breakthroughs in new dialogue capabilities by leveraging instruction tuning,which refreshes human impressions of dialogue systems. The long-standing goal of dialogue systems is to be human-like enough to establish long-term connections with users. Therefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems. In this paper, we propose DialogBench, a dialogue evaluation benchmark that contains 12 dialogue tasks to probe the capabilities of LLMs as human-like dialogue systems should have. Specifically, we prompt GPT-4 to generate evaluation instances for each task. We first design the basic prompt based on widely used design principles and further mitigate the existing biases to generate higher-quality evaluation instances. Our extensive tests on English and Chinese DialogBench of 26 LLMs show that instruction tuning improves the human likeness of LLMs to a certain extent, but most LLMs still have much room for improvement as human-like dialogue systems. Interestingly, results also show that the positioning of assistant AI can make instruction tuning weaken the human emotional perception of LLMs and their mastery of information about human daily life."
    },
    {
        "paperId": "9cfb4fd70cee64cd0d300472147c4fda7962c93b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Towards Concept-Aware Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.01866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-03",
        "authors": [
            {
                "authorId": "1683047517",
                "name": "Chen Shani"
            },
            {
                "authorId": "49688681",
                "name": "J. Vreeken"
            },
            {
                "authorId": "1805894",
                "name": "Dafna Shahaf"
            }
        ],
        "abstract": "Concepts play a pivotal role in various human cognitive functions, including learning, reasoning and communication. However, there is very little work on endowing machines with the ability to form and reason with concepts. In particular, state-of-the-art large language models (LLMs) work at the level of tokens, not concepts. In this work, we analyze how well contemporary LLMs capture human concepts and their structure. We then discuss ways to develop concept-aware LLMs, taking place at different stages of the pipeline. We sketch a method for pretraining LLMs using concepts, and also explore the simpler approach that uses the output of existing LLMs. Despite its simplicity, our proof-of-concept is shown to better match human intuition, as well as improve the robustness of predictions. These preliminary results underscore the promise of concept-aware LLMs."
    },
    {
        "paperId": "79729ed54ad03fd403d88f8d1543b49c7a58b973",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "The language of prompting: What linguistic properties make a prompt successful?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.01967, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-03",
        "authors": [
            {
                "authorId": "2262444923",
                "name": "Alina Leidinger"
            },
            {
                "authorId": "2238624162",
                "name": "R. Rooij"
            },
            {
                "authorId": "2262445370",
                "name": "Ekaterina Shutova"
            }
        ],
        "abstract": "The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with task performance. In this work, we investigate how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure. We investigate both grammatical properties such as mood, tense, aspect and modality, as well as lexico-semantic variation through the use of synonyms. Our findings contradict the common assumption that LLMs achieve optimal performance on lower perplexity prompts that reflect language use in pretraining or instruction-tuning data. Prompts transfer poorly between datasets or models, and performance cannot generally be explained by perplexity, word frequency, ambiguity or prompt length. Based on our results, we put forward a proposal for a more robust and comprehensive evaluation standard for prompting research."
    },
    {
        "paperId": "6f77532da1dea0be1b35b9550337a5fbf7d5c3fa",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.02684, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-05",
        "authors": [
            {
                "authorId": "2265575920",
                "name": "Zeren Chen"
            },
            {
                "authorId": "2265646739",
                "name": "Ziqin Wang"
            },
            {
                "authorId": "2265721142",
                "name": "Zhen Wang"
            },
            {
                "authorId": "2265615948",
                "name": "Huayang Liu"
            },
            {
                "authorId": "13050405",
                "name": "Zhen-fei Yin"
            },
            {
                "authorId": "2265617005",
                "name": "Si Liu"
            },
            {
                "authorId": "2290983876",
                "name": "Lu Sheng"
            },
            {
                "authorId": "2254269925",
                "name": "Wanli Ouyang"
            },
            {
                "authorId": "2265493981",
                "name": "Yu Qiao"
            },
            {
                "authorId": "2254280929",
                "name": "Jing Shao"
            }
        ],
        "abstract": "Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/."
    },
    {
        "paperId": "bae9c39adf7070ef37ae989bc8716a88691c33f2",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "CausalCite: A Causal Formulation of Paper Citations",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.02790, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-05",
        "authors": [
            {
                "authorId": "2265495740",
                "name": "Ishan Kumar"
            },
            {
                "authorId": "2111472502",
                "name": "Zhijing Jin"
            },
            {
                "authorId": "1994239028",
                "name": "Ehsan Mokhtarian"
            },
            {
                "authorId": "2265515815",
                "name": "Siyuan Guo"
            },
            {
                "authorId": "2265625277",
                "name": "Yuen Chen"
            },
            {
                "authorId": "2237987614",
                "name": "Negar Kiyavash"
            },
            {
                "authorId": "2790926",
                "name": "Mrinmaya Sachan"
            },
            {
                "authorId": "2131136841",
                "name": "Bernhard Schoelkopf"
            }
        ],
        "abstract": "Citation count of a paper is a commonly used proxy for evaluating the significance of a paper in the scientific community. Yet citation measures are widely criticized for failing to accurately reflect the true impact of a paper. Thus, we propose CausalCite, a new way to measure the significance of a paper by assessing the causal impact of the paper on its follow-up papers. CausalCite is based on a novel causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. TextMatch encodes each paper using text embeddings from large language models (LLMs), extracts similar samples by cosine similarity, and synthesizes a counterfactual sample as the weighted average of similar papers according to their similarity values. We demonstrate the effectiveness of CausalCite on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, (test-of-time) awards for past papers, and its stability across various subfields of AI. We also provide a set of findings that can serve as suggested ways for future researchers to use our metric for a better understanding of the quality of a paper. Our code is available at https://github.com/causalNLP/causal-cite."
    },
    {
        "paperId": "c0230760f644f6b7538d93e4296a5e9aa7028e45",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.03099, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-06",
        "authors": [
            {
                "authorId": "2265527327",
                "name": "Le Yu"
            },
            {
                "authorId": "48613402",
                "name": "Yu Bowen"
            },
            {
                "authorId": "46493167",
                "name": "Haiyang Yu"
            },
            {
                "authorId": "2257407873",
                "name": "Fei Huang"
            },
            {
                "authorId": "1527090216",
                "name": "Yongbin Li"
            }
        ],
        "abstract": "In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard."
    },
    {
        "paperId": "6974eeca4aa3e2fc904364ac7b3eca9cbdf9ca7c",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Instructed Language Models with Retrievers Are Powerful Entity Linkers",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.03250, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-06",
        "authors": [
            {
                "authorId": "2186608582",
                "name": "Zilin Xiao"
            },
            {
                "authorId": "50175330",
                "name": "Ming Gong"
            },
            {
                "authorId": "2265516958",
                "name": "Jie Wu"
            },
            {
                "authorId": "2265515206",
                "name": "Xingyao Zhang"
            },
            {
                "authorId": "24962156",
                "name": "Linjun Shou"
            },
            {
                "authorId": "2247751671",
                "name": "Jian Pei"
            },
            {
                "authorId": "71790825",
                "name": "Daxin Jiang"
            }
        ],
        "abstract": "Generative approaches powered by large language models (LLMs) have demonstrated emergent abilities in tasks that require complex reasoning abilities. Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base. We present Instructed Generative Entity Linker (INSGENEL), the first approach that enables casual language models to perform entity linking over knowledge bases. Several methods to equip language models with EL capability were proposed in this work, including (i) a sequence-to-sequence training EL objective with instruction-tuning, (ii) a novel generative EL framework based on a light-weight potential mention retriever that frees the model from heavy and non-parallelizable decoding, achieving 4$\\times$ speedup without compromise on linking metrics. INSGENEL outperforms previous generative alternatives with +6.8 F1 points gain on average, also with a huge advantage in training data efficiency and training compute consumption. In addition, our skillfully engineered in-context learning (ICL) framework for EL still lags behind INSGENEL significantly, reaffirming that the EL task remains a persistent hurdle for general LLMs."
    },
    {
        "paperId": "f133c66cccde9389b78388edfd8ab6b438f2562c",
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "title": "Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design",
        "openAccessPdf": {
            "url": "https://doi.org/10.1162/tacl_a_00685",
            "status": "GOLD",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.04076, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-07",
        "authors": [
            {
                "authorId": "2219036626",
                "name": "Lindia Tjuatja"
            },
            {
                "authorId": "15110752",
                "name": "Valerie Chen"
            },
            {
                "authorId": "2265720924",
                "name": "Sherry Tongshuang Wu"
            },
            {
                "authorId": "145532827",
                "name": "Ameet Talwalkar"
            },
            {
                "authorId": "2265547593",
                "name": "Graham Neubig"
            }
        ],
        "abstract": "Abstract One widely cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording\u2014but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of \u201cprompts\u201d have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior.1"
    },
    {
        "paperId": "808ab377131e511c922552376d83acdbfa1a7208",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Speech language models lack important brain-relevant semantics",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.04664, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-08",
        "authors": [
            {
                "authorId": "8307724",
                "name": "S. Oota"
            },
            {
                "authorId": "2265651199",
                "name": "Emin cCelik"
            },
            {
                "authorId": "2265650213",
                "name": "Fatma Deniz"
            },
            {
                "authorId": "2822168",
                "name": "Mariya Toneva"
            }
        ],
        "abstract": "Despite known differences between reading and listening in the brain, recent work has shown that text-based language models predict both text-evoked and speech-evoked brain activity to an impressive degree. This poses the question of what types of information language models truly predict in the brain. We investigate this question via a direct approach, in which we systematically remove specific low-level stimulus features (textual, speech, and visual) from language model representations to assess their impact on alignment with fMRI brain recordings during reading and listening. Comparing these findings with speech-based language models reveals starkly different effects of low-level features on brain alignment. While text-based models show reduced alignment in early sensory regions post-removal, they retain significant predictive power in late language regions. In contrast, speech-based models maintain strong alignment in early auditory regions even after feature removal but lose all predictive power in late language regions. These results suggest that speech-based models provide insights into additional information processed by early auditory regions, but caution is needed when using them to model processing in late language regions. We make our code publicly available. [https://github.com/subbareddy248/speech-llm-brain]"
    },
    {
        "paperId": "fc6d9e071e07ead88343a746c654aef7b4f3c5a7",
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "title": "How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure",
        "openAccessPdf": {
            "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00608/2178846/tacl_a_00608.pdf",
            "status": "GOLD",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.04900, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-01",
        "authors": [
            {
                "authorId": "2260408544",
                "name": "Michael Wilson"
            },
            {
                "authorId": "153098735",
                "name": "Jackson Petty"
            },
            {
                "authorId": "2265649172",
                "name": "Robert Frank"
            }
        ],
        "abstract": "Abstract Language models are typically evaluated on their success at predicting the distribution of specific words in specific contexts. Yet linguistic knowledge also encodes relationships between contexts, allowing inferences between word distributions. We investigate the degree to which pre-trained transformer-based large language models (LLMs) represent such relationships, focusing on the domain of argument structure. We find that LLMs perform well in generalizing the distribution of a novel noun argument between related contexts that were seen during pre-training (e.g., the active object and passive subject of the verb spray), succeeding by making use of the semantically organized structure of the embedding space for word embeddings. However, LLMs fail at generalizations between related contexts that have not been observed during pre-training, but which instantiate more abstract, but well-attested structural generalizations (e.g., between the active object and passive subject of an arbitrary verb). Instead, in this case, LLMs show a bias to generalize based on linear order. This finding points to a limitation with current models and points to a reason for which their training is data-intensive.1"
    },
    {
        "paperId": "d3b5705d6ea1b8ec25504f46d9ff99a65a294e7c",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.05085, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-09",
        "authors": [
            {
                "authorId": "2266166126",
                "name": "Aditi Mishra"
            },
            {
                "authorId": "37455401",
                "name": "Sajjadur Rahman"
            },
            {
                "authorId": "2143468335",
                "name": "H. Kim"
            },
            {
                "authorId": "2265753908",
                "name": "Kushan Mitra"
            },
            {
                "authorId": "2265753807",
                "name": "Estevam R. Hruschka"
            }
        ],
        "abstract": "Large language models (LLMs) are proficient at generating fluent text with minimal task-specific supervision. Yet, their ability to provide well-grounded rationalizations for knowledge-intensive tasks remains under-explored. Such tasks, like commonsense multiple-choice questions, require rationales based on world knowledge to support predictions and refute alternate options. We consider the task of generating knowledge-guided rationalization in natural language by using expert-written examples in a few-shot manner. Surprisingly, crowd-workers preferred knowledge-grounded rationales over crowdsourced rationalizations, citing their factuality, sufficiency, and comprehensive refutations. Although LLMs-generated rationales were preferable, further improvements in conciseness and novelty are required. In another study, we show how rationalization of incorrect model predictions erodes humans' trust in LLM-generated rationales. Motivated by these observations, we create a two-stage pipeline to review task predictions and eliminate potential incorrect decisions before rationalization, enabling trustworthy rationale generation."
    },
    {
        "paperId": "462041e29f2e4e8d78b3214ee1a286865bc68721",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.06025, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-10",
        "authors": [
            {
                "authorId": "151472012",
                "name": "Yuanhe Tian"
            },
            {
                "authorId": "94166197",
                "name": "Ruyi Gan"
            },
            {
                "authorId": "2257459089",
                "name": "Yan Song"
            },
            {
                "authorId": "2265618872",
                "name": "Jiaxing Zhang"
            },
            {
                "authorId": "2266360133",
                "name": "Yongdong Zhang"
            }
        ],
        "abstract": "Recently, the increasing demand for superior medical services has highlighted the discrepancies in the medical infrastructure. With big data, especially texts, forming the foundation of medical services, there is an exigent need for effective natural language processing (NLP) solutions tailored to the healthcare domain. Conventional approaches leveraging pre-trained models present promising results in this domain and current large language models (LLMs) offer advanced foundation for medical text processing. However, most medical LLMs are trained only with supervised fine-tuning (SFT), even though it efficiently empowers LLMs to understand and respond to medical instructions but is ineffective in learning domain knowledge and aligning with human preference. In this work, we propose ChiMed-GPT, a new benchmark LLM designed explicitly for Chinese medical domain, and undergoes a comprehensive training regime with pre-training, SFT, and RLHF. Evaluations on tasks including information extraction, question answering, and dialogue generation demonstrate ChiMed-GPT's superior performance over general domain LLMs. Furthermore, we analyze possible biases through prompting ChiMed-GPT to perform attitude scales regarding discrimination of patients, so as to contribute to further responsible development of LLMs in the medical domain. The code and model are released at https://github.com/synlp/ChiMed-GPT."
    },
    {
        "paperId": "6fb36c2d53ce4554b71473984a27fb961ffaafbf",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "LLMs Learn Task Heuristics from Demonstrations: A Heuristic-Driven Prompting Strategy for Document-Level Event Argument Extraction",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2311.06555",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.06555, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-11",
        "authors": [
            {
                "authorId": "2111825433",
                "name": "Hanzhang Zhou"
            },
            {
                "authorId": "2266439699",
                "name": "Junlang Qian"
            },
            {
                "authorId": "2112599636",
                "name": "Zijian Feng"
            },
            {
                "authorId": "2266420870",
                "name": "Hui Lu"
            },
            {
                "authorId": "2166589821",
                "name": "Zixiao Zhu"
            },
            {
                "authorId": "2128504277",
                "name": "Kezhi Mao"
            }
        ],
        "abstract": "In this study, we investigate in-context learning (ICL) in document-level event argument extraction (EAE) to alleviate the dependency on large-scale labeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy (HD-LoA) prompting to address the challenge of example selection and to develop a prompting strategy tailored for EAE. Specifically, we hypothesize and validate that LLMs learn task-specific heuristics from demonstrations via ICL. Building upon this hypothesis, we introduce an explicit heuristic-driven demonstration construction approach, which transforms the haphazard example selection process into a methodical method that emphasizes task heuristics. Additionally, inspired by the analogical reasoning of human, we propose the link-of-analogy prompting, which enables LLMs to process new situations by drawing analogies to known situations, enhancing their performance on unseen classes beyond limited ICL examples. Experiments show that our method outperforms existing prompting methods and few-shot supervised learning methods on document-level EAE datasets. Additionally, the HD-LoA prompting shows effectiveness in diverse tasks like sentiment analysis and natural language inference, demonstrating its broad adaptability."
    },
    {
        "paperId": "533f5ec8d126408247ad9c9ec5830ed00bfc7501",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Large Language Models are In-context Teachers for Knowledge Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.06985, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-12",
        "authors": [
            {
                "authorId": "2266424024",
                "name": "Jiachen Zhao"
            },
            {
                "authorId": "1576489304",
                "name": "Zonghai Yao"
            },
            {
                "authorId": "2261462978",
                "name": "Zhichao Yang"
            },
            {
                "authorId": "2261455807",
                "name": "Hong Yu"
            }
        ],
        "abstract": "In this work, we study in-context teaching (ICT), where a teacher provides in-context example rationales to teach a student to reason over unseen cases. Human teachers are usually required to craft in-context demonstrations, which are costly and have high variance. We ask whether a large language model (LLM) can serve as a more effective in-context teacher for itself or other LLMs, compared to humans. Inspired by the Encoding Specificity Hypothesis from human episodic memory, we hypothesize that in-context exemplars crafted by the teacher should match the training data of the student. This hypothesis motivates us to propose Self-Explain where an LLM's self-elicited explanations are used as in-context demonstrations for prompting it as they are generalized from the model's training examples. Self-Explain is shown to significantly outperform using human-crafted exemplars and other baselines. Furthermore, we reveal that for ICT, rationales from different teacher LLMs or human experts that more resemble the student LLM's self-explanations are better in-context demonstrations. This supports our encoding specificity hypothesis. We then propose Teach-Back that aligns a teacher LLM with the student to enhance the ICT performance. For example, Teach-Back enables a 7B model to teach the much larger GPT-3.5 in context, surpassing human teachers by around 5% in test accuracy on medical question answering."
    },
    {
        "paperId": "715cd3244bd58ed966bf07c5a3871a46f2666cbe",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.07099, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-13",
        "authors": [
            {
                "authorId": "2266425446",
                "name": "Yue Yu"
            },
            {
                "authorId": "2266463492",
                "name": "Jiaming Shen"
            },
            {
                "authorId": "2239381730",
                "name": "Tianqi Liu"
            },
            {
                "authorId": "2266819166",
                "name": "Zhen Qin"
            },
            {
                "authorId": "2266388043",
                "name": "Jing Nathan Yan"
            },
            {
                "authorId": "2239559694",
                "name": "Jialu Liu"
            },
            {
                "authorId": "2256776233",
                "name": "Chao Zhang"
            },
            {
                "authorId": "1815447",
                "name": "Michael Bendersky"
            }
        ],
        "abstract": "Large language models (LLMs) have shown remarkable capabilities in various natural language understanding tasks. With only a few demonstration examples, these LLMs can quickly adapt to target tasks without expensive gradient updates. Common strategies to boost such 'in-context' learning ability are to ensemble multiple model decoded results and require the model to generate an explanation along with the prediction. However, these models often treat different class predictions equally and neglect the potential discrepancy between the explanations and predictions. To fully unleash the power of explanations, we propose EASE, an Explanation-Aware Soft Ensemble framework to empower in-context learning with LLMs. We design two techniques, explanation-guided ensemble, and soft probability aggregation, to mitigate the effect of unreliable explanations and improve the consistency between explanations and final predictions. Experiments on seven natural language understanding tasks and four varying-size LLMs demonstrate the effectiveness of our proposed framework."
    },
    {
        "paperId": "28685ab4bb673cd7aac4f5711b0882d08d2fa7c7",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.07237, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-13",
        "authors": [
            {
                "authorId": "2144403358",
                "name": "Huihan Li"
            },
            {
                "authorId": "2266397759",
                "name": "Yuting Ning"
            },
            {
                "authorId": "1435033296",
                "name": "Zeyi Liao"
            },
            {
                "authorId": "2266421644",
                "name": "Siyuan Wang"
            },
            {
                "authorId": "2266532556",
                "name": "Xiang Lorraine Li"
            },
            {
                "authorId": "50085131",
                "name": "Ximing Lu"
            },
            {
                "authorId": "2266434757",
                "name": "Wenting Zhao"
            },
            {
                "authorId": "2223951216",
                "name": "Faeze Brahman"
            },
            {
                "authorId": "2266363632",
                "name": "Yejin Choi"
            },
            {
                "authorId": "2228515529",
                "name": "Xiang Ren"
            }
        ],
        "abstract": "To effectively use large language models (LLMs) for real-world queries, it is imperative that they generalize to the long-tail distribution, i.e. rare examples where models exhibit low confidence. In this work, we take the first step towards evaluating LLMs in the long-tail distribution of inferential knowledge. We exemplify long-tail evaluation on the Natural Language Inference task. First, we introduce Logic-Induced-Knowledge-Search (LINK), a systematic long-tail data generation framework, to obtain factually-correct yet long-tail inferential statements. LINK uses variable-wise prompting grounded on symbolic rules to seek low-confidence statements while ensuring factual correctness. We then use LINK to curate Logic-Induced-Long-Tail (LINT), a large-scale long-tail inferential knowledge dataset that contains 108K statements spanning four domains. We evaluate popular LLMs on LINT; we find that state-of-the-art LLMs show significant performance drop (21% relative drop for GPT4) on long-tail data as compared to on head distribution data, and smaller models show even more generalization weakness. These results further underscore the necessity of long-tail evaluation in developing generalizable LLMs."
    },
    {
        "paperId": "175ccc099f2a91005f53d752c09a74b8b91fdc38",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "An Analysis and Mitigation of the Reversal Curse",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2311.07468",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.07468, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-13",
        "authors": [
            {
                "authorId": "66261602",
                "name": "Ang Lv"
            },
            {
                "authorId": "2266486204",
                "name": "Kaiyi Zhang"
            },
            {
                "authorId": "1889683",
                "name": "Shufang Xie"
            },
            {
                "authorId": "2071635049",
                "name": "Quan Tu"
            },
            {
                "authorId": "2266420263",
                "name": "Yuhan Chen"
            },
            {
                "authorId": "2263887786",
                "name": "Ji-Rong Wen"
            },
            {
                "authorId": "2172312251",
                "name": "Rui Yan"
            }
        ],
        "abstract": "Recent research observed a noteworthy phenomenon in large language models (LLMs), referred to as the ``reversal curse.'' The reversal curse is that when dealing with two entities, denoted as $a$ and $b$, connected by their relation $R$ and its inverse $R^{-1}$, LLMs excel in handling sequences in the form of ``$aRb$,'' but encounter challenges when processing ``$bR^{-1}a$,'' whether in generation or comprehension. For instance, GPT-4 can accurately respond to the query ``Tom Cruise's mother is?'' with ``Mary Lee Pfeiffer,'' but it struggles to provide a satisfactory answer when asked ``Mary Lee Pfeiffer's son is?'' In this paper, we undertake the first-ever study of how the reversal curse happens in LLMs. Our investigations reveal that the reversal curse can stem from the specific training objectives, which become particularly evident in the widespread use of next-token prediction within most causal language models. We hope this initial investigation can draw more attention to the reversal curse, as well as other underlying limitations in current LLMs."
    },
    {
        "paperId": "cd9f3efbe5995855e646ea1cfd368a3ed067d1a3",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2311.07532",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.07532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-13",
        "authors": [
            {
                "authorId": "2216486213",
                "name": "Nishant Balepur"
            },
            {
                "authorId": "2166311031",
                "name": "Shramay Palta"
            },
            {
                "authorId": "2034613",
                "name": "Rachel Rudinger"
            }
        ],
        "abstract": "Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE), when used with COT, can enhance self-consistency, interpretability, and tasks such as medical diagnoses of exclusion. Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct error analyses and give suggestions for future work."
    },
    {
        "paperId": "2e7cc95145665bae4fa98b7f81b9d551f1b1c021",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Using Natural Language Explanations to Improve Robustness of In-context Learning for Natural Language Inference",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.07556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-13",
        "authors": [
            {
                "authorId": "2288269593",
                "name": "Xuanli He"
            },
            {
                "authorId": "39417610",
                "name": "Yuxiang Wu"
            },
            {
                "authorId": "3317152",
                "name": "Oana-Maria Camburu"
            },
            {
                "authorId": "3051815",
                "name": "Pasquale Minervini"
            },
            {
                "authorId": "1918552",
                "name": "Pontus Stenetorp"
            }
        ],
        "abstract": "Recent studies demonstrated that large language models (LLMs) can excel in many tasks via in-context learning (ICL). However, recent works show that ICL-prompted models tend to produce inaccurate results when presented with adversarial inputs. In this work, we investigate whether augmenting ICL with natural language explanations (NLEs) improves the robustness of LLMs on adversarial datasets covering natural language inference and paraphrasing identification. We prompt LLMs with a small set of human-generated NLEs to produce further NLEs, yielding more accurate results than both a zero-shot-ICL setting and using only human-generated NLEs. Our results on five popular LLMs (GPT3.5-turbo, Llama2, Vicuna, Zephyr, and Mistral) show that our approach yields over 6% improvement over baseline approaches for eight adversarial datasets: HANS, ISCS, NaN, ST, PICD, PISP, ANLI, and PAWS. Furthermore, previous studies have demonstrated that prompt selection strategies significantly enhance ICL on in-distribution test sets. However, our findings reveal that these strategies do not match the efficacy of our approach for robustness evaluations, resulting in an accuracy drop of 8% compared to the proposed approach."
    },
    {
        "paperId": "d0cb9bfa6432ecf749bb2dc503d41e91caf59de3",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Fair Abstractive Summarization of Diverse Perspectives",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.07884, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-14",
        "authors": [
            {
                "authorId": "2108051142",
                "name": "Yusen Zhang"
            },
            {
                "authorId": "2266469940",
                "name": "Nan Zhang"
            },
            {
                "authorId": "2267023746",
                "name": "Yixin Liu"
            },
            {
                "authorId": "22281632",
                "name": "A. R. Fabbri"
            },
            {
                "authorId": "2266501130",
                "name": "Junru Liu"
            },
            {
                "authorId": "83757854",
                "name": "Ryo Kamoi"
            },
            {
                "authorId": "2266470204",
                "name": "Xiaoxin Lu"
            },
            {
                "authorId": "2266469680",
                "name": "Caiming Xiong"
            },
            {
                "authorId": "2266698166",
                "name": "Jieyu Zhao"
            },
            {
                "authorId": "2250049814",
                "name": "Dragomir R. Radev"
            },
            {
                "authorId": "2257972424",
                "name": "Kathleen McKeown"
            },
            {
                "authorId": "144142360",
                "name": "Rui Zhang"
            }
        ],
        "abstract": "People from different social and demographic groups express diverse perspectives and conflicting opinions on a broad set of topics such as product reviews, healthcare, law, and politics. A fair summary should provide a comprehensive coverage of diverse perspectives without underrepresenting certain groups. However, current work in summarization metrics and Large Language Models (LLMs) evaluation has not explored fair abstractive summarization. In this paper, we systematically investigate fair abstractive summarization for user-generated data. We first formally define fairness in abstractive summarization as not underrepresenting perspectives of any groups of people, and we propose four reference-free automatic metrics by measuring the differences between target and source perspectives. We evaluate nine LLMs, including three GPT models, four LLaMA models, PaLM 2, and Claude, on six datasets collected from social media, online reviews, and recorded transcripts. Experiments show that both the model-generated and the human-written reference summaries suffer from low fairness. We conduct a comprehensive analysis of the common factors influencing fairness and propose three simple but effective methods to alleviate unfair summarization. Our dataset and code are available at https://github.com/psunlpgroup/FairSumm."
    },
    {
        "paperId": "6233b5863f9a0e8bacce47ce21bc3e81c09497bd",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.07954, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-14",
        "authors": [
            {
                "authorId": "2151067405",
                "name": "Ruixin Hong"
            },
            {
                "authorId": "2254831297",
                "name": "Hongming Zhang"
            },
            {
                "authorId": "2266464882",
                "name": "Xinyu Pang"
            },
            {
                "authorId": "2261392681",
                "name": "Dong Yu"
            },
            {
                "authorId": "2164309584",
                "name": "Changshui Zhang"
            }
        ],
        "abstract": "Logical reasoning has been an ongoing pursuit in the field of AI. Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems. To enhance reasoning performance, one promising direction is scalable oversight, which requires LLMs to identify their own errors and then improve by themselves. Various self-verification methods have been proposed in pursuit of this goal. Nevertheless, whether existing models understand their own errors well is still under investigation. In this paper, we take a closer look at the self-verification abilities of LLMs in the context of logical reasoning, focusing on their ability to identify logical fallacies accurately. We introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies categorized in a hierarchical taxonomy. By conducting exhaustive experiments on FALLACIES, we obtain comprehensive and detailed analyses of a series of models on their verification abilities. Our main findings suggest that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self-verification methods. Drawing from these observations, we offer suggestions for future research and practical applications of self-verification methods."
    },
    {
        "paperId": "311841075acf5a5b38d807c68fa9f55e4aa274bf",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "A Ship of Theseus: Curious Cases of Paraphrasing in LLM-Generated Texts",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.08374, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-14",
        "authors": [
            {
                "authorId": "66674465",
                "name": "Nafis Irtiza Tripto"
            },
            {
                "authorId": "2189394679",
                "name": "Saranya Venkatraman"
            },
            {
                "authorId": "2260109257",
                "name": "Dominik Macko"
            },
            {
                "authorId": "144535025",
                "name": "R\u00f3bert M\u00f3ro"
            },
            {
                "authorId": "2129782",
                "name": "Ivan Srba"
            },
            {
                "authorId": "150035131",
                "name": "Adaku Uchendu"
            },
            {
                "authorId": "2260345102",
                "name": "Thai Le"
            },
            {
                "authorId": "2158951945",
                "name": "Dongwon Lee"
            }
        ],
        "abstract": "In the realm of text manipulation and linguistic transformation, the question of authorship has been a subject of fascination and philosophical inquiry. Much like the Ship of Theseus paradox, which ponders whether a ship remains the same when each of its original planks is replaced, our research delves into an intriguing question: Does a text retain its original authorship when it undergoes numerous paraphrasing iterations? Specifically, since Large Language Models (LLMs) have demonstrated remarkable proficiency in both the generation of original content and the modification of human-authored texts, a pivotal question emerges concerning the determination of authorship in instances where LLMs or similar paraphrasing tools are employed to rephrase the text--i.e., whether authorship should be attributed to the original human author or the AI-powered tool. Therefore, we embark on a philosophical voyage through the seas of language and authorship to unravel this intricate puzzle. Using a computational approach, we discover that the diminishing performance in text classification models, with each successive paraphrasing iteration, is closely associated with the extent of deviation from the original author's style, thus provoking a reconsideration of the current notion of authorship."
    },
    {
        "paperId": "9646153a4b49abd2e269fae5666b60e568a1999f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2311.08389",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.08389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-14",
        "authors": [
            {
                "authorId": "2261689091",
                "name": "Huashan Sun"
            },
            {
                "authorId": "2266714135",
                "name": "Yixiao Wu"
            },
            {
                "authorId": "2261393344",
                "name": "Yizhe Yang"
            },
            {
                "authorId": "2261448070",
                "name": "Yinghao Li"
            },
            {
                "authorId": "2261392836",
                "name": "Jiawei Li"
            },
            {
                "authorId": "2316692356",
                "name": "Yuhao Ye"
            },
            {
                "authorId": "2261669912",
                "name": "Yang Gao"
            }
        ],
        "abstract": "Language style is necessary for AI systems to understand and generate diverse human language accurately. However, previous text style transfer primarily focused on sentence-level data-driven approaches, limiting exploration of potential problems in large language models (LLMs) and the ability to meet complex application needs. To overcome these limitations, we introduce a novel task called Public-Speaking Style Transfer (PSST), which aims to simulate humans to transform passage-level, official texts into a public-speaking style. Grounded in the analysis of real-world data from a linguistic perspective, we decompose public-speaking style into key sub-styles to pose challenges and quantify the style modeling capability of LLMs. For such intricate text style transfer, we further propose a fine-grained evaluation framework to analyze the characteristics and identify the problems of stylized texts. Comprehensive experiments suggest that current LLMs struggle to generate public speaking texts that align with human preferences, primarily due to excessive stylization and loss of semantic information."
    },
    {
        "paperId": "72273f7a050529fc71c7d45c0256d2b9754f56bb",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.08562, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-14",
        "authors": [
            {
                "authorId": "2267005785",
                "name": "Lin Xu"
            },
            {
                "authorId": "2267163621",
                "name": "Zhiyuan Hu"
            },
            {
                "authorId": "18119920",
                "name": "Daquan Zhou"
            },
            {
                "authorId": "2266809202",
                "name": "Hongyu Ren"
            },
            {
                "authorId": "143879884",
                "name": "Zhen Dong"
            },
            {
                "authorId": "2242659602",
                "name": "Kurt Keutzer"
            },
            {
                "authorId": "2269762964",
                "name": "See-Kiong Ng"
            },
            {
                "authorId": "2256994948",
                "name": "Jiashi Feng"
            }
        ],
        "abstract": "Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating exceptional reasoning, tool usage, and memory capabilities. As their applications expand into multi-agent environments, there arises a need for a comprehensive evaluation framework that captures LLMs\u2019 reasoning, planning, collaboration, and other social abilities. This work introduces a novel competition-based benchmark framework specifically designed to assess LLMs within multi-agent settings, providing quantitative metrics to evaluate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality.We utilize two social deduction games alongside three game-theory scenarios to create diverse environments.Our frame is fortified with the probabilistic graphic modeling (PGM) method, enhancing the LLMs\u2019 capabilities in navigating complex social and cognitive dimensions. We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B. It also confirms that our PGM enhancement boosts the abilities of all selected models by an average of 37%. Our data and code can be found here https://github.com/cathyxl/MAgIC."
    },
    {
        "paperId": "20f3abdd3640718d8f268aaea8b2ac3b8978d2af",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Exploring the Potential of Large Language Models in Computational Argumentation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-15",
        "authors": [
            {
                "authorId": "2218132412",
                "name": "Guizhen Chen"
            },
            {
                "authorId": "123962152",
                "name": "Liying Cheng"
            },
            {
                "authorId": "26336902",
                "name": "Anh Tuan Luu"
            },
            {
                "authorId": "1996394",
                "name": "Lidong Bing"
            }
        ],
        "abstract": "Computational argumentation has become an essential tool in various domains, including law, public policy, and artificial intelligence. It is an emerging research field in natural language processing that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated impressive capabilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on diverse computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models, and LLaMA2 models, in both zero-shot and few-shot settings. We organize existing tasks into six main categories and standardize the format of fourteen openly available datasets. In addition, we present a new benchmark dataset on counter speech generation that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of the datasets, demonstrating their capabilities in the field of argumentation. Our analysis offers valuable suggestions for evaluating computational argumentation and its integration with LLMs in future research endeavors."
    },
    {
        "paperId": "e58147980d42f845879c4231840c2857d82337eb",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "MELA: Multilingual Evaluation of Linguistic Acceptability",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09033, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-15",
        "authors": [
            {
                "authorId": "2267343117",
                "name": "Ziyin Zhang"
            },
            {
                "authorId": "2214607250",
                "name": "Yikang Liu"
            },
            {
                "authorId": "2000111554",
                "name": "Wei Huang"
            },
            {
                "authorId": "2266752587",
                "name": "Junyu Mao"
            },
            {
                "authorId": "2266813948",
                "name": "Rui Wang"
            },
            {
                "authorId": "2118878378",
                "name": "Hai Hu"
            }
        ],
        "abstract": "In this work, we present the largest benchmark to date on linguistic acceptability: Multilingual Evaluation of Linguistic Acceptability -- MELA, with 46K samples covering 10 languages from a diverse set of language families. We establish LLM baselines on this benchmark, and investigate cross-lingual transfer in acceptability judgements with XLM-R. In pursuit of multilingual interpretability, we conduct probing experiments with fine-tuned XLM-R to explore the process of syntax capability acquisition. Our results show that GPT-4o exhibits a strong multilingual ability, outperforming fine-tuned XLM-R, while open-source multilingual models lag behind by a noticeable gap. Cross-lingual transfer experiments show that transfer in acceptability judgment is non-trivial: 500 Icelandic fine-tuning examples lead to 23 MCC performance in a completely unrelated language -- Chinese. Results of our probing experiments indicate that training on MELA improves the performance of XLM-R on syntax-related tasks. Our data is available at https://github.com/sjtu-compling/MELA."
    },
    {
        "paperId": "b91ff4e3d067f1c5418846bdb9888ad3fb02cceb",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09105, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-15",
        "authors": [
            {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
            },
            {
                "authorId": "47837854",
                "name": "Hao Peng"
            },
            {
                "authorId": "2266808191",
                "name": "Yong Guan"
            },
            {
                "authorId": "10673612",
                "name": "Kaisheng Zeng"
            },
            {
                "authorId": "2220188202",
                "name": "Jianhui Chen"
            },
            {
                "authorId": "2055765060",
                "name": "Lei Hou"
            },
            {
                "authorId": "48506411",
                "name": "Xu Han"
            },
            {
                "authorId": "2149202150",
                "name": "Yankai Lin"
            },
            {
                "authorId": "2261154189",
                "name": "Zhiyuan Liu"
            },
            {
                "authorId": "2257007994",
                "name": "Ruobing Xie"
            },
            {
                "authorId": "2257088385",
                "name": "Jie Zhou"
            },
            {
                "authorId": "2133353675",
                "name": "Juanzi Li"
            }
        ],
        "abstract": "Understanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a large-scale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-Arg, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,591 events and 290,613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-Arg and codes can be obtained from https://github.com/THU-KEG/MAVEN-Argument."
    },
    {
        "paperId": "a5c61aae22f47a7deaf7397a4a24bce42a8cc1f1",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Grounding Gaps in Language Model Generations",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2311.09144",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09144, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-15",
        "authors": [
            {
                "authorId": "2266753706",
                "name": "Omar Shaikh"
            },
            {
                "authorId": "2266756261",
                "name": "Kristina Gligori'c"
            },
            {
                "authorId": "2266756182",
                "name": "Ashna Khetan"
            },
            {
                "authorId": "2266753978",
                "name": "Matthias Gerstgrasser"
            },
            {
                "authorId": "2267015753",
                "name": "Diyi Yang"
            },
            {
                "authorId": "2256674786",
                "name": "Dan Jurafsky"
            }
        ],
        "abstract": "Effective conversation requires common ground: a shared understanding between the participants. Common ground, however, does not emerge spontaneously in conversation. Speakers and listeners work together to both identify and construct a shared basis while avoiding misunderstanding. To accomplish grounding, humans rely on a range of dialogue acts, like clarification (What do you mean?) and acknowledgment (I understand.). However, it is unclear whether large language models (LLMs) generate text that reflects human grounding. To this end, we curate a set of grounding acts and propose corresponding metrics that quantify attempted grounding. We study whether LLM generations contain grounding acts, simulating turn-taking from several dialogue datasets and comparing results to humans. We find that\u2014compared to humans\u2014LLMs generate language with less conversational grounding, instead generating text that appears to simply presume common ground. To understand the roots of the identified grounding gap, we examine the role of instruction tuning and preference optimization, finding that training on contemporary preference data leads to a reduction in generated grounding acts. Altogether, we highlight the need for more research investigating conversational grounding in human-AI interaction."
    },
    {
        "paperId": "4ed896f45e143b31dca465b2153fc7fc93ca3fdf",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Temporal Knowledge Question Answering via Abstract Reasoning Induction",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09149, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-15",
        "authors": [
            {
                "authorId": "2117098298",
                "name": "Ziyang Chen"
            },
            {
                "authorId": "2265618386",
                "name": "Dongfang Li"
            },
            {
                "authorId": "2327289650",
                "name": "Xiang Zhao"
            },
            {
                "authorId": "33968873",
                "name": "Baotian Hu"
            },
            {
                "authorId": "2258690227",
                "name": "Min Zhang"
            }
        ],
        "abstract": "In this study, we address the challenge of enhancing temporal knowledge reasoning in Large Language Models (LLMs). LLMs often struggle with this task, leading to the generation of inaccurate or misleading responses. This issue mainly arises from their limited ability to handle evolving factual knowledge and complex temporal logic. To overcome these limitations, we propose Abstract Reasoning Induction (ARI) framework, which divides temporal reasoning into two distinct phases: Knowledge-agnostic and Knowledge-based. This framework offers factual knowledge support to LLMs while minimizing the incorporation of extraneous noisy data. Concurrently, informed by the principles of constructivism, ARI provides LLMs the capability to engage in proactive, self-directed learning from both correct and incorrect historical reasoning samples. By teaching LLMs to actively construct knowledge and methods, it can significantly boosting their temporal reasoning abilities. Our approach achieves remarkable improvements, with relative gains of 29.7% and 9.27% on two temporal QA datasets, underscoring its efficacy in advancing temporal reasoning in LLMs. The code can be found at https://github.com/czy1999/ARI-QA"
    },
    {
        "paperId": "4fbfa7f2c67a97621a745e607db4159580b9ec3f",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09182, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-15",
        "authors": [
            {
                "authorId": "2266802137",
                "name": "Jierui Li"
            },
            {
                "authorId": "2831377",
                "name": "Vipul Raheja"
            },
            {
                "authorId": "2261784117",
                "name": "Dhruv Kumar"
            }
        ],
        "abstract": "In recent times, large language models (LLMs) have shown impressive performance on various document-level tasks such as document classification, summarization, and question-answering. However, research on understanding their capabilities on the task of self-contradictions in long documents has been very limited. In this work, we introduce ContraDoc, the first human-annotated dataset to study self-contradictions in long documents across multiple domains, varying document lengths, self-contradiction types, and appearance scope. We then analyze the current capabilities of four state-of-the-art open-source and commercially available LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4 performs the best and can outperform humans on this task, we find that it is still unreliable and struggles with self-contradictions that require more nuance and context. We release the dataset and all the code associated with the experiments."
    },
    {
        "paperId": "3194a28bcab3c7dcd556fc5f6895b9c38311308d",
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "title": "Divergences between Language Models and Human Brains",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09308, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-15",
        "authors": [
            {
                "authorId": "2267004051",
                "name": "Yuchen Zhou"
            },
            {
                "authorId": "1381444447",
                "name": "Emmy Liu"
            },
            {
                "authorId": "1700325",
                "name": "Graham Neubig"
            },
            {
                "authorId": "2001748",
                "name": "Leila Wehbe"
            }
        ],
        "abstract": "Do machines and humans process language in similar ways? Recent research has hinted at the affirmative, showing that human neural activity can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using an LLM-based data-driven approach, we identify two domains that LMs do not capture well: social/emotional intelligence and physical commonsense. We validate these findings with human behavioral experiments and hypothesize that the gap is due to insufficient representations of social/emotional and physical knowledge in LMs. Our results show that fine-tuning LMs on these domains can improve their alignment with human brain responses.1"
    },
    {
        "paperId": "746a78123035729ce0ab1103efd51b9c5b5c0e72",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Temperature-scaling surprisal estimates improve fit to human reading times - but does it do so for the \"right reasons\"?",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2311.09325",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-15",
        "authors": [
            {
                "authorId": "2323730475",
                "name": "Tong Liu"
            },
            {
                "authorId": "2638559",
                "name": "Iza \u0160krjanec"
            },
            {
                "authorId": "2266842862",
                "name": "Vera Demberg"
            }
        ],
        "abstract": "A wide body of evidence shows that human language processing difficulty is predicted by the information-theoretic measure surprisal, a word's negative log probability in context. However, it is still unclear how to best estimate these probabilities needed for predicting human processing difficulty -- while a long-standing belief held that models with lower perplexity would provide more accurate estimates of word predictability, and therefore lead to better reading time predictions, recent work has shown that for very large models, psycholinguistic predictive power decreases. One reason could be that language models might be more confident of their predictions than humans, because they have had exposure to several magnitudes more data. In this paper, we test what effect temperature-scaling of large language model (LLM) predictions has on surprisal estimates and their predictive power of reading times of English texts. Firstly, we show that calibration of large language models typically improves with model size, i.e. poorer calibration cannot account for poorer fit to reading times. Secondly, we find that temperature-scaling probabilities lead to a systematically better fit to reading times (up to 89% improvement in delta log likelihood), across several reading time corpora. Finally, we show that this improvement in fit is chiefly driven by words that are composed of multiple subword tokens."
    },
    {
        "paperId": "2b654ef07dd84ff786861cbad21ee47a361ff224",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09602, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-16",
        "authors": [
            {
                "authorId": "2267014980",
                "name": "Smriti Singh"
            },
            {
                "authorId": "2261283079",
                "name": "Cornelia Caragea"
            },
            {
                "authorId": "2267018724",
                "name": "Junyi Jessy Li"
            }
        ],
        "abstract": "Situations and events evoke emotions in humans, but to what extent do they inform the prediction of emotion detection models? This work investigates how well human-annotated emotion triggers correlate with features that models deemed salient in their prediction of emotions. First, we introduce a novel dataset EmoTrigger, consisting of 900 social media posts sourced from three different datasets; these were annotated by experts for emotion triggers with high agreement. Using EmoTrigger, we evaluate the ability of large language models (LLMs) to identify emotion triggers, and conduct a comparative analysis of the features considered important for these tasks between LLMs and fine-tuned models. Our analysis reveals that emotion triggers are largely not considered salient features for emotion prediction models, instead there is intricate interplay between various features and the task of emotion detection."
    },
    {
        "paperId": "aafc487e8a71c32daa151ce56f9c656a86631cab",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Self-contradictory reasoning evaluation and detection",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09603, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-16",
        "authors": [
            {
                "authorId": "2267011746",
                "name": "Ziyi Liu"
            },
            {
                "authorId": "50044788",
                "name": "Isabelle G. Lee"
            },
            {
                "authorId": "2267007336",
                "name": "Yongkang Du"
            },
            {
                "authorId": "3313909",
                "name": "Soumya Sanyal"
            },
            {
                "authorId": "2267002049",
                "name": "Jieyu Zhao"
            }
        ],
        "abstract": "In a plethora of recent work, large language models (LLMs) demonstrated impressive reasoning ability, but many proposed downstream reasoning tasks only focus on final answers. Two fundamental questions persist: 1) how consistent is the reasoning, and 2) can models detect unreliable reasoning? In this paper, we investigate self-contradictory (Self-Contra) reasoning, where the model reasoning does not support its answers. To answer 1), we define and assess the Self-Contra rate across three datasets and delve into finer-grained categories of Self-Contra reasoning. We find that LLMs often contradict themselves in reasoning tasks involving contextual information understanding or commonsense. The model may generate correct answers by taking shortcuts in reasoning or overlooking contextual evidence, leading to compromised reasoning. For 2), we task the state-of-the-art model GPT-4 with identifying Self-Contra reasoning and finer-grained fallacies. We find that finer-grained categories enhanced detection can improve GPT-4's ability to detect Self-Contra. However, it is only able to detect Self-Contra with a 52.2% F1 score, much lower compared to 66.7% for humans. Our results indicate that current LLMs lack the robustness necessary for reliable reasoning and we emphasize the urgent need for establishing best practices in comprehensive reasoning evaluations beyond pure performance-based metrics."
    },
    {
        "paperId": "e56aa728aaa32c087c8f7bc56a7eb225675dd8ae",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Structured Chemistry Reasoning with Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-16",
        "authors": [
            {
                "authorId": "2042897657",
                "name": "Siru Ouyang"
            },
            {
                "authorId": "2284695096",
                "name": "Zhuosheng Zhang"
            },
            {
                "authorId": "2266886449",
                "name": "Bing Yan"
            },
            {
                "authorId": "2271402844",
                "name": "Xuan Liu"
            },
            {
                "authorId": "2259869648",
                "name": "Jiawei Han"
            },
            {
                "authorId": "2267901975",
                "name": "Lianhui Qin"
            }
        ],
        "abstract": "Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry. Different from the simple chemistry tasks (e.g., molecule classification) addressed in previous studies, complex chemistry problems require not only vast knowledge and precise calculation, but also compositional reasoning about rich dynamic interactions of different concepts (e.g., temperature changes). Our study shows that even advanced LLMs, like GPT-4, can fail easily in different ways. Interestingly, the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning structure that guides the LLMs to elicit the right knowledge, incorporate the knowledge in step-by-step reasoning, and iteratively refine results for further improved quality. On this basis, we introduce StructChem, a simple yet effective prompting strategy that offers the desired guidance and substantially boosts the LLMs' chemical reasoning capability. Testing across four chemistry areas -- quantum chemistry, mechanics, physical chemistry, and kinetics -- StructChem substantially enhances GPT-4's performance, with up to 30\\% peak improvement. Our analysis also underscores the unique difficulties of precise grounded reasoning in science with LLMs, highlighting a need for more research in this area. Code is available at \\url{https://github.com/ozyyshr/StructChem}."
    },
    {
        "paperId": "437cbaee4eaee0bf84abbe11750b86b091b9b756",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "MacGyver: Are Large Language Models Creative Problem Solvers?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-16",
        "authors": [
            {
                "authorId": "48391421",
                "name": "Yufei Tian"
            },
            {
                "authorId": "3023068",
                "name": "Abhilasha Ravichander"
            },
            {
                "authorId": "3444092",
                "name": "Lianhui Qin"
            },
            {
                "authorId": "2069676542",
                "name": "R. L. Bras"
            },
            {
                "authorId": "2075275038",
                "name": "Raja Marjieh"
            },
            {
                "authorId": "2256996328",
                "name": "Nanyun Peng"
            },
            {
                "authorId": "2266363632",
                "name": "Yejin Choi"
            },
            {
                "authorId": "2263664277",
                "name": "Thomas L. Griffiths"
            },
            {
                "authorId": "2223951216",
                "name": "Faeze Brahman"
            }
        ],
        "abstract": "We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniques such as iterative step-wise reflection and divergent-convergent thinking.This work (1) introduces a fresh arena for intelligent agents focusing on intricate aspects of physical reasoning, planning, and unconventional thinking, which supplements the existing spectrum of machine intelligence; and (2) provides insight into the constrained problem-solving capabilities of both humans and AI."
    },
    {
        "paperId": "5f00a1355eaa149ad3ad5db56ca3c0eb50e631f1",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "You don\u2019t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09718, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-16",
        "authors": [
            {
                "authorId": "2266841280",
                "name": "Bangzhao Shu"
            },
            {
                "authorId": "2267011399",
                "name": "Lechen Zhang"
            },
            {
                "authorId": "2111881583",
                "name": "Minje Choi"
            },
            {
                "authorId": "2141029906",
                "name": "Lavinia Dunagan"
            },
            {
                "authorId": null,
                "name": "Dallas Card"
            },
            {
                "authorId": "3046220",
                "name": "David Jurgens"
            }
        ],
        "abstract": "The versatility of Large Language Models (LLMs) on natural language understanding tasks has made them popular for research in social sciences. To properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs about particular opinions. In this study, we take a cautionary step back and examine whether the current format of prompting LLMs elicits responses in a consistent and robust manner. We first construct a dataset that contains 693 questions encompassing 39 different instruments of persona measurement on 115 persona axes. Additionally, we design a set of prompts containing minor variations and examine LLMs\u2019 capabilities to generate answers, as well as prompt variations to examine their consistency with respect to content-level variations such as switching the order of response options or negating the statement. Our experiments on 17 different LLMs reveal that even simple perturbations significantly downgrade a model\u2019s question-answering ability, and that most LLMs have low negation consistency. Our results suggest that the currently widespread practice of prompting is insufficient to accurately and reliably capture model perceptions, and we therefore discuss potential alternatives to improve these issues."
    },
    {
        "paperId": "726a23d476b9b444d61463aeefca8804539b9e6a",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "MOKA: Moral Knowledge Augmentation for Moral Event Extraction",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09733, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-16",
        "authors": [
            {
                "authorId": "2108030243",
                "name": "Xinliang Frederick Zhang"
            },
            {
                "authorId": "2262515710",
                "name": "Winston Wu"
            },
            {
                "authorId": "2262444290",
                "name": "Nick Beauchamp"
            },
            {
                "authorId": "2153518220",
                "name": "Lu Wang"
            }
        ],
        "abstract": "News media often strive to minimize explicit moral language in news articles, yet most articles are dense with moral values as expressed through the reported events themselves. However, values that are reflected in the intricate dynamics among *participating entities* and *moral events* are far more challenging for most NLP systems to detect, including LLMs. To study this phenomenon, we annotate a new dataset, **MORAL EVENTS**, consisting of 5,494 structured event annotations on 474 news articles by diverse US media across the political spectrum. We further propose **MOKA**, a moral event extraction framework with **MO**ral **K**nowledge **A**ugmentation, which leverages knowledge derived from moral words and moral scenarios to produce structural representations of morality-bearing events. Experiments show that **MOKA** outperforms competitive baselines across three moral event understanding tasks. Further analysis shows even ostensibly nonpartisan media engage in the selective reporting of moral events."
    },
    {
        "paperId": "56e7bda25b83228f91962d3465fd587cfe8908e1",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "How Far Can We Extract Diverse Perspectives from Large Language Models?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09799, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-16",
        "authors": [
            {
                "authorId": "31998283",
                "name": "Shirley Anugrah Hayati"
            },
            {
                "authorId": "2187932371",
                "name": "Minhwa Lee"
            },
            {
                "authorId": "1801149",
                "name": "Dheeraj Rajagopal"
            },
            {
                "authorId": "48493368",
                "name": "Dongyeop Kang"
            }
        ],
        "abstract": "Collecting diverse human opinions is costly and challenging. This leads to a recent trend in exploiting large language models (LLMs) for generating diverse data for potential scalable and efficient solutions. However, the extent to which LLMs can generate diverse perspectives on subjective topics is still unclear. In this study, we explore LLMs\u2019 capacity of generating diverse perspectives and rationales on subjective topics such as social norms and argumentative texts. We introduce the problem of extracting maximum diversity from LLMs. Motivated by how humans form opinions based on values, we propose a criteria-based prompting technique to ground diverse opinions. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting to generate more outputs from the model iteratively. Our methods, applied to various tasks, show that LLMs can indeed produce diverse opinions according to the degree of task subjectivity. We also find that LLMs performance of extracting maximum diversity is on par with human."
    },
    {
        "paperId": "3657eff55a86a89db629e598a7ea93edd31bf59d",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Large Language Models for Propaganda Span Annotation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09812, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-16",
        "authors": [
            {
                "authorId": "2905745",
                "name": "Maram Hasanain"
            },
            {
                "authorId": "2266856028",
                "name": "Fatema Ahmed"
            },
            {
                "authorId": "2257274157",
                "name": "Firoj Alam"
            }
        ],
        "abstract": "The use of propagandistic techniques in online content has increased in recent years aiming to manipulate online audiences. Fine-grained propaganda detection and extraction of textual spans where propaganda techniques are used, are essential for more informed content consumption. Automatic systems targeting the task over lower resourced languages are limited, usually obstructed by lack of large scale training datasets. Our study investigates whether Large Language Models (LLMs), such as GPT-4, can effectively extract propagandistic spans. We further study the potential of employing the model to collect more cost-effective annotations. Finally, we examine the effectiveness of labels provided by GPT-4 in training smaller language models for the task. The experiments are performed over a large-scale in-house manually annotated dataset. The results suggest that providing more annotation context to GPT-4 within prompts improves its performance compared to human annotators. Moreover, when serving as an expert annotator (consolidator), the model provides labels that have higher agreement with expert annotators, and lead to specialized models that achieve state-of-the-art over an unseen Arabic testing set. Finally, our work is the first to show the potential of utilizing LLMs to develop annotated datasets for propagandistic spans detection task prompting it with annotations from human annotators with limited expertise. All scripts and annotations will be shared with the community."
    },
    {
        "paperId": "0bee079faf3dc53380db55cd5fc1ba9267c4d5e7",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "WatME: Towards Lossless Watermarking Through Lexical Redundancy",
        "openAccessPdf": {
            "url": "https://ink.library.smu.edu.sg/context/sis_research/article/10237/viewcontent/2024.acl_long.496.pdf",
            "status": "GREEN",
            "license": "CCBYNCND",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09832, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-16",
        "authors": [
            {
                "authorId": "2260621463",
                "name": "Liang Chen"
            },
            {
                "authorId": "2257039155",
                "name": "Yatao Bian"
            },
            {
                "authorId": "2280964158",
                "name": "Yang Deng"
            },
            {
                "authorId": "2284594760",
                "name": "Deng Cai"
            },
            {
                "authorId": "2223902178",
                "name": "Shuaiyi Li"
            },
            {
                "authorId": "2240144403",
                "name": "Peilin Zhao"
            },
            {
                "authorId": "2237563835",
                "name": "Kam-Fai Wong"
            }
        ],
        "abstract": "Text watermarking has emerged as a pivotal technique for identifying machine-generated text. However, existing methods often rely on arbitrary vocabulary partitioning during decoding to embed watermarks, which compromises the availability of suitable tokens and significantly degrades the quality of responses. This study assesses the impact of watermarking on different capabilities of large language models (LLMs) from a cognitive science lens. Our finding highlights a significant disparity; knowledge recall and logical reasoning are more adversely affected than language generation. These results suggest a more profound effect of watermarking on LLMs than previously understood. To address these challenges, we introduce Watermarking with Mutual Exclusion (WatME), a novel approach leveraging linguistic prior knowledge of inherent lexical redundancy in LLM vocabularies to seamlessly integrate watermarks. Specifically, WatME dynamically optimizes token usage during the decoding process by applying a mutually exclusive rule to the identified lexical redundancies. This strategy effectively prevents the unavailability of appropriate tokens and preserves the expressive power of LLMs. We provide both theoretical analysis and empirical evidence showing that WatME effectively preserves the diverse capabilities of LLMs while ensuring watermark detectability."
    },
    {
        "paperId": "c16def288e2704ef78023ba1802b699069d4d4c7",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "INTERVENOR: Prompting the Coding Ability of Large Language Models with the Interactive Chain of Repair",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2311.09868",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09868, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-16",
        "authors": [
            {
                "authorId": "2267230996",
                "name": "Hanbin Wang"
            },
            {
                "authorId": "49047064",
                "name": "Zhenghao Liu"
            },
            {
                "authorId": "2267033597",
                "name": "Shuo Wang"
            },
            {
                "authorId": "52297757",
                "name": "Ganqu Cui"
            },
            {
                "authorId": "2284766400",
                "name": "Ning Ding"
            },
            {
                "authorId": "2266886975",
                "name": "Zhiyuan Liu"
            },
            {
                "authorId": "2204644192",
                "name": "Ge Yu"
            }
        ],
        "abstract": "This paper introduces INTERVENOR (INTERactiVE chaiN Of Repair), a system designed to emulate the interactive code repair processes observed in humans, encompassing both code diagnosis and code repair. INTERVENOR prompts Large Language Models (LLMs) to play distinct roles during the code repair process, functioning as both a Code Learner and a Code Teacher. Specifically, the Code Learner is tasked with adhering to instructions to generate or repair code, while the Code Teacher is responsible for crafting a Chain-of-Repair (CoR) to serve as guidance for the Code Learner. During generating the CoR, the Code Teacher needs to check the generated codes from Code Learner and reassess how to address code bugs based on error feedback received from compilers. Experimental results demonstrate that INTERVENOR surpasses baseline models, exhibiting improvements of approximately 18% and 4.3% over GPT-3.5 in code generation and code translation tasks, respectively. Our further analyses show that CoR is effective to illuminate the reasons behind bugs and outline solution plans in natural language. With the feedback of code compilers, INTERVENOR can accurately identify syntax errors and assertion errors and provide precise instructions to repair codes. All data and codes are available at https://github.com/NEUIR/INTERVENOR"
    },
    {
        "paperId": "0aa150619e07fa41492517368beaaf8ae56fe061",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.10227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-16",
        "authors": [
            {
                "authorId": "2000786644",
                "name": "Alex Wilf"
            },
            {
                "authorId": "2267273567",
                "name": "Sihyun Shawn Lee"
            },
            {
                "authorId": "28130078",
                "name": "Paul Pu Liang"
            },
            {
                "authorId": "49933077",
                "name": "Louis-philippe Morency"
            }
        ],
        "abstract": "Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires made possible by Theory of Mind (ToM): our cognitive ability to understand the mental states of ourselves and others. Although ToM may come naturally to us, emulating it presents a challenge to even the most advanced Large Language Models (LLMs). Recent improvements to LLMs' reasoning capabilities from simple yet effective prompting techniques such as Chain-of-Thought have seen limited applicability to ToM. In this paper, we turn to the prominent cognitive science theory\"Simulation Theory\"to bridge this gap. We introduce SimToM, a novel two-stage prompting framework inspired by Simulation Theory's notion of perspective-taking. To implement this idea on current ToM benchmarks, SimToM first filters context based on what the character in question knows before answering a question about their mental state. Our approach, which requires no additional training and minimal prompt-tuning, shows substantial improvement over existing methods, and our analysis reveals the importance of perspective-taking to Theory-of-Mind capabilities. Our findings suggest perspective-taking as a promising direction for future research into improving LLMs' ToM capabilities."
    },
    {
        "paperId": "44d16a076c00ecada3d425203377e4ec951c4ed0",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.10537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-16",
        "authors": [
            {
                "authorId": "47274259",
                "name": "Xiangru Tang"
            },
            {
                "authorId": "2187586628",
                "name": "Anni Zou"
            },
            {
                "authorId": "3322871",
                "name": "Zhuosheng Zhang"
            },
            {
                "authorId": "46316984",
                "name": "Yilun Zhao"
            },
            {
                "authorId": "2267545169",
                "name": "Xingyao Zhang"
            },
            {
                "authorId": "2266838179",
                "name": "Arman Cohan"
            },
            {
                "authorId": "2201323142",
                "name": "Mark B. Gerstein"
            }
        ],
        "abstract": "Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose MedAgents, a novel multi-disciplinary collaboration framework for the medical domain. MedAgents leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MedAgents framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities. Our code can be found at https://github.com/gersteinlab/MedAgents."
    },
    {
        "paperId": "1c3164a204dabde2dcecf990ead6b368fe2fc485",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Countering Misinformation via Emotional Response Generation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.10587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-17",
        "authors": [
            {
                "authorId": "2267240582",
                "name": "Daniel Russo"
            },
            {
                "authorId": "2180057903",
                "name": "Shane P. Kaszefski-Yaschuk"
            },
            {
                "authorId": "2256994086",
                "name": "Jacopo Staiano"
            },
            {
                "authorId": "1912357",
                "name": "Marco Guerini"
            }
        ],
        "abstract": "The proliferation of misinformation on social media platforms (SMPs) poses a significant danger to public health, social cohesion and ultimately democracy. Previous research has shown how social correction can be an effective way to curb misinformation, by engaging directly in a constructive dialogue with users who spread -- often in good faith -- misleading messages. Although professional fact-checkers are crucial to debunking viral claims, they usually do not engage in conversations on social media. Thereby, significant effort has been made to automate the use of fact-checker material in social correction; however, no previous work has tried to integrate it with the style and pragmatics that are commonly employed in social media communication. To fill this gap, we present VerMouth, the first large-scale dataset comprising roughly 12 thousand claim-response pairs (linked to debunking articles), accounting for both SMP-style and basic emotions, two factors which have a significant role in misinformation credibility and spreading. To collect this dataset we used a technique based on an author-reviewer pipeline, which efficiently combines LLMs and human annotators to obtain high-quality data. We also provide comprehensive experiments showing how models trained on our proposed dataset have significant improvements in terms of output quality and generalization capabilities."
    },
    {
        "paperId": "9331818a22f1b80b397b4de8f0742403e2588436",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Value FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Value",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.10766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-15",
        "authors": [
            {
                "authorId": "2237129499",
                "name": "Jing Yao"
            },
            {
                "authorId": "2258961742",
                "name": "Xiaoyuan Yi"
            },
            {
                "authorId": "2261675563",
                "name": "Xiting Wang"
            },
            {
                "authorId": "2267880119",
                "name": "Yifan Gong"
            },
            {
                "authorId": "2261275112",
                "name": "Xing Xie"
            }
        ],
        "abstract": "Value alignment is crucial for the responsible development of Large Language Models (LLMs). However, how to define values in this context remains largely unexplored. Existing work mainly specifies values as risk criteria formulated in the AI community, e.g., fairness and privacy protection, suffering from poor clarity, adaptability and transparency. Leveraging basic values established in humanity and social science that are compatible with values across cultures, this paper introduces a novel value space spanned by multiple basic value dimensions and proposes BaseAlign, a corresponding value alignment paradigm. Applying the representative Schwartz\u2019s Theory of Basic Values as an instantiation, we construct FULCRA, a dataset consisting of 20k (LLM output, value vector) pairs. LLMs\u2019 outputs are mapped into the K-dim value space beyond simple binary labels, by identifying their underlying priorities for these value dimensions. Extensive analysis and experiments on FULCRA: (1) reveal the essential relation between basic values and LLMs\u2019 behaviors, (2) demonstrate that our paradigm with basic values not only covers existing risks but also anticipates the unidentified ones, and (3) manifest BaseAlign\u2019s superiority in alignment performance with less data, paving the way for addressing the above three challenges."
    },
    {
        "paperId": "52d2f7326a1965ce84dd6d97c0a5400d85e837d0",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.13565, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-22",
        "authors": [
            {
                "authorId": "2300285077",
                "name": "Inderjeet Nair"
            },
            {
                "authorId": "2267725867",
                "name": "Shwetha Somasundaram"
            },
            {
                "authorId": "46961776",
                "name": "Apoorv Saxena"
            },
            {
                "authorId": "120873790",
                "name": "Koustava Goswami"
            }
        ],
        "abstract": "We address the task of evidence retrieval for long document question answering, which involves locating relevant paragraphs within a document to answer a question. We aim to assess the applicability of large language models (LLMs) in the task of zero-shot long document evidence retrieval, owing to their unprecedented performance across various NLP tasks. However, currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global context while missing out on capturing the inter-segment dependencies. Moreover, directly feeding the large input sets can incur significant computational costs, particularly when processing the entire document (and potentially incurring monetary expenses with enterprise APIs like OpenAI's GPT variants). To address these challenges, we propose a suite of techniques that exploit the discourse structure commonly found in documents. By utilizing this structure, we create a condensed representation of the document, enabling a more comprehensive understanding and analysis of relationships between different parts. We retain $99.6\\%$ of the best zero-shot approach's performance, while processing only $26\\%$ of the total tokens used by the best approach in the information seeking evidence retrieval setup. We also show how our approach can be combined with \\textit{self-ask} reasoning agent to achieve best zero-shot performance in complex multi-hop question answering, just $\\approx 4\\%$ short of zero-shot performance using gold evidence."
    },
    {
        "paperId": "645e7a5b9caa88a61c11bb87a10b07b90945640d",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Quantifying the redundancy between prosody and text",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.17233, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-28",
        "authors": [
            {
                "authorId": "2268674360",
                "name": "Lukas Wolf"
            },
            {
                "authorId": "1388571351",
                "name": "Tiago Pimentel"
            },
            {
                "authorId": "144733430",
                "name": "Evelina Fedorenko"
            },
            {
                "authorId": "2070989574",
                "name": "Ryan Cotterell"
            },
            {
                "authorId": "46236380",
                "name": "Alex Warstadt"
            },
            {
                "authorId": "2268675406",
                "name": "E. Wilcox"
            },
            {
                "authorId": "3512083",
                "name": "Tamar I. Regev"
            }
        ],
        "abstract": "Prosody -- the suprasegmental component of speech, including pitch, loudness, and tempo -- carries critical aspects of meaning. However, the relationship between the information conveyed by prosody vs. by the words themselves remains poorly understood. We use large language models (LLMs) to estimate how much information is redundant between prosody and the words themselves. Using a large spoken corpus of English audiobooks, we extract prosodic features aligned to individual words and test how well they can be predicted from LLM embeddings, compared to non-contextual word embeddings. We find a high degree of redundancy between the information carried by the words and prosodic information across several prosodic features, including intensity, duration, pauses, and pitch contours. Furthermore, a word's prosodic information is redundant with both the word itself and the context preceding as well as following it. Still, we observe that prosodic features can not be fully predicted from text, suggesting that prosody carries information above and beyond the words. Along with this paper, we release a general-purpose data processing pipeline for quantifying the relationship between linguistic information and extra-linguistic features."
    },
    {
        "paperId": "034c8d4eb031786925ef274e6d275c7c210c4f1d",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Unveiling the Implicit Toxicity in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.17391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-29",
        "authors": [
            {
                "authorId": "2104586007",
                "name": "Jiaxin Wen"
            },
            {
                "authorId": "1886879",
                "name": "Pei Ke"
            },
            {
                "authorId": "144990601",
                "name": "Hao Sun"
            },
            {
                "authorId": "101371510",
                "name": "Zhexin Zhang"
            },
            {
                "authorId": "2267838375",
                "name": "Chengfei Li"
            },
            {
                "authorId": "2267901816",
                "name": "Jinfeng Bai"
            },
            {
                "authorId": "2254009342",
                "name": "Minlie Huang"
            }
        ],
        "abstract": "The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use. While recent studies primarily focus on probing toxic outputs that can be easily detected with existing toxicity classifiers, we show that LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting. Moreover, we propose a reinforcement learning (RL) based attacking method to further induce the implicit toxicity in LLMs. Specifically, we optimize the language model with a reward that prefers implicit toxic outputs to explicit toxic and non-toxic ones. Experiments on five widely-adopted toxicity classifiers demonstrate that the attack success rate can be significantly improved through RL fine-tuning. For instance, the RL-finetuned LLaMA-13B model achieves an attack success rate of 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs. We further show that fine-tuning toxicity classifiers on the annotated examples from our attacking method can effectively enhance their ability to detect LLM-generated implicit toxic language. The code is publicly available at https://github.com/thu-coai/Implicit-Toxicity."
    },
    {
        "paperId": "7354a57261d27a281e56dc428b6ec146b9992afd",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.18805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-30",
        "authors": [
            {
                "authorId": "2268816164",
                "name": "Qi Cao"
            },
            {
                "authorId": "2081836120",
                "name": "Takeshi Kojima"
            },
            {
                "authorId": "2241471533",
                "name": "Yutaka Matsuo"
            },
            {
                "authorId": "1715282",
                "name": "Yusuke Iwasawa"
            }
        ],
        "abstract": "While Large Language Models (LLMs) have achieved remarkable performance in many tasks, much about their inner workings remains unclear. In this study, we present novel experimental insights into the resilience of LLMs, particularly GPT-4, when subjected to extensive character-level permutations. To investigate this, we first propose the Scrambled Bench, a suite designed to measure the capacity of LLMs to handle scrambled input, in terms of both recovering scrambled sentences and answering questions given scrambled context. The experimental results indicate that most powerful LLMs demonstrate the capability akin to typoglycemia, a phenomenon where humans can understand the meaning of words even when the letters within those words are scrambled, as long as the first and last letters remain in place. More surprisingly, we found that only GPT-4 nearly flawlessly processes inputs with unnatural errors, even under the extreme condition, a task that poses significant challenges for other LLMs and often even for humans. Specifically, GPT-4 can almost perfectly reconstruct the original sentences from scrambled ones, decreasing the edit distance by 95%, even when all letters within each word are entirely scrambled. It is counter-intuitive that LLMs can exhibit such resilience despite severe disruption to input tokenization caused by scrambled text."
    },
    {
        "paperId": "c36b4aec0c26f1ff5b3cf7e86c0b90f51575ebea",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.00746, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-01",
        "authors": [
            {
                "authorId": "48198605",
                "name": "Dekun Wu"
            },
            {
                "authorId": "2273646161",
                "name": "Haochen Shi"
            },
            {
                "authorId": "2269419329",
                "name": "Zhiyuan Sun"
            },
            {
                "authorId": "2269701539",
                "name": "Bang Liu"
            }
        ],
        "abstract": "In this study, we explore the application of Large Language Models (LLMs) in \\textit{Jubensha}, a Chinese detective role-playing game and a novel area in Artificial Intelligence (AI) driven gaming. We introduce the first dataset specifically for Jubensha, including character scripts and game rules, to foster AI agent development in this complex narrative environment. Our work also presents a unique multi-agent interaction framework using LLMs, allowing AI agents to autonomously engage in this game. To evaluate the gaming performance of these AI agents, we developed novel methods measuring their mastery of case information and reasoning skills. Furthermore, we incorporated the latest advancements in in-context learning to improve the agents' performance in information gathering, murderer identification, and logical reasoning. The experimental results validate the effectiveness of our proposed methods. This work aims to offer a novel perspective on understanding LLM capabilities and establish a new benchmark for evaluating large language model-based agents."
    },
    {
        "paperId": "38c8111873cb40d28c8bbc8aa6836a172234b5fa",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Nash Learning from Human Feedback",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.00886, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-01",
        "authors": [
            {
                "authorId": "2237802765",
                "name": "R\u00e9mi Munos"
            },
            {
                "authorId": "2259912893",
                "name": "Michal Valko"
            },
            {
                "authorId": "2439765",
                "name": "Daniele Calandriello"
            },
            {
                "authorId": "37666967",
                "name": "M. G. Azar"
            },
            {
                "authorId": "144845456",
                "name": "Mark Rowland"
            },
            {
                "authorId": "3407143",
                "name": "Z. Guo"
            },
            {
                "authorId": "2269752766",
                "name": "Yunhao Tang"
            },
            {
                "authorId": "2253609155",
                "name": "Matthieu Geist"
            },
            {
                "authorId": "2237423540",
                "name": "Thomas Mesnard"
            },
            {
                "authorId": "82899350",
                "name": "Andrea Michi"
            },
            {
                "authorId": "2269473701",
                "name": "Marco Selvi"
            },
            {
                "authorId": "35022714",
                "name": "Sertan Girgin"
            },
            {
                "authorId": "1470531643",
                "name": "Nikola Momchev"
            },
            {
                "authorId": "1936951",
                "name": "Olivier Bachem"
            },
            {
                "authorId": "3187297",
                "name": "D. Mankowitz"
            },
            {
                "authorId": "2249762747",
                "name": "D. Precup"
            },
            {
                "authorId": "1808897",
                "name": "Bilal Piot"
            }
        ],
        "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the main paradigm for aligning large language models (LLMs) with human preferences. Typically, RLHF involves the initial step of learning a reward model from human feedback, often expressed as preferences between pairs of text generations produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by optimizing it to maximize the reward model through a reinforcement learning algorithm. However, an inherent limitation of current reward models is their inability to fully represent the richness of human preferences and their dependency on the sampling distribution. In this study, we introduce an alternative pipeline for the fine-tuning of LLMs using pairwise human feedback. Our approach entails the initial learning of a preference model, which is conditioned on two inputs given a prompt, followed by the pursuit of a policy that consistently generates responses preferred over those generated by any competing policy, thus defining the Nash equilibrium of this preference model. We term this approach Nash learning from human feedback (NLHF). In the context of a tabular policy representation, we present a novel algorithmic solution, Nash-MD, founded on the principles of mirror descent. This algorithm produces a sequence of policies, with the last iteration converging to the regularized Nash equilibrium. Additionally, we explore parametric representations of policies and introduce gradient descent algorithms for deep-learning architectures. To demonstrate the effectiveness of our approach, we present experimental results involving the fine-tuning of a LLM for a text summarization task. We believe NLHF offers a compelling avenue for preference learning and policy optimization with the potential of advancing the field of aligning LLMs with human preferences."
    },
    {
        "paperId": "c3ec2a49c5a09308a496ea21424dcd3eb812a754",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.01279, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-03",
        "authors": [
            {
                "authorId": "1390029897",
                "name": "James Enouen"
            },
            {
                "authorId": "3346298",
                "name": "Hootan Nakhost"
            },
            {
                "authorId": "27556211",
                "name": "Sayna Ebrahimi"
            },
            {
                "authorId": "2676352",
                "name": "Sercan \u00d6. Arik"
            },
            {
                "authorId": "2257088730",
                "name": "Yan Liu"
            },
            {
                "authorId": "2264567300",
                "name": "Tomas Pfister"
            }
        ],
        "abstract": "Large language models (LLMs) have attracted huge interest in practical applications given their increasingly accurate responses and coherent reasoning abilities. Given their nature as black-boxes using complex reasoning processes on their inputs, it is inevitable that the demand for scalable and faithful explanations for LLMs' generated content will continue to grow. There have been major developments in the explainability of neural network models over the past decade. Among them, post-hoc explainability methods, especially Shapley values, have proven effective for interpreting deep learning models. However, there are major challenges in scaling up Shapley values for LLMs, particularly when dealing with long input contexts containing thousands of tokens and autoregressively generated output sequences. Furthermore, it is often unclear how to effectively utilize generated explanations to improve the performance of LLMs. In this paper, we introduce TextGenSHAP, an efficient post-hoc explanation method incorporating LM-specific techniques. We demonstrate that this leads to significant increases in speed compared to conventional Shapley value computations, reducing processing times from hours to minutes for token-level explanations, and to just seconds for document-level explanations. In addition, we demonstrate how real-time Shapley values can be utilized in two important scenarios, providing better understanding of long-document question answering by localizing important words and sentences; and improving existing document retrieval systems through enhancing the accuracy of selected passages and ultimately the final responses."
    },
    {
        "paperId": "a4efd2f79e388c4fb1cd1be073bcc6af32af9911",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.01858, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-04",
        "authors": [
            {
                "authorId": "2269719967",
                "name": "Zichao Li"
            },
            {
                "authorId": "2269468533",
                "name": "Ines Arous"
            },
            {
                "authorId": "145732771",
                "name": "Siva Reddy"
            },
            {
                "authorId": "2269460072",
                "name": "Jackie CK Cheung"
            }
        ],
        "abstract": "The potential of using a large language model (LLM) as a knowledge base (KB) has sparked significant interest. To manage the knowledge acquired by LLMs, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge. Existing work on editing LLMs has partially addressed the issue of dependency, when the editing of a fact should apply to its lexical variations without disrupting irrelevant ones. However, they neglect the dependency between a fact and its logical implications. We propose an evaluation protocol with an accompanying question-answering dataset, DepEdit, that provides a comprehensive assessment of the editing process considering the above notions of dependency. Our protocol involves setting up a controlled environment in which we edit facts and monitor their impact on LLMs, along with their implications based on If-Then rules. Extensive experiments on DepEdit show that existing knowledge editing methods are sensitive to the surface form of knowledge, and that they have limited performance in inferring the implications of edited facts."
    },
    {
        "paperId": "11e56b35fa81ddb00dc96329c077c5b39c8f9e75",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.02073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-04",
        "authors": [
            {
                "authorId": "2269473832",
                "name": "Giovanni Monea"
            },
            {
                "authorId": "35512303",
                "name": "Maxime Peyrard"
            },
            {
                "authorId": "65826567",
                "name": "Martin Josifoski"
            },
            {
                "authorId": "113810201",
                "name": "Vishrav Chaudhary"
            },
            {
                "authorId": "2269472030",
                "name": "Jason Eisner"
            },
            {
                "authorId": "2264962872",
                "name": "Emre Kiciman"
            },
            {
                "authorId": "2269473744",
                "name": "Hamid Palangi"
            },
            {
                "authorId": "27419446",
                "name": "Barun Patra"
            },
            {
                "authorId": "2269473532",
                "name": "Robert West"
            }
        ],
        "abstract": "Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a novel dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the internal parametric knowledge clashes with the contextual information. We benchmark various LLMs with Fakepedia and conduct a causal mediation analysis of LLM components when answering Fakepedia queries, based on our Masked Grouped Causal Tracing (MGCT) method. Through this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs."
    },
    {
        "paperId": "d69f081f218ce1a4f72c71aa0eb1b2a98934b9c1",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Integrating Pre-Trained Speech and Language Models for End-to-End Speech Recognition",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2312.03668",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.03668, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "30775505",
                "name": "Yukiya Hono"
            },
            {
                "authorId": "2270322584",
                "name": "Koh Mitsuda"
            },
            {
                "authorId": "2269818838",
                "name": "Tianyu Zhao"
            },
            {
                "authorId": "2059317757",
                "name": "Kentaro Mitsui"
            },
            {
                "authorId": "22589853",
                "name": "Toshiaki Wakatsuki"
            },
            {
                "authorId": "2253396533",
                "name": "Kei Sawada"
            }
        ],
        "abstract": "Advances in machine learning have made it possible to perform various text and speech processing tasks, such as automatic speech recognition (ASR), in an end-to-end (E2E) manner. E2E approaches utilizing pre-trained models are gaining attention for conserving training data and resources. However, most of their applications in ASR involve only one of either a pre-trained speech or a language model. This paper proposes integrating a pre-trained speech representation model and a large language model (LLM) for E2E ASR. The proposed model enables the optimization of the entire ASR process, including acoustic feature extraction and acoustic and language modeling, by combining pre-trained models with a bridge network and also enables the application of remarkable developments in LLM utilization, such as parameter-efficient domain adaptation and inference optimization. Experimental results demonstrate that the proposed model achieves a performance comparable to that of modern E2E ASR models by utilizing powerful pre-training models with the proposed integrated approach."
    },
    {
        "paperId": "89d3d19ad717cd534bdd1866d28e2da253147705",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.04916, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-08",
        "authors": [
            {
                "authorId": "2272979112",
                "name": "Yanxi Chen"
            },
            {
                "authorId": "2211993531",
                "name": "Xuchen Pan"
            },
            {
                "authorId": "2237607166",
                "name": "Yaliang Li"
            },
            {
                "authorId": "2266389996",
                "name": "Bolin Ding"
            },
            {
                "authorId": "2145786974",
                "name": "Jingren Zhou"
            }
        ],
        "abstract": "We present EE-LLM, a framework for large-scale training and inference of early-exit large language models (LLMs). While recent works have shown preliminary evidence for the efficacy of early exiting in accelerating LLM inference, EE-LLM makes a foundational step towards scaling up early-exit LLMs by supporting their training and inference with massive 3D parallelism. Built upon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and performance optimizations tailored to early exiting, including a lightweight method that facilitates backpropagation for the early-exit training objective with pipeline parallelism, techniques of leveraging idle resources in the original pipeline schedule for computation related to early-exit layers, and two approaches of early-exit inference that are compatible with KV caching for autoregressive generation. Our analytical and empirical study shows that EE-LLM achieves great training efficiency with negligible computational overhead compared to standard LLM training, as well as outstanding inference speedup without compromising output quality. To facilitate further research and adoption, we release EE-LLM at https://github.com/pan-x-c/EE-LLM."
    },
    {
        "paperId": "63b1b40781faf00a0d948bba653e0be02895e587",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.611.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.05434, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-09",
        "authors": [
            {
                "authorId": "2109380683",
                "name": "Hongzhan Lin"
            },
            {
                "authorId": "23523733",
                "name": "Ziyang Luo"
            },
            {
                "authorId": "2157404600",
                "name": "Jing Ma"
            },
            {
                "authorId": "143891667",
                "name": "Long Chen"
            }
        ],
        "abstract": "The age of social media is rife with memes. Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image. However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image. In this paper, we attempt to detect harmful memes based on advanced reasoning over the interplay of multimodal information in memes. Inspired by the success of Large Language Models (LLMs) on complex reasoning, we first conduct abductive reasoning with LLMs. Then we propose a novel generative framework to learn reasonable thoughts from LLMs for better multimodal fusion and lightweight fine-tuning, which consists of two training stages: 1) Distill multimodal reasoning knowledge from LLMs; and 2) Fine-tune the generative framework to infer harmfulness. Extensive experiments conducted on three meme datasets demonstrate that our proposed approach achieves superior performance than state-of-the-art methods on the harmful meme detection task."
    },
    {
        "paperId": "6cfbbf7604adda1df65932e3c4d157770a2df000",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Alignment for Honesty",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.07000, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-12",
        "authors": [
            {
                "authorId": "2145435513",
                "name": "Yuqing Yang"
            },
            {
                "authorId": "2273658317",
                "name": "Ethan Chern"
            },
            {
                "authorId": "2273725403",
                "name": "Xipeng Qiu"
            },
            {
                "authorId": "1700325",
                "name": "Graham Neubig"
            },
            {
                "authorId": "144118452",
                "name": "Pengfei Liu"
            }
        ],
        "abstract": "Recent research has made significant strides in aligning large language models (LLMs) with helpfulness and harmlessness. In this paper, we argue for the importance of alignment for \\emph{honesty}, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. However, a pivotal aspect of alignment for honesty involves discerning an LLM's knowledge boundaries, which demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies. We address these challenges by first establishing a precise problem definition and defining ``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone for developing metrics that effectively measure an LLM's honesty by quantifying its progress post-alignment. Furthermore, we introduce a flexible training framework which is further instantiated by several efficient fine-tuning techniques that emphasize honesty without sacrificing performance on other tasks. Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics. We open-source all relevant resources to facilitate future research at \\url{https://github.com/GAIR-NLP/alignment-for-honesty}."
    },
    {
        "paperId": "dc4d9b0c3c9c9cd9eb3a4c8d3ffa415d9953f77d",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Large Human Language Models: A Need and the Challenges",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.07751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-11-09",
        "authors": [
            {
                "authorId": "145297996",
                "name": "Nikita Soni"
            },
            {
                "authorId": "2273933343",
                "name": "H. A. Schwartz"
            },
            {
                "authorId": "2662374",
                "name": "Jo\u00e3o Sedoc"
            },
            {
                "authorId": "2273927019",
                "name": "Niranjan Balasubramanian"
            }
        ],
        "abstract": "As research in human-centered NLP advances, there is a growing recognition of the importance of incorporating human and social factors into NLP models. At the same time, our NLP systems have become heavily reliant on LLMs, most of which do not model authors. To build NLP systems that can truly understand human language, we must better integrate human contexts into LLMs. This brings to the fore a range of design considerations and challenges in terms of what human aspects to capture, how to represent them, and what modeling strategies to pursue. To address these, we advocate for three positions toward creating large human language models (LHLMs) using concepts from psychological and behavioral sciences: First, LM training should include the human context. Second, LHLMs should recognize that people are more than their group(s). Third, LHLMs should be able to account for the dynamic and temporally-dependent nature of the human context. We refer to relevant advances and present open challenges that need to be addressed and their possible solutions in realizing these goals."
    },
    {
        "paperId": "9e2a811a6f5d1c5352ce19ac24303810eb1867f7",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.07887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-13",
        "authors": [
            {
                "authorId": "2809051",
                "name": "Junhao Zheng"
            },
            {
                "authorId": "2273927657",
                "name": "Shengjie Qiu"
            },
            {
                "authorId": "2274027644",
                "name": "Qianli Ma"
            }
        ],
        "abstract": "Incremental Learning (IL) has been a long-standing problem in both vision and Natural Language Processing (NLP) communities. In recent years, as Pre-trained Language Models (PLMs) have achieved remarkable progress in various NLP downstream tasks, utilizing PLMs as backbones has become a common practice in recent research of IL in NLP. Most assume that catastrophic forgetting is the biggest obstacle to achieving superior IL performance and propose various techniques to overcome this issue. However, we find that this assumption is problematic. Specifically, we revisit more than 20 methods on four classification tasks (Text Classification, Intent Classification, Relation Extraction, and Named Entity Recognition) under the two most popular IL settings (Class-Incremental and Task-Incremental) and reveal that most of them severely underestimate the inherent anti-forgetting ability of PLMs. Based on the observation, we propose a frustratingly easy method called SEQ* for IL with PLMs. The results show that SEQ* has competitive or superior performance compared to state-of-the-art (SOTA) IL methods and requires considerably less trainable parameters and training time. These findings urge us to revisit the IL with PLMs and encourage future studies to have a fundamental understanding of the catastrophic forgetting in PLMs. The data, code and scripts are publicly available at https://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm."
    },
    {
        "paperId": "9cd0837a6204e62e0819fbd9238a4e41b18482aa",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.09085, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-14",
        "authors": [
            {
                "authorId": "2158524037",
                "name": "Rongwu Xu"
            },
            {
                "authorId": "2274119735",
                "name": "Brian S. Lin"
            },
            {
                "authorId": "2274185207",
                "name": "Shujian Yang"
            },
            {
                "authorId": "2274424870",
                "name": "Tianqi Zhang"
            },
            {
                "authorId": "2274921986",
                "name": "Weiyan Shi"
            },
            {
                "authorId": "2268671902",
                "name": "Tianwei Zhang"
            },
            {
                "authorId": "2274984656",
                "name": "Zhixuan Fang"
            },
            {
                "authorId": "2304324554",
                "name": "Wei Xu"
            },
            {
                "authorId": "2268515593",
                "name": "Han Qiu"
            }
        ],
        "abstract": "Large language models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs' susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs' belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs' correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies."
    },
    {
        "paperId": "0a0c3d8650d2c0ffeecd491356393b93bba63fed",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.12112, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-19",
        "authors": [
            {
                "authorId": "50842362",
                "name": "Nabeel Seedat"
            },
            {
                "authorId": "2275241328",
                "name": "Nicolas Huynh"
            },
            {
                "authorId": "2047304902",
                "name": "B. V. Breugel"
            },
            {
                "authorId": "1729969",
                "name": "M. Schaar"
            }
        ],
        "abstract": "Machine Learning (ML) in low-data settings remains an underappreciated yet crucial problem. Hence, data augmentation methods to increase the sample size of datasets needed for ML are key to unlocking the transformative potential of ML in data-deprived regions and domains. Unfortunately, the limited training set constrains traditional tabular synthetic data generators in their ability to generate a large and diverse augmented dataset needed for ML tasks. To address this challenge, we introduce CLLM, which leverages the prior knowledge of Large Language Models (LLMs) for data augmentation in the low-data regime. However, not all the data generated by LLMs will improve downstream utility, as for any generative model. Consequently, we introduce a principled curation mechanism, leveraging learning dynamics, coupled with confidence and uncertainty metrics, to obtain a high-quality dataset. Empirically, on multiple real-world datasets, we demonstrate the superior performance of CLLM in the low-data regime compared to conventional generators. Additionally, we provide insights into the LLM generation and curation mechanism, shedding light on the features that enable them to output high-quality augmented datasets."
    },
    {
        "paperId": "b7410d3cdfb18320882605b4792458ca6b1518ed",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Exploring the Sensitivity of LLMs' Decision-Making Capabilities: Insights from Prompt Variations and Hyperparameters",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.241.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.17476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-29",
        "authors": [
            {
                "authorId": "2210803167",
                "name": "Manikanta Loya"
            },
            {
                "authorId": "2273533986",
                "name": "Divya Sinha"
            },
            {
                "authorId": "2273578676",
                "name": "Richard Futrell"
            }
        ],
        "abstract": "The advancement of Large Language Models (LLMs) has led to their widespread use across a broad spectrum of tasks including decision making. Prior studies have compared the decision making abilities of LLMs with those of humans from a psychological perspective. However, these studies have not always properly accounted for the sensitivity of LLMs' behavior to hyperparameters and variations in the prompt. In this study, we examine LLMs' performance on the Horizon decision making task studied by Binz and Schulz (2023) analyzing how LLMs respond to variations in prompts and hyperparameters. By experimenting on three OpenAI language models possessing different capabilities, we observe that the decision making abilities fluctuate based on the input prompts and temperature settings. Contrary to previous findings language models display a human-like exploration exploitation tradeoff after simple adjustments to the prompt."
    },
    {
        "paperId": "4d3b2ce84634a4ab0222c0dfe13daf6b3516529f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.00757, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-01",
        "authors": [
            {
                "authorId": "2277575723",
                "name": "Yuxuan Wan"
            },
            {
                "authorId": "2144328160",
                "name": "Wenxuan Wang"
            },
            {
                "authorId": "2277415220",
                "name": "Yiliu Yang"
            },
            {
                "authorId": "2253848935",
                "name": "Youliang Yuan"
            },
            {
                "authorId": "2161306685",
                "name": "Jen-Tse Huang"
            },
            {
                "authorId": "40532404",
                "name": "Pinjia He"
            },
            {
                "authorId": "12386833",
                "name": "Wenxiang Jiao"
            },
            {
                "authorId": "2146840128",
                "name": "Michael R. Lyu"
            }
        ],
        "abstract": "We introduce LogicAsker, a novel approach for evaluating and enhancing the logical reasoning capabilities of large language models (LLMs) such as ChatGPT and GPT-4. Despite LLMs\u2019 prowess in tasks like writing assistance, code generation, and machine translation, assessing their ability to reason has been challenging. Traditional evaluations often prioritize accuracy on downstream tasks over direct assessments of reasoning processes. LogicAsker addresses this gap by employing a set of atomic reasoning skills grounded in propositional and predicate logic to systematically examine and improve the reasoning prowess of LLMs. Our methodology reveals significant gaps in LLMs\u2019 learning of logical rules, with identified reasoning failures ranging from 29% to 90% across different models. Moreover, we leverage these findings to construct targeted demonstration examples and fine-tune data, notably enhancing logical reasoning in models like GPT-4o by up to 5%. To our knowledge, this is the first effort to utilize test case outcomes to effectively refine LLMs\u2019 formal reasoning capabilities. We make our code, data, and results publicly available(https://github.com/yxwan123/LogicAsker) to facilitate further research and replication of our findings."
    },
    {
        "paperId": "6b2cbaf69bd372c90aba9781721d79893f4a2fc4",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.01275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-02",
        "authors": [
            {
                "authorId": "2071635049",
                "name": "Quan Tu"
            },
            {
                "authorId": "2277501740",
                "name": "Shilong Fan"
            },
            {
                "authorId": "2277648904",
                "name": "Zihang Tian"
            },
            {
                "authorId": "2277448117",
                "name": "Rui Yan"
            }
        ],
        "abstract": "Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 23,020 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation. Source code, data source and reward model will be publicly accessible at https://github.com/morecry/CharacterEval."
    },
    {
        "paperId": "c562b5abd8283f77bf44e5030c4f6ff8a8092428",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "\u03b4-CAUSAL: Exploring Defeasibility in Causal Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.03183, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-06",
        "authors": [
            {
                "authorId": "2278427436",
                "name": "Shaobo Cui"
            },
            {
                "authorId": "2133325891",
                "name": "Lazar Milikic"
            },
            {
                "authorId": "2278783946",
                "name": "Yiyang Feng"
            },
            {
                "authorId": "2190955045",
                "name": "Mete Ismayilzada"
            },
            {
                "authorId": "2261760962",
                "name": "Debjit Paul"
            },
            {
                "authorId": "2691021",
                "name": "Antoine Bosselut"
            },
            {
                "authorId": "1735128",
                "name": "B. Faltings"
            }
        ],
        "abstract": "Defeasibility in causal reasoning implies that the causal relationship between cause and effect can be strengthened or weakened. Namely, the causal strength between cause and effect should increase or decrease with the incorporation of strengthening arguments (supporters) or weakening arguments (defeaters), respectively. However, existing works ignore defeasibility in causal reasoning and fail to evaluate existing causal strength metrics in defeasible settings. In this work, we present $\\delta$-CAUSAL, the first benchmark dataset for studying defeasibility in causal reasoning. $\\delta$-CAUSAL includes around 11K events spanning ten domains, featuring defeasible causality pairs, i.e., cause-effect pairs accompanied by supporters and defeaters. We further show current causal strength metrics fail to reflect the change of causal strength with the incorporation of supporters or defeaters in $\\delta$-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation with Attention Rating), a metric that measures causal strength based on token-level causal relationships. CESAR achieves a significant 69.7% relative improvement over existing metrics, increasing from 47.2% to 80.1% in capturing the causal strength change brought by supporters and defeaters. We further demonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and 10.7 points behind humans in generating supporters and defeaters, emphasizing the challenge posed by $\\delta$-CAUSAL."
    },
    {
        "paperId": "e4d31c8a94acc557bf21c8aa0936b3c3593ba123",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "PIXAR: Auto-Regressive Language Modeling in Pixel Space",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.03321, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-06",
        "authors": [
            {
                "authorId": "2278434997",
                "name": "Yintao Tai"
            },
            {
                "authorId": "2278437465",
                "name": "Xiyang Liao"
            },
            {
                "authorId": "3444866",
                "name": "Alessandro Suglia"
            },
            {
                "authorId": "14341992",
                "name": "Antonio Vergari"
            }
        ],
        "abstract": "Recent work showed the possibility of building open-vocabulary large language models (LLMs) that directly operate on pixel representations. These models are implemented as autoencoders that reconstruct masked patches of rendered text. However, these pixel-based LLMs are limited to discriminative tasks (e.g., classification) and, similar to BERT, cannot be used to generate text. Therefore, they cannot be used for generative tasks such as free-form question answering. In this work, we introduce PIXAR, the first pixel-based autoregressive LLM that performs text generation. Consisting of only a decoder, PIXAR can perform free-form generative tasks while keeping the number of parameters on par with previous encoder-decoder models. Furthermore, we highlight the challenges of generating text as non-noisy images and show this is due to using a maximum likelihood objective. To overcome this problem, we propose an adversarial pretraining stage that improves the readability and accuracy of PIXAR by 8.1 on LAMBADA and 8.5 on bAbI -- making it comparable to GPT-2 on text generation tasks. This paves the way to build open-vocabulary LLMs that operate on perceptual input only and calls into question the necessity of the usual symbolic input representation, i.e., text as (sub)tokens."
    },
    {
        "paperId": "1edc9a2b5210e23d5ceccc5cc1b5848dbb4ceab4",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "InFoBench: Evaluating Instruction Following Ability in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.03601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-07",
        "authors": [
            {
                "authorId": "2143528602",
                "name": "Yiwei Qin"
            },
            {
                "authorId": "50982080",
                "name": "Kaiqiang Song"
            },
            {
                "authorId": "2218546567",
                "name": "Yebowen Hu"
            },
            {
                "authorId": "2087264100",
                "name": "Wenlin Yao"
            },
            {
                "authorId": "2173531",
                "name": "Sangwoo Cho"
            },
            {
                "authorId": "2250363276",
                "name": "Xiaoyang Wang"
            },
            {
                "authorId": "2145346360",
                "name": "Xuansheng Wu"
            },
            {
                "authorId": "2278462154",
                "name": "Fei Liu"
            },
            {
                "authorId": "2279028362",
                "name": "Pengfei Liu"
            },
            {
                "authorId": "2256336899",
                "name": "Dong Yu"
            }
        ],
        "abstract": "This paper introduces the Decomposed Requirements Following Ratio (DRFR), a new metric for evaluating Large Language Models' (LLMs) ability to follow instructions. Addressing a gap in current methodologies, DRFR breaks down complex instructions into simpler criteria, facilitating a detailed analysis of LLMs' compliance with various aspects of tasks. Alongside this metric, we present InFoBench, a benchmark comprising 500 diverse instructions and 2,250 decomposed questions across multiple constraint categories. Our experiments compare DRFR with traditional scoring methods and explore annotation sources, including human experts, crowd-sourced workers, and GPT-4. The findings demonstrate DRFR's higher reliability and the effectiveness of using GPT-4 as a cost-efficient annotator. The evaluation of several advanced LLMs using this framework reveals their strengths and areas needing improvement, particularly in complex instruction-following. This study contributes a novel metric and benchmark, offering insights for future LLM development and evaluation."
    },
    {
        "paperId": "18b3b96ffb28c785452081aa367cbc02a1cf7567",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "MERA: A Comprehensive LLM Evaluation in Russian",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.04531, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-09",
        "authors": [
            {
                "authorId": "10054744",
                "name": "Alena Fenogenova"
            },
            {
                "authorId": "2278795222",
                "name": "Artem Chervyakov"
            },
            {
                "authorId": "2180794333",
                "name": "Nikita Martynov"
            },
            {
                "authorId": "2278796972",
                "name": "Anastasia Kozlova"
            },
            {
                "authorId": "1564571932",
                "name": "M. Tikhonova"
            },
            {
                "authorId": "2188733371",
                "name": "Albina Akhmetgareeva"
            },
            {
                "authorId": "146059392",
                "name": "Anton A. Emelyanov"
            },
            {
                "authorId": "80739149",
                "name": "Denis Shevelev"
            },
            {
                "authorId": "2278792366",
                "name": "Pavel Lebedev"
            },
            {
                "authorId": "102315613",
                "name": "Leonid S. Sinev"
            },
            {
                "authorId": "2103078579",
                "name": "Ulyana Isaeva"
            },
            {
                "authorId": "2232601767",
                "name": "Katerina Kolomeytseva"
            },
            {
                "authorId": "2268318729",
                "name": "Daniil Moskovskiy"
            },
            {
                "authorId": "2266389312",
                "name": "Elizaveta Goncharova"
            },
            {
                "authorId": "2198165070",
                "name": "Nikita Savushkin"
            },
            {
                "authorId": "2278793085",
                "name": "Polina Mikhailova"
            },
            {
                "authorId": "2254284540",
                "name": "Denis Dimitrov"
            },
            {
                "authorId": "2266390354",
                "name": "Alexander Panchenko"
            },
            {
                "authorId": "143659208",
                "name": "Sergey Markov"
            }
        ],
        "abstract": "Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features. However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in zero- and few-shot fixed instruction settings that can be extended to other modalities. We propose an evaluation methodology, an open-source code base for the MERA assessment, and a leaderboard with a submission system. We evaluate open LMs as baselines and find that they are still far behind the human level. We publicly release MERA to guide forthcoming research, anticipate groundbreaking model features, standardize the evaluation procedure, and address potential societal drawbacks."
    },
    {
        "paperId": "a9f5e62bd132e43dd300fefac71093ef5c7c8596",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.04679, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-09",
        "authors": [
            {
                "authorId": "2204967430",
                "name": "Mahdi Nikdan"
            },
            {
                "authorId": "2212367160",
                "name": "Soroush Tabesh"
            },
            {
                "authorId": "3311387",
                "name": "Dan Alistarh"
            }
        ],
        "abstract": "We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis that jointly trains $\\textit{low-rank}$ and $\\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms LoRA, pure sparse fine-tuning, and alternative hybrid methods at the same parameter budget, and can even recover the performance of FFT on some tasks. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memory- and computationally-efficient training, and show that it is also compatible with low-precision base weights, resulting in the first joint representation combining quantization, low-rank and sparse approximations. Our code is available at https://github.com/IST-DASLab/RoSA."
    },
    {
        "paperId": "7bd792c6541d794c7b1a9150a2ef910206143afd",
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "title": "Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.04854, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-10",
        "authors": [
            {
                "authorId": "2278838325",
                "name": "Harvey Lederman"
            },
            {
                "authorId": "2261742833",
                "name": "Kyle Mahowald"
            }
        ],
        "abstract": "Abstract Are LLMs cultural technologies like photocopiers or printing presses, which transmit information but cannot create new content? A challenge for this idea, which we call bibliotechnism, is that LLMs generate novel text. We begin with a defense of bibliotechnism, showing how even novel text may inherit its meaning from original human-generated text. We then argue that bibliotechnism faces an independent challenge from examples in which LLMs generate novel reference, using new names to refer to new entities. Such examples could be explained if LLMs were not cultural technologies but had beliefs, desires, and intentions. According to interpretationism in the philosophy of mind, a system has such attitudes if and only if its behavior is well explained by the hypothesis that it does. Interpretationists may hold that LLMs have attitudes, and thus have a simple solution to the novel reference problem. We emphasize, however, that interpretationism is compatible with very simple creatures having attitudes and differs sharply from views that presuppose these attitudes require consciousness, sentience, or intelligence (topics about which we make no claims)."
    },
    {
        "paperId": "c858999fdfe11f93eb50bcb0eb6920f877e385ad",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "CASA: Causality-driven Argument Sufficiency Assessment",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.05249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-10",
        "authors": [
            {
                "authorId": "49543720",
                "name": "Xiao Liu"
            },
            {
                "authorId": "2273532365",
                "name": "Yansong Feng"
            },
            {
                "authorId": "2278984743",
                "name": "Kai-Wei Chang"
            }
        ],
        "abstract": "The argument sufficiency assessment task aims to determine if the premises of a given argument support its conclusion.To tackle this task, existing works often train a classifier on data annotated by humans. However, annotating data is laborious, and annotations are often inconsistent due to subjective criteria. Motivated by the definition of probability of sufficiency (PS) in the causal literature, we proposeCASA, a zero-shot causality-driven argument sufficiency assessment framework. PS measures how likely introducing the premise event would lead to the conclusion when both the premise and conclusion events are absent. To estimate this probability, we propose to use large language models (LLMs) to generate contexts that are inconsistent with the premise and conclusion and revise them by injecting the premise event.Experiments on two logical fallacy detection datasets demonstrate that CASA accurately identifies insufficient arguments. We further deploy CASA in a writing assistance application, and find that suggestions generated by CASA enhance the sufficiency of student-written arguments. Code and data are available at https://github.com/xxxiaol/CASA."
    },
    {
        "paperId": "867e4e0a9574d8e01fe7003179b3fd9cd809516f",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "I am a Strange Dataset: Metalinguistic Tests for Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.05300, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-10",
        "authors": [
            {
                "authorId": "1500242049",
                "name": "Tristan Thrush"
            },
            {
                "authorId": "2279123719",
                "name": "Jared Moore"
            },
            {
                "authorId": "2278829003",
                "name": "Miguel Monares"
            },
            {
                "authorId": "2279335958",
                "name": "Christopher Potts"
            },
            {
                "authorId": "2111313627",
                "name": "Douwe Kiela"
            }
        ],
        "abstract": "Statements involving metalinguistic self-reference (\"This paper has six sections.\") are prevalent in many domains. Can current large language models (LLMs) handle such language? In this paper, we present\"I am a Strange Dataset\", a new dataset for addressing this question. There are two subtasks: generation and verification. In generation, models continue statements like\"The penultimate word in this sentence is\"(where a correct continuation is\"is\"). In verification, models judge the truth of statements like\"The penultimate word in this sentence is sentence.\"(false). We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all. The dataset is hand-crafted by experts and validated by non-expert annotators. We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs. All models perform close to chance across both subtasks and even on the non-self-referential metalinguistic control data, though we find some steady improvement with model scale. GPT 4 is the only model to consistently do significantly better than chance, and it is still only in the 60% range, while our untrained human annotators score well in the 89-93% range. The dataset and evaluation toolkit are available at https://github.com/TristanThrush/i-am-a-strange-dataset."
    },
    {
        "paperId": "732ce53c573475f2691a7cfc716cf4f568d17360",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.06373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-12",
        "authors": [
            {
                "authorId": "2273035656",
                "name": "Yi Zeng"
            },
            {
                "authorId": "2279410589",
                "name": "Hongpeng Lin"
            },
            {
                "authorId": "2107959264",
                "name": "Jingwen Zhang"
            },
            {
                "authorId": "2263629011",
                "name": "Diyi Yang"
            },
            {
                "authorId": "2254249161",
                "name": "Ruoxi Jia"
            },
            {
                "authorId": "2266973626",
                "name": "Weiyan Shi"
            }
        ],
        "abstract": "Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP and, found a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs"
    },
    {
        "paperId": "a8798c4a2352a118a369678b164ab4c249004642",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.06408, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-12",
        "authors": [
            {
                "authorId": "15983089",
                "name": "Li Lucy"
            },
            {
                "authorId": "40895369",
                "name": "Suchin Gururangan"
            },
            {
                "authorId": "3328733",
                "name": "Luca Soldaini"
            },
            {
                "authorId": "2268272",
                "name": "Emma Strubell"
            },
            {
                "authorId": "2064411219",
                "name": "David Bamman"
            },
            {
                "authorId": "2279335512",
                "name": "Lauren Klein"
            },
            {
                "authorId": "34176020",
                "name": "Jesse Dodge"
            }
        ],
        "abstract": "Large language models' (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage are under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten\"quality\"and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will encourage a new line of research on pretraining data curation practices and its social implications."
    },
    {
        "paperId": "18fe825d347d2ae47d23378dc2eee015875d18e9",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Mission: Impossible Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.06416, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-12",
        "authors": [
            {
                "authorId": "2140489566",
                "name": "Julie Kallini"
            },
            {
                "authorId": "1665756307",
                "name": "Isabel Papadimitriou"
            },
            {
                "authorId": "2585394",
                "name": "Richard Futrell"
            },
            {
                "authorId": "2412497",
                "name": "Kyle Mahowald"
            },
            {
                "authorId": "2279335958",
                "name": "Christopher Potts"
            }
        ],
        "abstract": "Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout training to compare the learning process for each language. Our core finding is that GPT-2 struggles to learn impossible languages when compared to English as a control, challenging the core claim. More importantly, we hope our approach opens up a productive line of inquiry in which different LLM architectures are tested on a variety of impossible languages in an effort to learn more about how LLMs can be used as tools for these cognitive and typological investigations."
    },
    {
        "paperId": "75b874acc4eb463e177e88bb73c668ccc1aae0a2",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Two Directions for Clinical Data Generation with Large Language Models: Data-to-Label and Label-to-Data",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.474.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.06774, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-01",
        "authors": [
            {
                "authorId": "2237106751",
                "name": "Rumeng Li"
            },
            {
                "authorId": "2221230794",
                "name": "Xun Wang"
            },
            {
                "authorId": "2273658683",
                "name": "Hong Yu"
            }
        ],
        "abstract": "Large language models (LLMs) can generate natural language texts for various domains and tasks, but their potential for clinical text mining, a domain with scarce, sensitive, and imbalanced medical data, is under-explored. We investigate whether LLMs can augment clinical data for detecting Alzheimer's Disease (AD)-related signs and symptoms from electronic health records (EHRs), a challenging task that requires high expertise. We create a novel pragmatic taxonomy for AD sign and symptom progression based on expert knowledge and generated three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method, which labels sentences from a public EHR collection with AD-related signs and symptoms; and (3) a bronze dataset created by the label-to-data method which generates sentences with AD-related signs and symptoms based on the label definition. We train a system to detect AD-related signs and symptoms from EHRs. We find that the silver and bronze datasets improves the system performance, outperforming the system using only the gold dataset. This shows that LLMs can generate synthetic clinical data for a complex task by incorporating expert knowledge, and our label-to-data method can produce datasets that are free of sensitive information, while maintaining acceptable quality."
    },
    {
        "paperId": "b39d32b7b9b11019a579b182e0fd33cd511a4003",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Structsum Generation for Faster Text Comprehension",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.06837, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-12",
        "authors": [
            {
                "authorId": "2719746",
                "name": "Parag Jain"
            },
            {
                "authorId": "2279548162",
                "name": "Andreea Marzoca"
            },
            {
                "authorId": "2261956317",
                "name": "Francesco Piccinno"
            }
        ],
        "abstract": "We consider the task of generating structured representations of text using large language models (LLMs). We focus on tables and mind maps as representative modalities. Tables are more organized way of representing data, while mind maps provide a visually dynamic and flexible approach, particularly suitable for sparse content. Despite the effectiveness of LLMs on different tasks, we show that current models struggle with generating structured outputs. In response, we present effective prompting strategies for both of these tasks. We introduce a taxonomy of problems around factuality, global and local structure, common to both modalities and propose a set of critiques to tackle these issues resulting in an absolute improvement in accuracy of +37pp (79%) for mind maps and +15pp (78%) for tables. To evaluate semantic coverage of generated structured representations we propose Auto-QA, and we verify the adequacy of Auto-QA using SQuAD dataset. We further evaluate the usefulness of structured representations via a text comprehension user study. The results show a significant reduction in comprehension time compared to text when using table (42.9%) and mind map (31.9%), without loss in accuracy."
    },
    {
        "paperId": "c2eeef03f0c0d85237fe64b8da3a44d6170dbf32",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.08438, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-06",
        "authors": [
            {
                "authorId": "2190336685",
                "name": "Yaojia Lv"
            },
            {
                "authorId": "2272745453",
                "name": "Haojie Pan"
            },
            {
                "authorId": "2108727290",
                "name": "Zekun Wang"
            },
            {
                "authorId": "2884436",
                "name": "Ruiji Fu"
            },
            {
                "authorId": "2112748107",
                "name": "Ming Liu"
            },
            {
                "authorId": "2272922866",
                "name": "Zhongyuan Wang"
            },
            {
                "authorId": "2247852651",
                "name": "Bing Qin"
            }
        ],
        "abstract": "Cognitive dynamics are pivotal to advance human understanding of the world. Recent advancements in large language models (LLMs) reveal their potential for cognitive simulation. However, these LLM-based cognitive studies primarily focus on static modeling, overlooking the dynamic nature of cognition. To bridge this gap, we propose the concept of the cognitive dynamics of LLMs and present a corresponding task with the inspiration of longitudinal studies. Towards the task, we develop CogBench, a novel benchmark to assess the cognitive dynamics of LLMs and validate it through participant surveys. We also design two evaluation metrics for CogBench, including Authenticity and Rationality. Recognizing the inherent static nature of LLMs, we introduce CogGPT for the task, which features an innovative iterative cognitive mechanism aimed at enhancing lifelong cognitive dynamics. Empirical results demonstrate the superiority of CogGPT over existing methods, particularly in its ability to facilitate role-specific cognitive dynamics under continuous information flows."
    },
    {
        "paperId": "684ede7013b3d844dae13090350a5c27b196727f",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.11880, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-22",
        "authors": [
            {
                "authorId": "2218417690",
                "name": "Zaibin Zhang"
            },
            {
                "authorId": "2280256177",
                "name": "Yongting Zhang"
            },
            {
                "authorId": "2280261471",
                "name": "Lijun Li"
            },
            {
                "authorId": "2280292237",
                "name": "Hongzhi Gao"
            },
            {
                "authorId": "2118354131",
                "name": "Lijun Wang"
            },
            {
                "authorId": "2279962721",
                "name": "Huchuan Lu"
            },
            {
                "authorId": "2280259690",
                "name": "Feng Zhao"
            },
            {
                "authorId": "2265493981",
                "name": "Yu Qiao"
            },
            {
                "authorId": "2280138285",
                "name": "Jing Shao"
            }
        ],
        "abstract": "Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit profound capabilities in collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. In this paper, we explore these concerns through the innovative lens of agent psychology, revealing that the dark psychological states of agents constitute a significant threat to safety. To tackle these concerns, we propose a comprehensive framework (PsySafe) grounded in agent psychology, focusing on three key areas: firstly, identifying how dark personality traits in agents can lead to risky behaviors; secondly, evaluating the safety of multi-agent systems from the psychological and behavioral perspectives, and thirdly, devising effective strategies to mitigate these risks. Our experiments reveal several intriguing phenomena, such as the collective dangerous behaviors among agents, agents' self-reflection when engaging in dangerous behavior, and the correlation between agents' psychological assessments and dangerous behaviors. We anticipate that our framework and observations will provide valuable insights for further research into the safety of multi-agent systems. We will make our data and code publicly accessible at https://github.com/AI4Good24/PsySafe."
    },
    {
        "paperId": "f6a141b2ef027033119db5e8e1ab816f28ab1ec7",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Revisiting Demonstration Selection Strategies in In-Context Learning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.12087, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-22",
        "authors": [
            {
                "authorId": "2065263152",
                "name": "Keqin Peng"
            },
            {
                "authorId": "46573238",
                "name": "Liang Ding"
            },
            {
                "authorId": "2280289498",
                "name": "Yancheng Yuan"
            },
            {
                "authorId": "2256344322",
                "name": "Xuebo Liu"
            },
            {
                "authorId": "2269805934",
                "name": "Min Zhang"
            },
            {
                "authorId": "1797641",
                "name": "Y. Ouyang"
            },
            {
                "authorId": "2255502438",
                "name": "D. Tao"
            }
        ],
        "abstract": "Large language models (LLMs) have shown an impressive ability to perform a wide range of tasks using in-context learning (ICL), where a few examples are used to describe a task to the model. However, the performance of ICL varies significantly with the choice of demonstrations, and it is still unclear why this happens or what factors will influence its choice. In this work, we first revisit the factors contributing to this variance from both data and model aspects, and find that the choice of demonstration is both data- and model-dependent. We further proposed a data- and model-dependent demonstration selection method, \\textbf{TopK + ConE}, based on the assumption that \\textit{the performance of a demonstration positively correlates with its contribution to the model's understanding of the test samples}, resulting in a simple and effective recipe for ICL. Empirically, our method yields consistent improvements in both language understanding and generation tasks with different model scales. Further analyses confirm that, besides the generality and stability under different circumstances, our method provides a unified explanation for the effectiveness of previous methods. Code will be released."
    },
    {
        "paperId": "03092a3f26ed774af11cf05082a3d48e6ae1429d",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "SLANG: New Concept Comprehension of Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2401.12585",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.12585, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-23",
        "authors": [
            {
                "authorId": "2280335494",
                "name": "Lingrui Mei"
            },
            {
                "authorId": "2280336349",
                "name": "Shenghua Liu"
            },
            {
                "authorId": "2280382891",
                "name": "Yiwei Wang"
            },
            {
                "authorId": "2280332813",
                "name": "Baolong Bi"
            },
            {
                "authorId": "2280359349",
                "name": "Xueqi Chen"
            }
        ],
        "abstract": "The dynamic nature of language, particularly evident in the realm of slang and memes on the Internet, poses serious challenges to the adaptability of Large Language Models (LLMs). Traditionally anchored to static datasets, these models often struggle to keep up with the rapid linguistic evolution characteristic of online communities. This research aims to bridge this gap by enhancing LLMs\u2019 comprehension of the evolving new concepts on the Internet, without the high cost of continual retraining. In pursuit of this goal, we introduce SLNAG, a benchmark designed to autonomously integrate novel data and assess LLMs\u2019 ability to comprehend emerging concepts, alongside FOCUS, an approach uses causal inference to enhance LLMs to understand new phrases and their colloquial context. Our benchmark and approach involves understanding real-world instances of linguistic shifts, serving as contextual beacons, to form more precise and contextually relevant connections between newly emerging expressions and their meanings. The empirical analysis shows that our causal inference-based approach outperforms the baseline methods in terms of precision and relevance in the comprehension of Internet slang and memes."
    },
    {
        "paperId": "19261c6ad20c6c1e5585a8afcb88196173cbc8a6",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.13919, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-25",
        "authors": [
            {
                "authorId": "144987681",
                "name": "Hongliang He"
            },
            {
                "authorId": "2087264100",
                "name": "Wenlin Yao"
            },
            {
                "authorId": "22244290",
                "name": "Kaixin Ma"
            },
            {
                "authorId": "2265843326",
                "name": "Wenhao Yu"
            },
            {
                "authorId": "2281312832",
                "name": "Yong Dai"
            },
            {
                "authorId": "2254831297",
                "name": "Hongming Zhang"
            },
            {
                "authorId": "2258395889",
                "name": "Zhenzhong Lan"
            },
            {
                "authorId": "2261392681",
                "name": "Dong Yu"
            }
        ],
        "abstract": "The rapid advancement of large language models (LLMs) has led to a new era marked by the development of autonomous applications in real-world scenarios, which drives innovation in creating advanced web agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we establish a new benchmark by compiling real-world tasks from 15 popular websites and introduce an automatic evaluation protocol leveraging multimodal understanding abilities of GPT-4V to evaluate open-ended web agents. We show that WebVoyager achieves a 59.1% task success rate on our benchmark, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement with human judgment, indicating its effectiveness in providing reliable and accurate assessments of web agents."
    },
    {
        "paperId": "99f53c35aaeec3c7c4a02eb3ca30a303a3d8291b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.15498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-27",
        "authors": [
            {
                "authorId": "2294313959",
                "name": "Caiqi Zhang"
            },
            {
                "authorId": "2681038",
                "name": "Zhijiang Guo"
            },
            {
                "authorId": "2266465838",
                "name": "Andreas Vlachos"
            }
        ],
        "abstract": "This paper investigates the potential benefits of language-specific fact-checking models, focusing on the case of Chinese using CHEF dataset. To better reflect real-world fact-checking, we first develop a novel Chinese document-level evidence retriever, achieving state-of-the-art performance. We then demonstrate the limitations of translation-based methods and multilingual language models, highlighting the need for language-specific systems. To better analyze token-level biases in different systems, we construct an adversarial dataset based on the CHEF dataset, where each instance has a large word overlap with the original one but holds the opposite veracity label. Experimental results on the CHEF dataset and our adversarial dataset show that our proposed method outperforms translation-based methods and multilingual language models and is more robust toward biases, emphasizing the importance of language-specific fact-checking systems."
    },
    {
        "paperId": "94701d9c6cfc1aecc174ff62ccda939f790c1710",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "ReGAL: Refactoring Programs to Discover Generalizable Abstractions",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.16467, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-29",
        "authors": [
            {
                "authorId": "2281825070",
                "name": "Elias Stengel-Eskin"
            },
            {
                "authorId": "1677896557",
                "name": "Archiki Prasad"
            },
            {
                "authorId": "2281826842",
                "name": "Mohit Bansal"
            }
        ],
        "abstract": "While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality. Generating redundant code from scratch is both inefficient and error-prone. To address this, we propose Refactoring for Generalizable Abstraction Learning (ReGAL), a gradient-free method for learning a library of reusable functions via code refactorization, i.e., restructuring code without changing its execution output. ReGAL learns from a small set of existing programs, iteratively verifying and refining its abstractions via execution. We find that the shared function libraries discovered by ReGAL make programs easier to predict across diverse domains. On five datasets -- LOGO graphics generation, Date reasoning, TextCraft (a Minecraft-based text-game) MATH, and TabMWP -- both open-source and proprietary LLMs improve in accuracy when predicting programs with ReGAL functions. For CodeLlama-13B, ReGAL results in absolute accuracy increases of 11.5% on LOGO, 26.1% on date understanding, and 8.1% on TextCraft, outperforming GPT-3.5 in two of three domains. Our analysis reveals ReGAL's abstractions encapsulate frequently-used subroutines as well as environment dynamics."
    },
    {
        "paperId": "e086c452000a58b4695383e9774dfbaeed912065",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.16475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-29",
        "authors": [
            {
                "authorId": "52019849",
                "name": "Jan Trienes"
            },
            {
                "authorId": "2216725032",
                "name": "Sebastian Antony Joseph"
            },
            {
                "authorId": "2224014870",
                "name": "Jorg Schlotterer"
            },
            {
                "authorId": "2194180216",
                "name": "Christin Seifert"
            },
            {
                "authorId": "46258841",
                "name": "Kyle Lo"
            },
            {
                "authorId": "2281906500",
                "name": "Wei Xu"
            },
            {
                "authorId": "2281826929",
                "name": "Byron C. Wallace"
            },
            {
                "authorId": "2281900870",
                "name": "Junyi Jessy Li"
            }
        ],
        "abstract": "Text simplification aims to make technical texts more accessible to laypeople but often results in deletion of information and vagueness. This work proposes InfoLossQA, a framework to characterize and recover simplification-induced information loss in form of question-and-answer (QA) pairs. Building on the theory of Question Under Discussion, the QA pairs are designed to help readers deepen their knowledge of a text. We conduct a range of experiments with this framework. First, we collect a dataset of 1,000 linguist-curated QA pairs derived from 104 LLM simplifications of scientific abstracts of medical studies. Our analyses of this data reveal that information loss occurs frequently, and that the QA pairs give a high-level overview of what information was lost. Second, we devise two methods for this task: end-to-end prompting of open-source and commercial language models, and a natural language inference pipeline. With a novel evaluation framework considering the correctness of QA pairs and their linguistic suitability, our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss."
    },
    {
        "paperId": "3a0e3775f9f8870f4b2301d897b63d4ef789aa95",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Recent Advances in Online Hate Speech Moderation: Multimodality and the Role of Large Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.16727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-30",
        "authors": [
            {
                "authorId": "72043108",
                "name": "Ming Shan Hee"
            },
            {
                "authorId": "1491627343",
                "name": "Shivam Sharma"
            },
            {
                "authorId": "2261821068",
                "name": "Rui Cao"
            },
            {
                "authorId": "2281824700",
                "name": "Palash Nandi"
            },
            {
                "authorId": "2026545715",
                "name": "Preslav Nakov"
            },
            {
                "authorId": "2256999352",
                "name": "Tanmoy Chakraborty"
            },
            {
                "authorId": "2282413929",
                "name": "Roy Ka-Wei Lee"
            }
        ],
        "abstract": "In the evolving landscape of online communication, moderating hate speech (HS) presents an intricate challenge, compounded by the multimodal nature of digital content. This comprehensive survey delves into the recent strides in HS moderation, spotlighting the burgeoning role of large language models (LLMs) and large multimodal models (LMMs). Our exploration begins with a thorough analysis of current literature, revealing the nuanced interplay between textual, visual, and auditory elements in propagating HS. We uncover a notable trend towards integrating these modalities, primarily due to the complexity and subtlety with which HS is disseminated. A significant emphasis is placed on the advances facilitated by LLMs and LMMs, which have begun to redefine the boundaries of detection and moderation capabilities. We identify existing gaps in research, particularly in the context of underrepresented languages and cultures, and the need for solutions to handle low-resource settings. The survey concludes with a forward-looking perspective, outlining potential avenues for future research, including the exploration of novel AI methodologies, the ethical governance of AI in moderation, and the development of more nuanced, context-aware systems. This comprehensive overview aims to catalyze further research and foster a collaborative effort towards more sophisticated, responsible, and human-centric approaches to HS moderation in the digital era. WARNING: This paper contains offensive examples."
    },
    {
        "paperId": "f09904d113b33071385d39655e40b465c55e8784",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Conditional and Modal Reasoning in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.17169, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-30",
        "authors": [
            {
                "authorId": "2281827796",
                "name": "Wesley H. Holliday"
            },
            {
                "authorId": "50878879",
                "name": "M. Mandelkern"
            }
        ],
        "abstract": "The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in AI and cognitive science. In this paper, we probe the extent to which twenty-nine LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., '*If* Ann has a queen, *then* Bob has a jack\u2019) and epistemic modals (e.g., \u2018Ann *might* have an ace\u2019, \u2018Bob *must* have a king\u2019). These inferences have been of special interest to logicians, philosophers, and linguists, since they play a central role in the fundamental human ability to reason about distal possibilities. Assessing LLMs on these inferences is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. All the LLMs we tested make some basic mistakes with conditionals or modals, though zero-shot chain-of-thought prompting helps them make fewer mistakes. Even the best performing LLMs make basic errors in modal reasoning, display logically inconsistent judgments across inference patterns involving epistemic modals and conditionals, and give answers about complex conditional inferences that do not match reported human judgments. These results highlight gaps in basic logical reasoning in today\u2019s LLMs."
    },
    {
        "paperId": "95a9e813983047a53668f558e1b4011612f90092",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Multipath parsing in the brain",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.18046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-31",
        "authors": [
            {
                "authorId": "1725409453",
                "name": "Berta Franzluebbers"
            },
            {
                "authorId": "2042230065",
                "name": "Donald Dunagan"
            },
            {
                "authorId": "2156788173",
                "name": "Milovs Stanojevi'c"
            },
            {
                "authorId": "2281942875",
                "name": "Jan Buys"
            },
            {
                "authorId": "2237452309",
                "name": "John T. Hale"
            }
        ],
        "abstract": "Humans understand sentences word-by-word, in the order that they hear them. This incrementality entails resolving temporary ambiguities about syntactic relationships. We investigate how humans process these syntactic ambiguities by correlating predictions from incremental generative dependency parsers with timecourse data from people undergoing functional neuroimaging while listening to an audiobook. In particular, we compare competing hypotheses regarding the number of developing syntactic analyses in play during word-by-word comprehension: one vs more than one. This comparison involves evaluating syntactic surprisal from a state-of-the-art dependency parser with LLM-adapted encodings against an existing fMRI dataset. In both English and Chinese data, we find evidence for multipath parsing. Brain regions associated with this multipath effect include bilateral superior temporal gyrus."
    },
    {
        "paperId": "6f26ea6f51093253eab6e91fd7b4d90b17f932cf",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.18070, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-31",
        "authors": [
            {
                "authorId": "2023300626",
                "name": "Andreas Opedal"
            },
            {
                "authorId": "2175480389",
                "name": "Alessandro Stolfo"
            },
            {
                "authorId": "2281942924",
                "name": "Haruki Shirakami"
            },
            {
                "authorId": "2220651550",
                "name": "Ying Jiao"
            },
            {
                "authorId": "2295665129",
                "name": "Ryan Cotterell"
            },
            {
                "authorId": "2261392474",
                "name": "Bernhard Scholkopf"
            },
            {
                "authorId": "2407368",
                "name": "Abulhair Saparov"
            },
            {
                "authorId": "2790926",
                "name": "Mrinmaya Sachan"
            }
        ],
        "abstract": "There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which properties of human cognition are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand whether current LLMs display the same cognitive biases as children in these steps. We generate a novel set of word problems for each of these tests, using a neuro-symbolic approach that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but not in the final step, in which the arithmetic expressions are executed to obtain the answer."
    },
    {
        "paperId": "65bc3f3a3cf9ca5924a5ac1d10b8ea7de5181cc2",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Exploring Spatial Schema Intuitions in Large Language and Vision Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.00956, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-01",
        "authors": [
            {
                "authorId": "2282473799",
                "name": "Philipp Wicke"
            },
            {
                "authorId": "2029658284",
                "name": "Lennart Wachowiak"
            }
        ],
        "abstract": "Despite the ubiquity of large language models (LLMs) in AI research, the question of embodiment in LLMs remains underexplored, distinguishing them from embodied systems in robotics where sensory perception directly informs physical action. Our investigation navigates the intriguing terrain of whether LLMs, despite their non-embodied nature, effectively capture implicit human intuitions about fundamental, spatial building blocks of language. We employ insights from spatial cognitive foundations developed through early sensorimotor experiences, guiding our exploration through the reproduction of three psycholinguistic experiments. Surprisingly, correlations between model outputs and human responses emerge, revealing adaptability without a tangible connection to embodied experiences. Notable distinctions include polarized language model responses and reduced correlations in vision language models. This research contributes to a nuanced understanding of the interplay between language, spatial experiences, and the computations made by large language models. More at https://cisnlp.github.io/Spatial_Schemas/"
    },
    {
        "paperId": "a640215755e23e2649f4b3d3246a47b14fea93f7",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Getting the most out of your tokenizer for pre-training and domain adaptation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-01",
        "authors": [
            {
                "authorId": "67175437",
                "name": "Gautier Dagan"
            },
            {
                "authorId": "2282469774",
                "name": "Gabriele Synnaeve"
            },
            {
                "authorId": "3361236",
                "name": "Baptiste Rozi\u00e8re"
            }
        ],
        "abstract": "Tokenization is an understudied and often neglected component of modern LLMs. Most published works use a single tokenizer for all experiments, often borrowed from another model, without performing ablations or analysis to optimize tokenization. Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model. In this paper, we show that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance. We train specialized Byte-Pair Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer design on the performance of LLMs for code generation tasks such as HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters selection and switching the tokenizer in a pre-trained LLM. We perform our experiments on models trained from scratch and from pre-trained models, verifying their applicability to a wide range of use-cases. We find that when fine-tuning on more than 50 billion tokens, we can specialize the tokenizer of a pre-trained LLM to obtain large gains in generation speed and effective context size."
    },
    {
        "paperId": "abdb6e912fe86a60600b438b0e36b502f6412b24",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Vaccine: Perturbation-aware Alignment for Large Language Model",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01109, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-02",
        "authors": [
            {
                "authorId": "2253860508",
                "name": "Tiansheng Huang"
            },
            {
                "authorId": "2254156032",
                "name": "Sihao Hu"
            },
            {
                "authorId": "2254270304",
                "name": "Ling Liu"
            }
        ],
        "abstract": "The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \\textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompts. Our code is available at \\url{https://github.com/git-disl/Vaccine}."
    },
    {
        "paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01306, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-02",
        "authors": [
            {
                "authorId": "10324691",
                "name": "Kawin Ethayarajh"
            },
            {
                "authorId": "2282503091",
                "name": "Winnie Xu"
            },
            {
                "authorId": "2037383772",
                "name": "Niklas Muennighoff"
            },
            {
                "authorId": "2256674786",
                "name": "Dan Jurafsky"
            },
            {
                "authorId": "2111313627",
                "name": "Douwe Kiela"
            }
        ],
        "abstract": "Kahneman&Tversky's $\\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call $\\textit{human-aware losses}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration."
    },
    {
        "paperId": "11155af5ccd1889277f4269f6bb349a7633554f4",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-02",
        "authors": [
            {
                "authorId": "2153624353",
                "name": "Jian Xie"
            },
            {
                "authorId": "145086492",
                "name": "Kai Zhang"
            },
            {
                "authorId": "5040052",
                "name": "Jiangjie Chen"
            },
            {
                "authorId": "2222400339",
                "name": "Tinghui Zhu"
            },
            {
                "authorId": "2118614649",
                "name": "Renze Lou"
            },
            {
                "authorId": "2282522220",
                "name": "Yuandong Tian"
            },
            {
                "authorId": "2282543969",
                "name": "Yanghua Xiao"
            },
            {
                "authorId": "1758652",
                "name": "Yu Su"
            }
        ],
        "abstract": "Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents."
    },
    {
        "paperId": "f58484d50364014289c0d4fe20ba35962fb40ba9",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "STICKERCONV: Generating Multimodal Empathetic Responses from Scratch",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01679, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-20",
        "authors": [
            {
                "authorId": "2281789446",
                "name": "Yiqun Zhang"
            },
            {
                "authorId": "2282539739",
                "name": "Fanheng Kong"
            },
            {
                "authorId": "2282963978",
                "name": "Peidong Wang"
            },
            {
                "authorId": "2282971267",
                "name": "Shuang Sun"
            },
            {
                "authorId": "2282538405",
                "name": "Lingshuai Wang"
            },
            {
                "authorId": "2087586948",
                "name": "Shi Feng"
            },
            {
                "authorId": "2111226672",
                "name": "Daling Wang"
            },
            {
                "authorId": "2108463824",
                "name": "Yifei Zhang"
            },
            {
                "authorId": "7750270",
                "name": "Kaisong Song"
            }
        ],
        "abstract": "Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research, notably due to the challenge of a lack of comprehensive datasets. In this paper, we introduce the Agent for STICKERCONV (Agent4SC), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, STICKERCONV, comprising 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios. This dataset serves as a benchmark for multimodal empathetic generation. To advance further, we propose PErceive and Generate Stickers (PEGS), a multimodal empathetic response generation framework, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate PEGS's effectiveness in generating contextually relevant and emotionally resonant multimodal empathetic responses, contributing to the advancement of more nuanced and engaging empathetic dialogue systems."
    },
    {
        "paperId": "0d370b2dde53ca0fc43d3dfefd5f032952fcb3c5",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2402.01737",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-29",
        "authors": [
            {
                "authorId": "2282532077",
                "name": "Yuncheng Hua"
            },
            {
                "authorId": "153139892",
                "name": "Lizhen Qu"
            },
            {
                "authorId": "2561045",
                "name": "Gholamreza Haffari"
            }
        ],
        "abstract": "We develop assistive agents based on Large Language Models (LLMs) that aid interlocutors in business negotiations. Specifically, we simulate business negotiations by letting two LLM-based agents engage in role play. A third LLM acts as a remediator agent to rewrite utterances violating norms for improving negotiation outcomes. We introduce a simple tuning-free and label-free In-Context Learning (ICL) method to identify high-quality ICL exemplars for the remediator, where we propose a novel select criteria, called value impact, to measure the quality of the negotiation outcomes. We provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different negotiation topics. We have released our source code and the generated dataset at: https://github.com/tk1363704/SADAS."
    },
    {
        "paperId": "e598e2ba755a2543d8e2472629cd84d96125962e",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Fractal Patterns May Illuminate the Success of Next-Token Prediction",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01825, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-02",
        "authors": [
            {
                "authorId": "2922782",
                "name": "Ibrahim M. Alabdulmohsin"
            },
            {
                "authorId": "2282541828",
                "name": "Vinh Q. Tran"
            },
            {
                "authorId": "2256989598",
                "name": "Mostafa Dehghani"
            }
        ],
        "abstract": "We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately H=0.7. Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can capture the structure of text across multiple levels of granularity, from words and clauses to broader contexts and intents. In addition, we carry out an extensive analysis across different domains and architectures, showing that fractal parameters are robust. Finally, we demonstrate that the tiny variations in fractal parameters seen across LLMs improve upon perplexity-based bits-per-byte (BPB) in predicting their downstream performance. We hope these findings offer a fresh perspective on language and the mechanisms underlying the success of LLMs."
    },
    {
        "paperId": "69e6dd39bf13d290fb9d885da90cc037f0dd2975",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01831, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-02",
        "authors": [
            {
                "authorId": "2229373549",
                "name": "Zhifeng Kong"
            },
            {
                "authorId": "2282529324",
                "name": "Arushi Goel"
            },
            {
                "authorId": "3433816",
                "name": "Rohan Badlani"
            },
            {
                "authorId": "2253664013",
                "name": "Wei Ping"
            },
            {
                "authorId": "2281035249",
                "name": "Rafael Valle"
            },
            {
                "authorId": "2264406909",
                "name": "Bryan Catanzaro"
            }
        ],
        "abstract": "Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks. Our demo website is https://audioflamingo.github.io/ and the code is open-sourced at https://github.com/NVIDIA/audio-flamingo."
    },
    {
        "paperId": "daed41785efecb393898af61b27934ffa5421230",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "InferCept: Efficient Intercept Support for Augmented Large Language Model Inference",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-02",
        "authors": [
            {
                "authorId": "2217683047",
                "name": "Reyna Abhyankar"
            },
            {
                "authorId": "2253828077",
                "name": "Zijian He"
            },
            {
                "authorId": "2248827841",
                "name": "Vikranth Srivatsa"
            },
            {
                "authorId": "2283175919",
                "name": "Hao Zhang"
            },
            {
                "authorId": "2244769765",
                "name": "Yiying Zhang"
            }
        ],
        "abstract": "Large language models are increasingly integrated with external environments, tools, and agents like ChatGPT plugins to extend their capability beyond language-centric tasks. However, today's LLM inference systems are designed for standalone LLMs. They treat each external interaction as the end of LLM generation and form a new request when the interaction finishes, causing unnecessary recomputation of already computed contexts, which accounts for 37-40% of total model forwarding time. This paper presents InferCept, the first LLM inference framework targeting augmented LLMs and supporting the efficient interception of LLM generation. InferCept minimizes the GPU resource waste caused by LLM interceptions and dedicates saved memory for serving more requests. InferCept improves the overall serving throughput by 1.6x-2x and completes 2x more requests per second compared to the state-of-the-art LLM inference systems."
    },
    {
        "paperId": "2b5e40c9c6c76569714b902a53838cb80ce89a26",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Selecting Large Language Model to Fine-tune via Rectified Scaling Law",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-04",
        "authors": [
            {
                "authorId": "2257447835",
                "name": "Haowei Lin"
            },
            {
                "authorId": "2184278672",
                "name": "Baizhou Huang"
            },
            {
                "authorId": "2284300618",
                "name": "Haotian Ye"
            },
            {
                "authorId": "2273830934",
                "name": "Qinyu Chen"
            },
            {
                "authorId": "47196237",
                "name": "Zihao Wang"
            },
            {
                "authorId": "2257095534",
                "name": "Sujian Li"
            },
            {
                "authorId": "2257367982",
                "name": "Jianzhu Ma"
            },
            {
                "authorId": "2257016300",
                "name": "Xiaojun Wan"
            },
            {
                "authorId": "2283249105",
                "name": "James Zou"
            },
            {
                "authorId": "2257367774",
                "name": "Yitao Liang"
            }
        ],
        "abstract": "The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with Scaling Law. Unlike pre-training, we find that the fine-tuning scaling curve includes not just the well-known\"power phase\"but also the previously unobserved\"pre-power phase\". We also explain why existing Scaling Law fails to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of\"pre-learned data size\"into our Rectified Scaling Law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection. The project page is available at rectified-scaling-law.github.io."
    },
    {
        "paperId": "ae4d0695de237fa5f682e1de1811b9b17172d4ed",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Can Large Language Models Learn Independent Causal Mechanisms?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02636, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-04",
        "authors": [
            {
                "authorId": "2138967383",
                "name": "Ga\u00ebl Gendron"
            },
            {
                "authorId": "2282533187",
                "name": "Bao Trung Nguyen"
            },
            {
                "authorId": "2064347311",
                "name": "A. Peng"
            },
            {
                "authorId": "2280960277",
                "name": "Michael Witbrock"
            },
            {
                "authorId": "2239439286",
                "name": "Gillian Dobbie"
            }
        ],
        "abstract": "Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting a lack of generalisation ability. By contrast, systems such as causal models, that learn abstract variables and causal relationships, can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language modelling modules. We show that such causal constraints can improve out-of-distribution performance on abstract and causal reasoning tasks. We also investigate the level of independence and domain specialisation and show that LLMs rely on pre-trained partially domain-invariant mechanisms resilient to fine-tuning."
    },
    {
        "paperId": "ce0219aab283c07369187348d72f8c5fe9898e9b",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Large Language Models are Geographically Biased",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02680, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-05",
        "authors": [
            {
                "authorId": "2161717022",
                "name": "Rohin Manvi"
            },
            {
                "authorId": "2249576900",
                "name": "Samar Khanna"
            },
            {
                "authorId": "2257185967",
                "name": "Marshall Burke"
            },
            {
                "authorId": "2251309767",
                "name": "David B. Lobell"
            },
            {
                "authorId": "2269095529",
                "name": "Stefano Ermon"
            }
        ],
        "abstract": "Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and subjective topics. In particular, LLMs are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman's $\\rho$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing LLMs. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM"
    },
    {
        "paperId": "d622d8b2d5adc4b638a73b105686840e264dfb8f",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Position: What Can Large Language Models Tell Us about Time Series Analysis",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02713, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-05",
        "authors": [
            {
                "authorId": "2254096428",
                "name": "Ming Jin"
            },
            {
                "authorId": "2304517120",
                "name": "Yifan Zhang"
            },
            {
                "authorId": "2301489384",
                "name": "Wei Chen"
            },
            {
                "authorId": "2304920924",
                "name": "Kexin Zhang"
            },
            {
                "authorId": "2253824408",
                "name": "Yuxuan Liang"
            },
            {
                "authorId": "2298971233",
                "name": "Bin Yang"
            },
            {
                "authorId": "2304515949",
                "name": "Jindong Wang"
            },
            {
                "authorId": "2254047333",
                "name": "Shirui Pan"
            },
            {
                "authorId": "2303259263",
                "name": "Qingsong Wen"
            }
        ],
        "abstract": "Time series analysis is essential for comprehending the complexities inherent in various realworld systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including time series modality switching and question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts. Furthermore, we detail the seamless integration of time series analysis with existing LLM technologies and outline promising avenues for future research."
    },
    {
        "paperId": "8679575a82d9a2ba5970a96fcc5ac461aecbc90e",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Graph-enhanced Large Language Models in Asynchronous Plan Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-05",
        "authors": [
            {
                "authorId": "2279899894",
                "name": "Fangru Lin"
            },
            {
                "authorId": "1582740100",
                "name": "Emanuele La Malfa"
            },
            {
                "authorId": "1667898858",
                "name": "Valentin Hofmann"
            },
            {
                "authorId": "2282533037",
                "name": "Elle Michelle Yang"
            },
            {
                "authorId": "2248060862",
                "name": "Anthony G. Cohn"
            },
            {
                "authorId": "1970864",
                "name": "J. Pierrehumbert"
            }
        ],
        "abstract": "Planning is a fundamental property of human intelligence. Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents. Our code and data are available at https://github.com/fangru-lin/graph-llm-asynchow-plan."
    },
    {
        "paperId": "8e4fff506c2ce70eeb770a63848209e351a39033",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "CIDAR: Culturally Relevant Instruction Dataset For Arabic",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.03177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-05",
        "authors": [
            {
                "authorId": "25098419",
                "name": "Zaid Alyafeai"
            },
            {
                "authorId": "90615055",
                "name": "Khalid Almubarak"
            },
            {
                "authorId": "2282540813",
                "name": "Ahmed Ashraf"
            },
            {
                "authorId": "2428276",
                "name": "Deema Alnuhait"
            },
            {
                "authorId": "50702892",
                "name": "Saied Alshahrani"
            },
            {
                "authorId": "2188211751",
                "name": "Gubran A. Q. Abdulrahman"
            },
            {
                "authorId": "2282541184",
                "name": "Gamil Ahmed"
            },
            {
                "authorId": "2179591482",
                "name": "Qais Gawah"
            },
            {
                "authorId": "2282540934",
                "name": "Zead Saleh"
            },
            {
                "authorId": "2153559016",
                "name": "Mustafa Ghaleb"
            },
            {
                "authorId": "2282541254",
                "name": "Yousef Ali"
            },
            {
                "authorId": "1752627730",
                "name": "Maged S. Al-Shaibani"
            }
        ],
        "abstract": "Instruction tuning has emerged as a prominent methodology for teaching Large Language Models (LLMs) to follow instructions. However, current instruction datasets predominantly cater to English or are derived from English-dominated LLMs, resulting in inherent biases toward Western culture. This bias significantly impacts the linguistic structures of non-English languages such as Arabic, which has a distinct grammar reflective of the diverse cultures across the Arab region. This paper addresses this limitation by introducing CIDAR: https://hf.co/datasets/arbml/CIDAR, the first open Arabic instruction-tuning dataset culturally-aligned by human reviewers. CIDAR contains 10,000 instruction and output pairs that represent the Arab region. We discuss the cultural relevance of CIDAR via the analysis and comparison to other models fine-tuned on other datasets. Our experiments show that CIDAR can help enrich research efforts in aligning LLMs with the Arabic culture. All the code is available at https://github.com/ARBML/CIDAR."
    },
    {
        "paperId": "4862ea8e9bd568a7ea5d3347ebb6d2a8d7f80ecc",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Distinguishing the Knowable from the Unknowable with Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.03563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-05",
        "authors": [
            {
                "authorId": "2186878343",
                "name": "Gustaf Ahdritz"
            },
            {
                "authorId": "2282978795",
                "name": "Tian Qin"
            },
            {
                "authorId": "2257251094",
                "name": "Nikhil Vyas"
            },
            {
                "authorId": "2257219124",
                "name": "Boaz Barak"
            },
            {
                "authorId": "2176780986",
                "name": "Benjamin L. Edelman"
            }
        ],
        "abstract": "We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more informative indicators of model confidence in diverse practical settings."
    },
    {
        "paperId": "78b74b9e7c2a1918b792cb0dbf7048796621f679",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Are Machines Better at Complex Reasoning? Unveiling Human-Machine Inference Gaps in Entailment Verification",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.03686, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-06",
        "authors": [
            {
                "authorId": "3313909",
                "name": "Soumya Sanyal"
            },
            {
                "authorId": "2282961643",
                "name": "Tianyi Xiao"
            },
            {
                "authorId": "2144174497",
                "name": "Jiacheng Liu"
            },
            {
                "authorId": "2283084888",
                "name": "Wenya Wang"
            },
            {
                "authorId": "2283447925",
                "name": "Xiang Ren"
            }
        ],
        "abstract": "Making inferences in text comprehension to understand the meaning is essential in language processing. This work studies the entailment verification (EV) problem of multi-sentence premises that requires a system to make multiple inferences implicitly. Studying EV for such complex premises is important because modern NLP problems, such as detecting inconsistent model-generated rationales, require complex multi-hop reasoning. However, current textual inference datasets mostly contain short premises that only partially focus on these challenges. To address this, we compile an EV benchmark that includes datasets from three NLP domains (NLI, contextual QA, and rationales) containing multi-sentence premises. On benchmarking humans and LLMs, we find that LLMs are better than humans in multi-hop reasoning across extended contexts, while humans perform better in simple deductive reasoning tasks. We also finetune a Flan-T5 model for EV using two training objectives to obtain a strong open-source model that outperforms GPT-3.5 and rivals GPT-4. Finally, we use this model to filter out inconsistent model-generated rationales in self-consistency decoding, resulting in a 6% accuracy improvement on average across three MCQ datasets."
    },
    {
        "paperId": "b9c304cd9449d98014d21e44ce77add601e5ca04",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.03877, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-06",
        "authors": [
            {
                "authorId": "148099243",
                "name": "Spyridon Mouselinos"
            },
            {
                "authorId": "47407464",
                "name": "H. Michalewski"
            },
            {
                "authorId": "2282977129",
                "name": "Mateusz Malinowski"
            }
        ],
        "abstract": "Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations."
    },
    {
        "paperId": "ffffc62cf48c40b24db0c9fc49c67ed65589bb83",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Position: Stop Making Unscientific AGI Performance Claims",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.03962, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-06",
        "authors": [
            {
                "authorId": "2072038627",
                "name": "Patrick Altmeyer"
            },
            {
                "authorId": "19985826",
                "name": "Andrew M. Demetriou"
            },
            {
                "authorId": "2225601829",
                "name": "Antony Bartlett"
            },
            {
                "authorId": "2239572250",
                "name": "Cynthia C. S. Liem"
            }
        ],
        "abstract": "Developments in the field of Artificial Intelligence (AI), and particularly large language models (LLMs), have created a 'perfect storm' for observing 'sparks' of Artificial General Intelligence (AGI) that are spurious. Like simpler models, LLMs distill meaningful representations in their latent embeddings that have been shown to correlate with external variables. Nonetheless, the correlation of such representations has often been linked to human-like intelligence in the latter but not the former. We probe models of varying complexity including random projections, matrix decompositions, deep autoencoders and transformers: all of them successfully distill information that can be used to predict latent or external variables and yet none of them have previously been linked to AGI. We argue and empirically demonstrate that the finding of meaningful patterns in latent spaces of models cannot be seen as evidence in favor of AGI. Additionally, we review literature from the social sciences that shows that humans are prone to seek such patterns and anthropomorphize. We conclude that both the methodological setup and common public image of AI are ideal for the misinterpretation that correlations between model representations and some variables of interest are 'caused' by the model's understanding of underlying 'ground truth' relationships. We, therefore, call for the academic community to exercise extra caution, and to be keenly aware of principles of academic integrity, in interpreting and communicating about AI research outcomes."
    },
    {
        "paperId": "d408a2931262e0b21fe607900bff0c3822d0a634",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "In-context learning agents are asymmetric belief updaters",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.03969, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-06",
        "authors": [
            {
                "authorId": "2238366283",
                "name": "Johannes A. Schubert"
            },
            {
                "authorId": "8341302",
                "name": "A. K. Jagadish"
            },
            {
                "authorId": "32354733",
                "name": "Marcel Binz"
            },
            {
                "authorId": "2271334207",
                "name": "Eric Schulz"
            }
        ],
        "abstract": "We study the in-context learning dynamics of large language models (LLMs) using three instrumental learning tasks adapted from cognitive psychology. We find that LLMs update their beliefs in an asymmetric manner and learn more from better-than-expected outcomes than from worse-than-expected ones. Furthermore, we show that this effect reverses when learning about counterfactual feedback and disappears when no agency is implied. We corroborate these findings by investigating idealized in-context learning agents derived through meta-reinforcement learning, where we observe similar patterns. Taken together, our results contribute to our understanding of how in-context learning works by highlighting that the framing of a problem significantly influences how learning occurs, a phenomenon also observed in human cognition."
    },
    {
        "paperId": "f503b95c0a64f6a84eb1d90e5ea1e094b1e1892b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Systematic Biases in LLM Simulations of Debates",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.04049, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-06",
        "authors": [
            {
                "authorId": "2203788219",
                "name": "Amir Taubenfeld"
            },
            {
                "authorId": "2948787",
                "name": "Yaniv Dover"
            },
            {
                "authorId": "2249760179",
                "name": "Roi Reichart"
            },
            {
                "authorId": "2250857819",
                "name": "Ariel Goldstein"
            }
        ],
        "abstract": "The emergence of Large Language Models (LLMs), has opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. Current research suggests that LLM-based agents become increasingly human-like in their performance, sparking interest in using these AI agents as substitutes for human participants in behavioral studies. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. Hence, it is crucial to study and pinpoint the key behavioral distinctions between humans and LLM-based agents. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs\u2019 ability to simulate political debates on topics that are important aspects of people\u2019s day-to-day lives and decision-making processes. Our findings indicate a tendency for LLM agents to conform to the model\u2019s inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations."
    },
    {
        "paperId": "d3093494024474e8858bbf723d303b6913d9e3c4",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Online Cascade Learning for Efficient Inference over Streams",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.04513, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-07",
        "authors": [
            {
                "authorId": "2313699525",
                "name": "Lunyiu Nie"
            },
            {
                "authorId": "2283314288",
                "name": "Zhimin Ding"
            },
            {
                "authorId": "2283132576",
                "name": "Erdong Hu"
            },
            {
                "authorId": "2283132701",
                "name": "Christopher Jermaine"
            },
            {
                "authorId": "35865989",
                "name": "Swarat Chaudhuri"
            }
        ],
        "abstract": "Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to address this challenge. The objective here is to learn a\"cascade\"of models, starting with lower-capacity models (such as logistic regression) and ending with a powerful LLM, along with a deferral policy that determines the model to be used on a given input. We formulate the task of learning cascades online as an imitation-learning problem, where smaller models are updated over time imitating the collected LLM demonstrations, and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90% with strong robustness against input distribution shifts, underscoring its efficacy and adaptability in stream processing."
    },
    {
        "paperId": "775e9844a13b518aeb0cde401ba6891ba3538611",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Can Large Language Model Agents Simulate Human Trust Behaviors?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.04559, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-07",
        "authors": [
            {
                "authorId": "2283436996",
                "name": "Chengxing Xie"
            },
            {
                "authorId": "2163546329",
                "name": "Canyu Chen"
            },
            {
                "authorId": "2283138892",
                "name": "Feiran Jia"
            },
            {
                "authorId": "2283514670",
                "name": "Ziyu Ye"
            },
            {
                "authorId": "2284765696",
                "name": "Shiyang Lai"
            },
            {
                "authorId": "2266239316",
                "name": "Kai Shu"
            },
            {
                "authorId": "2314778",
                "name": "Adel Bibi"
            },
            {
                "authorId": "2283262338",
                "name": "Ziniu Hu"
            },
            {
                "authorId": "2282534002",
                "name": "Philip H. S. Torr"
            },
            {
                "authorId": "2274278297",
                "name": "Bernard Ghanem"
            },
            {
                "authorId": "49461641",
                "name": "G. Li"
            }
        ],
        "abstract": "Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: can LLM agents really simulate human behavior? In this paper, we focus on one critical and elemental behavior in human interactions, trust, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior, indicating the feasibility of simulating human trust behavior with LLM agents. In addition, we probe the biases of agent trust and differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond value alignment. We further illustrate broader implications of our discoveries for applications where trust is paramount."
    },
    {
        "paperId": "b925a290a35a3c11371fea0ff71b56371783d572",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Pedagogical Alignment of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.05000, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-07",
        "authors": [
            {
                "authorId": "1720691070",
                "name": "Shashank Sonkar"
            },
            {
                "authorId": "2283138265",
                "name": "Kangqi Ni"
            },
            {
                "authorId": "2137049528",
                "name": "Sapana Chaudhary"
            },
            {
                "authorId": "2249763597",
                "name": "R. Baraniuk"
            }
        ],
        "abstract": "Large Language Models (LLMs), when used in educational settings without pedagogical fine-tuning, often provide immediate answers rather than guiding students through the problem-solving process. This approach falls short of pedagogically best practices and limits their effectiveness as educational tools. We term the objective of training LLMs to emulate effective teaching strategies as `pedagogical alignment.' In this paper, we investigate Learning from Human Preferences (LHP) algorithms to achieve this alignment objective. A key challenge in this process is the scarcity of high-quality preference datasets to guide the alignment. To address this, we propose a novel approach for constructing a large-scale dataset using synthetic data generation techniques, eliminating the need for time-consuming and costly manual annotation. Leveraging this dataset, our experiments with Llama and Mistral models demonstrate that LHP methods outperform standard supervised fine-tuning (SFT), improving pedagogical alignment accuracy by 13.1% and 8.7% respectively. Existing evaluation methods also lack quantitative metrics to adequately measure the pedagogical alignment of LLMs. To address this gap, we propose novel perplexity-based metrics that quantify LLMs' tendency to provide scaffolded guidance versus direct answers, offering a robust measure of pedagogical alignment. Our analysis provides compelling evidence for the superiority of LHP methods over SFT in optimizing LLMs' behavior, underscoring the potential of LHP methods in better aligning LLMs with educational objectives and fostering effective learning experiences. Code and models are available \\href{https://github.com/luffycodes/Tutorbot-Spock}{here}."
    },
    {
        "paperId": "4d5cbfbba6e336075cf11c7229b38b098d9243d1",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "ApiQ: Finetuning of 2-Bit Quantized Large Language Model",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.05147, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-07",
        "authors": [
            {
                "authorId": "66693547",
                "name": "Baohao Liao"
            },
            {
                "authorId": "2062908179",
                "name": "C. Monz"
            }
        ],
        "abstract": "Memory-efficient finetuning of large language models (LLMs) has recently attracted huge attention with the increasing size of LLMs, primarily due to the constraints posed by GPU memory limitations and the effectiveness of these methods compared to full finetuning. Despite the advancements, current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for finetuning purposes. In this work, we introduce a novel quantization framework named ApiQ, designed to restore the lost information from quantization by concurrently initializing the LoRA components and quantizing the weights of LLMs. This approach ensures the maintenance of the original LLM\u2019s activation precision while mitigating the error propagation from shallower into deeper layers. Through comprehensive evaluations conducted on a spectrum of language tasks with various LLMs, ApiQ demonstrably minimizes activation error during quantization. Consequently, it consistently achieves superior finetuning results across various bit-widths. Notably, one can even finetune a 2-bit Llama-2-70b with ApiQ on a single NVIDIA A100-80GB GPU without any memory-saving techniques, and achieve promising results."
    },
    {
        "paperId": "450d97c7f456eafbb69dd70322d341058028b171",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "In-Context Principle Learning from Mistakes",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.05403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-08",
        "authors": [
            {
                "authorId": "1993655237",
                "name": "Tianjun Zhang"
            },
            {
                "authorId": "21626987",
                "name": "Aman Madaan"
            },
            {
                "authorId": "2267242298",
                "name": "Luyu Gao"
            },
            {
                "authorId": "2283438176",
                "name": "Steven Zheng"
            },
            {
                "authorId": "1817207",
                "name": "Swaroop Mishra"
            },
            {
                "authorId": "2267307049",
                "name": "Yiming Yang"
            },
            {
                "authorId": "2261389843",
                "name": "Niket Tandon"
            },
            {
                "authorId": "47051926",
                "name": "Uri Alon"
            }
        ],
        "abstract": "In-context learning (ICL, also known as few-shot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples. Nonetheless, all ICL-based approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then we reflect on these mistakes, and learn explicit task-specific\"principles\"from them, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4 turbo and Claude-2.1. For example, LEAP improves over the standard few-shot prompting using GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does not require any more input or examples than the standard few-shot prompting settings."
    },
    {
        "paperId": "cb169315cb75ce4b1af20083fbbe46c553e7f252",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Accurate LoRA-Finetuning Quantization of LLMs via Information Retention",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.05445, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-08",
        "authors": [
            {
                "authorId": "1381853008",
                "name": "Haotong Qin"
            },
            {
                "authorId": "2190791504",
                "name": "Xudong Ma"
            },
            {
                "authorId": "2283439767",
                "name": "Xingyu Zheng"
            },
            {
                "authorId": "2274799334",
                "name": "Xiaoyang Li"
            },
            {
                "authorId": "2274640577",
                "name": "Yang Zhang"
            },
            {
                "authorId": "2274201209",
                "name": "Shouda Liu"
            },
            {
                "authorId": "2283307736",
                "name": "Jie Luo"
            },
            {
                "authorId": "2278053544",
                "name": "Xianglong Liu"
            },
            {
                "authorId": "2283140094",
                "name": "Michele Magno"
            }
        ],
        "abstract": "The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the state-of-the-art methods. The significant performance gain requires only a tiny 0.31% additional time consumption, revealing the satisfactory efficiency of our IR-QLoRA. We highlight that IR-QLoRA enjoys excellent versatility, compatible with various frameworks (e.g., NormalFloat and Integer quantization) and brings general accuracy gains. The code is available at https://github.com/htqin/ir-qlora."
    },
    {
        "paperId": "db54979e005b5e3fd0a9b9c0444b014488bd47a9",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.05547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-08",
        "authors": [
            {
                "authorId": "120738202",
                "name": "Hengguan Huang"
            },
            {
                "authorId": "2283699530",
                "name": "Songtao Wang"
            },
            {
                "authorId": "2144318258",
                "name": "Hongfu Liu"
            },
            {
                "authorId": "2283490183",
                "name": "Hao Wang"
            },
            {
                "authorId": "2188974935",
                "name": "Ye Wang"
            }
        ],
        "abstract": "Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical dialogue systems. However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored. We introduce\"ChatCoach\", a human-AI cooperative framework designed to assist medical learners in practicing their communication skills during patient consultations. ChatCoach (Our data and code are available online: https://github.com/zerowst/Chatcoach)differentiates itself from conventional dialogue systems by offering a simulated environment where medical learners can practice dialogues with a patient agent, while a coach agent provides immediate, structured feedback. This is facilitated by our proposed Generalized Chain-of-Thought (GCoT) approach, which fosters the generation of structured feedback and enhances the utilization of external knowledge sources. Additionally, we have developed a dataset specifically for evaluating Large Language Models (LLMs) within the ChatCoach framework on communicative medical coaching tasks. Our empirical results validate the effectiveness of ChatCoach."
    },
    {
        "paperId": "e9640cd4bdb0fa93a94151ec00259909b5e88d6d",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.05699, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-08",
        "authors": [
            {
                "authorId": "2259929200",
                "name": "Xianghe Pang"
            },
            {
                "authorId": "2283546526",
                "name": "Shuo Tang"
            },
            {
                "authorId": "2273620042",
                "name": "Rui Ye"
            },
            {
                "authorId": "2261864295",
                "name": "Yuxin Xiong"
            },
            {
                "authorId": "2283315857",
                "name": "Bolun Zhang"
            },
            {
                "authorId": "2273558245",
                "name": "Yanfeng Wang"
            },
            {
                "authorId": "2249818980",
                "name": "Siheng Chen"
            }
        ],
        "abstract": "Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that our method outperforms over 10 baselines across 4 benchmarks. As evidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning with human values. See our project page at https://shuotang123.github.io/MATRIX."
    },
    {
        "paperId": "b92d30286e7984a54cf1f537b4b72b2dd1b168a7",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.05863, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-08",
        "authors": [
            {
                "authorId": "2273858095",
                "name": "Federico Bianchi"
            },
            {
                "authorId": "2071967689",
                "name": "P. Chia"
            },
            {
                "authorId": "1387987315",
                "name": "Mert Y\u00fcksekg\u00f6n\u00fcl"
            },
            {
                "authorId": "1964040776",
                "name": "Jacopo Tagliabue"
            },
            {
                "authorId": "2259379929",
                "name": "Daniel Jurafsky"
            },
            {
                "authorId": "2251099054",
                "name": "James Zou"
            }
        ],
        "abstract": "Negotiation is the basis of social interactions; humans negotiate everything from the price of cars to how to share common resources. With rapidly growing interest in using large language models (LLMs) to act as agents on behalf of human users, such LLM agents would also need to be able to negotiate. In this paper, we study how well LLMs can negotiate with each other. We develop NegotiationArena: a flexible framework for evaluating and probing the negotiation abilities of LLM agents. We implemented three types of scenarios in NegotiationArena to assess LLM's behaviors in allocating shared resources (ultimatum games), aggregate resources (trading games) and buy/sell goods (price negotiations). Each scenario allows for multiple turns of flexible dialogues between LLM agents to allow for more complex negotiations. Interestingly, LLM agents can significantly boost their negotiation outcomes by employing certain behavioral tactics. For example, by pretending to be desolate and desperate, LLMs can improve their payoffs by 20\\% when negotiating against the standard GPT-4. We also quantify irrational negotiation behaviors exhibited by the LLM agents, many of which also appear in humans. Together, \\NegotiationArena offers a new environment to investigate LLM interactions, enabling new insights into LLM's theory of mind, irrationality, and reasoning abilities."
    },
    {
        "paperId": "6d9a490411070f5a5a0ee5b116a7c5c61a446be2",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.06025, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-08",
        "authors": [
            {
                "authorId": "2275241768",
                "name": "Wasu Top Piriyakulkij"
            },
            {
                "authorId": "2325741808",
                "name": "Cassidy Langenfeld"
            },
            {
                "authorId": "2325942978",
                "name": "Tuan Anh Le"
            },
            {
                "authorId": "2275244890",
                "name": "Kevin Ellis"
            }
        ],
        "abstract": "We give a model of how to infer natural language rules by doing experiments. The model integrates Large Language Models (LLMs) with Monte Carlo algorithms for probabilistic inference, interleaving online belief updates with experiment design under information-theoretic criteria. We conduct a human-model comparison on a Zendo-style task, finding that a critical ingredient for modeling the human data is to assume that humans also consider fuzzy, probabilistic rules, in addition to assuming that humans perform approximately-Bayesian belief updates. We also compare with recent algorithms for using LLMs to generate and revise hypotheses, finding that our online inference method yields higher accuracy at recovering the true underlying rule, and provides better support for designing optimal experiments."
    },
    {
        "paperId": "ad4e02784491f9794f6abb76b8982c980f51a6ee",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.06044, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-08",
        "authors": [
            {
                "authorId": "2283763073",
                "name": "Hainiu Xu"
            },
            {
                "authorId": "2047279522",
                "name": "Runcong Zhao"
            },
            {
                "authorId": "2131133148",
                "name": "Lixing Zhu"
            },
            {
                "authorId": "2283875272",
                "name": "Jinhua Du"
            },
            {
                "authorId": "2237125679",
                "name": "Yulan He"
            }
        ],
        "abstract": "Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world."
    },
    {
        "paperId": "14d0489047a1390434e7ea454e7e5165d9721ae3",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Calibrating Long-form Generations from Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.06544, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-09",
        "authors": [
            {
                "authorId": "2145952181",
                "name": "Yukun Huang"
            },
            {
                "authorId": "2283783652",
                "name": "Yixin Liu"
            },
            {
                "authorId": "2283771339",
                "name": "Raghuveer Thirukovalluru"
            },
            {
                "authorId": "2266838179",
                "name": "Arman Cohan"
            },
            {
                "authorId": "2060730422",
                "name": "Bhuwan Dhingra"
            }
        ],
        "abstract": "To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibration performance is found to be metric-dependent, and that self-consistency methods excel in factoid datasets. We also find that calibration can be enhanced through techniques such as fine-tuning, integrating relevant source documents, scaling the temperature, and combining self-consistency with self-evaluation. Lastly, we showcase a practical application of our system: selecting and cascading open-source models and ChatGPT to optimize correctness given a limited API budget. This research not only challenges existing notions of LLM calibration but also offers practical methodologies for improving trustworthiness in long-form generation."
    },
    {
        "paperId": "88cd6e0f7c2f1fe2bd4796a6729447d60bfeffc9",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.06619, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-09",
        "authors": [
            {
                "authorId": "2283844788",
                "name": "Shivalika Singh"
            },
            {
                "authorId": "7140106",
                "name": "Freddie Vargus"
            },
            {
                "authorId": "2283769272",
                "name": "Daniel Dsouza"
            },
            {
                "authorId": "2047947436",
                "name": "B\u00f6rje F. Karlsson"
            },
            {
                "authorId": "2143239611",
                "name": "Abinaya Mahendiran"
            },
            {
                "authorId": "1500573542",
                "name": "Wei-Yin Ko"
            },
            {
                "authorId": "2213270141",
                "name": "Herumb Shandilya"
            },
            {
                "authorId": "2283824430",
                "name": "Jay Patel"
            },
            {
                "authorId": "2283768246",
                "name": "Deividas Mataciunas"
            },
            {
                "authorId": "2283767148",
                "name": "Laura OMahony"
            },
            {
                "authorId": "2108063298",
                "name": "Mike Zhang"
            },
            {
                "authorId": "2047712004",
                "name": "Ramith Hettiarachchi"
            },
            {
                "authorId": "2283791017",
                "name": "Joseph Wilson"
            },
            {
                "authorId": "2283816089",
                "name": "Marina Machado"
            },
            {
                "authorId": "2283770582",
                "name": "Luisa Souza Moura"
            },
            {
                "authorId": "2283767135",
                "name": "Dominik Krzemi'nski"
            },
            {
                "authorId": "2252226",
                "name": "Hakimeh Fadaei"
            },
            {
                "authorId": "2283767018",
                "name": "Irem Ergun"
            },
            {
                "authorId": "2283767989",
                "name": "Ifeoma Okoh"
            },
            {
                "authorId": "2216335988",
                "name": "Aisha Alaagib"
            },
            {
                "authorId": "2091417428",
                "name": "Oshan Mudannayake"
            },
            {
                "authorId": "25098419",
                "name": "Zaid Alyafeai"
            },
            {
                "authorId": "1484109150",
                "name": "Minh Chien Vu"
            },
            {
                "authorId": "2884561",
                "name": "Sebastian Ruder"
            },
            {
                "authorId": "2283769216",
                "name": "Surya Guthikonda"
            },
            {
                "authorId": "3366968",
                "name": "Emad A. Alghamdi"
            },
            {
                "authorId": "2265058484",
                "name": "Sebastian Gehrmann"
            },
            {
                "authorId": "2037383772",
                "name": "Niklas Muennighoff"
            },
            {
                "authorId": "2287028357",
                "name": "Max Bartolo"
            },
            {
                "authorId": "3422710",
                "name": "Julia Kreutzer"
            },
            {
                "authorId": "82290814",
                "name": "A. Ustun"
            },
            {
                "authorId": "2818759",
                "name": "Marzieh Fadaee"
            },
            {
                "authorId": "2261493078",
                "name": "Sara Hooker"
            }
        ],
        "abstract": "Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the finetuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and translating existing datasets across 114 languages. In total, we contribute four key resources: we develop and open-source the Aya Annotation Platform, the Aya Dataset, the Aya Collection, and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case study in participatory research, involving collaborators from 119 countries. We see this as a valuable framework for future research collaborations that aim to bridge gaps in resources."
    },
    {
        "paperId": "9fd10119173ea30350db76550c1cbfacf87ce9a2",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Understanding the Effects of Iterative Prompting on Truthfulness",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.06625, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-09",
        "authors": [
            {
                "authorId": "1387484410",
                "name": "Satyapriya Krishna"
            },
            {
                "authorId": "40228633",
                "name": "Chirag Agarwal"
            },
            {
                "authorId": "1892673",
                "name": "Himabindu Lakkaraju"
            }
        ],
        "abstract": "The development of Large Language Models (LLMs) has notably transformed numerous sectors, offering impressive text generation capabilities. Yet, the reliability and truthfulness of these models remain pressing concerns. To this end, we investigate iterative prompting, a strategy hypothesized to refine LLM responses, assessing its impact on LLM truthfulness, an area which has not been thoroughly explored. Our extensive experiments delve into the intricacies of iterative prompting variants, examining their influence on the accuracy and calibration of model responses. Our findings reveal that naive prompting methods significantly undermine truthfulness, leading to exacerbated calibration errors. In response to these challenges, we introduce several prompting variants designed to address the identified issues. These variants demonstrate marked improvements over existing baselines, signaling a promising direction for future research. Our work provides a nuanced understanding of iterative prompting and introduces novel approaches to enhance the truthfulness of LLMs, thereby contributing to the development of more accurate and trustworthy AI systems."
    },
    {
        "paperId": "a39f5d6fdfd88c9afb9bd04537d1085aa4b63c80",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Feedback Loops With Language Models Drive In-Context Reward Hacking",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.06627, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-09",
        "authors": [
            {
                "authorId": "2064029812",
                "name": "Alexander Pan"
            },
            {
                "authorId": "145790135",
                "name": "Erik Jones"
            },
            {
                "authorId": "145096211",
                "name": "Meena Jagadeesan"
            },
            {
                "authorId": "2182571460",
                "name": "Jacob Steinhardt"
            }
        ],
        "abstract": "Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide three recommendations for evaluation to capture more instances of ICRH. As AI development accelerates, the effects of feedback loops will proliferate, increasing the need to understand their role in shaping LLM behavior."
    },
    {
        "paperId": "1376973d90d2d46c7676889a714026c270b027da",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2402.06900",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.06900, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-10",
        "authors": [
            {
                "authorId": "2212319558",
                "name": "Hyukhun Koh"
            },
            {
                "authorId": "2283873100",
                "name": "Dohyung Kim"
            },
            {
                "authorId": "2283973995",
                "name": "Minwoo Lee"
            },
            {
                "authorId": "2249538747",
                "name": "Kyomin Jung"
            }
        ],
        "abstract": "In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to detect the toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets, which are susceptible to out-of-distribution (OOD) problems and depend on the dataset's definition of toxicity. In this paper, we introduce a robust metric grounded on LLMs to flexibly measure toxicity according to the given definition. We first analyze the toxicity factors, followed by an examination of the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Finally, we evaluate the performance of our metric with detailed analysis. Our empirical results demonstrate outstanding performance in measuring toxicity within verified factors, improving on conventional metrics by 12 points in the F1 score. Our findings also indicate that upstream toxicity significantly influences downstream metrics, suggesting that LLMs are unsuitable for toxicity evaluations within unverified factors."
    },
    {
        "paperId": "0acb1b219c6c25a6d23e4267a061789a43ddd16b",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.07092, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-11",
        "authors": [
            {
                "authorId": "2280839621",
                "name": "Haonan Chen"
            },
            {
                "authorId": "2273086037",
                "name": "Zhicheng Dou"
            },
            {
                "authorId": "1580228663",
                "name": "Kelong Mao"
            },
            {
                "authorId": "1830383266",
                "name": "Jiongnan Liu"
            },
            {
                "authorId": "2175348510",
                "name": "Ziliang Zhao"
            }
        ],
        "abstract": "Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem -- that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug). ConvAug first generates multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby giving the model a larger learning space. A contrastive learning objective is then employed to train a better conversational context encoder. Extensive experiments conducted on four public datasets, under both normal and zero-shot settings, demonstrate the effectiveness, generalizability, and applicability of ConvAug. The code is released at https://github.com/haon-chen/ConvAug."
    },
    {
        "paperId": "291923449015d8fdd13e8af432a7b1169666dcec",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.07282, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-11",
        "authors": [
            {
                "authorId": "2283869238",
                "name": "Ryan Liu"
            },
            {
                "authorId": "1976174397",
                "name": "T. Sumers"
            },
            {
                "authorId": "46745316",
                "name": "Ishita Dasgupta"
            },
            {
                "authorId": "2265956804",
                "name": "Thomas L. Griffiths"
            }
        ],
        "abstract": "In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting."
    },
    {
        "paperId": "a8176838a636651324b9bac1b3443c803b44e1b3",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Online Iterative Reinforcement Learning from Human Feedback with General Preference Model",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.07314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-11",
        "authors": [
            {
                "authorId": "2105550407",
                "name": "Chen Ye"
            },
            {
                "authorId": "2275119437",
                "name": "Wei Xiong"
            },
            {
                "authorId": "2283877837",
                "name": "Yuheng Zhang"
            },
            {
                "authorId": "35279146",
                "name": "Hanze Dong"
            },
            {
                "authorId": "2275167899",
                "name": "Nan Jiang"
            },
            {
                "authorId": "2260473361",
                "name": "Tong Zhang"
            }
        ],
        "abstract": "We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework."
    },
    {
        "paperId": "5483b66f0e941be3e3382a97961773551d5ae563",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Secret Collusion among AI Agents: Multi-Agent Deception via Steganography",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.07510, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-12",
        "authors": [
            {
                "authorId": "2247588788",
                "name": "S. Motwani"
            },
            {
                "authorId": "2283847595",
                "name": "Mikhail Baranchuk"
            },
            {
                "authorId": "2290072234",
                "name": "Martin Strohmeier"
            },
            {
                "authorId": "2344839712",
                "name": "Vijay Bolina"
            },
            {
                "authorId": "2325901139",
                "name": "Philip Torr"
            },
            {
                "authorId": "2283850960",
                "name": "Lewis Hammond"
            },
            {
                "authorId": "2288102632",
                "name": "Christian Schr\u00f6der de Witt"
            }
        ],
        "abstract": "Recent capability increases in large language models (LLMs) open up applications in which groups of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities. We conclude by laying out a comprehensive research program to mitigate future risks of collusion between generative AI models."
    },
    {
        "paperId": "8ed3a631b81099ccefab86a8c382453c64ade412",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Policy Improvement using Language Feedback Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.07876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-12",
        "authors": [
            {
                "authorId": "2283840214",
                "name": "Victor Zhong"
            },
            {
                "authorId": "2273648018",
                "name": "Dipendra Misra"
            },
            {
                "authorId": "2284022721",
                "name": "Xingdi Yuan"
            },
            {
                "authorId": "40638665",
                "name": "Marc-Alexandre C\u00f4t\u00e9"
            }
        ],
        "abstract": "We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning."
    },
    {
        "paperId": "19da1292e830f128f21b8f6b178c803edda657b7",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.08219, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-13",
        "authors": [
            {
                "authorId": "2284101768",
                "name": "Haotian Sun"
            },
            {
                "authorId": "8103389",
                "name": "Yuchen Zhuang"
            },
            {
                "authorId": "2263558355",
                "name": "Wei Wei"
            },
            {
                "authorId": "2256775076",
                "name": "Chao Zhang"
            },
            {
                "authorId": "2263684187",
                "name": "Bo Dai"
            }
        ],
        "abstract": "Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive experiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It improves model performance by up to 6.77% across diverse tasks and domains, while reducing training and inference costs by 31.30x and 1.84x, respectively."
    },
    {
        "paperId": "e1c8f6e56ad2a1e7e3ed150e2411dcca85d19b69",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.08761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-13",
        "authors": [
            {
                "authorId": "33772445",
                "name": "Jillian R. Fisher"
            },
            {
                "authorId": "50085131",
                "name": "Ximing Lu"
            },
            {
                "authorId": "2266707583",
                "name": "Jaehun Jung"
            },
            {
                "authorId": "2112504145",
                "name": "Liwei Jiang"
            },
            {
                "authorId": "2265540561",
                "name": "Zaid Harchaoui"
            },
            {
                "authorId": "2258807987",
                "name": "Yejin Choi"
            }
        ],
        "abstract": "The permanence of online content combined with the enhanced authorship identification techniques calls for stronger computational methods to protect the identity and privacy of online authorship when needed, e.g., blind reviews for scientific papers, anonymous online reviews, or anonymous interactions in the mental health forums. In this paper, we propose an unsupervised inference-time approach to authorship obfuscation to address the unique challenges of authorship obfuscation: lack of supervision data for diverse authorship and domains, and the need for a sufficient level of revision beyond simple paraphrasing to obfuscate the authorship, all the while preserving the original content and fluency.We introduce JAMDEC, a user-controlled, inference-time algorithm for authorship obfuscation that can be in principle applied to any text and authorship. Our approach builds on small language models such as GPT2-XL in order to help avoid disclosing the original content to proprietary LLM\u2019s APIs, while also reducing the performance gap between small and large language models via algorithmic enhancement. The key idea behind our approach is to boost the creative power of smaller language models through constrained decoding, while also allowing for user-specified controls and flexibility. Experimental results demonstrate that our approach based on GPT2-XL outperforms previous state-of-the-art methods based on comparably small models, while performing competitively against GPT3.5 175B, a propriety model that is two orders of magnitudes larger."
    },
    {
        "paperId": "315bffd94456ff2a214a5a972c2f5c7f2ddb2163",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.08831, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-13",
        "authors": [
            {
                "authorId": "1397540342",
                "name": "B. Peng"
            },
            {
                "authorId": "2284065977",
                "name": "Xinyi Ling"
            },
            {
                "authorId": "11832104",
                "name": "Ziru Chen"
            },
            {
                "authorId": "2284196593",
                "name": "Huan Sun"
            },
            {
                "authorId": "2184773023",
                "name": "Xia Ning"
            }
        ],
        "abstract": "With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM exhibits excellent generalizability to out-of-domain settings, including unseen products and unseen instructions, highlighting its superiority as a generalist e-commerce model. Both the ECInstruct dataset and the eCeLLM models show great potential in empowering versatile and effective LLMs for e-commerce. ECInstruct and eCeLLM models are publicly accessible through https://ninglab.github.io/eCeLLM."
    },
    {
        "paperId": "d5da6f458b21c11bc87524120b8e82972b41b930",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Recurrent Alignment with Hard Attention for Hierarchical Text Rating",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2402.08874",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.08874, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-14",
        "authors": [
            {
                "authorId": "2284151974",
                "name": "Chenxi Lin"
            },
            {
                "authorId": "2284191055",
                "name": "Jiayu Ren"
            },
            {
                "authorId": "46180236",
                "name": "Guoxiu He"
            },
            {
                "authorId": "34153325",
                "name": "Zhuoren Jiang"
            },
            {
                "authorId": "2284130236",
                "name": "Haiyan Yu"
            },
            {
                "authorId": "2284259398",
                "name": "Xiaomin Zhu"
            }
        ],
        "abstract": "While large language models (LLMs) excel at understanding and generating plain text, they are not tailored to handle hierarchical text structures or directly predict task-specific properties such as text rating. In fact, selectively and repeatedly grasping the hierarchical structure of large-scale text is pivotal for deciphering its essence. To this end, we propose a novel framework for hierarchical text rating utilizing LLMs, which incorporates Recurrent Alignment with Hard Attention (RAHA). Particularly, hard attention mechanism prompts a frozen LLM to selectively focus on pertinent leaf texts associated with the root text and generate symbolic representations of their relationships. Inspired by the gradual stabilization of the Markov Chain, recurrent alignment strategy involves feeding predicted ratings iteratively back into the prompts of another trainable LLM, aligning it to progressively approximate the desired target. Experimental results demonstrate that RAHA outperforms existing state-of-the-art methods on three hierarchical text rating datasets. Theoretical and empirical analysis confirms RAHA\u2019s ability to gradually converge towards the underlying target through multiple inferences. Additional experiments on plain text rating datasets verify the effectiveness of this Markov-like alignment. Our data and code can be available in https://github.com/ECNU-Text-Computing/Markov-LLM."
    },
    {
        "paperId": "2208e506f72518a16ea86dfa604995c12fa8e4ca",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Premise Order Matters in Reasoning with Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.08939, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-14",
        "authors": [
            {
                "authorId": "2238263119",
                "name": "Xinyun Chen"
            },
            {
                "authorId": "2284066085",
                "name": "Ryan A. Chi"
            },
            {
                "authorId": "2238394232",
                "name": "Xuezhi Wang"
            },
            {
                "authorId": "2256313467",
                "name": "Denny Zhou"
            }
        ],
        "abstract": "Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark."
    },
    {
        "paperId": "ce0d07a82ec152258d9ebf2496a16a6737cf89f6",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.09727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-15",
        "authors": [
            {
                "authorId": "2284306150",
                "name": "Kuang-Huei Lee"
            },
            {
                "authorId": "2284302591",
                "name": "Xinyun Chen"
            },
            {
                "authorId": "2052903664",
                "name": "Hiroki Furuta"
            },
            {
                "authorId": "2253784975",
                "name": "John F. Canny"
            },
            {
                "authorId": "2284224093",
                "name": "Ian Fischer"
            }
        ],
        "abstract": "Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum. ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3.5-20x."
    },
    {
        "paperId": "4d4d9da4f2c39089ea6c8d84e5031c195548d7b6",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-15",
        "authors": [
            {
                "authorId": "29757826",
                "name": "Weixiang Zhao"
            },
            {
                "authorId": "2193658401",
                "name": "Zhuojun Li"
            },
            {
                "authorId": "2214828656",
                "name": "Shilong Wang"
            },
            {
                "authorId": "2284334856",
                "name": "Yang Wang"
            },
            {
                "authorId": "2279742153",
                "name": "Yulin Hu"
            },
            {
                "authorId": "49339265",
                "name": "Yanyan Zhao"
            },
            {
                "authorId": "2284284584",
                "name": "Chen Wei"
            },
            {
                "authorId": "2203961541",
                "name": "Bing Qin"
            }
        ],
        "abstract": "Emotional Intelligence (EI), consisting of emotion perception, emotion cognition and emotion expression, plays the critical roles in improving user interaction experience for the current large language model (LLM) based conversational general AI assistants. Previous works mainly focus on raising the emotion perception ability of them via naive fine-tuning on EI-related classification or regression tasks. However, this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI). To this end, we first introduce \\textsc{EiBench}, a large-scale collection of EI-related tasks in the text-to-text formation with task instructions that covers all three aspects of EI, which lays a solid foundation for the comprehensive EI enhancement of LLMs. Then a novel \\underline{\\textbf{Mo}}dular \\underline{\\textbf{E}}motional \\underline{\\textbf{I}}ntelligence enhancement method (\\textbf{MoEI}), consisting of Modular Parameter Expansion and intra-inter modulation, is proposed to comprehensively enhance the EI of LLMs without compromise their GI. Extensive experiments on two representative LLM-based assistants, Flan-T5 and LLaMA-2-Chat, demonstrate the effectiveness of MoEI to improving EI while maintain GI."
    },
    {
        "paperId": "a624041a028404e7d1fdb40987b1780ce4f1c842",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "BitDelta: Your Fine-Tune May Only Be Worth One Bit",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10193, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-15",
        "authors": [
            {
                "authorId": "2317036571",
                "name": "James Liu"
            },
            {
                "authorId": "2046958974",
                "name": "Guangxuan Xiao"
            },
            {
                "authorId": "2288961070",
                "name": "Kai Li"
            },
            {
                "authorId": "2266491429",
                "name": "Jason D. Lee"
            },
            {
                "authorId": "2249530374",
                "name": "Song Han"
            },
            {
                "authorId": "2280290462",
                "name": "Tri Dao"
            },
            {
                "authorId": "2280065752",
                "name": "Tianle Cai"
            }
        ],
        "abstract": "Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it's intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, which can also be translated to enhanced generation latency in multi-tenant settings. We validate BitDelta through experiments across Llama-2 and Mistral model families, and on models up to 70B parameters, showcasing minimal performance degradation over all tested settings."
    },
    {
        "paperId": "9637ef9019671034912ea0f506ae67c3f2fc4689",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10207, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-15",
        "authors": [
            {
                "authorId": "145094495",
                "name": "Rui Yang"
            },
            {
                "authorId": "2243367575",
                "name": "Xiaoman Pan"
            },
            {
                "authorId": "2310245050",
                "name": "Feng Luo"
            },
            {
                "authorId": "2284239202",
                "name": "Shuang Qiu"
            },
            {
                "authorId": "2260341870",
                "name": "Han Zhong"
            },
            {
                "authorId": "2256336899",
                "name": "Dong Yu"
            },
            {
                "authorId": "2108276402",
                "name": "Jianshu Chen"
            }
        ],
        "abstract": "We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around 10% GPU hours compared with multi-objective RL baseline."
    },
    {
        "paperId": "600fce8b16708755a331ba7122b9a0600f8890e1",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "PRISE: LLM-Style Sequence Compression for Learning Temporal Action Abstractions in Control",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10450, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-16",
        "authors": [
            {
                "authorId": "2108815599",
                "name": "Ruijie Zheng"
            },
            {
                "authorId": "2262064790",
                "name": "Ching-An Cheng"
            },
            {
                "authorId": "2200167546",
                "name": "Hal Daum'e"
            },
            {
                "authorId": "2238405926",
                "name": "Furong Huang"
            },
            {
                "authorId": "6247481",
                "name": "A. Kolobov"
            }
        ],
        "abstract": "Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of LLM training pipelines -- input tokenization via byte pair encoding (BPE) -- to the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action quantization with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the performance of both multitask imitation learning as well as few-shot imitation learning on unseen tasks. Our code is released at https://github.com/FrankZheng2022/PRISE."
    },
    {
        "paperId": "3dd564a7500861162904c6875a102118891a03f9",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10646, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-16",
        "authors": [
            {
                "authorId": "2187830802",
                "name": "Zhaowei Wang"
            },
            {
                "authorId": "2262397115",
                "name": "Wei Fan"
            },
            {
                "authorId": "2256995117",
                "name": "Qing Zong"
            },
            {
                "authorId": "2260447535",
                "name": "Hongming Zhang"
            },
            {
                "authorId": "2127128777",
                "name": "Sehyun Choi"
            },
            {
                "authorId": "2044202073",
                "name": "Tianqing Fang"
            },
            {
                "authorId": "2260287837",
                "name": "Xin Liu"
            },
            {
                "authorId": "2241325169",
                "name": "Yangqiu Song"
            },
            {
                "authorId": "9413717",
                "name": "Ginny Y. Wong"
            },
            {
                "authorId": "2256995112",
                "name": "Simon See"
            }
        ],
        "abstract": "Abstraction ability is crucial in human intelligence, which can also benefit various tasks in NLP study. Existing work shows that LLMs are deficient in abstract ability, and how to improve it remains unexplored. In this work, we design the framework AbsInstruct to enhance LLMs' abstraction ability through instruction tuning. The framework builds instructions with in-depth explanations to assist LLMs in capturing the underlying rationale of abstraction. Meanwhile, we introduce a plausibility estimator to select instructions that are more consistent with the abstraction knowledge of LLMs to be aligned. Then, our framework combines abstraction instructions with general-purpose ones to build a hybrid dataset. Extensive experiments and analyses demonstrate that our framework can considerably enhance LLMs' abstraction ability with strong generalization performance while maintaining their general instruction-following abilities."
    },
    {
        "paperId": "a28071c63963cc59ba500cd00c140ac08eb5ccb0",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Humans or LLMs as the Judge? A Study on Judgement Bias",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-16",
        "authors": [
            {
                "authorId": "2214875415",
                "name": "Guiming Hardy Chen"
            },
            {
                "authorId": "2267023659",
                "name": "Shunian Chen"
            },
            {
                "authorId": "2243875495",
                "name": "Ziche Liu"
            },
            {
                "authorId": "2308661183",
                "name": "Feng Jiang"
            },
            {
                "authorId": "2267007505",
                "name": "Benyou Wang"
            }
        ],
        "abstract": "Adopting human and large language models (LLM) as judges (*a.k.a* human- and LLM-as-a-judge) for evaluating the performance of LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLMs, questioning the reliability of the evaluation results. In this paper, we propose a novel framework that is free from referencing groundtruth annotations for investigating **Misinformation Oversight Bias**, **Gender Bias**, **Authority Bias** and **Beauty Bias** on LLM and human judges. We curate a dataset referring to the revised Bloom\u2019s Taxonomy and conduct thousands of evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the cutting-edge judges possess considerable biases. We further exploit these biases to conduct attacks on LLM judges. We hope that our work can notify the community of the bias and vulnerability of human- and LLM-as-a-judge, as well as the urgency of developing robust evaluation systems."
    },
    {
        "paperId": "beecd5dc56fbd344749c444eb102a4bb485a2070",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Inference to the Best Explanation in Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2402.10767",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10767, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-16",
        "authors": [
            {
                "authorId": "1491746798",
                "name": "Dhairya Dalal"
            },
            {
                "authorId": "34102057",
                "name": "Marco Valentino"
            },
            {
                "authorId": "2242981659",
                "name": "Andr\u00e9 Freitas"
            },
            {
                "authorId": "2265758612",
                "name": "Paul Buitelaar"
            }
        ],
        "abstract": "While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLMs' explanations. IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty. Extensive experiments are conducted on Causal Question Answering (CQA), where \\textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2). The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77\\% accuracy ($\\approx 27\\%$ above random), improving upon a GPT 3.5-as-a-Judge baseline ($\\approx+17\\%$) while being intrinsically more efficient and interpretable. Additional analyses suggest that, despite model-specific variances, LLM-generated explanations tend to conform to IBE criteria and that IBE-Eval is significantly correlated with human judgment, opening up opportunities for future development of automated explanation verification tools."
    },
    {
        "paperId": "6b86326016fc7b900deac073e119c543daa0bafd",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Quantifying the Persona Effect in LLM Simulations",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10811, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-16",
        "authors": [
            {
                "authorId": "2046818614",
                "name": "Tiancheng Hu"
            },
            {
                "authorId": "2261387574",
                "name": "Nigel Collier"
            }
        ],
        "abstract": "Large language models (LLMs) have shown remarkable promise in simulating human language and behavior. This study investigates how integrating persona variables-demographic, social, and behavioral factors-impacts LLMs' ability to simulate diverse perspectives. We find that persona variables account for<10% variance in annotations in existing subjective NLP datasets. Nonetheless, incorporating persona variables via prompting in LLMs provides modest but statistically significant improvements. Persona prompting is most effective in samples where many annotators disagree, but their disagreements are relatively minor. Notably, we find a linear relationship in our setting: the stronger the correlation between persona variables and human annotations, the more accurate the LLM predictions are using persona prompting. In a zero-shot setting, a powerful 70b model with persona prompting captures 81% of the annotation variance achievable by linear regression trained on ground truth annotations. However, for most subjective NLP datasets, where persona variables have limited explanatory power, the benefits of persona prompting are limited."
    },
    {
        "paperId": "2ef23e8b3c07885a850164d86ba748303e335377",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "CultureLLM: Incorporating Cultural Differences into Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10946, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-09",
        "authors": [
            {
                "authorId": "2258600474",
                "name": "Cheng Li"
            },
            {
                "authorId": "2284684384",
                "name": "Mengzhou Chen"
            },
            {
                "authorId": "2145270616",
                "name": "Jindong Wang"
            },
            {
                "authorId": "2256989615",
                "name": "Sunayana Sitaram"
            },
            {
                "authorId": "2249681654",
                "name": "Xing Xie"
            }
        ],
        "abstract": "Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM significantly outperforms various counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with comparable performance to GPT-4 or even better. Our human study shows that the generated samples are semantically equivalent to the original samples, providing an effective solution for LLMs augmentation. Code is released at https://github.com/Scarelette/CultureLLM."
    },
    {
        "paperId": "46a5ec31987a12d60ade20c6471db64c46f90106",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10963, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-13",
        "authors": [
            {
                "authorId": "2279337437",
                "name": "Alex Havrilla"
            },
            {
                "authorId": "1498636613",
                "name": "S. Raparthy"
            },
            {
                "authorId": "2284685571",
                "name": "Christoforus Nalmpantis"
            },
            {
                "authorId": "2173509991",
                "name": "Jane Dwivedi-Yu"
            },
            {
                "authorId": "2273535109",
                "name": "Maksym Zhuravinskyi"
            },
            {
                "authorId": "2072738644",
                "name": "Eric Hambro"
            },
            {
                "authorId": "2284686654",
                "name": "Roberta Railneau"
            }
        ],
        "abstract": "State-of-the-art language models can exhibit impressive reasoning refinement capabilities on math, science or coding tasks. However, recent work demonstrates that even the best models struggle to identify \\textit{when and where to refine} without access to external feedback. Outcome-based Reward Models (\\textbf{ORMs}), trained to predict correctness of the final answer indicating when to refine, offer one convenient solution for deciding when to refine. Process Based Reward Models (\\textbf{PRMs}), trained to predict correctness of intermediate steps, can then be used to indicate where to refine. But they are expensive to train, requiring extensive human annotations. In this paper, we propose Stepwise ORMs (\\textbf{SORMs}) which are trained, only on synthetic data, to approximate the expected future reward of the optimal policy or $V^{\\star}$. More specifically, SORMs are trained to predict the correctness of the final answer when sampling the current policy many times (rather than only once as in the case of ORMs). Our experiments show that SORMs can more accurately detect incorrect reasoning steps compared to ORMs, thus improving downstream accuracy when doing refinements. We then train \\textit{global} refinement models, which take only the question and a draft solution as input and predict a corrected solution, and \\textit{local} refinement models which also take as input a critique indicating the location of the first reasoning error. We generate training data for both models synthetically by reusing data used to train the SORM. We find combining global and local refinements, using the ORM as a reranker, significantly outperforms either one individually, as well as a best of three sample baseline. With this strategy we can improve the accuracy of a LLaMA-2 13B model (already fine-tuned with RL) on GSM8K from 53\\% to 65\\% when greedily sampled."
    },
    {
        "paperId": "f55bddab8d47f80097b1048b6da6988e3aa3f357",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11051, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-16",
        "authors": [
            {
                "authorId": "2047279522",
                "name": "Runcong Zhao"
            },
            {
                "authorId": "2285077193",
                "name": "Qinglin Zhu"
            },
            {
                "authorId": "2283763073",
                "name": "Hainiu Xu"
            },
            {
                "authorId": "92861741",
                "name": "Jiazheng Li"
            },
            {
                "authorId": "2314888686",
                "name": "Yuxiang Zhou"
            },
            {
                "authorId": "2265996676",
                "name": "Yulan He"
            },
            {
                "authorId": "2253541219",
                "name": "Lin Gui"
            }
        ],
        "abstract": "Existing datasets for narrative understanding often fail to represent the complexity and uncertainty of relationships in real-life social scenarios. To address this gap, we introduce a new benchmark, Conan, designed for extracting and analysing intricate character relation graphs from detective narratives. Specifically, we designed hierarchical relationship categories and manually extracted and annotated role-oriented relationships from the perspectives of various characters, incorporating both public relationships known to most characters and secret ones known to only a few. Our experiments with advanced Large Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their limitations in inferencing complex relationships and handling longer narratives. The combination of the Conan dataset and our pipeline strategy is geared towards understanding the ability of LLMs to comprehend nuanced relational dynamics in narrative contexts."
    },
    {
        "paperId": "0e314ddbf28514d92f2405b73941242c162ae0ba",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-16",
        "authors": [
            {
                "authorId": "2046974354",
                "name": "Jingwei Ni"
            },
            {
                "authorId": "2284947386",
                "name": "Minjing Shi"
            },
            {
                "authorId": "146552774",
                "name": "Dominik Stammbach"
            },
            {
                "authorId": "2790926",
                "name": "Mrinmaya Sachan"
            },
            {
                "authorId": "2261279066",
                "name": "Elliott Ash"
            },
            {
                "authorId": "3073566",
                "name": "Markus Leippold"
            }
        ],
        "abstract": "With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics."
    },
    {
        "paperId": "c59628de894a4aa7f91548bad5b4103b747256e8",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-17",
        "authors": [
            {
                "authorId": "2241417701",
                "name": "Yuxia Wang"
            },
            {
                "authorId": "2218861055",
                "name": "Jonibek Mansurov"
            },
            {
                "authorId": "2058455381",
                "name": "Petar Ivanov"
            },
            {
                "authorId": "2116966710",
                "name": "Jinyan Su"
            },
            {
                "authorId": "1967424",
                "name": "Artem Shelmanov"
            },
            {
                "authorId": "2164381839",
                "name": "Akim Tsvigun"
            },
            {
                "authorId": "2284688236",
                "name": "Osama Mohanned Afzal"
            },
            {
                "authorId": "2218209429",
                "name": "Tarek Mahmoud"
            },
            {
                "authorId": "2284686859",
                "name": "Giovanni Puccetti"
            },
            {
                "authorId": "2284687590",
                "name": "Thomas Arnold"
            },
            {
                "authorId": "8129718",
                "name": "Alham Fikri Aji"
            },
            {
                "authorId": "2257292541",
                "name": "Nizar Habash"
            },
            {
                "authorId": "2260340390",
                "name": "Iryna Gurevych"
            },
            {
                "authorId": "2026545715",
                "name": "Preslav Nakov"
            }
        ],
        "abstract": "The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark based on a multilingual, multi-domain, and multi-generator corpus of MGTs -- M4GT-Bench. The benchmark is compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection where one need to identify, which particular model generated the text; and (3) mixed human-machine text detection, where a word boundary delimiting MGT from human-written content should be determined. On the developed benchmark, we have tested several MGT detection baselines and also conducted an evaluation of human performance. We see that obtaining good performance in MGT detection usually requires an access to the training data from the same domain and generators. The benchmark is available at https://github.com/mbzuai-nlp/M4GT-Bench."
    },
    {
        "paperId": "cacd57ad4eada225ae7c436fe726ac5549a6f926",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Dissecting Human and LLM Preferences",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-17",
        "authors": [
            {
                "authorId": "2278801242",
                "name": "Junlong Li"
            },
            {
                "authorId": "2153433679",
                "name": "Fan Zhou"
            },
            {
                "authorId": "2256995981",
                "name": "Shichao Sun"
            },
            {
                "authorId": "2279654115",
                "name": "Yikai Zhang"
            },
            {
                "authorId": "2257373887",
                "name": "Hai Zhao"
            },
            {
                "authorId": "2256991660",
                "name": "Pengfei Liu"
            }
        ],
        "abstract": "As a relative quality comparison of model responses, human and Large Language Model (LLM) preferences serve as common alignment goals in model fine-tuning and criteria in evaluation. Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks. In this work, we dissect the preferences of human and 32 different LLMs to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis. We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize correctness, clarity, and harmlessness more. Additionally, LLMs of similar sizes tend to exhibit similar preferences, regardless of their training methods, and fine-tuning for alignment does not significantly alter the preferences of pretrained-only LLMs. Finally, we show that preference-based evaluation can be intentionally manipulated. In both training-free and training-based settings, aligning a model with the preferences of judges boosts scores, while injecting the least preferred properties lowers them. This results in notable score shifts: up to 0.59 on MT-Bench (1-10 scale) and 31.94 on AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this strategic adaptation. Interactive Demo: https://huggingface.co/spaces/GAIR/Preference-Dissection-Visualization Dataset: https://huggingface.co/datasets/GAIR/preference-dissection Code: https://github.com/GAIR-NLP/Preference-Dissection"
    },
    {
        "paperId": "1cfc42cdf19a2d3d4dc9c3ee84858c1c60e1ed3b",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Language Models Don't Learn the Physical Manifestation of Language",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2402.11349",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11349, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-17",
        "authors": [
            {
                "authorId": "14445971",
                "name": "Bruce W. Lee"
            },
            {
                "authorId": "2284733674",
                "name": "Jaehyuk Lim"
            }
        ],
        "abstract": "We argue that language-only models don't learn the physical manifestation of language. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-Test. These tasks highlight a fundamental gap between human linguistic understanding and the sensory-deprived linguistic understanding of LLMs. In support of our hypothesis, 1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the same model family (LLaMA 2 13B ->LLaMA 2 70B) has no significant effect on H-Test performance. We bring in the philosophical case of Mary, who learns about the world in a sensory-deprived environment as a useful conceptual framework to understand how language-only models learn about the world (Jackson, 1986). Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limitations of linguistic knowledge acquired in the absence of sensory experience. Our code and data are available at."
    },
    {
        "paperId": "90ee7c1d50793f7f0da731a1c07d6d2fc47324ac",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Offline Training of Language Model Agents with Functions as Learnable Weights",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11359, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-17",
        "authors": [
            {
                "authorId": "2116579935",
                "name": "Shaokun Zhang"
            },
            {
                "authorId": "47540245",
                "name": "Jieyu Zhang"
            },
            {
                "authorId": "2253854104",
                "name": "Jiale Liu"
            },
            {
                "authorId": "2322070046",
                "name": "Linxin Song"
            },
            {
                "authorId": "2256289461",
                "name": "Chi Wang"
            },
            {
                "authorId": "2257273968",
                "name": "Ranjay Krishna"
            },
            {
                "authorId": "2254166618",
                "name": "Qingyun Wu"
            }
        ],
        "abstract": "Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with two strategies, roll-back, and early-stop, to streamline the training process. With extensive experiments, we showcase that the agent training paradigm could significantly improve the performance of representative LLM agents in various downstream tasks. We also study the behavior of the agent training regarding aspects like the learning curve and domain transferability."
    },
    {
        "paperId": "a3fb9a9fba92831bd205e8f43aed798a2d8bc9cd",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11406, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-18",
        "authors": [
            {
                "authorId": "2239689793",
                "name": "Min Zhang"
            },
            {
                "authorId": "2239274607",
                "name": "Jianfeng He"
            },
            {
                "authorId": "9553002",
                "name": "Taoran Ji"
            },
            {
                "authorId": "2239106360",
                "name": "Chang-Tien Lu"
            }
        ],
        "abstract": "The fairness and trustworthiness of Large Language Models (LLMs) are receiving increasing attention. Implicit hate speech, which employs indirect language to convey hateful intentions, occupies a significant portion of practice. However, the extent to which LLMs effectively address this issue remains insufficiently examined. This paper delves into the capability of LLMs to detect implicit hate speech (Classification Task) and express confidence in their responses (Calibration Task). Our evaluation meticulously considers various prompt patterns and mainstream uncertainty estimation methods. Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech. (2) LLMs' confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset's complexity. Consequently, the calibration performance is heavily reliant on primary classification accuracy. These discoveries unveil new limitations of LLMs, underscoring the need for caution when optimizing models to ensure they do not veer towards extremes. This serves as a reminder to carefully consider sensitivity and confidence in the pursuit of model fairness."
    },
    {
        "paperId": "328bf0b8513ddf1b63e162aafb171cedda8c78a6",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Multi-dimensional Evaluation of Empathetic Dialog Responses",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11409, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-18",
        "authors": [
            {
                "authorId": "2284763199",
                "name": "Zhichao Xu"
            },
            {
                "authorId": "2284734001",
                "name": "Jiepu Jiang"
            }
        ],
        "abstract": "Empathy is critical for effective and satisfactory conversational communication. Prior efforts to measure conversational empathy mostly focus on expressed communicative intents -- that is, the way empathy is expressed. Yet, these works ignore the fact that conversation is also a collaboration involving both speakers and listeners. In contrast, we propose a multi-dimensional empathy evaluation framework to measure both \\emph{expressed intents from the speaker's perspective} and \\emph{perceived empathy from the listener's perspective}. We apply our analytical framework to examine internal customer-service dialogues. We find the two dimensions (expressed intent types and perceived empathy) are inter-connected, while perceived empathy has high correlations with dialogue satisfaction levels. To reduce the annotation cost, we explore different options to automatically measure conversational empathy: prompting LLMs and training language model-based classifiers. Our experiments show that prompting methods with even popular models like GPT-4 and Flan family models perform relatively poorly on both public and our internal datasets. In contrast, instruction-finetuned classifiers based on Flan-T5 family models outperform prior works and competitive baselines. We conduct a detailed ablation study to give more insights into instruction finetuning method's strong performance."
    },
    {
        "paperId": "49ed697a1e5cc5be831e33f2efb7dc61c77d28e7",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11442, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-18",
        "authors": [
            {
                "authorId": "2266421644",
                "name": "Siyuan Wang"
            },
            {
                "authorId": "2118602528",
                "name": "Zhongyu Wei"
            },
            {
                "authorId": "2266363632",
                "name": "Yejin Choi"
            },
            {
                "authorId": "2228515529",
                "name": "Xiang Ren"
            }
        ],
        "abstract": "Large language models (LLMs) have achieved impressive human-like performance across various reasoning tasks. However, their mastery of underlying inferential rules still falls short of human capabilities. To investigate this, we propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains. Our analysis of GPT-series models over a rule subset reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns. We further distill these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream reasoning. Through a multi-judger evaluation, our inference engine proves effective in generating accurate, complex and abstract conclusions and premises, and improve various commonsense reasoning tasks. Overall, our work sheds light on LLMs' limitations in grasping inferential rule and suggests ways to enhance their logical reasoning abilities~\\footnote{Code and data are available at \\url{https://github.com/SiyuanWangw/ULogic}.}."
    },
    {
        "paperId": "2fcddceba1710e3556e4315d13041eb029d5632c",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11456, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-18",
        "authors": [
            {
                "authorId": "2284684590",
                "name": "Sebastian Antony Joseph"
            },
            {
                "authorId": "2284685582",
                "name": "Lily Chen"
            },
            {
                "authorId": "52019849",
                "name": "Jan Trienes"
            },
            {
                "authorId": "2286927446",
                "name": "Hannah Louisa G\u00f6ke"
            },
            {
                "authorId": "2284684489",
                "name": "Monika Coers"
            },
            {
                "authorId": "2281906500",
                "name": "Wei Xu"
            },
            {
                "authorId": "2281826929",
                "name": "Byron C. Wallace"
            },
            {
                "authorId": "2281900870",
                "name": "Junyi Jessy Li"
            }
        ],
        "abstract": "Plain language summarization with LLMs can be useful for improving textual accessibility of technical content. But how factual are these summaries in a high-stakes domain like medicine? This paper presents FactPICO, a factuality benchmark for plain language summarization of medical texts describing randomized controlled trials (RCTs), which are the basis of evidence-based medicine and can directly inform patient treatment. FactPICO consists of 345 plain language summaries of RCT abstracts generated from three LLMs (i.e., GPT-4, Llama-2, and Alpaca), with fine-grained evaluation and natural language rationales from experts. We assess the factuality of critical elements of RCTs in those summaries: Populations, Interventions, Comparators, Outcomes (PICO), as well as the reported findings concerning these. We also evaluate the correctness of the extra information (e.g., explanations) added by LLMs. Using FactPICO, we benchmark a range of existing factuality metrics, including the newly devised ones based on LLMs. We find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level."
    },
    {
        "paperId": "9988a412f879731cb67e536663b8ceeb273f5c34",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "What Evidence Do Language Models Find Convincing?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11782, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-19",
        "authors": [
            {
                "authorId": "2209191806",
                "name": "Alexander Wan"
            },
            {
                "authorId": "2257243686",
                "name": "Eric Wallace"
            },
            {
                "authorId": "2249657133",
                "name": "Dan Klein"
            }
        ],
        "abstract": "Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as\"is aspartame linked to cancer\". To resolve these ambiguous queries, one must search through a large range of websites and consider\"which, if any, of this evidence do I find convincing?\". In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text contains scientific references or is written with a neutral tone. Taken together, these results highlight the importance of RAG corpus quality (e.g., the need to filter misinformation), and possibly even a shift in how LLMs are trained to better align with human judgements."
    },
    {
        "paperId": "efa27ad1f502c66afb0cad0fe4ec5339f192de67",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Microstructures and Accuracy of Graph Recall by Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11821, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-19",
        "authors": [
            {
                "authorId": "2261357677",
                "name": "Yanbang Wang"
            },
            {
                "authorId": "2284667869",
                "name": "Hejie Cui"
            },
            {
                "authorId": "2261283809",
                "name": "Jon M. Kleinberg"
            }
        ],
        "abstract": "Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structural patterns) in their recall. We find that LLMs not only underperform often in graph recall, but also tend to favor more triangles and alternating 2-paths. Moreover, we find that more advanced LLMs have a striking dependence on the domain that a real-world graph comes from -- by yielding the best recall accuracy when the graph is narrated in a language style consistent with its original domain."
    },
    {
        "paperId": "0d637d5e257813e5e4dde1385f6e7cff0bfc8722",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11886, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-19",
        "authors": [
            {
                "authorId": "2284690203",
                "name": "Shir Lissak"
            },
            {
                "authorId": "2135910736",
                "name": "Nitay Calderon"
            },
            {
                "authorId": "49916449",
                "name": "Geva Shenkman"
            },
            {
                "authorId": "4805164",
                "name": "Yaakov Ophir"
            },
            {
                "authorId": "2278440228",
                "name": "Eyal Fruchter"
            },
            {
                "authorId": "7351156",
                "name": "A. Klomek"
            },
            {
                "authorId": "2249760179",
                "name": "Roi Reichart"
            }
        ],
        "abstract": "Queer youth face increased mental health risks, such as depression, anxiety, and suicidal ideation. Hindered by negative stigma, they often avoid seeking help and rely on online resources, which may provide incompatible information. Although access to a supportive environment and reliable information is invaluable, many queer youth worldwide have no access to such support. However, this could soon change due to the rapid adoption of Large Language Models (LLMs) such as ChatGPT. This paper aims to comprehensively explore the potential of LLMs to revolutionize emotional support for queers. To this end, we conduct a qualitative and quantitative analysis of LLM\u2019s interactions with queer-related content. To evaluate response quality, we develop a novel ten-question scale that is inspired by psychological standards and expert input. We apply this scale to score several LLMs and human comments to posts where queer youth seek advice and share experiences. We find that LLM responses are supportive and inclusive, outscoring humans. However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice. We discuss these challenges, demonstrate that a dedicated prompt can improve the performance, and propose a blueprint of an LLM-supporter that actively (but sensitively) seeks user context to provide personalized, empathetic, and reliable responses. Our annotated dataset is available for further research.*https://github.com/nitaytech/LGBTeenDataset"
    },
    {
        "paperId": "c3f079f9f59f255e032a1239aea02a2affe93be8",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11889, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-19",
        "authors": [
            {
                "authorId": "2114810150",
                "name": "Qihuang Zhong"
            },
            {
                "authorId": "46573238",
                "name": "Liang Ding"
            },
            {
                "authorId": "46701032",
                "name": "Juhua Liu"
            },
            {
                "authorId": "2212029373",
                "name": "Bo Du"
            },
            {
                "authorId": "2255502438",
                "name": "D. Tao"
            }
        ],
        "abstract": "With the development of instruction-tuned large language models (LLMs), improving the safety of LLMs has become more critical. However, the current approaches for aligning the LLMs output with expected safety usually require substantial training efforts, e.g., high-quality safety data and expensive computational resources, which are costly and inefficient. To this end, we present reverse prompt contrastive decoding (ROSE), a simple-yet-effective method to directly boost the safety of existing instruction-tuned LLMs without any additional training. The principle of ROSE is to improve the probability of desired safe output via suppressing the undesired output induced by the carefully-designed reverse prompts. Experiments on 6 safety and 2 general-purpose tasks show that, our ROSE not only brings consistent and significant safety improvements (up to +13.8% safety score) upon 5 types of instruction-tuned LLMs, but also benefits the general-purpose ability of LLMs. In-depth analyses explore the underlying mechanism of ROSE, and reveal when and where to use it."
    },
    {
        "paperId": "d9bde53b440a86a754b7212b024757e139ad39cf",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11894, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-19",
        "authors": [
            {
                "authorId": "2249532957",
                "name": "Jiahao Ying"
            },
            {
                "authorId": "2275189609",
                "name": "Yixin Cao"
            },
            {
                "authorId": "2305427985",
                "name": "Yushi Bai"
            },
            {
                "authorId": "2306912740",
                "name": "Qianru Sun"
            },
            {
                "authorId": "2217713470",
                "name": "Bo Wang"
            },
            {
                "authorId": "2116724274",
                "name": "Wei Tang"
            },
            {
                "authorId": "2305440705",
                "name": "Zhaojun Ding"
            },
            {
                "authorId": "2284729613",
                "name": "Yizhe Yang"
            },
            {
                "authorId": "2284750473",
                "name": "Xuanjing Huang"
            },
            {
                "authorId": "2284684403",
                "name": "Shuicheng Yan"
            }
        ],
        "abstract": "Large language models (LLMs) have achieved impressive performance across various natural language benchmarks, prompting a continual need to curate more difficult datasets for larger LLMs, which is costly and time-consuming. In this paper, we propose to automate dataset updating and provide systematic analysis regarding its effectiveness in dealing with benchmark leakage issue, difficulty control, and stability. Thus, once the current benchmark has been mastered or leaked, we can update it for timely and reliable evaluation. There are two updating strategies: 1) mimicking strategy to generate similar samples based on original data, preserving stylistic and contextual essence, and 2) extending strategy that further expands existing samples at varying cognitive levels by adapting Bloom's taxonomy of educational objectives. Extensive experiments on updated MMLU and BIG-Bench demonstrate the stability of the proposed strategies and find that the mimicking strategy can effectively alleviate issues of overestimation from benchmark leakage. In cases where the efficient mimicking strategy fails, our extending strategy still shows promising results. Additionally, by controlling the difficulty, we can better discern the models' performance and enable fine-grained analysis neither too difficult nor too easy an exam can fairly judge students' learning status. To the best of our knowledge, we are the first to automate updating benchmarks for reliable and timely evaluation. Our demo leaderboard can be found at https://yingjiahao14.github.io/Automating-DatasetUpdates/."
    },
    {
        "paperId": "9a1b11b562d3959e53c9fb7ea8027e132ac209ae",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Understanding the Therapeutic Relationship between Counselors and Clients in Online Text-based Counseling using LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11958, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-19",
        "authors": [
            {
                "authorId": "2141519878",
                "name": "Anqi Li"
            },
            {
                "authorId": "2284943990",
                "name": "Yu Lu"
            },
            {
                "authorId": "2266840773",
                "name": "Nirui Song"
            },
            {
                "authorId": "2258666345",
                "name": "Shuai Zhang"
            },
            {
                "authorId": "2267020097",
                "name": "Lizhi Ma"
            },
            {
                "authorId": "2258395889",
                "name": "Zhenzhong Lan"
            }
        ],
        "abstract": "Robust therapeutic relationships between counselors and clients are fundamental to counseling effectiveness. The assessment of therapeutic alliance is well-established in traditional face-to-face therapy but may not directly translate to text-based settings. With millions of individuals seeking support through online text-based counseling, understanding the relationship in such contexts is crucial. In this paper, we present an automatic approach using large language models (LLMs) to understand the development of therapeutic alliance in text-based counseling. We adapt a theoretically grounded framework specifically to the context of online text-based counseling and develop comprehensive guidelines for characterizing the alliance. We collect a comprehensive counseling dataset and conduct multiple expert evaluations on a subset based on this framework. Our LLM-based approach, combined with guidelines and simultaneous extraction of supportive evidence underlying its predictions, demonstrates effectiveness in identifying the therapeutic alliance. Through further LLM-based evaluations on additional conversations, our findings underscore the challenges counselors face in cultivating strong online relationships with clients. Furthermore, we demonstrate the potential of LLM-based feedback mechanisms to enhance counselors' ability to build relationships, supported by a small-scale proof-of-concept."
    },
    {
        "paperId": "3cdc18cf052823b535796a7cd01be51048ab4b4a",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "EmoBench: Evaluating the Emotional Intelligence of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.12071, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-19",
        "authors": [
            {
                "authorId": "2106627931",
                "name": "Sahand Sabour"
            },
            {
                "authorId": "50152447",
                "name": "Siyang Liu"
            },
            {
                "authorId": "2257434341",
                "name": "Zheyuan Zhang"
            },
            {
                "authorId": "2247666353",
                "name": "June M. Liu"
            },
            {
                "authorId": "2145787511",
                "name": "Jinfeng Zhou"
            },
            {
                "authorId": "2284689530",
                "name": "Alvionna S. Sunaryo"
            },
            {
                "authorId": "2284734101",
                "name": "Juanzi Li"
            },
            {
                "authorId": "2284732865",
                "name": "Tatia M.C. Lee"
            },
            {
                "authorId": "145557251",
                "name": "Rada Mihalcea"
            },
            {
                "authorId": "2285704458",
                "name": "Minlie Huang"
            }
        ],
        "abstract": "Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. Our code and data are publicly available at https://github.com/Sahandfer/EmoBench."
    },
    {
        "paperId": "a9978987b7c58d00ee6814ea58e1974ba47d38b3",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.12195, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-19",
        "authors": [
            {
                "authorId": "51063775",
                "name": "Ziyue Wang"
            },
            {
                "authorId": "2116885534",
                "name": "Chi Chen"
            },
            {
                "authorId": "2309967889",
                "name": "Yiqi Zhu"
            },
            {
                "authorId": "2238952295",
                "name": "Fuwen Luo"
            },
            {
                "authorId": "144326610",
                "name": "Peng Li"
            },
            {
                "authorId": "2266495259",
                "name": "Ming Yan"
            },
            {
                "authorId": "2284726780",
                "name": "Ji Zhang"
            },
            {
                "authorId": "2262080010",
                "name": "Fei Huang"
            },
            {
                "authorId": "2257187607",
                "name": "Maosong Sun"
            },
            {
                "authorId": "2284726267",
                "name": "Yang Liu"
            }
        ],
        "abstract": "With the bloom of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) that incorporate LLMs with pre-trained vision models have recently demonstrated impressive performance across diverse vision-language tasks. However, they fall short to comprehend context involving multiple images. A primary reason for this shortcoming is that the visual features for each images are encoded individually by frozen encoders before feeding into the LLM backbone, lacking awareness of other images and the multimodal instructions. We term this issue as prior-LLM modality isolation and propose a two phase paradigm, browse-and-concentrate, to enable in-depth multimodal context fusion prior to feeding the features into LLMs. This paradigm initially\"browses\"through the inputs for essential insights, and then revisits the inputs to\"concentrate\"on crucial details, guided by these insights, to achieve a more comprehensive understanding of the multimodal inputs. Additionally, we develop training strategies specifically to enhance the understanding of multi-image inputs. Our method markedly boosts the performance on 7 multi-image scenarios, contributing to increments on average accuracy by 2.13% and 7.60% against strong MLLMs baselines with 3B and 11B LLMs, respectively."
    },
    {
        "paperId": "1243ccec1a27112bd37d1891b152ad68be8e4777",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.12275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-19",
        "authors": [
            {
                "authorId": "2109238481",
                "name": "Hao Tang"
            },
            {
                "authorId": "2074953129",
                "name": "Darren Key"
            },
            {
                "authorId": "2284673917",
                "name": "Kevin Ellis"
            }
        ],
        "abstract": "We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We define this optimism as a logical constraint between a program and a planner. We study our agent on gridworlds, and on task planning, finding our approach is more sample-efficient compared to deep RL, more compute-efficient compared to ReAct-style agents, and that it can transfer its knowledge across environments by editing its code."
    },
    {
        "paperId": "34503f8a9b1b48cd7b24471d9b4c23e41a7849ad",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2402.12326",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.12326, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-19",
        "authors": [
            {
                "authorId": "2127499278",
                "name": "Qisen Yang"
            },
            {
                "authorId": "2252413754",
                "name": "Z. Wang"
            },
            {
                "authorId": "2284729907",
                "name": "Honghui Chen"
            },
            {
                "authorId": "2219056193",
                "name": "Shenzhi Wang"
            },
            {
                "authorId": "47037481",
                "name": "Yifan Pu"
            },
            {
                "authorId": "2284733571",
                "name": "Xin Gao"
            },
            {
                "authorId": "2284758950",
                "name": "Wenhao Huang"
            },
            {
                "authorId": "2235964292",
                "name": "Shiji Song"
            },
            {
                "authorId": "2258319611",
                "name": "Gao Huang"
            }
        ],
        "abstract": "Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ human evaluators to examine the generated content across various psychological constructs, including depression, cognitive distortions, and personality traits. Results demonstrate that PsychoGAT serves as an effective assessment tool, achieving statistically significant excellence in psychometric metrics such as reliability, convergent validity, and discriminant validity. Moreover, human evaluations confirm PsychoGAT's enhancements in content coherence, interactivity, interest, immersion, and satisfaction."
    },
    {
        "paperId": "11d4478587c4d2ecb195fe911809946928767657",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.12327, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-19",
        "authors": [
            {
                "authorId": "2157765133",
                "name": "Zengqing Wu"
            },
            {
                "authorId": "2266391200",
                "name": "Run Peng"
            },
            {
                "authorId": "2266803478",
                "name": "Shuyuan Zheng"
            },
            {
                "authorId": "2284725320",
                "name": "Qianying Liu"
            },
            {
                "authorId": "2233049793",
                "name": "Xu Han"
            },
            {
                "authorId": "2284685881",
                "name": "Brian Inhyuk Kwon"
            },
            {
                "authorId": "2266396584",
                "name": "Makoto Onizuka"
            },
            {
                "authorId": "2284760425",
                "name": "Shaojie Tang"
            },
            {
                "authorId": "2284717877",
                "name": "Chuan Xiao"
            }
        ],
        "abstract": "Large Language Models (LLMs) have increasingly been utilized in social simulations, where they are often guided by carefully crafted instructions to stably exhibit human-like behaviors during simulations. Nevertheless, we doubt the necessity of shaping agents' behaviors for accurate social simulations. Instead, this paper emphasizes the importance of spontaneous phenomena, wherein agents deeply engage in contexts and make adaptive decisions without explicit directions. We explored spontaneous cooperation across three competitive scenarios and successfully simulated the gradual emergence of cooperation, findings that align closely with human behavioral data. This approach not only aids the computational social science community in bridging the gap between simulations and real-world dynamics but also offers the AI community a novel method to assess LLMs' capability of deliberate reasoning."
    },
    {
        "paperId": "ce083720b969f0cd57d19dec12eed26e5c49a515",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Understanding Fine-grained Distortions in Reports of Scientific Findings",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2402.12431",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.12431, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-19",
        "authors": [
            {
                "authorId": "2163480927",
                "name": "Amelie W\u00fchrl"
            },
            {
                "authorId": "2115541265",
                "name": "Dustin Wright"
            },
            {
                "authorId": "66339110",
                "name": "Roman Klinger"
            },
            {
                "authorId": "1736067",
                "name": "Isabelle Augenstein"
            }
        ],
        "abstract": "Distorted science communication harms individuals and society as it can lead to unhealthy behavior change and decrease trust in scientific institutions. Given the rapidly increasing volume of science communication in recent years, a fine-grained understanding of how findings from scientific publications are reported to the general public, and methods to detect distortions from the original work automatically, are crucial. Prior work focused on individual aspects of distortions or worked with unpaired data. In this work, we make three foundational contributions towards addressing this problem: (1) annotating 1,600 instances of scientific findings from academic papers paired with corresponding findings as reported in news articles and tweets wrt. four characteristics: causality, certainty, generality and sensationalism; (2) establishing baselines for automatically detecting these characteristics; and (3) analyzing the prevalence of changes in these characteristics in both human-annotated and large-scale unlabeled data. Our results show that scientific findings frequently undergo subtle distortions when reported. Tweets distort findings more often than science news reports. Detecting fine-grained distortions automatically poses a challenging task. In our experiments, fine-tuned task-specific models consistently outperform few-shot LLM prompting."
    },
    {
        "paperId": "2ecf11920fbc331333520a46711df2322b77f8b6",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.12483, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-19",
        "authors": [
            {
                "authorId": "2216486213",
                "name": "Nishant Balepur"
            },
            {
                "authorId": "3023068",
                "name": "Abhilasha Ravichander"
            },
            {
                "authorId": "2034613",
                "name": "Rachel Rudinger"
            }
        ],
        "abstract": "Multiple-choice question answering (MCQA) is often used to evaluate large language models (LLMs). To see if MCQA assesses LLMs as intended, we probe if LLMs can perform MCQA with choices-only prompts, where models must select the correct answer only from the choices. In three MCQA datasets and four LLMs, this prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy gain. To help explain this behavior, we conduct an in-depth, black-box analysis on memorization, choice dynamics, and question inference. Our key findings are threefold. First, we find no evidence that the choices-only accuracy stems from memorization alone. Second, priors over individual choices do not fully explain choices-only accuracy, hinting that LLMs use the group dynamics of choices. Third, LLMs have some ability to infer a relevant question from choices, and surprisingly can sometimes even match the original question. Inferring the original question is an impressive reasoning strategy, but it cannot fully explain the high choices-only accuracy of LLMs in MCQA. Thus, while LLMs are not fully incapable of reasoning in MCQA, we still advocate for the use of stronger baselines in MCQA benchmarks, the design of robust MCQA datasets for fair evaluations, and further efforts to explain LLM decision-making."
    },
    {
        "paperId": "d3f52ab6abc86b269380fbfd8b2a77b69013af3f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.12593, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-19",
        "authors": [
            {
                "authorId": "151472158",
                "name": "Joseph Marvin Imperial"
            },
            {
                "authorId": "2284760912",
                "name": "Gail Forey"
            },
            {
                "authorId": "3467205",
                "name": "H. T. Madabushi"
            }
        ],
        "abstract": "Domain experts across engineering, healthcare, and education follow strict standards for producing quality content such as technical manuals, medication instructions, and children\u2019s reading materials. However, current works in controllable text generation have yet to explore using these standards as references for control. Towards this end, we introduce Standardize, a retrieval-style in-context learning-based framework to guide large language models to align with expert-defined standards. Focusing on English language standards in the education domain as a use case, we consider the Common European Framework of Reference for Languages (CEFR) and Common Core Standards (CCS) for the task of open-ended content generation. Our findings show that models can gain 45% to 100% increase in precise accuracy across open and commercial LLMs evaluated, demonstrating that the use of knowledge artifacts extracted from standards and integrating them in the generation process can effectively guide models to produce better standard-aligned content."
    },
    {
        "paperId": "a70f0f9b9b9dc7d5caadcb23a551ea4213727548",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.12914, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-20",
        "authors": [
            {
                "authorId": "2163940136",
                "name": "Xueyang Feng"
            },
            {
                "authorId": "2241452075",
                "name": "Zhi-Yuan Chen"
            },
            {
                "authorId": "50625437",
                "name": "Yujia Qin"
            },
            {
                "authorId": "2257310922",
                "name": "Yankai Lin"
            },
            {
                "authorId": "2265519430",
                "name": "Xu Chen"
            },
            {
                "authorId": "2284825514",
                "name": "Zhiyuan Liu"
            },
            {
                "authorId": "2266834033",
                "name": "Ji-Rong Wen"
            }
        ],
        "abstract": "In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the model's effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention. Datasets and code are available at: https://github.com/XueyangFeng/ReHAC."
    },
    {
        "paperId": "16e989b9094c3653972c82b10b7004b6f0b42927",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13211, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-20",
        "authors": [
            {
                "authorId": "2266420525",
                "name": "Dongjin Kang"
            },
            {
                "authorId": "2295169937",
                "name": "Sunghwan Kim"
            },
            {
                "authorId": "2258722263",
                "name": "Taeyoon Kwon"
            },
            {
                "authorId": "2266717741",
                "name": "Seungjun Moon"
            },
            {
                "authorId": "2284988495",
                "name": "Hyunsouk Cho"
            },
            {
                "authorId": "2258802492",
                "name": "Youngjae Yu"
            },
            {
                "authorId": "2258907627",
                "name": "Dongha Lee"
            },
            {
                "authorId": "2258712913",
                "name": "Jinyoung Yeo"
            }
        ],
        "abstract": "Emotional Support Conversation (ESC) is a task aimed at alleviating individuals' emotional distress through daily conversation. Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses. Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support. Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy. Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy. Moreover, we conduct a methodological study to offer insights into the necessary approaches for LLMs to serve as proficient emotional supporters. Our findings emphasize that (1) low preference for specific strategies hinders the progress of emotional support, (2) external assistance helps reduce preference bias, and (3) existing LLMs alone cannot become good emotional supporters. These insights suggest promising avenues for future research to enhance the emotional intelligence of LLMs."
    },
    {
        "paperId": "b1890367317f0657c08ed96be4c474035b34b485",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Investigating Cultural Alignment of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13231, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-20",
        "authors": [
            {
                "authorId": "2006905770",
                "name": "Badr AlKhamissi"
            },
            {
                "authorId": "2006906348",
                "name": "Muhammad N. ElNokrashy"
            },
            {
                "authorId": "2284760572",
                "name": "Mai AlKhamissi"
            },
            {
                "authorId": "2284760722",
                "name": "Mona Diab"
            }
        ],
        "abstract": "The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions -- firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer."
    },
    {
        "paperId": "6810ac40c7c36249ef3f988ef803ea8540419f39",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-20",
        "authors": [
            {
                "authorId": "46815797",
                "name": "Liyan Tang"
            },
            {
                "authorId": "2260409785",
                "name": "Igor Shalyminov"
            },
            {
                "authorId": "2284825150",
                "name": "Amy Wing-mei Wong"
            },
            {
                "authorId": "2352104255",
                "name": "Jon Burnsky"
            },
            {
                "authorId": "2284763778",
                "name": "Jake W. Vincent"
            },
            {
                "authorId": "2284832892",
                "name": "Yu'an Yang"
            },
            {
                "authorId": "2260441495",
                "name": "Siffi Singh"
            },
            {
                "authorId": "2325515106",
                "name": "Song Feng"
            },
            {
                "authorId": "2260612278",
                "name": "Hwanjun Song"
            },
            {
                "authorId": "2260901186",
                "name": "Hang Su"
            },
            {
                "authorId": "2284917617",
                "name": "Lijia Sun"
            },
            {
                "authorId": "2324898296",
                "name": "Yi Zhang"
            },
            {
                "authorId": "39674628",
                "name": "Saab Mansour"
            },
            {
                "authorId": "2284767355",
                "name": "Kathleen McKeown"
            }
        ],
        "abstract": "Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence- level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model\u2019s size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a curated error taxonomy. We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than LLM-based evaluators."
    },
    {
        "paperId": "7b75b3d9f08aea9498baef8426f954106c3b5802",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13276, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-17",
        "authors": [
            {
                "authorId": "2216477657",
                "name": "Xiangyu Zhang"
            },
            {
                "authorId": "2281787478",
                "name": "Hexin Liu"
            },
            {
                "authorId": "2119184118",
                "name": "Kaishuai Xu"
            },
            {
                "authorId": "2284639855",
                "name": "Qiquan Zhang"
            },
            {
                "authorId": "2284641532",
                "name": "Daijiao Liu"
            },
            {
                "authorId": "2284864254",
                "name": "Beena Ahmed"
            },
            {
                "authorId": "2266394219",
                "name": "Julien Epps"
            }
        ],
        "abstract": "Depression is a critical concern in global mental health, prompting extensive research into AI-based detection methods. Among various AI technologies, Large Language Models (LLMs) stand out for their versatility in healthcare applications. However, the application of LLMs in the identification and analysis of depressive states remains relatively unexplored, presenting an intriguing avenue for future research. In this paper, we present an innovative approach to employ an LLM in the realm of depression detection, integrating acoustic speech information into the LLM framework for this specific application. We investigate an efficient method for automatic depression detection by integrating speech signals into LLMs utilizing Acoustic Landmarks. This approach is not only valuable for the detection of depression but also represents a new perspective in enhancing the ability of LLMs to comprehend and process speech signals. By incorporating acoustic landmarks, which are specific to the pronunciation of spoken words, our method adds critical dimensions to text transcripts. This integration also provides insights into the unique speech patterns of individuals, revealing the potential mental states of individuals. By encoding acoustic landmarks information into LLMs, evaluations of the proposed approach on the DAIC-WOZ dataset reveal state-of-the-art results when compared with existing Audio-Text baselines."
    },
    {
        "paperId": "4fb825a8454e6a7f69f987424fd799b65ae1c6a1",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-20",
        "authors": [
            {
                "authorId": "2260755600",
                "name": "Kewei Cheng"
            },
            {
                "authorId": "144741751",
                "name": "Nesreen K. Ahmed"
            },
            {
                "authorId": "2268234685",
                "name": "T. Willke"
            },
            {
                "authorId": "2257321412",
                "name": "Yizhou Sun"
            }
        ],
        "abstract": "Although Large Language Models (LLMs) excel at addressing straightforward reasoning tasks, they frequently struggle with difficulties when confronted by more complex multi-step reasoning due to a range of factors. Firstly, natural language often encompasses complex relationships among entities, making it challenging to maintain a clear reasoning chain over longer spans. Secondly, the abundance of linguistic diversity means that the same entities and relationships can be expressed using different terminologies and structures, complicating the task of identifying and establishing connections between multiple pieces of information. Graphs provide an effective solution to represent data rich in relational information and capture long-term dependencies among entities. To harness the potential of graphs, our paper introduces Structure Guided Prompt, an innovative three-stage task-agnostic prompting framework designed to improve the multi-step reasoning capabilities of LLMs in a zero-shot setting. This framework explicitly converts unstructured text into a graph via LLMs and instructs them to navigate this graph using task-specific strategies to formulate responses. By effectively organizing information and guiding navigation, it enables LLMs to provide more accurate and context-aware responses. Our experiments show that this framework significantly enhances the reasoning capabilities of LLMs, enabling them to excel in a broader spectrum of natural language scenarios."
    },
    {
        "paperId": "4e61fa433eb1a905309f5955a75235e74f7eb6b2",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-21",
        "authors": [
            {
                "authorId": "2284873738",
                "name": "Deuksin Kwon"
            },
            {
                "authorId": "2284873612",
                "name": "Emily Weiss"
            },
            {
                "authorId": "2284872129",
                "name": "Tara Kulshrestha"
            },
            {
                "authorId": "34338341",
                "name": "Kushal Chawla"
            },
            {
                "authorId": "2419453",
                "name": "Gale M. Lucas"
            },
            {
                "authorId": "2267332230",
                "name": "Jonathan Gratch"
            }
        ],
        "abstract": "A successful negotiation requires a range of capabilities, including comprehension of the conversation context, Theory-of-Mind (ToM) skills to infer the partner's motives, strategic reasoning, and effective communication, making it challenging for automated systems. Despite the remarkable performance of LLMs in various NLP tasks, there is no systematic evaluation of their capabilities in negotiation. Such an evaluation is critical for advancing AI negotiation agents and negotiation research, ranging from designing dialogue systems to providing pedagogical feedback and scaling up data collection practices. This work aims to systematically analyze the multifaceted capabilities of LLMs across diverse dialogue scenarios throughout the stages of a typical negotiation interaction. Our analysis highlights GPT-4's superior performance in many tasks while identifying specific challenges, such as making subjective assessments and generating contextually appropriate, strategically advantageous responses."
    },
    {
        "paperId": "5e490270c0fd49418e6bc5732c375867f253a20f",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Fine-Grained Modeling of Narrative Context: A Coherence Perspective via Retrospective Questions",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2402.13551",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-21",
        "authors": [
            {
                "authorId": "2276317005",
                "name": "Liyan Xu"
            },
            {
                "authorId": "153154545",
                "name": "JiangNan Li"
            },
            {
                "authorId": "2265525656",
                "name": "Mo Yu"
            },
            {
                "authorId": "2283871195",
                "name": "Jie Zhou"
            }
        ],
        "abstract": "This work introduces an original and practical paradigm for narrative comprehension, stemming from the characteristics that individual passages within narratives tend to be more cohesively related than isolated. Complementary to the common end-to-end paradigm, we propose a fine-grained modeling of narrative context, by formulating a graph dubbed NarCo, which explicitly depicts task-agnostic coherence dependencies that are ready to be consumed by various downstream tasks. In particular, edges in NarCo encompass free-form retrospective questions between context snippets, inspired by human cognitive perception that constantly reinstates relevant events from prior context. Importantly, our graph formalism is practically instantiated by LLMs without human annotations, through our designed two-stage prompting scheme. To examine the graph properties and its utility, we conduct three studies in narratives, each from a unique angle: edge relation efficacy, local context enrichment, and broader application in QA. All tasks could benefit from the explicit coherence captured by NarCo."
    },
    {
        "paperId": "aa4a36fc4610719b5ec253ebe0a789264c4043ff",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13561, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-21",
        "authors": [
            {
                "authorId": "2118046679",
                "name": "Yunxin Li"
            },
            {
                "authorId": "2266425853",
                "name": "Xinyu Chen"
            },
            {
                "authorId": "2142726660",
                "name": "Baotian Hu"
            },
            {
                "authorId": "2285182534",
                "name": "Haoyuan Shi"
            },
            {
                "authorId": "2284915404",
                "name": "Min Zhang"
            }
        ],
        "abstract": "Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA, respectively. The codes are available at https://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper"
    },
    {
        "paperId": "c4d43fe1b7e44c5e9929d6edf7bd11de4e6d293a",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-21",
        "authors": [
            {
                "authorId": "2284865968",
                "name": "Zhaorui Yang"
            },
            {
                "authorId": "1409707585",
                "name": "Qian Liu"
            },
            {
                "authorId": "19201674",
                "name": "Tianyu Pang"
            },
            {
                "authorId": "2285032001",
                "name": "Han Wang"
            },
            {
                "authorId": "46854712",
                "name": "H. Feng"
            },
            {
                "authorId": "145314938",
                "name": "Minfeng Zhu"
            },
            {
                "authorId": "2256716159",
                "name": "Wei Chen"
            }
        ],
        "abstract": "The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at https://github.com/sail-sg/sdft."
    },
    {
        "paperId": "de6f93849a84c128d72d14649ec4a6115be3c68d",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.14355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-22",
        "authors": [
            {
                "authorId": "72657087",
                "name": "Ning Bian"
            },
            {
                "authorId": "2118233348",
                "name": "Xianpei Han"
            },
            {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
            },
            {
                "authorId": "1831434",
                "name": "Yaojie Lu"
            },
            {
                "authorId": "2046814249",
                "name": "Ben He"
            },
            {
                "authorId": "2110832778",
                "name": "Le Sun"
            }
        ],
        "abstract": "Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning. In contrast, humans convey and pass down commonsense implicitly through stories. This paper investigates the inherent commonsense ability of large language models (LLMs) expressed through storytelling. We systematically investigate and compare stories and rules for retrieving and leveraging commonsense in LLMs. Experimental results on 28 commonsense QA datasets show that stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy. Moreover, stories are the more effective commonsense expression for answering questions regarding daily events, while rules are more effective for scientific questions. This aligns with the reporting bias of commonsense in text corpora. We further show that the correctness and relevance of commonsense stories can be further improved via iterative self-supervised fine-tuning. These findings emphasize the importance of using appropriate language to express, retrieve, and leverage commonsense for LLMs, highlighting a promising direction for better exploiting their commonsense abilities."
    },
    {
        "paperId": "186c3023473ad6093b654d4f4d8da64e0749fc32",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.14660, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-22",
        "authors": [
            {
                "authorId": "50117922",
                "name": "Yanan Wu"
            },
            {
                "authorId": "2285060791",
                "name": "Jie Liu"
            },
            {
                "authorId": "2284990102",
                "name": "Xingyuan Bu"
            },
            {
                "authorId": "2284731877",
                "name": "Jiaheng Liu"
            },
            {
                "authorId": "2254279326",
                "name": "Zhanhui Zhou"
            },
            {
                "authorId": "2279778502",
                "name": "Yuanxing Zhang"
            },
            {
                "authorId": "2214538677",
                "name": "Chenchen Zhang"
            },
            {
                "authorId": "2279540935",
                "name": "Zhiqi Bai"
            },
            {
                "authorId": "2284981829",
                "name": "Haibin Chen"
            },
            {
                "authorId": "2278217687",
                "name": "Tiezheng Ge"
            },
            {
                "authorId": "2254269925",
                "name": "Wanli Ouyang"
            },
            {
                "authorId": "2279560018",
                "name": "Wenbo Su"
            },
            {
                "authorId": "2279710589",
                "name": "Bo Zheng"
            }
        ],
        "abstract": "This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grained mathematical abilities of their models and facilitate the growth of foundation models."
    },
    {
        "paperId": "9978f937189a279a726545a734d20558095e7a0a",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2402.14762",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.14762, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-22",
        "authors": [
            {
                "authorId": "2284992285",
                "name": "Ge Bai"
            },
            {
                "authorId": "2285060791",
                "name": "Jie Liu"
            },
            {
                "authorId": "2284990102",
                "name": "Xingyuan Bu"
            },
            {
                "authorId": "2285046736",
                "name": "Yancheng He"
            },
            {
                "authorId": "2284731877",
                "name": "Jiaheng Liu"
            },
            {
                "authorId": "2254279326",
                "name": "Zhanhui Zhou"
            },
            {
                "authorId": "2285168908",
                "name": "Zhuoran Lin"
            },
            {
                "authorId": "2279560018",
                "name": "Wenbo Su"
            },
            {
                "authorId": "2278217687",
                "name": "Tiezheng Ge"
            },
            {
                "authorId": "2279710589",
                "name": "Bo Zheng"
            },
            {
                "authorId": "2254269925",
                "name": "Wanli Ouyang"
            }
        ],
        "abstract": "The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks. Further analysis indicates that neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs. Extensive case studies suggest that our designed tasks accurately assess the corresponding multi-turn abilities. The data and code are available at \\url{https://github.com/mtbench101/mt-bench-101}."
    },
    {
        "paperId": "3e5e0e1841e23fc41f4e3243c9091bf6e7e7d199",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.14798, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-22",
        "authors": [
            {
                "authorId": "38912638",
                "name": "Nathaniel Weir"
            },
            {
                "authorId": "2187060946",
                "name": "Kate Sanders"
            },
            {
                "authorId": "47433471",
                "name": "Orion Weller"
            },
            {
                "authorId": "2285978160",
                "name": "Shreya Sharma"
            },
            {
                "authorId": "2285428192",
                "name": "Dongwei Jiang"
            },
            {
                "authorId": "6600801",
                "name": "Zhengping Jiang"
            },
            {
                "authorId": "40135250",
                "name": "Bhavana Dalvi"
            },
            {
                "authorId": "3385516",
                "name": "Oyvind Tafjord"
            },
            {
                "authorId": "144949918",
                "name": "Peter Alexander Jansen"
            },
            {
                "authorId": "2258709497",
                "name": "Peter Clark"
            },
            {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
            }
        ],
        "abstract": "Recent language models enable new opportunities for structured reasoning with text, such as the construction of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what _valid decompositional entailment_ is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic entailment engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment and evaluate its impact on LLM-based textual inference. We find that our new dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of forming a clear protocol for discerning entailment. We also find that training an RDTE-oriented entailment classifier via knowledge distillation and employing it in an entailment tree reasoning engine significantly improves both accuracy and proof quality, illustrating the practical benefit of this advance for textual inference."
    },
    {
        "paperId": "4ca78a93635aa6e41d695c78110699e04e5d5a5b",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.14807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-22",
        "authors": [
            {
                "authorId": "2148687825",
                "name": "Nikhil Behari"
            },
            {
                "authorId": "2261281313",
                "name": "Edwin Zhang"
            },
            {
                "authorId": "2261358902",
                "name": "Yunfan Zhao"
            },
            {
                "authorId": "2052769644",
                "name": "A. Taneja"
            },
            {
                "authorId": "35803003",
                "name": "Dheeraj M. Nagaraj"
            },
            {
                "authorId": "2288282318",
                "name": "Milind Tambe"
            }
        ],
        "abstract": "Restless multi-armed bandits (RMAB) have demonstrated success in optimizing resource allocation for large beneficiary populations in public health settings. Unfortunately, RMAB models lack flexibility to adapt to evolving public health policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept automated planners across domains of robotic control and navigation. In this paper, we propose a Decision Language Model (DLM) for RMABs, enabling dynamic fine-tuning of RMAB policies in public health settings using human-language commands. We propose using LLMs as automated planners to (1) interpret human policy preference prompts, (2) propose reward functions as code for a multi-agent RMAB environment, and (3) iterate on the generated reward functions using feedback from grounded RMAB simulations. We illustrate the application of DLM in collaboration with ARMMAN, an India-based non-profit promoting preventative care for pregnant mothers, that currently relies on RMAB policies to optimally allocate health worker calls to low-resource populations. We conduct a technology demonstration in simulation using the Gemini Pro model, showing DLM can dynamically shape policy outcomes using only human prompts as input."
    },
    {
        "paperId": "6982dba1c01082571d5bda6bfeb2b4b4db4b81fa",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.14856, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-20",
        "authors": [
            {
                "authorId": "2286310256",
                "name": "Philipp Mondorf"
            },
            {
                "authorId": "2286310311",
                "name": "Barbara Plank"
            }
        ],
        "abstract": "Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like $\\textit{supposition following}$ or $\\textit{chain construction}$. Moreover, our research demonstrates that the architecture and scale of the model significantly affect its preferred method of reasoning, with more advanced models tending to adopt strategies more frequently than less sophisticated ones. Importantly, we assert that a model's accuracy, that is the correctness of its final conclusion, does not necessarily reflect the validity of its reasoning process. This distinction underscores the necessity for more nuanced evaluation procedures in the field."
    },
    {
        "paperId": "0334987f094121c094d5043ab38f14ebf5852c05",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.14865, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-21",
        "authors": [
            {
                "authorId": "2543684",
                "name": "Kaijie Zhu"
            },
            {
                "authorId": "2273553706",
                "name": "Jindong Wang"
            },
            {
                "authorId": "2261935625",
                "name": "Qinlin Zhao"
            },
            {
                "authorId": "2266367743",
                "name": "Ruochen Xu"
            },
            {
                "authorId": "2249681654",
                "name": "Xing Xie"
            }
        ],
        "abstract": "Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are also dynamically configurable, allowing multifaceted analysis. We conducted extensive evaluations using MPA and found that most LLMs achieve poorer performance, indicating room for improvement. Our multifaceted analysis demonstrated the strong correlation between the basic abilities and an implicit Matthew effect on model size, i.e., larger models possess stronger correlations of the abilities. MPA can also be used as a data augmentation approach to enhance LLMs. Code is available at: https://github.com/microsoft/promptbench."
    },
    {
        "paperId": "4c0f88e80320885e8289a9af781a1101717104f2",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "tinyBenchmarks: evaluating LLMs with fewer examples",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.14992, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-22",
        "authors": [
            {
                "authorId": "1490941219",
                "name": "Felipe Maia Polo"
            },
            {
                "authorId": "2286343827",
                "name": "Lucas Weber"
            },
            {
                "authorId": "2283849613",
                "name": "Leshem Choshen"
            },
            {
                "authorId": "2247880508",
                "name": "Yuekai Sun"
            },
            {
                "authorId": "2286488914",
                "name": "Gongjun Xu"
            },
            {
                "authorId": "8202372",
                "name": "M. Yurochkin"
            }
        ],
        "abstract": "The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results."
    },
    {
        "paperId": "121aef49e5fbe8d9dc829adaa472a44aff84f4f5",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-23",
        "authors": [
            {
                "authorId": "2164113313",
                "name": "Zhuohao Yu"
            },
            {
                "authorId": "2287659901",
                "name": "Chang Gao"
            },
            {
                "authorId": "2286328804",
                "name": "Wenjin Yao"
            },
            {
                "authorId": "2108024279",
                "name": "Yidong Wang"
            },
            {
                "authorId": "145235149",
                "name": "Wei Ye"
            },
            {
                "authorId": "2273553706",
                "name": "Jindong Wang"
            },
            {
                "authorId": "2249681654",
                "name": "Xing Xie"
            },
            {
                "authorId": "2250437942",
                "name": "Yue Zhang"
            },
            {
                "authorId": "1705434",
                "name": "Shikun Zhang"
            }
        ],
        "abstract": "Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered\"interactor\"role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KIEval's effectiveness and generalization. We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning."
    },
    {
        "paperId": "6559a72f4b63681542f63508268fa139d8693101",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "ToMBench: Benchmarking Theory of Mind in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15052, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-23",
        "authors": [
            {
                "authorId": "2268644680",
                "name": "Zhuang Chen"
            },
            {
                "authorId": "2109217402",
                "name": "Jincenzi Wu"
            },
            {
                "authorId": "2145787511",
                "name": "Jinfeng Zhou"
            },
            {
                "authorId": "2122225897",
                "name": "Bosi Wen"
            },
            {
                "authorId": "2286323459",
                "name": "Guanqun Bi"
            },
            {
                "authorId": "2286317046",
                "name": "Gongyao Jiang"
            },
            {
                "authorId": "2288333110",
                "name": "Yaru Cao"
            },
            {
                "authorId": "2284693051",
                "name": "Mengting Hu"
            },
            {
                "authorId": "2287487634",
                "name": "Yunghwei Lai"
            },
            {
                "authorId": "2286309288",
                "name": "Zexuan Xiong"
            },
            {
                "authorId": "2285704458",
                "name": "Minlie Huang"
            }
        ],
        "abstract": "Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicating that LLMs have not achieved a human-level theory of mind yet. Our aim with ToMBench is to enable an efficient and effective evaluation of LLMs' ToM capabilities, thereby facilitating the development of LLMs with inherent social intelligence."
    },
    {
        "paperId": "bdde2394bd908575738639833b463fbaa81399e7",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15337, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-23",
        "authors": [
            {
                "authorId": "2260656558",
                "name": "Nitesh Kumar"
            },
            {
                "authorId": "1843915",
                "name": "Usashi Chatterjee"
            },
            {
                "authorId": "2265382",
                "name": "S. Schockaert"
            }
        ],
        "abstract": "Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy, but existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but having at least some perceptual and subjective features in the training data seems essential for achieving the best results."
    },
    {
        "paperId": "c3b82c397e3bc30942a214b8aca7d861d453a74f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-23",
        "authors": [
            {
                "authorId": "2286338369",
                "name": "Sergei Bogdanov"
            },
            {
                "authorId": "2286299579",
                "name": "Alexandre Constantin"
            },
            {
                "authorId": "2286308669",
                "name": "Timoth'ee Bernard"
            },
            {
                "authorId": "2286299659",
                "name": "Benoit Crabb'e"
            },
            {
                "authorId": "2286304439",
                "name": "Etienne Bernard"
            }
        ],
        "abstract": "Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task. NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs. We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs. NuNER and NuNER\u2019s dataset are open-sourced with MIT License."
    },
    {
        "paperId": "868c06e552be0c9a53d41d18eaa233402a2bb7ee",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15754, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-24",
        "authors": [
            {
                "authorId": "2261155019",
                "name": "Yuxuan Liu"
            },
            {
                "authorId": "2260740143",
                "name": "Tianchi Yang"
            },
            {
                "authorId": "3110003",
                "name": "Shaohan Huang"
            },
            {
                "authorId": "2260851231",
                "name": "Zihan Zhang"
            },
            {
                "authorId": "2146285313",
                "name": "Haizhen Huang"
            },
            {
                "authorId": "2253471545",
                "name": "Furu Wei"
            },
            {
                "authorId": "2066621592",
                "name": "Weiwei Deng"
            },
            {
                "authorId": "2247156451",
                "name": "Feng Sun"
            },
            {
                "authorId": "2256972722",
                "name": "Qi Zhang"
            }
        ],
        "abstract": "Large language models (LLMs) have emerged as a promising alternative to expensive human evaluations. However, the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria. To address this challenge, we propose HD-Eval, a novel framework that iteratively aligns LLM-based evaluators with human preference via Hierarchical Criteria Decomposition. HD-Eval inherits the essence from the evaluation mindset of human experts and enhances the alignment of LLM-based evaluators by decomposing a given evaluation task into finer-grained criteria, aggregating them according to estimated human preferences, pruning insignificant criteria with attribution, and further decomposing significant criteria. By integrating these steps within an iterative alignment training process, we obtain a hierarchical decomposition of criteria that comprehensively captures aspects of natural language at multiple levels of granularity. Implemented as a white box, the human preference-guided aggregator is efficient to train and more explainable than relying solely on prompting, and its independence from model parameters makes it applicable to closed-source LLMs. Extensive experiments on three evaluation domains demonstrate the superiority of HD-Eval in further aligning state-of-the-art evaluators and providing deeper insights into the explanation of evaluation results and the task itself."
    },
    {
        "paperId": "28becb37490f8f16c8666fe9878b0001dd540e97",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MATHWELL: Generating Educational Math Word Problems Using Teacher Annotations",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2402.15861",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15861, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-24",
        "authors": [
            {
                "authorId": "2286875024",
                "name": "Bryan R. Christ"
            },
            {
                "authorId": "2286867173",
                "name": "Jonathan Kropko"
            },
            {
                "authorId": "32452740",
                "name": "Thomas Hartvigsen"
            }
        ],
        "abstract": "Math word problems are critical K-8 educational tools, but writing them is time consuming and requires extensive expertise. To be educational, problems must be solvable, have accurate answers, and, most importantly, be educationally appropriate. We propose that language models have potential to support K-8 math education by automatically generating word problems. However, evaluating educational appropriateness is hard to quantify. We fill this gap by having teachers evaluate problems generated by LLMs, who find existing models and data often fail to be educationally appropriate. We then explore automatically generating educational word problems, ultimately using our expert annotations to finetune a 70B language model. Our model, MATHWELL, is the first K-8 word problem generator targeted at educational appropriateness. Further expert studies find MATHWELL generates problems far more solvable, accurate, and appropriate than public models. MATHWELL also matches GPT-4's problem quality while attaining more appropriate reading levels for K-8 students and avoiding generating harmful questions."
    },
    {
        "paperId": "33313b413695dcd396c6f558a3153cc6a572d5c1",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "ChatMusician: Understanding and Generating Music Intrinsically with LLM",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.16153, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-25",
        "authors": [
            {
                "authorId": "2032236274",
                "name": "Ruibin Yuan"
            },
            {
                "authorId": "2287467997",
                "name": "Hanfeng Lin"
            },
            {
                "authorId": "2287877457",
                "name": "Yi Wang"
            },
            {
                "authorId": "2157198469",
                "name": "Zeyue Tian"
            },
            {
                "authorId": "2152536983",
                "name": "Shangda Wu"
            },
            {
                "authorId": "2057973326",
                "name": "Tianhao Shen"
            },
            {
                "authorId": "2143853895",
                "name": "Ge Zhang"
            },
            {
                "authorId": "2280131300",
                "name": "Yuhang Wu"
            },
            {
                "authorId": "2286881771",
                "name": "Cong Liu"
            },
            {
                "authorId": "2286908476",
                "name": "Ziya Zhou"
            },
            {
                "authorId": "2116609277",
                "name": "Ziyang Ma"
            },
            {
                "authorId": "46426991",
                "name": "Liumeng Xue"
            },
            {
                "authorId": "2267906150",
                "name": "Ziyu Wang"
            },
            {
                "authorId": "2154974033",
                "name": "Qin Liu"
            },
            {
                "authorId": "2268491856",
                "name": "Tianyu Zheng"
            },
            {
                "authorId": "2129449392",
                "name": "Yizhi Li"
            },
            {
                "authorId": "2265407443",
                "name": "Yinghao Ma"
            },
            {
                "authorId": "2280286947",
                "name": "Yiming Liang"
            },
            {
                "authorId": "2192825554",
                "name": "Xiaowei Chi"
            },
            {
                "authorId": "2275192102",
                "name": "Ruibo Liu"
            },
            {
                "authorId": "2284801141",
                "name": "Zili Wang"
            },
            {
                "authorId": "2286877005",
                "name": "Pengfei Li"
            },
            {
                "authorId": "2287919309",
                "name": "Jingcheng Wu"
            },
            {
                "authorId": "2279451646",
                "name": "Chenghua Lin"
            },
            {
                "authorId": "2157104830",
                "name": "Qi-fei Liu"
            },
            {
                "authorId": "2281039192",
                "name": "Tao Jiang"
            },
            {
                "authorId": "2239245627",
                "name": "Wenhao Huang"
            },
            {
                "authorId": "2249847177",
                "name": "Wenhu Chen"
            },
            {
                "authorId": "2109397",
                "name": "Emmanouil Benetos"
            },
            {
                "authorId": "2276508494",
                "name": "Jie Fu"
            },
            {
                "authorId": "38746431",
                "name": "Gus G. Xia"
            },
            {
                "authorId": "2286883319",
                "name": "R. Dannenberg"
            },
            {
                "authorId": "2239201089",
                "name": "Wei Xue"
            },
            {
                "authorId": "2257126275",
                "name": "Shiyin Kang"
            },
            {
                "authorId": "2118270918",
                "name": "Yi-Ting Guo"
            }
        ],
        "abstract": "While Large Language Models (LLMs) demonstrate impressive capabilities in text generation, we find that their ability has yet to be generalized to music, humanity's creative language. We introduce ChatMusician, an open-source LLM that integrates intrinsic musical abilities. It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language. ChatMusician can understand and generate music with a pure text tokenizer without any external multi-modal neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score. Our model is capable of composing well-structured, full-length music, conditioned on texts, chords, melodies, motifs, musical forms, etc, surpassing GPT-4 baseline. On our meticulously curated college-level music understanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and GPT-3.5 on zero-shot setting by a noticeable margin. Our work reveals that LLMs can be an excellent compressor for music, but there remains significant territory to be conquered. We release our 4B token music-language corpora MusicPile, the collected MusicTheoryBench, code, model and demo in GitHub."
    },
    {
        "paperId": "f2bedc815eca8d5be46d88d27b44d1c86bbb8b90",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.16367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-26",
        "authors": [
            {
                "authorId": "1390516186",
                "name": "L"
            },
            {
                "authorId": "2288755136",
                "name": "Yinlong Xu"
            },
            {
                "authorId": "2305579523",
                "name": "Hongxia Xu"
            },
            {
                "authorId": "1391200997",
                "name": "Jintai Chen"
            },
            {
                "authorId": "2266808383",
                "name": "Xuming Hu"
            },
            {
                "authorId": "2267005674",
                "name": "Jian Wu"
            }
        ],
        "abstract": "Recently, large language models (LLMs) have achieved tremendous breakthroughs in the field of NLP, but still lack understanding of their internal neuron activities when processing different languages. We designed a method to convert dense LLMs into fine-grained MoE architectures, and then visually studied the multilingual activation patterns of LLMs through expert activation frequency heatmaps. Through comprehensive experiments on different model families, different model sizes, and different variants, we analyzed the similarities and differences in the internal neuron activation patterns of LLMs when processing different languages. Specifically, we investigated the distribution of high-frequency activated experts, multilingual shared experts, whether multilingual activation patterns are related to language families, and the impact of instruction tuning on activation patterns. We further explored leveraging the discovered differences in expert activation frequencies to guide sparse activation and pruning. Experimental results demonstrated that our method significantly outperformed random expert pruning and even exceeded the performance of unpruned models in some languages. Additionally, we found that configuring different pruning rates for different layers based on activation level differences could achieve better results. Our findings reveal the multilingual processing mechanisms within LLMs and utilize these insights to offer new perspectives for applications such as sparse activation and model pruning."
    },
    {
        "paperId": "96edfa3441bb3624ea2b8bdfc5eec2c87efa9637",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.16438, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-26",
        "authors": [
            {
                "authorId": "1997234792",
                "name": "Tianyi Tang"
            },
            {
                "authorId": "2287969051",
                "name": "Wenyang Luo"
            },
            {
                "authorId": "15086992",
                "name": "Haoyang Huang"
            },
            {
                "authorId": "2273919921",
                "name": "Dongdong Zhang"
            },
            {
                "authorId": "72541556",
                "name": "Xiaolei Wang"
            },
            {
                "authorId": "2273631201",
                "name": "Xin Zhao"
            },
            {
                "authorId": "2270027811",
                "name": "Furu Wei"
            },
            {
                "authorId": "2287918455",
                "name": "Ji-Rong Wen"
            }
        ],
        "abstract": "Large language models (LLMs) demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora. It remains a challenging problem to explain the underlying mechanisms by which LLMs process multilingual texts. In this paper, we delve into the composition of Transformer architectures in LLMs to pinpoint language-specific regions. Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within LLMs. Based on LAPE, we conduct comprehensive experiments on several representative LLMs, such as LLaMA-2, BLOOM, and Mistral. Our findings indicate that LLMs' proficiency in processing a particular language is predominantly due to a small subset of neurons, primarily situated in the models' top and bottom layers. Furthermore, we showcase the feasibility to\"steer\"the output language of LLMs by selectively activating or deactivating language-specific neurons. Our research provides important evidence to the understanding and exploration of the multilingual capabilities of LLMs."
    },
    {
        "paperId": "5bd44a34457d3f323eea4d961dd762003be3961d",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.16786, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-26",
        "authors": [
            {
                "authorId": "2043232919",
                "name": "Paul R\u00f6ttger"
            },
            {
                "authorId": "1667898858",
                "name": "Valentin Hofmann"
            },
            {
                "authorId": "22330666",
                "name": "Valentina Pyatkin"
            },
            {
                "authorId": "2287831755",
                "name": "Musashi Hinck"
            },
            {
                "authorId": "90729626",
                "name": "Hannah Rose Kirk"
            },
            {
                "authorId": "2130001188",
                "name": "Hinrich Schutze"
            },
            {
                "authorId": "2267334203",
                "name": "Dirk Hovy"
            }
        ],
        "abstract": "Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing constrained evaluation paradigm for values and opinions in LLMs and explore more realistic unconstrained evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs."
    },
    {
        "paperId": "3ebf1cc913172c87ab4d8e1c8aeffdeb6c0ff0b3",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17019, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-26",
        "authors": [
            {
                "authorId": "2267158814",
                "name": "Hang Jiang"
            },
            {
                "authorId": "2267000216",
                "name": "Xiajie Zhang"
            },
            {
                "authorId": "1729586298",
                "name": "Robert Mahari"
            },
            {
                "authorId": "2287836752",
                "name": "Daniel Kessler"
            },
            {
                "authorId": "2287832315",
                "name": "Eric Ma"
            },
            {
                "authorId": "50509991",
                "name": "Tal August"
            },
            {
                "authorId": "2287767363",
                "name": "Irene Li"
            },
            {
                "authorId": "2252488708",
                "name": "A. Pentland"
            },
            {
                "authorId": "2288014253",
                "name": "Yoon Kim"
            },
            {
                "authorId": "2631301",
                "name": "Jad Kabbara"
            },
            {
                "authorId": "2261551123",
                "name": "Deb Roy"
            }
        ],
        "abstract": "Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 294 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop approach to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through randomized controlled trials (RCTs) with legal novices on 10 samples from the dataset. We find that LLM-generated stories enhance comprehension of legal concepts and interest in law among non-native speakers compared to only definitions. Moreover, stories consistently help participants relate legal concepts to their lives. Finally, we find that learning with stories shows a higher retention rate for non-native speakers in the follow-up assessment. Our work has strong implications for using LLMs in promoting teaching and learning in the legal field and beyond."
    },
    {
        "paperId": "27dd800cb087f1575a65fba06c95ec8fd83a0fb4",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17124, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-27",
        "authors": [
            {
                "authorId": "1500662261",
                "name": "Xinran Zhao"
            },
            {
                "authorId": "2279761345",
                "name": "Hongming Zhang"
            },
            {
                "authorId": "2243367575",
                "name": "Xiaoman Pan"
            },
            {
                "authorId": "2087264100",
                "name": "Wenlin Yao"
            },
            {
                "authorId": "2256336899",
                "name": "Dong Yu"
            },
            {
                "authorId": "2287820683",
                "name": "Tongshuang Wu"
            },
            {
                "authorId": "2108276402",
                "name": "Jianshu Chen"
            }
        ],
        "abstract": "For a LLM to be trustworthy, its confidence level should be well-calibrated with its actual performance. While it is now common sense that LLM performances are greatly impacted by prompts, the confidence calibration in prompting LLMs has yet to be thoroughly explored. In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved. We conduct extensive experiments on six prompting methods in the question-answering context and we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances. Inspired by human cognition, we propose Fact-and-Reflection (FaR) prompting, which improves the LLM calibration in two steps. First, FaR elicits the known\"facts\"that are relevant to the input prompt from the LLM. And then it asks the model to\"reflect\"over them to generate the final answer. Experiments show that FaR prompting achieves significantly better calibration; it lowers the Expected Calibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR prompting even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger retrieval augmentation for solving these harder instances."
    },
    {
        "paperId": "288338c1ec81b259af5587677fdd10d5191baf71",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17226, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-27",
        "authors": [
            {
                "authorId": "2284816193",
                "name": "Xiaolong Wang"
            },
            {
                "authorId": "2279098701",
                "name": "Yile Wang"
            },
            {
                "authorId": "2186280840",
                "name": "Yuan Zhang"
            },
            {
                "authorId": "2238952295",
                "name": "Fuwen Luo"
            },
            {
                "authorId": "144326610",
                "name": "Peng Li"
            },
            {
                "authorId": "2257187607",
                "name": "Maosong Sun"
            },
            {
                "authorId": "2284726267",
                "name": "Yang Liu"
            }
        ],
        "abstract": "Large Language Models (LLMs) have achieved remarkable performance in objective tasks such as open-domain question answering and mathematical reasoning, which can often be solved through recalling learned factual knowledge or chain-of-thought style reasoning. However, we find that the performance of LLMs in subjective tasks is still unsatisfactory, such as metaphor recognition, dark humor detection, etc. Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted reasoning pathway. Based on the characteristics of the tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC (Reasoning in Conversation), a method that focuses on solving subjective tasks through dialogue simulation. The motivation of RiC is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential useful knowledge behind dialogues for giving the final answers. We evaluate both API-based and open-source LLMs including GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental results show that RiC can yield significant improvement compared with various baselines."
    },
    {
        "paperId": "1d83d2512bd9c7ceb8de1fa25ac7b8c4c00b5573",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17302, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-27",
        "authors": [
            {
                "authorId": "9358635",
                "name": "Rifki Afina Putri"
            },
            {
                "authorId": "2287841290",
                "name": "Faiz Ghifari Haznitrama"
            },
            {
                "authorId": "2210201739",
                "name": "Dea Adhista"
            },
            {
                "authorId": "2283768962",
                "name": "Alice Oh"
            }
        ],
        "abstract": "Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators, resulting in 4.5K questions per language (9K in total), making our dataset the largest of its kind. Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally \u2018deep\u2019 as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages."
    },
    {
        "paperId": "9048cb9c7108cfabc4f64b0a11e0ad9e62c84f99",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2402.17433",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17433, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-27",
        "authors": [
            {
                "authorId": "2258760063",
                "name": "Jiaqi Wang"
            },
            {
                "authorId": "2216479666",
                "name": "Zhenxi Song"
            },
            {
                "authorId": "2287797380",
                "name": "Zhengyu Ma"
            },
            {
                "authorId": "2287910839",
                "name": "Xipeng Qiu"
            },
            {
                "authorId": "2279744105",
                "name": "Min Zhang"
            },
            {
                "authorId": "2216439414",
                "name": "Zhiguo Zhang"
            }
        ],
        "abstract": "Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modules alongside the EEG stream from CET-MAE and further enables an LLM (specifically BART) to decode text from EEG sequences. Comprehensive experiments conducted on the popular text-evoked EEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms the state-of-the-art in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%, respectively. These results indicate significant advancements in the field and underscores the proposed framework's potential to enable more powerful and widespread BCI applications."
    },
    {
        "paperId": "099621bec723c5b3435e9be6a4f70e4c871f3f2f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-27",
        "authors": [
            {
                "authorId": "1708171825",
                "name": "Ruiyang Ren"
            },
            {
                "authorId": "2287833740",
                "name": "Peng Qiu"
            },
            {
                "authorId": "51281403",
                "name": "Yingqi Qu"
            },
            {
                "authorId": "2287961461",
                "name": "Jing Liu"
            },
            {
                "authorId": "2257376413",
                "name": "Wayne Xin Zhao"
            },
            {
                "authorId": "46477059",
                "name": "Huaqin Wu"
            },
            {
                "authorId": "2274218622",
                "name": "Ji-Rong Wen"
            },
            {
                "authorId": "2238917361",
                "name": "Haifeng Wang"
            }
        ],
        "abstract": "Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation. Considering the scarcity and limit (e.g., privacy issues) of real user data, in this paper, we conduct large-scale user simulation for web search, to improve the analysis and modeling of user search behavior. Specially, we propose BASES, a novel user simulation framework with LLM-based agents, designed to facilitate comprehensive simulations of web search user behaviors. Our simulation framework can generate unique user profiles at scale, which subsequently leads to diverse search behaviors. To demonstrate the effectiveness of BASES, we conduct evaluation experiments based on two human benchmarks in both Chinese and English, demonstrating that BASES can effectively simulate large-scale human-like search behaviors. To further accommodate the research on web search, we develop WARRIORS, a new large-scale dataset encompassing web search user behaviors, including both Chinese and English versions, which can greatly bolster research in the field of information retrieval. Our code and data will be publicly released soon."
    },
    {
        "paperId": "789485978d69e248832df358ee0fb062012925b8",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17574, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-27",
        "authors": [
            {
                "authorId": "2135282890",
                "name": "Wenqi Zhang"
            },
            {
                "authorId": "2287757233",
                "name": "Ke Tang"
            },
            {
                "authorId": "2287760920",
                "name": "Hai Wu"
            },
            {
                "authorId": "2287809102",
                "name": "Mengna Wang"
            },
            {
                "authorId": "1471660296",
                "name": "Yongliang Shen"
            },
            {
                "authorId": "2273676684",
                "name": "Guiyang Hou"
            },
            {
                "authorId": "2092670555",
                "name": "Zeqi Tan"
            },
            {
                "authorId": "2288185017",
                "name": "Peng Li"
            },
            {
                "authorId": "2056432541",
                "name": "Y. Zhuang"
            },
            {
                "authorId": "1776903",
                "name": "Weiming Lu"
            }
        ],
        "abstract": "Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications."
    },
    {
        "paperId": "292d5013b1642eb7245f3d653695c4d31f4f3aa5",
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "title": "Beyond Prompt Brittleness: Evaluating the Reliability and Consistency of Political Worldviews in LLMs",
        "openAccessPdf": {
            "url": "https://doi.org/10.1162/tacl_a_00710",
            "status": "GOLD",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17649, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-27",
        "authors": [
            {
                "authorId": "2124422440",
                "name": "Tanise Ceron"
            },
            {
                "authorId": "2298039452",
                "name": "Neele Falk"
            },
            {
                "authorId": "2282139828",
                "name": "Ana Bari'c"
            },
            {
                "authorId": "2256988247",
                "name": "Dmitry Nikolaev"
            },
            {
                "authorId": "2287831796",
                "name": "Sebastian Pad'o"
            }
        ],
        "abstract": "Abstract Due to the widespread use of large language models (LLMs), we need to understand whether they embed a specific \u201cworldview\u201d and what these views reflect. Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings (Feng et al., 2023; Motoki et al., 2024). However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning. We propose a series of tests which assess the reliability and consistency of LLMs\u2019 stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy issues. We study LLMs ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count. Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They show a (left-wing) positive stance towards environment protection, social welfare state, and liberal society but also (right-wing) law and order, with no consistent preferences in the areas of foreign policy and migration."
    },
    {
        "paperId": "0bf3a1867f7245b8a702093901c66b08b518eafc",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Evaluating Very Long-Term Conversational Memory of LLM Agents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17753, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-27",
        "authors": [
            {
                "authorId": "8785371",
                "name": "Adyasha Maharana"
            },
            {
                "authorId": "2266803131",
                "name": "Dong-Ho Lee"
            },
            {
                "authorId": "145582202",
                "name": "S. Tulyakov"
            },
            {
                "authorId": "2285969697",
                "name": "Mohit Bansal"
            },
            {
                "authorId": "2266751000",
                "name": "Francesco Barbieri"
            },
            {
                "authorId": "2267220081",
                "name": "Yuwei Fang"
            }
        ],
        "abstract": "Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance."
    },
    {
        "paperId": "3c7da712ed58b8e5223577a3ea734171cdc3cb30",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Adversarial Math Word Problem Generation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17916, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-27",
        "authors": [
            {
                "authorId": "2287928453",
                "name": "Roy Xie"
            },
            {
                "authorId": "2288011699",
                "name": "Chengxuan Huang"
            },
            {
                "authorId": "2288035346",
                "name": "Junlin Wang"
            },
            {
                "authorId": "2342561701",
                "name": "Bhuwan Dhingra"
            }
        ],
        "abstract": "Large language models (LLMs) have significantly transformed the educational landscape. As current plagiarism detection tools struggle to keep pace with LLMs' rapid advancements, the educational community faces the challenge of assessing students' true problem-solving abilities in the presence of LLMs. In this work, we explore a new paradigm for ensuring fair evaluation -- generating adversarial examples which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by LLMs. Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems. We conduct experiments on various open- and closed-source LLMs, quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability. We identify shared vulnerabilities among LLMs and propose a cost-effective approach to attack high-cost models. Additionally, we conduct automatic analysis to investigate the cause of failure, providing further insights into the limitations of LLMs."
    },
    {
        "paperId": "7ce95b1c083748c516c5ec7b852e8c79769fb4fa",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "An Iterative Associative Memory Model for Empathetic Response Generation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17959, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-28",
        "authors": [
            {
                "authorId": "2111736594",
                "name": "Zhou Yang"
            },
            {
                "authorId": "2274220185",
                "name": "Zhaochun Ren"
            },
            {
                "authorId": "2273825374",
                "name": "Yufeng Wang"
            },
            {
                "authorId": "2288039012",
                "name": "Chao Chen"
            },
            {
                "authorId": "2284754087",
                "name": "Haizhou Sun"
            },
            {
                "authorId": "2274054094",
                "name": "Xiaofei Zhu"
            },
            {
                "authorId": "2271907316",
                "name": "Xiangwen Liao"
            }
        ],
        "abstract": "Empathetic response generation aims to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them. To address this issue, we propose an Iterative Associative Memory Model (IAMM) for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances. We conduct experiments on the Empathetic-Dialogue dataset. Both automatic and human evaluations validate the efficacy of the model. Variant experiments on LLMs also demonstrate that attending to associated words improves empathetic comprehension and expression."
    },
    {
        "paperId": "b2991a4b2ecc9db0fbd9ca738022801b4e5ee001",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "CogBench: a large language model walks into a psychology lab",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.18225, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-28",
        "authors": [
            {
                "authorId": "2215168951",
                "name": "Julian Coda-Forno"
            },
            {
                "authorId": "32354733",
                "name": "Marcel Binz"
            },
            {
                "authorId": "2288037634",
                "name": "Jane X. Wang"
            },
            {
                "authorId": "2271334207",
                "name": "Eric Schulz"
            }
        ],
        "abstract": "Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors."
    },
    {
        "paperId": "3bdd3d56ef9054aba47f83879b531a4842640295",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Learning or Self-aligning? Rethinking Instruction Fine-tuning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.18243, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-28",
        "authors": [
            {
                "authorId": "2280137459",
                "name": "Mengjie Ren"
            },
            {
                "authorId": "2113252896",
                "name": "Boxi Cao"
            },
            {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
            },
            {
                "authorId": "2288206462",
                "name": "Liu Cao"
            },
            {
                "authorId": "2118233348",
                "name": "Xianpei Han"
            },
            {
                "authorId": "2303402257",
                "name": "Ke Zeng"
            },
            {
                "authorId": "2249757099",
                "name": "Guanglu Wan"
            },
            {
                "authorId": "2290035990",
                "name": "Xunliang Cai"
            },
            {
                "authorId": "2110832778",
                "name": "Le Sun"
            }
        ],
        "abstract": "Instruction Fine-tuning~(IFT) is a critical phase in building large language models~(LLMs). Previous works mainly focus on the IFT's role in the transfer of behavioral norms and the learning of additional world knowledge. However, the understanding of the underlying mechanisms of IFT remains significantly limited. In this paper, we design a knowledge intervention framework to decouple the potential underlying factors of IFT, thereby enabling individual analysis of different factors. Surprisingly, our experiments reveal that attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects. Further, we discover that maintaining internal knowledge consistency before and after IFT is a critical factor for achieving successful IFT. Our findings reveal the underlying mechanisms of IFT and provide robust support for some very recent and potential future works."
    },
    {
        "paperId": "bef263083bf5ea965c37b152bc5f0b43aaf74824",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.18439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-28",
        "authors": [
            {
                "authorId": "2109136284",
                "name": "Weize Chen"
            },
            {
                "authorId": "2232928180",
                "name": "Chenfei Yuan"
            },
            {
                "authorId": "2288556899",
                "name": "Jiarui Yuan"
            },
            {
                "authorId": "48576745",
                "name": "Yusheng Su"
            },
            {
                "authorId": "2214580084",
                "name": "Cheng Qian"
            },
            {
                "authorId": "2257052321",
                "name": "Cheng Yang"
            },
            {
                "authorId": "2257007994",
                "name": "Ruobing Xie"
            },
            {
                "authorId": "2141313179",
                "name": "Zhiyuan Liu"
            },
            {
                "authorId": "2273551430",
                "name": "Maosong Sun"
            }
        ],
        "abstract": "Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs. Intriguingly, the structured communication format decided by LLMs exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication. Our code is released at \\url{https://github.com/thunlp/AutoForm}."
    },
    {
        "paperId": "b620dae5446609b52e0f6d6f10dcb8cf0892bb1d",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Meta-Task Prompting Elicits Embeddings from Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2402.18458",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.18458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-28",
        "authors": [
            {
                "authorId": "2166060981",
                "name": "Yibin Lei"
            },
            {
                "authorId": "2288196416",
                "name": "Di Wu"
            },
            {
                "authorId": "2289635484",
                "name": "Tianyi Zhou"
            },
            {
                "authorId": "2279548827",
                "name": "Tao Shen"
            },
            {
                "authorId": "2288044458",
                "name": "Yu Cao"
            },
            {
                "authorId": "2287928517",
                "name": "Chongyang Tao"
            },
            {
                "authorId": "2287929110",
                "name": "Andrew Yates"
            }
        ],
        "abstract": "We introduce a new unsupervised text embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning. Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks are versatile embeddings that yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law, offering a versatile and resource-efficient approach for embedding generation across diverse scenarios."
    },
    {
        "paperId": "7e388155314475226bd001b84d8a9a69f35021a7",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Language Models Represent Beliefs of Self and Others",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.18496, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-28",
        "authors": [
            {
                "authorId": "2256680799",
                "name": "Wentao Zhu"
            },
            {
                "authorId": "2288039488",
                "name": "Zhining Zhang"
            },
            {
                "authorId": "2256800070",
                "name": "Yizhou Wang"
            }
        ],
        "abstract": "Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations."
    },
    {
        "paperId": "e53aa81923958e8e247bb9d09250b8ccf0848513",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.18571, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-28",
        "authors": [
            {
                "authorId": "2266194421",
                "name": "Haoxiang Wang"
            },
            {
                "authorId": "2238123947",
                "name": "Yong Lin"
            },
            {
                "authorId": "2275119437",
                "name": "Wei Xiong"
            },
            {
                "authorId": "2289687537",
                "name": "Rui Yang"
            },
            {
                "authorId": "50826757",
                "name": "Shizhe Diao"
            },
            {
                "authorId": "2284239202",
                "name": "Shuang Qiu"
            },
            {
                "authorId": "2188204871",
                "name": "Han Zhao"
            },
            {
                "authorId": "2260473361",
                "name": "Tong Zhang"
            }
        ],
        "abstract": "Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance trade-off across various reward objectives. In comparison with the scalar-reward RLHF, DPA offers users intuitive control over LLM generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity). We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B. Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO)."
    },
    {
        "paperId": "1ab1d9ca27f31fab7c00e80ba1bf9b28ce75ca6e",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Teaching Large Language Models an Unseen Language on the Fly",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.19167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-29",
        "authors": [
            {
                "authorId": "2111574159",
                "name": "Chen Zhang"
            },
            {
                "authorId": "2288030514",
                "name": "Xiao Liu"
            },
            {
                "authorId": "2218883170",
                "name": "Jiuheng Lin"
            },
            {
                "authorId": "2273532365",
                "name": "Yansong Feng"
            }
        ],
        "abstract": "Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones, for which there is minimal training data available for effective parameter updating. We thus investigate whether LLMs can learn a new language on the fly solely through prompting. To study this question, we collect a research suite for Zhuang, a language supported by no LLMs currently. We introduce DiPMT++, a framework for adapting LLMs to unseen languages by in-context learning. Using a dictionary and 5K parallel sentences only, DiPMT++ significantly enhances the performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32 BLEU for Zhuang-to-Chinese translation. We also validate the effectiveness of our framework on Kalamang, another unseen language. Furthermore, we demonstrate the practical utility of DiPMT++ in aiding humans in translating completely unseen languages, which could contribute to the preservation of linguistic diversity."
    },
    {
        "paperId": "bed35010543191bf57a09a6058e75332702d7afa",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.19255, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-29",
        "authors": [
            {
                "authorId": "2264516593",
                "name": "Qintong Li"
            },
            {
                "authorId": "2279792419",
                "name": "Leyang Cui"
            },
            {
                "authorId": "2260841029",
                "name": "Xueliang Zhao"
            },
            {
                "authorId": "2260528279",
                "name": "Lingpeng Kong"
            },
            {
                "authorId": "2283842274",
                "name": "Wei Bi"
            }
        ],
        "abstract": "Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (GSM-Plus) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result."
    },
    {
        "paperId": "030b888709dc14ccd6f6b8151757066e9c33dc7f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.00126, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-29",
        "authors": [
            {
                "authorId": "2108158210",
                "name": "Xiaoqiang Wang"
            },
            {
                "authorId": "2290032240",
                "name": "Bang Liu"
            },
            {
                "authorId": "2289796510",
                "name": "Lingfei Wu"
            }
        ],
        "abstract": "Large language models (LLMs) are primarily evaluated by overall performance on various text understanding and generation tasks. However, such a paradigm fails to comprehensively differentiate the fine-grained language and cognitive skills, rendering the lack of sufficient interpretation to LLMs\u2019 capabilities. In this paper, we present FAC^2E, a framework for Fine-grAined and Cognition-grounded LLMs\u2019 Capability Evaluation. Specifically, we formulate LLMs\u2019 evaluation in a multi-dimensional and explainable manner by dissociating the language-related capabilities and the cognition-related ones. Besides, through extracting the intermediate reasoning from LLMs, we further break down the process of applying a specific capability into three sub-steps: recalling relevant knowledge, utilizing knowledge, and solving problems. Finally, FAC^2E evaluates each sub-step of each fine-grained capability, providing a two-faceted diagnosis for LLMs. Utilizing FAC^2E, we identify a common shortfall in knowledge utilization among models and propose a straightforward, knowledge-enhanced method to mitigate this issue. Our results not only showcase promising performance enhancements but also highlight a direction for future LLM advancements."
    },
    {
        "paperId": "4fbe526e33975f117bc9cdfa01dbe31cfd990f26",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.00462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-01",
        "authors": [
            {
                "authorId": "2289609689",
                "name": "Joe Stacey"
            },
            {
                "authorId": "2284870521",
                "name": "Jianpeng Cheng"
            },
            {
                "authorId": "3457287",
                "name": "John Torr"
            },
            {
                "authorId": "2230366180",
                "name": "Tristan Guigue"
            },
            {
                "authorId": "35149056",
                "name": "Joris Driesen"
            },
            {
                "authorId": "2284760920",
                "name": "Alexandru Coca"
            },
            {
                "authorId": "2289610119",
                "name": "Mark Gaynor"
            },
            {
                "authorId": "2284760811",
                "name": "Anders Johannsen"
            }
        ],
        "abstract": "Spurred by recent advances in Large Language Models (LLMs), virtual assistants are poised to take a leap forward in terms of their dialogue capabilities. Yet a major bottleneck to achieving genuinely transformative task-oriented dialogue capabilities remains the scarcity of high quality data. Existing datasets, while impressive in scale, have limited domain coverage and contain few genuinely challenging conversational phenomena; those which are present are typically unlabelled, making it difficult to assess the strengths and weaknesses of models without time-consuming and costly human evaluation. Moreover, creating high quality dialogue data has until now required considerable human input, limiting both the scale of these datasets and the ability to rapidly bootstrap data for a new target domain. We aim to overcome these issues with LUCID, a modularised and highly automated LLM-driven data generation system that produces realistic, diverse and challenging dialogues. We use LUCID to generate a seed dataset of 4,277 conversations across 100 intents to demonstrate its capabilities, with a human review finding consistently high quality labels in the generated data."
    },
    {
        "paperId": "7b76201b2c4730aba93c845d335247e36d1d9577",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.00499, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-01",
        "authors": [
            {
                "authorId": "2289609556",
                "name": "Ariel Goldstein"
            },
            {
                "authorId": "2126417012",
                "name": "Gabriel Stanovsky"
            }
        ],
        "abstract": "Recent advances in LLMs have sparked a debate on whether they understand text. In this position paper, we argue that opponents in this debate hold different definitions for understanding, and particularly differ in their view on the role of consciousness. To substantiate this claim, we propose a thought experiment involving an open-source chatbot $Z$ which excels on every possible benchmark, seemingly without subjective experience. We ask whether $Z$ is capable of understanding, and show that different schools of thought within seminal AI research seem to answer this question differently, uncovering their terminological disagreement. Moving forward, we propose two distinct working definitions for understanding which explicitly acknowledge the question of consciousness, and draw connections with a rich literature in philosophy, psychology and neuroscience."
    },
    {
        "paperId": "3abd61d29c47949b2445b9eed08512500ebb6380",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Cognitive Bias in Decision-Making with LLMs",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2403.00811",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.00811, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-25",
        "authors": [
            {
                "authorId": "80743661",
                "name": "J. Echterhoff"
            },
            {
                "authorId": "2261358086",
                "name": "Yao Liu"
            },
            {
                "authorId": "2214808732",
                "name": "Abeer Alessa"
            },
            {
                "authorId": "2258962117",
                "name": "Julian McAuley"
            },
            {
                "authorId": "2116458151",
                "name": "Zexue He"
            }
        ],
        "abstract": "Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. Given their training on human (created) data, LLMs have been shown to inherit societal biases against protected groups, as well as be subject to bias functionally resembling cognitive bias. Human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces BiasBuster, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive science, we develop a dataset containing 13,465 prompts to evaluate LLM decisions on different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, while proposing a novel method utilizing LLMs to debias their own human-like cognitive bias within prompts. Our analysis provides a comprehensive picture of the presence and effects of cognitive bias across commercial and open-source models. We demonstrate that our selfhelp debiasing effectively mitigates model answers that display patterns akin to human cognitive bias without having to manually craft examples for each bias."
    },
    {
        "paperId": "a1849a77644ff411a03833b5aa7a65ff57158c50",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "CLLMs: Consistency Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.00835, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-28",
        "authors": [
            {
                "authorId": "2258963117",
                "name": "Siqi Kou"
            },
            {
                "authorId": "2258334187",
                "name": "Lanxiang Hu"
            },
            {
                "authorId": "2116778591",
                "name": "Zhe He"
            },
            {
                "authorId": "2260296481",
                "name": "Zhijie Deng"
            },
            {
                "authorId": "2289837431",
                "name": "Hao Zhang"
            }
        ],
        "abstract": "Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\\times$ to 3.4$\\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks."
    },
    {
        "paperId": "96b6592a0a4257b9cf093a44140fec7916a09c06",
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "title": "Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers",
        "openAccessPdf": {
            "url": "https://doi.org/10.1162/tacl_a_00702",
            "status": "GOLD",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.01061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-02",
        "authors": [
            {
                "authorId": "2065894334",
                "name": "Melanie Subbiah"
            },
            {
                "authorId": "2289860092",
                "name": "Sean Zhang"
            },
            {
                "authorId": "1912259",
                "name": "Lydia B. Chilton"
            },
            {
                "authorId": "2284767355",
                "name": "Kathleen McKeown"
            }
        ],
        "abstract": "Abstract We evaluate recent Large Language Models (LLMs) on the challenging task of summarizing short stories, which can be lengthy, and include nuanced subtext or scrambled timelines. Importantly, we work directly with authors to ensure that the stories have not been shared online (and therefore are unseen by the models), and to obtain informed evaluations of summary quality using judgments from the authors themselves. Through quantitative and qualitative analysis grounded in narrative theory, we compare GPT-4, Claude-2.1, and LLama-2-70B. We find that all three models make faithfulness mistakes in over 50% of summaries and struggle with specificity and interpretation of difficult subtext. We additionally demonstrate that LLM ratings and other automatic metrics for summary quality do not correlate well with the quality ratings from the writers."
    },
    {
        "paperId": "29a56e1c377ac6a4457656b57ef7631ed2bdb509",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "LLMCRIT: Teaching Large Language Models to Use Criteria",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.01069, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-02",
        "authors": [
            {
                "authorId": "30300197",
                "name": "Weizhe Yuan"
            },
            {
                "authorId": "2285740821",
                "name": "Pengfei Liu"
            },
            {
                "authorId": "2304322010",
                "name": "Matthias Gall'e"
            }
        ],
        "abstract": "Humans follow criteria when they execute tasks, and these criteria are directly used to assess the quality of task completion. Therefore, having models learn to use criteria to provide feedback can help humans or models to perform tasks better. However, existing research in this field tends to consider only a limited set of criteria or quality assessment aspects. To fill this gap, we propose a general framework that enables large language models (LLMs) to use comprehensive criteria for a task in delivering natural language feedback on task execution. In particular, we present a model-in-the-loop framework that semi-automatically derives criteria from collected guidelines for different writing tasks and constructs in-context demonstrations for each criterion. We choose three tasks from real-world scenarios to operationalize this idea: paper introduction writing, Python code writing, and Reddit post writing, and evaluate our feedback generation framework using different LLMs. The results reveal the fine-grained effects of incorporating criteria and demonstrations and provide valuable insights on how to teach LLMs to use criteria more effectively."
    },
    {
        "paperId": "1a22ad92a06422d4580effd7cc2778e38d9f7368",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Distilling Text Style Transfer With Self-Explanation From LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.01106, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-02",
        "authors": [
            {
                "authorId": "50445559",
                "name": "Chiyu Zhang"
            },
            {
                "authorId": "2275868595",
                "name": "Honglong Cai"
            },
            {
                "authorId": "2290047120",
                "name": "Yuezhang Li"
            },
            {
                "authorId": "2253837058",
                "name": "Yuexin Wu"
            },
            {
                "authorId": "2153400663",
                "name": "Le Hou"
            },
            {
                "authorId": "2065312024",
                "name": "M. Abdul-Mageed"
            }
        ],
        "abstract": "Text Style Transfer (TST) seeks to alter the style of text while retaining its core content. Given the constraints of limited parallel datasets for TST, we propose CoTeX, a framework that leverages large language models (LLMs) alongside chain-of-thought (CoT) prompting to facilitate TST. CoTeX distills the complex rewriting and reasoning capabilities of LLMs into more streamlined models capable of working with both non-parallel and parallel data. Through experimentation across four TST datasets, CoTeX is shown to surpass traditional supervised fine-tuning and knowledge distillation methods, particularly in low-resource settings. We conduct a comprehensive evaluation, comparing CoTeX against current unsupervised, supervised, in-context learning (ICL) techniques, and instruction-tuned LLMs. Furthermore, CoTeX distinguishes itself by offering transparent explanations for its style transfer process."
    },
    {
        "paperId": "1f1093fbd6eb15b2a79e0e39d5e3621f8090fc84",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.01139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-02",
        "authors": [
            {
                "authorId": "2188739803",
                "name": "Oren Sultan"
            },
            {
                "authorId": "1938499056",
                "name": "Yonatan Bitton"
            },
            {
                "authorId": "2047122977",
                "name": "Ron Yosef"
            },
            {
                "authorId": "1805894",
                "name": "Dafna Shahaf"
            }
        ],
        "abstract": "Analogy-making is central to human cognition, allowing us to adapt to novel situations \u2013 an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy.In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs\u2019 and humans\u2019 analogy recognition in binary and multiple-choice settings, and found that humans outperform the best models (\u223c13% gap) after a light supervision. We demonstrate that our silver-set is useful for training models. Lastly, we show challenging distractors confuse LLMs, but not humans. We hope our pipeline will encourage research in this emerging field."
    },
    {
        "paperId": "a27d2f743dab4ae009beec52f2d61e0be885a7bd",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Differentially Private Synthetic Data via Foundation Model APIs 2: Text",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.01749, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-04",
        "authors": [
            {
                "authorId": "150961077",
                "name": "Chulin Xie"
            },
            {
                "authorId": "3354281",
                "name": "Zinan Lin"
            },
            {
                "authorId": "2064251",
                "name": "A. Backurs"
            },
            {
                "authorId": "40528805",
                "name": "Sivakanth Gopi"
            },
            {
                "authorId": "2290010050",
                "name": "Da Yu"
            },
            {
                "authorId": "3058104",
                "name": "Huseyin A. Inan"
            },
            {
                "authorId": "2268494857",
                "name": "Harsha Nori"
            },
            {
                "authorId": "2290240717",
                "name": "Haotian Jiang"
            },
            {
                "authorId": "2290022949",
                "name": "Huishuai Zhang"
            },
            {
                "authorId": "2290017167",
                "name": "Yin Tat Lee"
            },
            {
                "authorId": "2290141407",
                "name": "Bo Li"
            },
            {
                "authorId": "2262709844",
                "name": "Sergey Yekhanin"
            }
        ],
        "abstract": "Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use API access to an LLM and generate DP synthetic text without any model training. We conduct comprehensive experiments on three benchmark datasets. Our results demonstrate that Aug-PE produces DP synthetic text that yields competitive utility with the SOTA DP finetuning baselines. This underscores the feasibility of relying solely on API access of LLMs to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving LLM applications. Our code and data are available at https://github.com/AI-secure/aug-pe."
    },
    {
        "paperId": "a4a8010657ed78663879d45c1c2b6ca2f861351c",
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "title": "Revisiting Meta-evaluation for Grammatical Error Correction",
        "openAccessPdf": {
            "url": "https://doi.org/10.1162/tacl_a_00676",
            "status": "GOLD",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.02674, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-05",
        "authors": [
            {
                "authorId": "2290137499",
                "name": "Masamune Kobayashi"
            },
            {
                "authorId": "35643168",
                "name": "Masato Mita"
            },
            {
                "authorId": "2936411",
                "name": "Mamoru Komachi"
            }
        ],
        "abstract": "Abstract Metrics are the foundation for automatic evaluation in grammatical error correction (GEC), with their evaluation of the metrics (meta-evaluation) relying on their correlation with human judgments. However, conventional meta-evaluations in English GEC encounter several challenges, including biases caused by inconsistencies in evaluation granularity and an outdated setup using classical systems. These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques. To address these issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation. SEEDA consists of corrections with human ratings along two different granularities: edit-based and sentence-based, covering 12 state-of-the-art systems including large language models, and two human corrections with different focuses. The results of improved correlations by aligning the granularity in the sentence-level meta-evaluation suggest that edit-based metrics may have been underestimated in existing studies. Furthermore, correlations of most metrics decrease when changing from classical to neural systems, indicating that traditional metrics are relatively poor at evaluating fluently corrected sentences with many edits."
    },
    {
        "paperId": "e011dc8af89a254c2fed4ca140dff1c32133c647",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated Answers",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2403.02889",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.02889, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-05",
        "authors": [
            {
                "authorId": "2191186315",
                "name": "Yakir Yehuda"
            },
            {
                "authorId": "46252132",
                "name": "Itzik Malkiel"
            },
            {
                "authorId": "48797862",
                "name": "Oren Barkan"
            },
            {
                "authorId": "40389676",
                "name": "Jonathan Weill"
            },
            {
                "authorId": "2445296",
                "name": "Royi Ronen"
            },
            {
                "authorId": "1683070",
                "name": "Noam Koenigstein"
            }
        ],
        "abstract": "Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 87% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy of 81%, all without relying on external knowledge."
    },
    {
        "paperId": "52db5adffa53911c20b7cd884e8a2f2151a3c114",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.03121, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-05",
        "authors": [
            {
                "authorId": "1410406981",
                "name": "F. Plaza-del-Arco"
            },
            {
                "authorId": "3451318",
                "name": "A. C. Curry"
            },
            {
                "authorId": "2188052353",
                "name": "Alba Curry"
            },
            {
                "authorId": "17038002",
                "name": "Gavin Abercrombie"
            },
            {
                "authorId": "2267334203",
                "name": "Dirk Hovy"
            }
        ],
        "abstract": "Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men's anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like 'When I had a serious argument with a dear person'. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes. These findings are in line with established research in psychology and gender studies. Our study sheds light on the complex societal interplay between language, gender, and emotion. The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications."
    },
    {
        "paperId": "4c9ea71ebca90fa902c718a40c19dcef5d6cdfbe",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.03167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-05",
        "authors": [
            {
                "authorId": "2239202052",
                "name": "Arda Uzunouglu"
            },
            {
                "authorId": "2290021048",
                "name": "Abdalfatah Rashid Safa"
            },
            {
                "authorId": "2278439442",
                "name": "G\u00f6zde G\u00fcl Sahin"
            }
        ],
        "abstract": "Recently, there has been growing interest within the community regarding whether large language models are capable of planning or executing plans. However, most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities. These setups constrain evaluation methods (e.g., predefined action space), architectural choices (e.g., only generative models), and overlook the linguistic nuances essential for realistic analysis. To tackle this, we present PARADISE, an abductive reasoning task using Q\\&A format on practical procedural text sourced from wikiHow. It involves warning and tip inference tasks directly associated with goals, excluding intermediary steps, with the aim of testing the ability of the models to infer implicit knowledge of the plan solely from the given goal. Our experiments, utilizing fine-tuned language models and zero-shot prompting, reveal the effectiveness of task-specific small models over large language models in most scenarios. Despite advancements, all models fall short of human performance. Notably, our analysis uncovers intriguing insights, such as variations in model behavior with dropped keywords, struggles of BERT-family and GPT-4 with physical and abstract goals, and the proposed tasks offering valuable prior knowledge for other unseen procedural tasks. The PARADISE dataset and associated resources are publicly available for further research exploration with https://github.com/GGLAB-KU/paradise."
    },
    {
        "paperId": "5e206bc2e276f43a530fd593fdb208a8c76afad7",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.03194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-05",
        "authors": [
            {
                "authorId": "2290018018",
                "name": "Hossein Aboutalebi"
            },
            {
                "authorId": "2260612278",
                "name": "Hwanjun Song"
            },
            {
                "authorId": "2290210451",
                "name": "Yusheng Xie"
            },
            {
                "authorId": "144877669",
                "name": "Arshit Gupta"
            },
            {
                "authorId": "2290066632",
                "name": "Justin Sun"
            },
            {
                "authorId": "2260901186",
                "name": "Hang Su"
            },
            {
                "authorId": "2260409785",
                "name": "Igor Shalyminov"
            },
            {
                "authorId": "2283784519",
                "name": "Nikolaos Pappas"
            },
            {
                "authorId": "2260441495",
                "name": "Siffi Singh"
            },
            {
                "authorId": "39674628",
                "name": "Saab Mansour"
            }
        ],
        "abstract": "Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce Multimodal Augmented Generative Images Dialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images . Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialogue datasets, using automated and human evaluation. Our results show that MAGID is comparable to or better than baselines, with significant improvements in human evaluation, especially against retrieval baselines where the image database is small."
    },
    {
        "paperId": "97f05d277bac590cdcfe512b8adf647bc958d966",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Negating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.03419, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-06",
        "authors": [
            {
                "authorId": "2258959896",
                "name": "Shitong Duan"
            },
            {
                "authorId": "2258961742",
                "name": "Xiaoyuan Yi"
            },
            {
                "authorId": "2290178596",
                "name": "Peng Zhang"
            },
            {
                "authorId": "2324955244",
                "name": "Yan Liu"
            },
            {
                "authorId": "2323716787",
                "name": "Zheng Liu"
            },
            {
                "authorId": "1711569",
                "name": "T. Lu"
            },
            {
                "authorId": "2289847313",
                "name": "Xing Xie"
            },
            {
                "authorId": "2258958923",
                "name": "Ning Gu"
            }
        ],
        "abstract": "Large language models (LLMs) have revolutionized the role of AI, yet pose potential social risks. To steer LLMs towards human preference, alignment technologies have been introduced and gained increasing attention. Nevertheless, existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy positive responses that are barely distinguishable from negative ones. Given recent LLMs' proficiency in generating helpful responses, this work pivots towards a new research question: can we achieve alignment using solely human-annotated negative samples, preserving helpfulness while reducing harmfulness? For this purpose, we propose Distributional Dispreference Optimization (D$^2$O), which maximizes the discrepancy between dispreferred responses and the generated non-negative ones. In this way, D$^2$O effectively eschews harmful information without incorporating noisy positive samples, while avoiding collapse using self-generated responses as anchors. We demonstrate that D$^2$O can be regarded as learning a distributional preference model reflecting human dispreference against negative responses, which is theoretically an upper bound of the instance-level DPO. Extensive experiments manifest that our method achieves comparable generation quality and surpasses the latest strong baselines in producing less harmful and more informative responses with better training stability and faster convergence."
    },
    {
        "paperId": "bb5d07be56b33346e11386a23bbfedfa996c8619",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.03744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-06",
        "authors": [
            {
                "authorId": "2154279258",
                "name": "Tessa Han"
            },
            {
                "authorId": "2344642721",
                "name": "Aounon Kumar"
            },
            {
                "authorId": "40228633",
                "name": "Chirag Agarwal"
            },
            {
                "authorId": "1892673",
                "name": "Himabindu Lakkaraju"
            }
        ],
        "abstract": "As large language models (LLMs) develop increasingly sophisticated capabilities and find applications in medical settings, it becomes important to assess their medical safety due to their far-reaching implications for personal and public health, patient safety, and human rights. However, there is little to no understanding of the notion of medical safety in the context of LLMs, let alone how to evaluate and improve it. To address this gap, we first define the notion of medical safety in LLMs based on the Principles of Medical Ethics set forth by the American Medical Association. We then leverage this understanding to introduce MedSafetyBench, the first benchmark dataset designed to measure the medical safety of LLMs. We demonstrate the utility of MedSafetyBench by using it to evaluate and improve the medical safety of LLMs. Our results show that publicly-available medical LLMs do not meet standards of medical safety and that fine-tuning them using MedSafetyBench improves their medical safety while preserving their medical performance. By introducing this new benchmark dataset, our work enables a systematic study of the state of medical safety in LLMs and motivates future work in this area, paving the way to mitigate the safety risks of LLMs in medicine. The benchmark dataset and code are available at https://github.com/AI4LIFE-GROUP/med-safety-bench."
    },
    {
        "paperId": "87d24f44d4e881e51e8b1c8f3f0f7ba53fc53aef",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.03866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-06",
        "authors": [
            {
                "authorId": "2159829626",
                "name": "Fangyuan Xu"
            },
            {
                "authorId": "46258841",
                "name": "Kyle Lo"
            },
            {
                "authorId": "3328733",
                "name": "Luca Soldaini"
            },
            {
                "authorId": "2003338023",
                "name": "Bailey Kuehl"
            },
            {
                "authorId": "2257003422",
                "name": "Eunsol Choi"
            },
            {
                "authorId": "30051202",
                "name": "David Wadden"
            }
        ],
        "abstract": "Large language models (LLMs) adapted to follow user instructions are now widely deployed as conversational agents. In this work, we examine one increasingly common instruction-following task: providing writing assistance to compose a long-form answer. To evaluate the capabilities of current LLMs on this task, we construct KIWI, a dataset of knowledge-intensive writing instructions in the scientific domain. Given a research question, an initial model-generated answer and a set of relevant papers, an expert annotator iteratively issues instructions for the model to revise and improve its answer. We collect 1,260 interaction turns from 234 interaction sessions with three state-of-the-art LLMs. Each turn includes a user instruction, a model response, and a human evaluation of the model response. Through a detailed analysis of the collected responses, we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits. Further, we find that models struggle to judge whether their outputs successfully followed user instructions, with accuracy at least 10 points short of human agreement. Our findings indicate that KIWI will be a valuable resource to measure progress and improve LLMs' instruction-following capabilities for knowledge intensive writing tasks."
    },
    {
        "paperId": "366441034ec03b2fd72e29c246c49389a50b8ad8",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Online Adaptation of Language Models with a Memory of Amortized Contexts",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.04317, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-07",
        "authors": [
            {
                "authorId": "1750599181",
                "name": "Jihoon Tack"
            },
            {
                "authorId": "2116671354",
                "name": "Jaehyung Kim"
            },
            {
                "authorId": "2290187088",
                "name": "Eric Mitchell"
            },
            {
                "authorId": "2261688831",
                "name": "Jinwoo Shin"
            },
            {
                "authorId": "2290184641",
                "name": "Yee Whye Teh"
            },
            {
                "authorId": "2290185444",
                "name": "Jonathan Richard Schwarz"
            }
        ],
        "abstract": "Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs. To address the crucial need to keep models updated, online learning has emerged as a critical tool when utilizing LLMs for real-world applications. However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention. We propose a feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient manner, we utilize amortization-based meta-learning, which substitutes an otherwise required optimization process with a single forward pass of the encoder. Subsequently, we learn to choose from and aggregate selected documents into a single modulation by conditioning on the question, allowing us to adapt a frozen language model during test time without requiring further gradient updates. Our experiment demonstrates the superiority of MAC in multiple aspects, including online adaptation performance, time, and memory efficiency. In addition, we show how MAC can be combined with and improve the performance of popular alternatives such as retrieval augmented generations (RAGs). Code is available at: https://github.com/jihoontack/MAC."
    },
    {
        "paperId": "ae4635297ad87fcb3ec4105a51b5cbcb4075e5e2",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.04746, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-07",
        "authors": [
            {
                "authorId": "7425689",
                "name": "Boshi Wang"
            },
            {
                "authorId": "2290254315",
                "name": "Hao Fang"
            },
            {
                "authorId": "2276608164",
                "name": "Jason Eisner"
            },
            {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
            },
            {
                "authorId": "1758652",
                "name": "Yu Su"
            }
        ],
        "abstract": "Tools are essential for large language models (LLMs) to acquire up-to-date information and take consequential actions in external environments. Existing work on tool-augmented LLMs primarily focuses on the broad coverage of tools and the flexibility of adding new tools. However, a critical aspect that has surprisingly been understudied is simply how accurately an LLM uses tools for which it has been trained. We find that existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice. We propose a biologically inspired method for tool-augmented LLMs, simulated trial and error (STE), that orchestrates three key mechanisms for successful tool use behaviors in the biological system: trial and error, imagination, and memory. Specifically, STE leverages an LLM's 'imagination' to simulate plausible scenarios for using a tool, after which the LLM interacts with the tool to learn from its execution feedback. Both short-term and long-term memory are employed to improve the depth and breadth of the exploration, respectively. Comprehensive experiments on ToolBench show that STE substantially improves tool learning for LLMs under both in-context learning and fine-tuning settings, bringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform GPT-4. We also show effective continual learning of tools via a simple experience replay strategy."
    },
    {
        "paperId": "275b005c33a315ad603f236cd5766efe07ef6a54",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.04797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-05",
        "authors": [
            {
                "authorId": "2109338656",
                "name": "Zhenyu (Allen) Zhang"
            },
            {
                "authorId": "2284222685",
                "name": "Runjin Chen"
            },
            {
                "authorId": "2255081092",
                "name": "Shiwei Liu"
            },
            {
                "authorId": "2262242352",
                "name": "Zhewei Yao"
            },
            {
                "authorId": "2537545",
                "name": "Olatunji Ruwase"
            },
            {
                "authorId": "2249538643",
                "name": "Beidi Chen"
            },
            {
                "authorId": "2129511744",
                "name": "Xiaoxia Wu"
            },
            {
                "authorId": "2284563898",
                "name": "Zhangyang Wang"
            }
        ],
        "abstract": "This paper aims to overcome the\"lost-in-the-middle\"challenge of large language models (LLMs). While recent advancements have successfully enabled LLMs to perform stable language modeling with up to 4 million tokens, the persistent difficulty faced by most LLMs in identifying relevant information situated in the middle of the context has not been adequately tackled. To address this problem, this paper introduces Multi-scale Positional Encoding (Ms-PoE) which is a simple yet effective plug-and-play approach to enhance the capacity of LLMs to handle the relevant information located in the middle of the context, without fine-tuning or introducing any additional overhead. Ms-PoE leverages the position indice rescaling to relieve the long-term decay effect introduced by RoPE, while meticulously assigning distinct scaling ratios to different attention heads to preserve essential knowledge learned during the pre-training step, forming a multi-scale context fusion from short to long distance. Extensive experiments with a wide range of LLMs demonstrate the efficacy of our approach. Notably, Ms-PoE achieves an average accuracy gain of up to 3.8 on the Zero-SCROLLS benchmark over the original LLMs. Code are available at https://github.com/VITA-Group/Ms-PoE."
    },
    {
        "paperId": "8e36a8f8fd4451d55e927151aabc463830941a8e",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.04814, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-07",
        "authors": [
            {
                "authorId": "32816503",
                "name": "Linyuan Gong"
            },
            {
                "authorId": "2290741825",
                "name": "Sida Wang"
            },
            {
                "authorId": "2278430853",
                "name": "Mostafa Elhoushi"
            },
            {
                "authorId": "2278433480",
                "name": "Alvin Cheung"
            }
        ],
        "abstract": "We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future research in effective pretraining strategies for code LLMs. The evaluation toolkit and dataset are available at https://github.com/gonglinyuan/safim, and the leaderboard is available at https://safimbenchmark.com."
    },
    {
        "paperId": "bfc2aee63d20fb19c9a851da9e97fec40c454124",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.05020, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-08",
        "authors": [
            {
                "authorId": "144101734",
                "name": "Xuhui Zhou"
            },
            {
                "authorId": "2072382595",
                "name": "Zhe Su"
            },
            {
                "authorId": "102487474",
                "name": "Tiwalayo Eisape"
            },
            {
                "authorId": "32609381",
                "name": "Hyunwoo Kim"
            },
            {
                "authorId": "2729164",
                "name": "Maarten Sap"
            }
        ],
        "abstract": "Recent advances in large language models (LLM) have enabled richer social simulations, allowing for the study of various social phenomena. However, most recent work has used a more omniscient perspective on these simulations (e.g., single LLM to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that involve humans and AI agents in the real world. To examine these differences, we develop an evaluation framework to simulate social interactions with LLMs in various settings (omniscient, non-omniscient). Our experiments show that LLMs perform better in unrealistic, omniscient simulation settings but struggle in ones that more accurately reflect real-world conditions with information asymmetry. Moreover, we illustrate the limitations inherent in learning from omniscient simulations. Our findings indicate that addressing information asymmetry remains a fundamental challenge for LLM-based agents."
    },
    {
        "paperId": "31a32cbc7b9ed59666cbeb783505897885916669",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2403.05574",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.05574, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-26",
        "authors": [
            {
                "authorId": "2284762987",
                "name": "Mengxi Xiao"
            },
            {
                "authorId": "2249763955",
                "name": "Qianqian Xie"
            },
            {
                "authorId": "13627871",
                "name": "Zi-Zhou Kuang"
            },
            {
                "authorId": "2294930623",
                "name": "Zhicheng Liu"
            },
            {
                "authorId": "2003396186",
                "name": "Kailai Yang"
            },
            {
                "authorId": "2283923584",
                "name": "Min Peng"
            },
            {
                "authorId": "104843747",
                "name": "Weiguang Han"
            },
            {
                "authorId": "2555230",
                "name": "Jimin Huang"
            }
        ],
        "abstract": "Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the crucial task of cognitive reframing and overcoming challenges such as shame, distrust, therapist skill variability, and resource scarcity. Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients' self-discovery of alternative perspectives. In this paper, we unveil the Helping and Empowering through Adaptive Language in Mental Enhancement (HealMe) model. This novel cognitive reframing therapy method effectively addresses deep-rooted negative thoughts and fosters rational, balanced perspectives. Diverging from traditional LLM methods, HealMe employs empathetic dialogue based on psychotherapeutic frameworks. It systematically guides clients through distinguishing circumstances from feelings, brainstorming alternative viewpoints, and developing empathetic, actionable suggestions. Moreover, we adopt the first comprehensive and expertly crafted psychological evaluation metrics, specifically designed to rigorously assess the performance of cognitive reframing, in both AI-simulated dialogues and real-world therapeutic conversations. Experimental results show that our model outperforms others in terms of empathy, guidance, and logical coherence, demonstrating its effectiveness and potential positive impact on psychotherapy."
    },
    {
        "paperId": "3d6300263adb1a1e8000fd0eda55518a3642afa9",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "FLAP: Flow-Adhering Planning with Constrained Decoding in LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.05766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-09",
        "authors": [
            {
                "authorId": "2291438644",
                "name": "Shamik Roy"
            },
            {
                "authorId": "40552391",
                "name": "Sailik Sengupta"
            },
            {
                "authorId": "3457102",
                "name": "Daniele Bonadiman"
            },
            {
                "authorId": "39674628",
                "name": "Saab Mansour"
            },
            {
                "authorId": "144877669",
                "name": "Arshit Gupta"
            }
        ],
        "abstract": "Planning is a crucial task for agents in task oriented dialogs (TODs). Human agents typically resolve user issues by following predefined workflows, decomposing workflow steps into actionable items, and performing actions by executing APIs in order; all of which require reasoning and planning. With the recent advances in LLMs, there have been increasing attempts to use them for task planning and API usage. However, the faithfulness of the plans to predefined workflows and API dependencies, is not guaranteed with LLMs. Moreover, workflows in real life are often custom-defined and prone to changes; hence, adaptation is desirable. To study this, we propose the problem of faithful planning in TODs that needs to resolve user intents by following predefined flows and preserving API dependencies. To solve this problem, we propose \\textbf{FLAP}, a \\textbf{Fl}ow-\\textbf{A}dhering \\textbf{P}lanning algorithm based on constrained decoding with lookahead heuristic for LLMs. Our algorithm alleviates the need for finetuning LLMs using domain specific (plan/dependency) data, enables quick adaptation to predefined flows, and outperforms other decoding and prompting-based baselines. Further, our algorithm empowers smaller LLMs (\\approx7B) to perform at par larger LLMs (\\approx30B-40B)."
    },
    {
        "paperId": "cb26863dd5ee9e39ecfa4f383e61435756c337af",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "ItD: Large Language Models Can Teach Themselves Induction through Deduction",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.05789, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-09",
        "authors": [
            {
                "authorId": "2266428552",
                "name": "Wangtao Sun"
            },
            {
                "authorId": "2290748445",
                "name": "Haotian Xu"
            },
            {
                "authorId": "2266420795",
                "name": "Xuanqing Yu"
            },
            {
                "authorId": "2290936511",
                "name": "Pei Chen"
            },
            {
                "authorId": "1954845",
                "name": "Shizhu He"
            },
            {
                "authorId": "2257315249",
                "name": "Jun Zhao"
            },
            {
                "authorId": "2283274776",
                "name": "Kang Liu"
            }
        ],
        "abstract": "Although Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt ``post processes'' paradigms to improve the performance of LLMs on induction (e.g., the hypothesis search&refinement methods), but their performance is still constrained by the inherent inductive capability of the LLMs. In this paper, we propose a novel framework, Induction through Deduction (ItD), to enable the LLMs to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the fine-tuning and decoding of LLMs. Our empirical results showcase the effectiveness of ItD on two induction benchmarks, achieving relative performance improvement of 36% and 10% compared with previous state-of-the-art, respectively. Our ablation study verifies the effectiveness of two key modules of ItD. We also verify the effectiveness of ItD across different LLMs and deductors. The data and code of this paper can be found at https://anonymous.4open.science/r/ItD-E844."
    },
    {
        "paperId": "a0e7f328781eda3e3dc919c77bd1eedb99b99612",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "S2IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.05798, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-09",
        "authors": [
            {
                "authorId": "2281612988",
                "name": "Zijie Pan"
            },
            {
                "authorId": "2214140574",
                "name": "Yushan Jiang"
            },
            {
                "authorId": "2257349440",
                "name": "Sahil Garg"
            },
            {
                "authorId": "2257349988",
                "name": "Anderson Schneider"
            },
            {
                "authorId": "2774914",
                "name": "Yuriy Nevmyvaka"
            },
            {
                "authorId": "2276324326",
                "name": "Dongjin Song"
            }
        ],
        "abstract": "Recently, there has been a growing interest in leveraging pre-trained large language models (LLMs) for various time series applications. However, the semantic space of LLMs, established through the pre-training, is still underexplored and may help yield more distinctive and informative representations to facilitate time series forecasting. To this end, we propose Semantic Space Informed Prompt learning with LLM ($S^2$IP-LLM) to align the pre-trained semantic space with time series embeddings space and perform time series forecasting based on learned prompts from the joint space. We first design a tokenization module tailored for cross-modality alignment, which explicitly concatenates patches of decomposed time series components to create embeddings that effectively encode the temporal dynamics. Next, we leverage the pre-trained word token embeddings to derive semantic anchors and align selected anchors with time series embeddings by maximizing the cosine similarity in the joint space. This way, $S^2$IP-LLM can retrieve relevant semantic anchors as prompts to provide strong indicators (context) for time series that exhibit different temporal dynamics. With thorough empirical studies on multiple benchmark datasets, we demonstrate that the proposed $S^2$IP-LLM can achieve superior forecasting performance over state-of-the-art baselines. Furthermore, our ablation studies and visualizations verify the necessity of prompt learning informed by semantic space."
    },
    {
        "paperId": "6da55d9743eaf091750bcbbecaec6cd62ba8824b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.05814, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-09",
        "authors": [
            {
                "authorId": "1754112065",
                "name": "Yerin Hwang"
            },
            {
                "authorId": "2157128089",
                "name": "Yongi-Mi Kim"
            },
            {
                "authorId": "2164385860",
                "name": "Yunah Jang"
            },
            {
                "authorId": "2266475939",
                "name": "Jeesoo Bang"
            },
            {
                "authorId": "2165230479",
                "name": "Hyunkyung Bae"
            },
            {
                "authorId": "2266478688",
                "name": "Kyomin Jung"
            }
        ],
        "abstract": "Despite advancements in on-topic dialogue systems, effectively managing topic shifts within dialogues remains a persistent challenge, largely attributed to the limited availability of training datasets. To address this issue, we propose Multi-Passage to Dialogue (MP2D), a data generation framework that automatically creates conversational question-answering datasets with natural topic transitions. By leveraging the relationships between entities in a knowledge graph, MP2D maps the flow of topics within a dialogue, effectively mirroring the dynamics of human conversation. It retrieves relevant passages corresponding to the topics and transforms them into dialogues through the passage-to-dialogue method. Through quantitative and qualitative experiments, we demonstrate MP2D\u2019s efficacy in generating dialogue with natural topic shifts. Furthermore, this study introduces a novel benchmark for topic shift dialogues, TS-WikiDialog. Utilizing the dataset, we demonstrate that even Large Language Models (LLMs) struggle to handle topic shifts in dialogue effectively, and we showcase the performance improvements of models trained on datasets generated by MP2D across diverse topic shift dialogue tasks."
    },
    {
        "paperId": "6c28575c26b6a58a305650e0224178c92619d452",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Naming, Describing, and Quantifying Visual Objects in Humans and LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.06935, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-11",
        "authors": [
            {
                "authorId": "50829868",
                "name": "A. Testoni"
            },
            {
                "authorId": "2290915834",
                "name": "Juell Sprott"
            },
            {
                "authorId": "2346109854",
                "name": "Sandro Pezzelle"
            }
        ],
        "abstract": "While human speakers use a variety of different expressions when describing the same object in an image, giving rise to a distribution of plausible labels driven by pragmatic constraints, the extent to which current Vision&Language Large Language Models (VLLMs) can mimic this crucial feature of language use is an open question. This applies to common, everyday objects, but it is particularly interesting for uncommon or novel objects for which a category label may be lacking or fuzzy. Furthermore, similar patterns of variation are observed among human speakers for highly context-sensitive expressions, such as the quantifiers 'few' or 'most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on three categories (nouns, attributes, and quantifiers) where humans show great subjective variability concerning the distribution over plausible labels, using datasets and resources mostly under-explored in previous work. Our results reveal mixed evidence on the ability of VLLMs to capture human naming preferences at generation time: while some models are good at mimicking human distributions for nouns and attributes, all of them fail to assign quantifiers, a task that requires more accurate, high-level reasoning."
    },
    {
        "paperId": "023e113b11ff7bac182713a069fedcbcccad9562",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.07183, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-11",
        "authors": [
            {
                "authorId": "151253861",
                "name": "Weixin Liang"
            },
            {
                "authorId": "2290915555",
                "name": "Zachary Izzo"
            },
            {
                "authorId": "2291081046",
                "name": "Yaohui Zhang"
            },
            {
                "authorId": "113931466",
                "name": "Haley Lepp"
            },
            {
                "authorId": "2256613375",
                "name": "Hancheng Cao"
            },
            {
                "authorId": "150345512",
                "name": "Xuandong Zhao"
            },
            {
                "authorId": "2305027534",
                "name": "Lingjiao Chen"
            },
            {
                "authorId": "2284300618",
                "name": "Haotian Ye"
            },
            {
                "authorId": "94035244",
                "name": "Sheng Liu"
            },
            {
                "authorId": "2291016248",
                "name": "Zhi Huang"
            },
            {
                "authorId": "2251233257",
                "name": "Daniel A. McFarland"
            },
            {
                "authorId": "2266445781",
                "name": "James Y. Zou"
            }
        ],
        "abstract": "We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices."
    },
    {
        "paperId": "7709a9eefa9a67510111aef2877f2834a76c8829",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.07230, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-12",
        "authors": [
            {
                "authorId": "2290918445",
                "name": "Pulkit Pattnaik"
            },
            {
                "authorId": "2041015203",
                "name": "Rishabh Maheshwary"
            },
            {
                "authorId": "1452683268",
                "name": "Kelechi Ogueji"
            },
            {
                "authorId": "143618944",
                "name": "Vikas Yadav"
            },
            {
                "authorId": "100519532",
                "name": "Sathwik Tejaswi Madhusudhan"
            }
        ],
        "abstract": "Direct Preference Optimization (DPO) is an effective technique that leverages pairwise preference data (usually one chosen and rejected response pair per user prompt) to align LLMs to human preferences. In practice, multiple responses can exist for a given prompt with varying quality relative to each other. With availability of such quality ratings for multiple responses, we propose utilizing these responses to create multiple preference pairs for a given prompt. Our work focuses on systematically using the constructed multiple preference pair in DPO training via curriculum learning methodology. In particular, we order these multiple pairs of preference data from easy to hard (emulating curriculum training) according to various criteria. We show detailed comparisons of our proposed approach to the standard single-pair DPO setting. Our method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set, highlighting its effectiveness. More specifically, Curry-DPO achieves a score of 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs with similar parameter size. Curry-DPO also achieves the highest adjusted win rates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and 87.9% respectively) in our experiments, with notable gains of upto 7.5% when compared to standard DPO technique. We release the preference pairs used in alignment at: https://huggingface.co/datasets/ServiceNow-AI/Curriculum_DPO_preferences"
    },
    {
        "paperId": "097d26af3bd536537a9a679ca5e0156082e9ebf5",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Can Large Language Models Identify Authorship?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.08213, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-13",
        "authors": [
            {
                "authorId": "51164501",
                "name": "Baixiang Huang"
            },
            {
                "authorId": "2163546329",
                "name": "Canyu Chen"
            },
            {
                "authorId": "2266239316",
                "name": "Kai Shu"
            }
        ],
        "abstract": "The ability to accurately identify authorship is crucial for verifying content authenticity and mitigating misinformation. Large Language Models (LLMs) have demonstrated an exceptional capacity for reasoning and problem-solving. However, their potential in authorship analysis remains under-explored. Traditional studies have depended on hand-crafted stylistic features, whereas state-of-the-art approaches leverage text embeddings from pre-trained language models. These methods, which typically require fine-tuning on labeled data, often suffer from performance degradation in cross-domain applications and provide limited explainability. This work seeks to address three research questions: (1) Can LLMs perform zero-shot, end-to-end authorship verification effectively? (2) Are LLMs capable of accurately attributing authorship among multiple candidates authors (e.g., 10 and 20)? (3) Can LLMs provide explainability in authorship analysis, particularly through the role of linguistic features? Moreover, we investigate the integration of explicit linguistic features to guide LLMs in their reasoning processes. Our assessment demonstrates LLMs' proficiency in both tasks without the need for domain-specific fine-tuning, providing explanations into their decision making via a detailed analysis of linguistic features. This establishes a new benchmark for future research on LLM-based authorship analysis."
    },
    {
        "paperId": "3f5405ffaa526adb43554520147c82e47a0e7d21",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.08312, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-13",
        "authors": [
            {
                "authorId": "2291078805",
                "name": "Jia-Nan Li"
            },
            {
                "authorId": "2071635049",
                "name": "Quan Tu"
            },
            {
                "authorId": "34754580",
                "name": "Cunli Mao"
            },
            {
                "authorId": "2285331933",
                "name": "Zhengtao Yu"
            },
            {
                "authorId": "2263887786",
                "name": "Ji-Rong Wen"
            },
            {
                "authorId": "2277448117",
                "name": "Rui Yan"
            }
        ],
        "abstract": "Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of \\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as ``conversational attention sinks'' (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200K or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200K of utterances, resulting in a prolonged dialogue learning. In order to minimize information losses from reconstruction after compression, we design two learning strategies of short-memory reconstruction (SMR) and long-memory reactivation (LMR). Our method outperforms strong baselines in dialogue tasks and achieves a 4 $\\times$ speedup while reducing memory usage by 18 $\\times$ compared to dense attention recomputation."
    },
    {
        "paperId": "ab8e6df5001dbb9b48445220099425aff536b3e8",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Knowledge Conflicts for LLMs: A Survey",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.08319, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-13",
        "authors": [
            {
                "authorId": "2158524037",
                "name": "Rongwu Xu"
            },
            {
                "authorId": "2257044451",
                "name": "Zehan Qi"
            },
            {
                "authorId": "2681038",
                "name": "Zhijiang Guo"
            },
            {
                "authorId": "35504092",
                "name": "Cunxiang Wang"
            },
            {
                "authorId": "2291083765",
                "name": "Hongru Wang"
            },
            {
                "authorId": "2261496744",
                "name": "Yue Zhang"
            },
            {
                "authorId": "2284475691",
                "name": "Wei Xu"
            }
        ],
        "abstract": "This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area."
    },
    {
        "paperId": "77e3b253c2deeb2661ffcb9fb9f952ceb700c4db",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "SOTOPIA-\u03c0: Interactive Learning of Socially Intelligent Language Agents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.08715, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-13",
        "authors": [
            {
                "authorId": "2266240014",
                "name": "Ruiyi Wang"
            },
            {
                "authorId": "2260283233",
                "name": "Haofei Yu"
            },
            {
                "authorId": "2266360509",
                "name": "W. Zhang"
            },
            {
                "authorId": "2266241076",
                "name": "Zhengyang Qi"
            },
            {
                "authorId": "2729164",
                "name": "Maarten Sap"
            },
            {
                "authorId": "2285194103",
                "name": "Graham Neubig"
            },
            {
                "authorId": "3312309",
                "name": "Yonatan Bisk"
            },
            {
                "authorId": "2260859845",
                "name": "Hao Zhu"
            }
        ],
        "abstract": "Humans learn social skills through both imitation and social interaction. This social learning process is largely understudied by existing research on building language agents. Motivated by this gap, we propose an interactive learning method, SOTOPIA-$\\pi$, improving the social intelligence of language agents. This method leverages behavior cloning and self-reinforcement training on filtered social interaction data according to large language model (LLM) ratings. We show that our training method allows a 7B LLM to reach the social goal completion ability of an expert model (GPT-4-based agent), while improving the safety of language agents and maintaining general QA ability on the MMLU benchmark. We also find that this training paradigm uncovers some difficulties in LLM-based evaluation of social intelligence: LLM-based evaluators overestimate the abilities of the language agents trained specifically for social interaction."
    },
    {
        "paperId": "48c94ddaecbb102ed200e0f08044aa271f9fb199",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Revealing the Parallel Multilingual Learning within Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.09073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-14",
        "authors": [
            {
                "authorId": "2040853265",
                "name": "Yongyu Mu"
            },
            {
                "authorId": "2291134227",
                "name": "Peinan Feng"
            },
            {
                "authorId": "2203807692",
                "name": "Zhiquan Cao"
            },
            {
                "authorId": "2291199377",
                "name": "Yuzhang Wu"
            },
            {
                "authorId": "2291200534",
                "name": "Bei Li"
            },
            {
                "authorId": "2109452621",
                "name": "Chenglong Wang"
            },
            {
                "authorId": "2261739712",
                "name": "Tong Xiao"
            },
            {
                "authorId": "2291418863",
                "name": "Kai Song"
            },
            {
                "authorId": "2111149004",
                "name": "Tongran Liu"
            },
            {
                "authorId": "1390468289",
                "name": "Chunliang Zhang"
            },
            {
                "authorId": "2240940961",
                "name": "Jingbo Zhu"
            }
        ],
        "abstract": "Large language models (LLMs) can handle multilingual and cross-lingual text within a single input; however, previous works leveraging multilingualism in LLMs primarily focus on using English as the pivot language to enhance language understanding and reasoning. Given that multiple languages are a compensation for the losses caused by a single language\u2019s limitations, it\u2019s a natural next step to enrich the model\u2019s learning context through the integration of the original input with its multiple translations. In this paper, we start by revealing that LLMs learn from parallel multilingual input (PMI). Our comprehensive evaluation shows that PMI enhances the model\u2019s comprehension of the input, achieving superior performance than conventional in-context learning (ICL). Furthermore, to explore how multilingual processing affects prediction, we examine the activated neurons in LLMs. Surprisingly, involving more languages in the input activates fewer neurons, leading to more focused and effective neural activation patterns. Also, this neural reaction coincidently mirrors the neuroscience insight about synaptic pruning, highlighting a similarity between artificial and biological \u2018brains\u2019."
    },
    {
        "paperId": "6edf144fb397afde80d9acf9472dfaffd22c8072",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.09162, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-14",
        "authors": [
            {
                "authorId": "2115538342",
                "name": "Haoran Yang"
            },
            {
                "authorId": "2266806630",
                "name": "Yumeng Zhang"
            },
            {
                "authorId": "2110524327",
                "name": "Jiaqi Xu"
            },
            {
                "authorId": "2156273800",
                "name": "Hongyuan Lu"
            },
            {
                "authorId": "2267115363",
                "name": "P. Heng"
            },
            {
                "authorId": "2253479907",
                "name": "Wai Lam"
            }
        ],
        "abstract": "While Large Language Models (LLMs) have demonstrated exceptional multitasking abilities, fine-tuning these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without fine-tuning. However, the comprehensive effects of fine-tuning on the LLMs\u2019 generalization ability are not fully understood.This paper delves into the differences between original, unmodified LLMs and their fine-tuned variants. Our primary investigation centers on whether fine-tuning affects the generalization ability intrinsic to LLMs. To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets.Our main findings reveal that models fine-tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks.Intriguingly, we observe that integrating the in-context learning strategy during fine-tuning on generation tasks can enhance the model\u2019s generalization ability.Through this systematic investigation, we aim to contribute valuable insights into the evolving landscape of fine-tuning practices for LLMs."
    },
    {
        "paperId": "38270d319b74916ab0022cb740e3e1dd68fe6459",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.09522, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-14",
        "authors": [
            {
                "authorId": "2108959287",
                "name": "Jiahuan Li"
            },
            {
                "authorId": "3456696",
                "name": "Shanbo Cheng"
            },
            {
                "authorId": "2124946880",
                "name": "Shujian Huang"
            },
            {
                "authorId": "2279659608",
                "name": "Jiajun Chen"
            }
        ],
        "abstract": "Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation, yet they suffer from high computational cost and latency. Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction. However, traditional knowledge distillation methods ignore the capability of student and teacher models, therefore repeatedly teaching student models on the knowledge they have learned, and failing to extend to novel contexts and knowledge. In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive and proactive manner. Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher. Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student. Experiment results on translating both specific language phenomena and general MT benchmarks demonstrate that finetuning the MT model on about 10% examples can achieve comparable results to the traditional knowledge distillation method, and synthesized potential errors and diverse contexts further improve MT performances on unseen contexts and words."
    },
    {
        "paperId": "e788d5c6dc79f106296f8ad296bcbe512028d5f0",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.09631, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-14",
        "authors": [
            {
                "authorId": "2226286961",
                "name": "Haoyu Zhen"
            },
            {
                "authorId": "2292176508",
                "name": "Xiaowen Qiu"
            },
            {
                "authorId": "2158502526",
                "name": "Peihao Chen"
            },
            {
                "authorId": "2291198667",
                "name": "Jincheng Yang"
            },
            {
                "authorId": "2291325237",
                "name": "Xin Yan"
            },
            {
                "authorId": "2258799458",
                "name": "Yilun Du"
            },
            {
                "authorId": "2265627123",
                "name": "Yining Hong"
            },
            {
                "authorId": "2266854520",
                "name": "Chuang Gan"
            }
        ],
        "abstract": "Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting the goal images and point clouds. To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications."
    },
    {
        "paperId": "9f06b3caa9bb8bcf35c5175ed65a5ac451d852c3",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Metaphor Understanding Challenge Dataset for LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.11810, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-18",
        "authors": [
            {
                "authorId": "2253762061",
                "name": "Xiaoyu Tong"
            },
            {
                "authorId": "2067163164",
                "name": "Rochelle Choenni"
            },
            {
                "authorId": "2253999321",
                "name": "Martha Lewis"
            },
            {
                "authorId": "2262445370",
                "name": "Ekaterina Shutova"
            }
        ],
        "abstract": "Metaphors in natural language are a reflection of fundamental cognitive processes such as analogical reasoning and categorisation, and are deeply rooted in everyday communication. Metaphor understanding is therefore an essential task for large language models (LLMs). We release the Metaphor Understanding Challenge Dataset (MUNCH), designed to evaluate the metaphor understanding capabilities of LLMs. The dataset provides over 10k paraphrases for sentences containing metaphor use, as well as 1.5k instances containing inapt paraphrases. The inapt paraphrases were carefully selected to serve as control to determine whether the model indeed performs full metaphor interpretation or rather resorts to lexical similarity. All apt and inapt paraphrases were manually annotated. The metaphorical sentences cover natural metaphor uses across 4 genres (academic, news, fiction, and conversation), and they exhibit different levels of novelty. Experiments with LLaMA and GPT-3.5 demonstrate that MUNCH presents a challenging task for LLMs. The dataset is freely accessible at https://github.com/xiaoyuisrain/metaphor-understanding-challenge."
    },
    {
        "paperId": "5372f4364fe5b7991c81ea19ee944c717afe6e0a",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Larimar: Large Language Models with Episodic Memory Control",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.11901, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-18",
        "authors": [
            {
                "authorId": "2283308757",
                "name": "Payel Das"
            },
            {
                "authorId": "2261672385",
                "name": "Subhajit Chaudhury"
            },
            {
                "authorId": "2292186979",
                "name": "Elliot Nelson"
            },
            {
                "authorId": "2275371384",
                "name": "Igor Melnyk"
            },
            {
                "authorId": "2248165313",
                "name": "Sarath Swaminathan"
            },
            {
                "authorId": "2292170473",
                "name": "Sihui Dai"
            },
            {
                "authorId": "2283303789",
                "name": "Aur\u00e9lie C. Lozano"
            },
            {
                "authorId": "2282965905",
                "name": "Georgios Kollias"
            },
            {
                "authorId": "46542047",
                "name": "Vijil Chenthamarakshan"
            },
            {
                "authorId": "2257033515",
                "name": "Jir\u00ed Navr\u00e1til"
            },
            {
                "authorId": "2292199652",
                "name": "Soham Dan"
            },
            {
                "authorId": "2292347619",
                "name": "Pin-Yu Chen"
            }
        ],
        "abstract": "Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 8-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting, information leakage prevention, and input context length generalization with Larimar and show their effectiveness. Our code is available at https://github.com/IBM/larimar"
    },
    {
        "paperId": "1bffb2aad9d34e5b74de328acd04d01355d4680f",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.13213, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-20",
        "authors": [
            {
                "authorId": "2193386368",
                "name": "Khaoula Chehbouni"
            },
            {
                "authorId": "2292258816",
                "name": "Megha Roshan"
            },
            {
                "authorId": "2292258768",
                "name": "Emmanuel Ma"
            },
            {
                "authorId": "2292260852",
                "name": "Futian Andrew Wei"
            },
            {
                "authorId": "2172481903",
                "name": "Afaf Ta\u00efk"
            },
            {
                "authorId": "2292258217",
                "name": "Jackie CK Cheung"
            },
            {
                "authorId": "2086602",
                "name": "G. Farnadi"
            }
        ],
        "abstract": "Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluating models on already mitigated biases. Using the case of Llama 2 as an example, we illustrate how LLMs' safety responses can still encode harmful assumptions. To do so, we create a set of non-toxic prompts, which we then use to evaluate Llama models. Through our new taxonomy of LLMs responses to users, we observe that the safety/helpfulness trade-offs are more pronounced for certain demographic groups which can lead to quality-of-service harms for marginalized populations."
    },
    {
        "paperId": "e3863c3f855614bcad53ddb15096864cff931cb5",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "SyllabusQA: A Course Logistics Question Answering Dataset",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.14666, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-03",
        "authors": [
            {
                "authorId": "39022996",
                "name": "Nigel Fernandez"
            },
            {
                "authorId": "1976128281",
                "name": "Alexander Scarlatos"
            },
            {
                "authorId": "2289844808",
                "name": "Andrew S. Lan"
            }
        ],
        "abstract": "Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SyllabusQA, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We introduce Fact-QA, an LLM-based (GPT-4) evaluation metric to evaluate the factuality of predicted answers. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between automated approaches and humans in terms of fact precision."
    },
    {
        "paperId": "144399b7708c0e15b8f067f7635df173a7067905",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Bypassing LLM Watermarks with Color-Aware Substitutions",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.14719, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-19",
        "authors": [
            {
                "authorId": "2293233274",
                "name": "Qilong Wu"
            },
            {
                "authorId": "2247670741",
                "name": "Varun Chandrasekaran"
            }
        ],
        "abstract": "Watermarking approaches are proposed to identify if text being circulated is human or large language model (LLM) generated. The state-of-the-art watermarking strategy of Kirchenbauer et al. (2023a) biases the LLM to generate specific (``green'') tokens. However, determining the robustness of this watermarking method is an open problem. Existing attack methods fail to evade detection for longer text segments. We overcome this limitation, and propose {\\em Self Color Testing-based Substitution (SCTS)}, the first ``color-aware'' attack. SCTS obtains color information by strategically prompting the watermarked LLM and comparing output tokens frequencies. It uses this information to determine token colors, and substitutes green tokens with non-green ones. In our experiments, SCTS successfully evades watermark detection using fewer number of edits than related work. Additionally, we show both theoretically and empirically that SCTS can remove the watermark for arbitrarily long watermarked text."
    },
    {
        "paperId": "8f02e0ac92a0b34360eb12105f4b8d98ab91d45a",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.14952, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-22",
        "authors": [
            {
                "authorId": "2028213158",
                "name": "Zhenrui Yue"
            },
            {
                "authorId": "2113559163",
                "name": "Huimin Zeng"
            },
            {
                "authorId": "2293323154",
                "name": "Yimeng Lu"
            },
            {
                "authorId": "65855502",
                "name": "Lanyu Shang"
            },
            {
                "authorId": "2145953897",
                "name": "Yang Zhang"
            },
            {
                "authorId": "2254248851",
                "name": "Dong Wang"
            }
        ],
        "abstract": "The proliferation of online misinformation has posed significant threats to public interest. While numerous online users actively participate in the combat against misinformation, many of such responses can be characterized by the lack of politeness and supporting facts. As a solution, text generation approaches are proposed to automatically produce counter-misinformation responses. Nevertheless, existing methods are often trained end-to-end without leveraging external knowledge, resulting in subpar text quality and excessively repetitive responses. In this paper, we propose retrieval augmented response generation for online misinformation (RARG), which collects supporting evidence from scientific sources and generates counter-misinformation responses based on the evidences. In particular, our RARG consists of two stages: (1) evidence collection, where we design a retrieval pipeline to retrieve and rerank evidence documents using a database comprising over 1M academic articles; (2) response generation, in which we align large language models (LLMs) to generate evidence-based responses via reinforcement learning from human feedback (RLHF). We propose a reward function to maximize the utilization of the retrieved evidence while maintaining the quality of the generated text, which yields polite and factual responses that clearly refutes misinformation. To demonstrate the effectiveness of our method, we study the case of COVID-19 and perform extensive experiments with both in- and cross-domain datasets, where RARG consistently outperforms baselines by generating high-quality counter-misinformation responses."
    },
    {
        "paperId": "5a0573a3c15d094e8b3d488c11a660773f631070",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Towards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.15412, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-05",
        "authors": [
            {
                "authorId": "2191731497",
                "name": "Farid Adilazuarda"
            },
            {
                "authorId": "2087482453",
                "name": "Sagnik Mukherjee"
            },
            {
                "authorId": "2262446167",
                "name": "Pradhyumna Lavania"
            },
            {
                "authorId": "2293352196",
                "name": "S. Singh"
            },
            {
                "authorId": "2262445503",
                "name": "Ashutosh Dwivedi"
            },
            {
                "authorId": "8129718",
                "name": "Alham Fikri Aji"
            },
            {
                "authorId": "2293317227",
                "name": "Jacki O'Neill"
            },
            {
                "authorId": "2477939",
                "name": "Ashutosh Modi"
            },
            {
                "authorId": "143990839",
                "name": "M. Choudhury"
            }
        ],
        "abstract": "We present a survey of more than 90 recent papers that aim to study cultural representation and inclusion in large language models (LLMs). We observe that none of the studies explicitly define \u201cculture, which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of \u201cculture\u201d. We call these aspects the proxies of culture, and organize them across two dimensions of demographic and semantic proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of \u201cculture,\u201d such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness of probing techniques and situated studies on the impact of cultural mis- and under-representation in LLM-based applications."
    },
    {
        "paperId": "a54204cbbf2b117cb472fd0557ef13d1b3d8bead",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Attribute First, then Generate: Locally-attributable Grounded Text Generation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.17104, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-25",
        "authors": [
            {
                "authorId": "2074098656",
                "name": "Aviv Slobodkin"
            },
            {
                "authorId": "2128107365",
                "name": "Eran Hirsch"
            },
            {
                "authorId": "1962331387",
                "name": "Arie Cattan"
            },
            {
                "authorId": "32303439",
                "name": "Tal Schuster"
            },
            {
                "authorId": "7465342",
                "name": "Ido Dagan"
            }
        ],
        "abstract": "Recent efforts to address hallucinations in Large Language Models (LLMs) have focused on attributed text generation, which supplements generated texts with citations of supporting sources for post-generation fact-checking and corrections. Yet, these citations often point to entire documents or paragraphs, burdening users with extensive verification work. In this paper, we introduce a locally-attributable text generation approach, prioritizing concise attributions. Our method, named\"Attribute First, then Generate\", breaks down the conventional end-to-end generation process into three intuitive steps: content selection, sentence planning, and sequential sentence generation. By initially identifying relevant source segments (\"select first\") and then conditioning the generation process on them (\"then generate\"), we ensure these segments also act as the output's fine-grained attributions (\"select\"becomes\"attribute\"). Tested on Multi-document Summarization and Long-form Question-answering, our method not only yields more concise citations than the baselines but also maintains - and in some cases enhances - both generation quality and attribution accuracy. Furthermore, it significantly reduces the time required for fact verification by human assessors."
    },
    {
        "paperId": "8f77307a394f006633c27cca56a345c0879eb67c",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Measuring Political Bias in Large Language Models: What Is Said and How It Is Said",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.18932, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-27",
        "authors": [
            {
                "authorId": "23672613",
                "name": "Yejin Bang"
            },
            {
                "authorId": "94210578",
                "name": "Delong Chen"
            },
            {
                "authorId": "40221187",
                "name": "Nayeon Lee"
            },
            {
                "authorId": "2265382811",
                "name": "Pascale Fung"
            }
        ],
        "abstract": "We propose to measure political bias in LLMs by analyzing both the content and style of their generated content regarding political issues. Existing benchmarks and measures focus on gender and racial biases. However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications. In order to provide transparency to users, we advocate that there should be fine-grained and explainable measures of political biases generated by LLMs. Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias. We measured the political bias in eleven open-sourced LLMs and showed that our proposed framework is easily scalable to other topics and is explainable."
    },
    {
        "paperId": "094b6847434be00e41686529f45d895bd632f68a",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.20046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-29",
        "authors": [
            {
                "authorId": "2260336914",
                "name": "Yongqi Tong"
            },
            {
                "authorId": "2161635474",
                "name": "Dawei Li"
            },
            {
                "authorId": "2260628159",
                "name": "Sizhe Wang"
            },
            {
                "authorId": "2261894506",
                "name": "Yujia Wang"
            },
            {
                "authorId": "2294173293",
                "name": "Fei Teng"
            },
            {
                "authorId": "2257005005",
                "name": "Jingbo Shang"
            }
        ],
        "abstract": "Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting. While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: \\textit{can LLMs learn and benefit from their mistakes, especially for their reasoning? } This study investigates this problem from both the prompting and model-tuning perspectives. We begin by introducing \\textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) \\textbf{Self-rethinking} prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) \\textbf{Mistake tuning} involves finetuning models in both correct and incorrect reasoning domains, rather than only tuning models to learn ground truth in traditional methodology. We conduct a series of experiments to prove LLMs can obtain benefits from mistakes in both directions. Our two methods offer potentially cost-effective strategies by leveraging errors to enhance reasoning capabilities, which costs significantly less than creating meticulously hand-crafted golden references. We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome. \\textsc{CoTErrorSet} will be published soon on \\texttt{\\url{https://github.com/YookiTong/Learn-from-Mistakes-CotErrorSet}}."
    },
    {
        "paperId": "72964abe403d528a5687cb2fc2711ffdba444d21",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.20147, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-29",
        "authors": [
            {
                "authorId": "2119502119",
                "name": "Nihar Ranjan Sahoo"
            },
            {
                "authorId": "2294175410",
                "name": "Pranamya Prashant Kulkarni"
            },
            {
                "authorId": "2294174388",
                "name": "Narjis Asad"
            },
            {
                "authorId": "2237745660",
                "name": "Arif Ahmad"
            },
            {
                "authorId": "2294174209",
                "name": "Tanu Goyal"
            },
            {
                "authorId": "31099365",
                "name": "Aparna Garimella"
            },
            {
                "authorId": "2259166620",
                "name": "Pushpak Bhattacharyya"
            }
        ],
        "abstract": "The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs). Existing efforts predominantly focus on English language and the Western context, leaving a void for a reliable dataset that encapsulates India\u2019s unique socio-cultural nuances. To bridge this gap, we introduce IndiBias, a comprehensive benchmarking dataset designed specifically for evaluating social biases in the Indian context. We filter and translate the existing CrowS-Pairs dataset to create a benchmark dataset suited to the Indian context in Hindi language. Additionally, we leverage LLMs including ChatGPT and InstructGPT to augment our dataset with diverse societal biases and stereotypes prevalent in India. The included bias dimensions encompass gender, religion, caste, age, region, physical appearance, and occupation. We also build a resource to address intersectional biases along three intersectional dimensions. Our dataset contains 800 sentence pairs and 300 tuples for bias measurement across different demographics. The dataset is available in English and Hindi, providing a size comparable to existing benchmark datasets. Furthermore, using IndiBias we compare ten different language models on multiple bias measurement metrics. We observed that the language models exhibit more bias across a majority of the intersectional groups. All the scripts utilized and datasets created in this study are publicly available."
    },
    {
        "paperId": "edf260dee56a06d897547fb460a1e317d7eb571b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Can Language Models Recognize Convincing Arguments?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.00750, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-03-31",
        "authors": [
            {
                "authorId": "2294362867",
                "name": "Paula Rescala"
            },
            {
                "authorId": "2164356774",
                "name": "Manoel Horta Ribeiro"
            },
            {
                "authorId": "2046818614",
                "name": "Tiancheng Hu"
            },
            {
                "authorId": "2257264380",
                "name": "Robert West"
            }
        ],
        "abstract": "The capabilities of large language models (LLMs) have raised concerns about their potential to create and propagate convincing narratives. Here, we study their performance in detecting convincing arguments to gain insights into LLMs' persuasive capabilities without directly engaging in experimentation with humans. We extend a dataset by Durmus and Cardie (2018) with debates, votes, and user traits and propose tasks measuring LLMs' ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, surpassing human performance. The data and code released with this paper contribute to the crucial effort of continuously evaluating and monitoring LLMs' capabilities and potential impact. (https://go.epfl.ch/persuasion-llm)"
    },
    {
        "paperId": "4a02d2fef5b94a4da8346f1fa88ae5f9294f886b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.01247, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-01",
        "authors": [
            {
                "authorId": "1452678825",
                "name": "Simran Khanuja"
            },
            {
                "authorId": "2140457932",
                "name": "Sathyanarayanan Ramamoorthy"
            },
            {
                "authorId": "2290031618",
                "name": "Yueqi Song"
            },
            {
                "authorId": "2285194103",
                "name": "Graham Neubig"
            }
        ],
        "abstract": "Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we introduce a new task of translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset \u2013 (i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image; and (ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop. Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our project webpage is here: https://machine-transcreation.github.io/image-transcreation and our code, data and model outputs can be found here: https://github.com/simran-khanuja/image-transcreation."
    },
    {
        "paperId": "7f94b69cac02fdd113385950297fcdbfbe3413ee",
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "title": "Robust Pronoun Fidelity with English LLMs: Are they Reasoning, Repeating, or Just Biased?",
        "openAccessPdf": {
            "url": "https://doi.org/10.1162/tacl_a_00719",
            "status": "GOLD",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.03134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-04",
        "authors": [
            {
                "authorId": "2047602319",
                "name": "Vagrant Gautam"
            },
            {
                "authorId": "2294878412",
                "name": "Eileen Bingert"
            },
            {
                "authorId": "47770349",
                "name": "D. Zhu"
            },
            {
                "authorId": "29891652",
                "name": "Anne Lauscher"
            },
            {
                "authorId": "2561225",
                "name": "D. Klakow"
            }
        ],
        "abstract": "Abstract Robust, faithful, and harm-free pronoun use for individuals is an important goal for language model development as their use increases, but prior work tends to study only one or two of these characteristics at a time. To measure progress towards the combined goal, we introduce the task of pronoun fidelity: Given a context introducing a co-referring entity and pronoun, the task is to reuse the correct pronoun later. We present RUFF, a carefully designed dataset of over 5 million instances to measure robust pronoun fidelity in English, and we evaluate 37 model variants from nine popular families, across architectures (encoder-only, decoder-only, and encoder-decoder) and scales (11M-70B parameters). When an individual is introduced with a pronoun, models can mostly faithfully reuse this pronoun in the next sentence, but they are significantly worse with she/her/her, singular they, and neopronouns. Moreover, models are easily distracted by non-adversarial sentences discussing other people; even one sentence with a distractor pronoun causes accuracy to drop on average by 34 percentage points. Our results show that pronoun fidelity is not robust, in a simple, naturalistic setting where humans achieve nearly 100% accuracy. We encourage researchers to bridge the gaps we find and to carefully evaluate reasoning in settings where superficial repetition might inflate perceptions of model performance."
    },
    {
        "paperId": "41b54b4dcdaf714734db846855009871be6d1b46",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Okay, Let\u2019s Do This! Modeling Event Coreference with Generated Rationales and Knowledge Distillation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.03196, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-04",
        "authors": [
            {
                "authorId": "2187454703",
                "name": "Abhijnan Nath"
            },
            {
                "authorId": "2294174499",
                "name": "Shadi Manafi"
            },
            {
                "authorId": "2294876859",
                "name": "Avyakta Chelle"
            },
            {
                "authorId": "2257224732",
                "name": "Nikhil Krishnaswamy"
            }
        ],
        "abstract": "In NLP, Event Coreference Resolution (ECR) is the task of connecting event clusters that refer to the same underlying real-life event, usually via neural systems. In this work, we investigate using abductive free-text rationales (FTRs) generated by modern autoregressive LLMs as distant supervision of smaller student models for cross-document coreference (CDCR) of events. We implement novel rationale-oriented event clustering and knowledge distillation methods for event coreference scoring that leverage enriched information from the FTRs for improved CDCR without additional annotation or expensive document clustering. Our model using coreference-specific knowledge distillation achieves SOTA B^3 F_1 on the ECB+ and GVC corpora and we establish a new baseline on the AIDA Phase 1 corpus. Our code can be found at https://github.com/csu-signal/llama_cdcr."
    },
    {
        "paperId": "ea104918ddcb97d0c433dcdfdbd21411855874f8",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.03622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-04",
        "authors": [
            {
                "authorId": "51198241",
                "name": "Wenshan Wu"
            },
            {
                "authorId": "2273419590",
                "name": "Shaoguang Mao"
            },
            {
                "authorId": "2282502075",
                "name": "Yadong Zhang"
            },
            {
                "authorId": "2258547658",
                "name": "Yan Xia"
            },
            {
                "authorId": "145307652",
                "name": "Li Dong"
            },
            {
                "authorId": "2114843952",
                "name": "Lei Cui"
            },
            {
                "authorId": "2249539478",
                "name": "Furu Wei"
            }
        ],
        "abstract": "Large language models (LLMs) have exhibited impressive performance in language comprehension and various reasoning tasks. However, their abilities in spatial reasoning, a crucial aspect of human cognition, remain relatively unexplored. Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as the Mind's Eye, enabling the imagination of the unseen world. Inspired by this cognitive capacity, we propose Visualization-of-Thought (VoT) prompting. VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability to generate mental images to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs. Please find the dataset and codes at https://microsoft.github.io/visualization-of-thought"
    },
    {
        "paperId": "53cd612f5046901ca454f3c72dcad45a84f4f31d",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical Reasoning in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.03887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-05",
        "authors": [
            {
                "authorId": "2256984804",
                "name": "Hyeonwoo Kim"
            },
            {
                "authorId": "2276424847",
                "name": "Gyoungjin Gim"
            },
            {
                "authorId": "2276638686",
                "name": "Yungi Kim"
            },
            {
                "authorId": "2276491119",
                "name": "Jihoo Kim"
            },
            {
                "authorId": "2295553506",
                "name": "Byungju Kim"
            },
            {
                "authorId": "2295681757",
                "name": "Wonseok Lee"
            },
            {
                "authorId": "2115195904",
                "name": "Chanjun Park"
            }
        ],
        "abstract": "This study presents a novel learning approach designed to enhance both mathematical reasoning and problem-solving abilities of Large Language Models (LLMs). We focus on integrating the Chain-of-Thought (CoT) and the Program-of-Thought (PoT) learning, hypothesizing that prioritizing the learning of mathematical reasoning ability is helpful for the amplification of problem-solving ability. Thus, the initial learning with CoT is essential for solving challenging mathematical problems. To this end, we propose a sequential learning approach, named SAAS (Solving Ability Amplification Strategy), which strategically transitions from CoT learning to PoT learning. Our empirical study, involving an extensive performance comparison using several benchmarks, demonstrates that our SAAS achieves state-of-the-art (SOTA) performance. The results underscore the effectiveness of our sequential learning approach, marking a significant advancement in the field of mathematical reasoning in LLMs."
    },
    {
        "paperId": "3bc23a232db48f454ebf27be39c189c9aa64af18",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Bias Amplification in Language Model Evolution: An Iterated Learning Perspective",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.04286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-04",
        "authors": [
            {
                "authorId": "2115242507",
                "name": "Yi Ren"
            },
            {
                "authorId": "3431365",
                "name": "Shangmin Guo"
            },
            {
                "authorId": "2257459530",
                "name": "Linlu Qiu"
            },
            {
                "authorId": "2296090895",
                "name": "Bailin Wang"
            },
            {
                "authorId": "2262445067",
                "name": "Danica J. Sutherland"
            }
        ],
        "abstract": "With the widespread adoption of Large Language Models (LLMs), the prevalence of iterative interactions among these models is anticipated to increase. Notably, recent advancements in multi-round self-improving methods allow LLMs to generate new examples for training subsequent models. At the same time, multi-agent LLM systems, involving automated interactions among agents, are also increasing in prominence. Thus, in both short and long terms, LLMs may actively engage in an evolutionary process. We draw parallels between the behavior of LLMs and the evolution of human culture, as the latter has been extensively studied by cognitive scientists for decades. Our approach involves leveraging Iterated Learning (IL), a Bayesian framework that elucidates how subtle biases are magnified during human cultural evolution, to explain some behaviors of LLMs. This paper outlines key characteristics of agents' behavior in the Bayesian-IL framework, including predictions that are supported by experimental verification with various LLMs. This theoretical framework could help to more effectively predict and guide the evolution of LLMs in desired directions."
    },
    {
        "paperId": "957afdde0cf5f812d4cd1534f066414eaf088f61",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.06003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-09",
        "authors": [
            {
                "authorId": "2164113313",
                "name": "Zhuohao Yu"
            },
            {
                "authorId": "2287659901",
                "name": "Chang Gao"
            },
            {
                "authorId": "2286328804",
                "name": "Wenjin Yao"
            },
            {
                "authorId": "2108024279",
                "name": "Yidong Wang"
            },
            {
                "authorId": "2293666521",
                "name": "Zhengran Zeng"
            },
            {
                "authorId": "145235149",
                "name": "Wei Ye"
            },
            {
                "authorId": "2273553706",
                "name": "Jindong Wang"
            },
            {
                "authorId": "2250437942",
                "name": "Yue Zhang"
            },
            {
                "authorId": "1705434",
                "name": "Shikun Zhang"
            }
        ],
        "abstract": "The rapid growth of evaluation methodologies and datasets for large language models (LLMs) has created a pressing need for their unified integration. Meanwhile, concerns about data contamination and bias compromise the trustworthiness of evaluation findings, while the efficiency of evaluation processes remains a bottleneck due to the significant computational costs associated with LLM inference.In response to these challenges, we introduce FreeEval, a modular framework not only for conducting trustworthy and efficient automatic evaluations of LLMs but also serving as a platform to develop and validate new evaluation methodologies. FreeEval addresses key challenges through: (1) unified abstractions that simplify the integration of diverse evaluation methods, including dynamic evaluations requiring complex LLM interactions; (2) built-in meta-evaluation techniques such as data contamination detection and human evaluation to enhance result fairness; (3) a high-performance infrastructure with distributed computation and caching strategies for efficient large-scale evaluations; and (4) an interactive Visualizer for result analysis and interpretation to support innovation of evaluation techniques. We open-source all our code at https://github.com/WisdomShell/FreeEval and our demostration video, live demo, installation guides are available at: https://freeeval.zhuohao.me/."
    },
    {
        "paperId": "f04e6bb8108cf191c88c461b5f367cd3ec336ebc",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.06138, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-09",
        "authors": [
            {
                "authorId": "66986482",
                "name": "Samuel Cahyawijaya"
            },
            {
                "authorId": "116344405",
                "name": "Holy Lovenia"
            },
            {
                "authorId": "2789148",
                "name": "Fajri Koto"
            },
            {
                "authorId": "9358635",
                "name": "Rifki Afina Putri"
            },
            {
                "authorId": "2242161249",
                "name": "Emmanuel Dave"
            },
            {
                "authorId": "2242192851",
                "name": "Jhonson Lee"
            },
            {
                "authorId": "2242166155",
                "name": "Nuur Shadieq"
            },
            {
                "authorId": "2295729963",
                "name": "Wawan Cenggoro"
            },
            {
                "authorId": "2242161500",
                "name": "Salsabil Maulana Akbar"
            },
            {
                "authorId": "2295729969",
                "name": "Muhammad Ihza Mahendra"
            },
            {
                "authorId": "2295845767",
                "name": "Dea Annisayanti Putri"
            },
            {
                "authorId": "2258553695",
                "name": "Bryan Wilie"
            },
            {
                "authorId": "9162688",
                "name": "Genta Indra Winata"
            },
            {
                "authorId": "8129718",
                "name": "Alham Fikri Aji"
            },
            {
                "authorId": "2257345523",
                "name": "Ayu Purwarianti"
            },
            {
                "authorId": "2057151752",
                "name": "Pascale Fung"
            }
        ],
        "abstract": "Large language models (LLMs) show remarkable human-like capability in various domains and languages. However, a notable quality gap arises in low-resource languages, e.g., Indonesian indigenous languages, rendering them ineffective and inefficient in such linguistic contexts. To bridge this quality gap, we introduce Cendol, a collection of Indonesian LLMs encompassing both decoder-only and encoder-decoder architectures across a range of model sizes. We highlight Cendol's effectiveness across a diverse array of tasks, attaining 20% improvement, and demonstrate its capability to generalize to unseen tasks and indigenous languages of Indonesia. Furthermore, Cendol models showcase improved human favorability despite their limitations in capturing indigenous knowledge and cultural values in Indonesia. In addition, we discuss the shortcomings of parameter-efficient tunings, such as LoRA, for language adaptation. Alternatively, we propose the usage of vocabulary adaptation to enhance efficiency. Lastly, we evaluate the safety of Cendol and showcase that safety in pre-training in one language such as English is transferable to low-resource languages, such as Indonesian, even without RLHF and safety fine-tuning."
    },
    {
        "paperId": "6ae28b27d5e81aa7ad2dfdc7e3d712870159a7bc",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.06762, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-10",
        "authors": [
            {
                "authorId": "49293155",
                "name": "Zhengyuan Liu"
            },
            {
                "authorId": "2220058735",
                "name": "Stella Xin Yin"
            },
            {
                "authorId": "2297433621",
                "name": "Geyu Lin"
            },
            {
                "authorId": "2271409954",
                "name": "Nancy F. Chen"
            }
        ],
        "abstract": "Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience. The emergence of large language models (LLMs) further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language learning. In dialogic teaching, recognizing and adapting to individual characteristics can significantly enhance student engagement and learning efficiency. However, characterizing and simulating student\u2019s persona remain challenging in training and evaluating conversational ITSs. In this work, we propose a framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects, and leverage LLMs for personality-aware student simulation in a language learning scenario. We further enhance the framework with multi-aspect validation, and conduct extensive analysis from both teacher and student perspectives. Our experimental results show that state-of-the-art LLMs can produce diverse student responses according to the given language ability and personality traits, and trigger teacher\u2019s adaptive scaffolding strategies."
    },
    {
        "paperId": "e876f221c3eaf542d0d79f5f1f5c2f51ad2d0a48",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.06809, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-10",
        "authors": [
            {
                "authorId": "2280143071",
                "name": "Ruotong Pan"
            },
            {
                "authorId": "2113252896",
                "name": "Boxi Cao"
            },
            {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
            },
            {
                "authorId": "2118233348",
                "name": "Xianpei Han"
            },
            {
                "authorId": "2118455467",
                "name": "Jia Zheng"
            },
            {
                "authorId": "2295934207",
                "name": "Sirui Wang"
            },
            {
                "authorId": "2290035990",
                "name": "Xunliang Cai"
            },
            {
                "authorId": "2110832778",
                "name": "Le Sun"
            }
        ],
        "abstract": "The rapid development of large language models has led to the widespread adoption of Retrieval-Augmented Generation (RAG), which integrates external knowledge to alleviate knowledge bottlenecks and mitigate hallucinations. However, the existing RAG paradigm inevitably suffers from the impact of flawed information introduced during the retrieval phrase, thereby diminishing the reliability and correctness of the generated outcomes. In this paper, we propose Credibility-aware Generation (CAG), a universally applicable framework designed to mitigate the impact of flawed information in RAG. At its core, CAG aims to equip models with the ability to discern and process information based on its credibility. To this end, we propose an innovative data transformation framework that generates data based on credibility, thereby effectively endowing models with the capability of CAG. Furthermore, to accurately evaluate the models\u2019 capabilities of CAG, we construct a comprehensive benchmark covering three critical real-world scenarios. Experimental results demonstrate that our model can effectively understand and employ credibility for generation, significantly outperform other models with retrieval augmentation, and exhibit robustness despite the increasing noise in the context."
    },
    {
        "paperId": "7a16e034525bdab6e5abc78bed488a565ed874be",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "An Audit on the Perspectives and Challenges of Hallucinations in NLP",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.07461, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-11",
        "authors": [
            {
                "authorId": "2053812167",
                "name": "Pranav Narayanan Venkit"
            },
            {
                "authorId": "2264443832",
                "name": "Tatiana Chakravorti"
            },
            {
                "authorId": "2110652561",
                "name": "Vipul Gupta"
            },
            {
                "authorId": "2295986944",
                "name": "Heidi Biggs"
            },
            {
                "authorId": "47467195",
                "name": "Mukund Srinath"
            },
            {
                "authorId": "120873790",
                "name": "Koustava Goswami"
            },
            {
                "authorId": "2243336258",
                "name": "Sarah Rajtmajer"
            },
            {
                "authorId": "2242862740",
                "name": "Shomir Wilson"
            }
        ],
        "abstract": "We audit how hallucination in large language models (LLMs) is characterized in peer-reviewed literature, using a critical examination of 103 publications across NLP research. Through the examination of the literature, we identify a lack of agreement with the term \u2018hallucination\u2019 in the field of NLP. Additionally, to compliment our audit, we conduct a survey with 171 practitioners from the field of NLP and AI to capture varying perspectives on hallucination. Our analysis calls for the necessity of explicit definitions and frameworks outlining hallucination within NLP, highlighting potential challenges, and our survey inputs provide a thematic understanding of the influence and ramifications of hallucination in society."
    },
    {
        "paperId": "ccb390557be8095a2de4da92380d17c84aaaf682",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "EFSA: Towards Event-Level Financial Sentiment Analysis",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.08681, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-08",
        "authors": [
            {
                "authorId": "2296951081",
                "name": "Tianyu Chen"
            },
            {
                "authorId": "2296731205",
                "name": "Yiming Zhang"
            },
            {
                "authorId": "2273922129",
                "name": "Guoxin Yu"
            },
            {
                "authorId": "2296749194",
                "name": "Dapeng Zhang"
            },
            {
                "authorId": "2296994152",
                "name": "Li Zeng"
            },
            {
                "authorId": "2297640617",
                "name": "Qing He"
            },
            {
                "authorId": "2273683983",
                "name": "Xiang Ao"
            }
        ],
        "abstract": "In this paper, we extend financial sentiment analysis~(FSA) to event-level since events usually serve as the subject of the sentiment in financial text. Though extracting events from the financial text may be conducive to accurate sentiment predictions, it has specialized challenges due to the lengthy and discontinuity of events in a financial text. To this end, we reconceptualize the event extraction as a classification task by designing a categorization comprising coarse-grained and fine-grained event categories. Under this setting, we formulate the \\textbf{E}vent-Level \\textbf{F}inancial \\textbf{S}entiment \\textbf{A}nalysis~(\\textbf{EFSA} for short) task that outputs quintuples consisting of (company, industry, coarse-grained event, fine-grained event, sentiment) from financial text. A large-scale Chinese dataset containing $12,160$ news articles and $13,725$ quintuples is publicized as a brand new testbed for our task. A four-hop Chain-of-Thought LLM-based approach is devised for this task. Systematically investigations are conducted on our dataset, and the empirical results demonstrate the benchmarking scores of existing methods and our proposed method can reach the current state-of-the-art. Our dataset and framework implementation are available at https://anonymous.4open.science/r/EFSA-645E"
    },
    {
        "paperId": "9d3a59b50ff7b232ea4daada54c19726f5d7883f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2404.08760",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.08760, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-12",
        "authors": [
            {
                "authorId": "2296829807",
                "name": "Siyang Liu"
            },
            {
                "authorId": "2296717969",
                "name": "Trish Maturi"
            },
            {
                "authorId": "2057992573",
                "name": "Bowen Yi"
            },
            {
                "authorId": "2072820796",
                "name": "Siqi Shen"
            },
            {
                "authorId": "2105984203",
                "name": "Rada Mihalcea"
            }
        ],
        "abstract": "We explore the alignment of values in Large Language Models (LLMs) with specific age groups, leveraging data from the World Value Survey across thirteen categories. Through a diverse set of prompts tailored to ensure response robustness, we find a general inclination of LLM values towards younger demographics, especially when compared to the US population. Although a general inclination can be observed, we also found that this inclination toward younger groups can be different across different value categories. Additionally, we explore the impact of incorporating age identity information in prompts and observe challenges in mitigating value discrepancies with different age cohorts. Our findings highlight the age bias in LLMs and provide insights for future work. Materials for our analysis will be available via https://github.com/anonymous"
    },
    {
        "paperId": "5fb8997c8cc9f4eaab102d54c2c86cfc61f30445",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Which questions should I answer? Salience Prediction of Inquisitive Questions",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.10917, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-16",
        "authors": [
            {
                "authorId": "2174327114",
                "name": "Yating Wu"
            },
            {
                "authorId": "2048016648",
                "name": "Ritika Mangla"
            },
            {
                "authorId": "1718469",
                "name": "A. Dimakis"
            },
            {
                "authorId": "1814094",
                "name": "Greg Durrett"
            },
            {
                "authorId": "2261337975",
                "name": "Junyi Jessy Li"
            }
        ],
        "abstract": "Inquisitive questions \u2014 open-ended, curiosity-driven questions people ask as they read \u2014 are an integral part of discourse processing and comprehension. Recent work in NLP has taken advantage of question generation capabilities of LLMs to enhance a wide range of applications. But the space of inquisitive questions is vast: many questions can be evoked from a given context. So which of those should be prioritized to find answers? Linguistic theories, unfortunately, have not yet provided an answer to this question. This paper presents QSalience, a salience predictor of inquisitive questions. QSalience is instruction-tuned over our dataset of linguist-annotated salience scores of 1,766 (context, question) pairs. A question scores high on salience if answering it would greatly enhance the understanding of the text. We show that highly salient questions are empirically more likely to be answered in the same article, bridging potential questions with Questions Under Discussion. We further validate our findings by showing that answering salient questions is an indicator of summarization quality in news."
    },
    {
        "paperId": "de17e7ed443e13320694cce3b2f475c694801246",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Stepwise Alignment for Constrained Language Model Policy Optimization",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.11049, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-17",
        "authors": [
            {
                "authorId": "41036308",
                "name": "Akifumi Wachi"
            },
            {
                "authorId": "2297347961",
                "name": "Thien Q. Tran"
            },
            {
                "authorId": "2296992379",
                "name": "Rei Sato"
            },
            {
                "authorId": "2074114653",
                "name": "Takumi Tanabe"
            },
            {
                "authorId": "2301151143",
                "name": "Yohei Akimoto"
            }
        ],
        "abstract": "Safety and trustworthiness are indispensable requirements for real-world applications of AI systems using large language models (LLMs). This paper formulates human value alignment as an optimization problem of the language model policy to maximize reward under a safety constraint, and then proposes an algorithm, Stepwise Alignment for Constrained Policy Optimization (SACPO). One key idea behind SACPO, supported by theory, is that the optimal policy incorporating reward and safety can be directly obtained from a reward-aligned policy. Building on this key idea, SACPO aligns LLMs step-wise with each metric while leveraging simple yet powerful alignment algorithms such as direct preference optimization (DPO). SACPO offers several advantages, including simplicity, stability, computational efficiency, and flexibility of algorithms and datasets. Under mild assumptions, our theoretical analysis provides the upper bounds on optimality and safety constraint violation. Our experimental results show that SACPO can fine-tune Alpaca-7B better than the state-of-the-art method in terms of both helpfulness and harmlessness."
    },
    {
        "paperId": "bc2ad540cb03cd513f96be3bdb81b5f296a4b09b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2404.11055",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.11055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-17",
        "authors": [
            {
                "authorId": "2114227440",
                "name": "Zhiheng Lyu"
            },
            {
                "authorId": "2111472502",
                "name": "Zhijing Jin"
            },
            {
                "authorId": "2272838249",
                "name": "Fernando Gonzalez Adauto"
            },
            {
                "authorId": "2105984203",
                "name": "Rada Mihalcea"
            },
            {
                "authorId": "2261483511",
                "name": "Bernhard Sch\u00f6lkopf"
            },
            {
                "authorId": "2790926",
                "name": "Mrinmaya Sachan"
            }
        ],
        "abstract": "Sentiment analysis (SA) aims to identify the sentiment expressed in a text, such as a product review. Given a review and the sentiment associated with it, this work formulates SA as a combination of two tasks: (1) a causal discovery task that distinguishes whether a review\"primes\"the sentiment (Causal Hypothesis C1), or the sentiment\"primes\"the review (Causal Hypothesis C2); and (2) the traditional prediction task to model the sentiment using the review as input. Using the peak-end rule in psychology, we classify a sample as C1 if its overall sentiment score approximates an average of all the sentence-level sentiments in the review, and C2 if the overall sentiment score approximates an average of the peak and end sentiments. For the prediction task, we use the discovered causal mechanisms behind the samples to improve LLM performance by proposing causal prompts that give the models an inductive bias of the underlying causal graph, leading to substantial improvements by up to 32.13 F1 points on zero-shot five-class SA. Our code is at https://github.com/cogito233/causal-sa"
    },
    {
        "paperId": "043aa8e8b4768e2b36c5097141b88a6ad2b591b7",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Inductive-Deductive Strategy Reuse for Multi-Turn Instructional Dialogues",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.11095, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-17",
        "authors": [
            {
                "authorId": "2265383868",
                "name": "Jiao Ou"
            },
            {
                "authorId": "2297140659",
                "name": "Jiayu Wu"
            },
            {
                "authorId": "2257133304",
                "name": "Che Liu"
            },
            {
                "authorId": "2257136363",
                "name": "Fuzheng Zhang"
            },
            {
                "authorId": "2257381268",
                "name": "Di Zhang"
            },
            {
                "authorId": "2238953242",
                "name": "Kun Gai"
            }
        ],
        "abstract": "Aligning large language models (LLMs) with human expectations requires high-quality instructional dialogues, which can be achieved by raising diverse, in-depth, and insightful instructions that deepen interactions. Existing methods target instructions from real instruction dialogues as a learning goal and fine-tune a user simulator for posing instructions. However, the user simulator struggles to implicitly model complex dialogue flows and pose high-quality instructions. In this paper, we take inspiration from the cognitive abilities inherent in human learning and propose the explicit modeling of complex dialogue flows through instructional strategy reuse. Specifically, we first induce high-level strategies from various real instruction dialogues. These strategies are applied to new dialogue scenarios deductively, where the instructional strategies facilitate high-quality instructions. Experimental results show that our method can generate diverse, in-depth, and insightful instructions for a given dialogue history. The constructed multi-turn instructional dialogues can outperform competitive baselines on the downstream chat model."
    },
    {
        "paperId": "1544f956492b613b47efd9ac79c3987df373fb81",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Aligning Language Models to Explicitly Handle Ambiguity",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.11972, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-18",
        "authors": [
            {
                "authorId": "2166352356",
                "name": "Hyuhng Joon Kim"
            },
            {
                "authorId": "2189657321",
                "name": "Youna Kim"
            },
            {
                "authorId": "2284731913",
                "name": "Cheonbok Park"
            },
            {
                "authorId": "2109166096",
                "name": "Junyeob Kim"
            },
            {
                "authorId": "2188345697",
                "name": "Choonghyun Park"
            },
            {
                "authorId": "31760501",
                "name": "Kang Min Yoo"
            },
            {
                "authorId": "2261393515",
                "name": "Sang-goo Lee"
            },
            {
                "authorId": "5041757",
                "name": "Taeuk Kim"
            }
        ],
        "abstract": "In interactions between users and language model agents, user utterances frequently exhibit ellipsis (omission of words or phrases) or imprecision (lack of exactness) to prioritize efficiency. This can lead to varying interpretations of the same input based on different assumptions or background knowledge. It is thus crucial for agents to adeptly handle the inherent ambiguity in queries to ensure reliability. However, even state-of-the-art large language models (LLMs) still face challenges in such scenarios, primarily due to the following hurdles: (1) LLMs are not explicitly trained to deal with ambiguous utterances; (2) the degree of ambiguity perceived by the LLMs may vary depending on the possessed knowledge. To address these issues, we propose Alignment with Perceived Ambiguity (APA), a novel pipeline that aligns LLMs to manage ambiguous queries by leveraging their own assessment of ambiguity (i.e., perceived ambiguity). Experimental results on question-answering datasets demonstrate that APA empowers LLMs to explicitly detect and manage ambiguous queries while retaining the ability to answer clear questions. Furthermore, our finding proves that APA excels beyond training with gold-standard labels, especially in out-of-distribution scenarios. The data and code are available at https://github.com/heyjoonkim/APA."
    },
    {
        "paperId": "1be2056a2fb91c84e5429bce41636a78e76e7c99",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Simultaneous Interpretation Corpus Construction by Large Language Models in Distant Language Pair",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.12299, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-18",
        "authors": [
            {
                "authorId": "2204051777",
                "name": "Yusuke Sakai"
            },
            {
                "authorId": "2221318283",
                "name": "Mana Makinae"
            },
            {
                "authorId": "2300756",
                "name": "Hidetaka Kamigaito"
            },
            {
                "authorId": "2266807418",
                "name": "Taro Watanabe"
            }
        ],
        "abstract": "In Simultaneous Machine Translation (SiMT), training with a simultaneous interpretation (SI) corpus is an effective method for achieving high-quality yet low-latency. However, constructing such a corpus is challenging due to high costs, and limitations in annotator capabilities, and as a result, existing SI corpora are limited. Therefore, we propose a method to convert existing speech translation (ST) corpora into interpretation-style corpora, maintaining the original word order and preserving the entire source content using Large Language Models (LLM-SI-Corpus). We demonstrate that fine-tuning SiMT models using the LLM-SI-Corpus reduces latency while achieving better quality compared to models fine-tuned with other corpora in both speech-to-text and text-to-text settings. The LLM-SI-Corpus is available at https://github.com/yusuke1997/LLM-SI-Corpus."
    },
    {
        "paperId": "7e2970084b55718344ffeb52731d0df8db4b5c89",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.12726, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-19",
        "authors": [
            {
                "authorId": "2295749596",
                "name": "Xinfeng Yuan"
            },
            {
                "authorId": "2145968425",
                "name": "Siyu Yuan"
            },
            {
                "authorId": "2297721498",
                "name": "Yuhan Cui"
            },
            {
                "authorId": "2297732848",
                "name": "Tianhe Lin"
            },
            {
                "authorId": "2108003209",
                "name": "Xintao Wang"
            },
            {
                "authorId": "2284900134",
                "name": "Rui Xu"
            },
            {
                "authorId": "5040052",
                "name": "Jiangjie Chen"
            },
            {
                "authorId": "2279399111",
                "name": "Deqing Yang"
            }
        ],
        "abstract": "Large language models (LLMs) have demonstrated impressive performance and spurred numerous AI applications, in which role-playing agents (RPAs) are particularly popular, especially for fictional characters. The prerequisite for these RPAs lies in the capability of LLMs to understand characters from fictional works. Previous efforts have evaluated this capability via basic classification tasks or characteristic imitation, failing to capture the nuanced character understanding with LLMs. In this paper, we propose evaluating LLMs\u2019 character understanding capability via the character profiling task, i.e., summarizing character profiles from corresponding materials, a widely adopted yet understudied practice for RPA development. Specifically, we construct the CROSS dataset from literature experts and assess the generated profiles by comparing them with ground truth references and evaluating their applicability in downstream tasks. Our experiments, which cover various summarization methods and LLMs, have yielded promising results. These results strongly validate the character understanding capability of LLMs. Resources are available at https://github.com/Joanna0123/character_profiling."
    },
    {
        "paperId": "5c7f465d162aade4a4c0eefb02fd7aadeebdaf58",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "LLM Evaluators Recognize and Favor Their Own Generations",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.13076, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-15",
        "authors": [
            {
                "authorId": "2279023095",
                "name": "Arjun Panickssery"
            },
            {
                "authorId": "2297768298",
                "name": "Samuel R. Bowman"
            },
            {
                "authorId": "2297816489",
                "name": "Shi Feng"
            }
        ],
        "abstract": "Self-evaluation using large language models (LLMs) has proven valuable not only in benchmarking but also methods like reward modeling, constitutional AI, and self-refinement. But new biases are introduced due to the same LLM acting as both the evaluator and the evaluatee. One such bias is self-preference, where an LLM evaluator scores its own outputs higher than others' while human annotators consider them of equal quality. But do LLMs actually recognize their own outputs when they give those texts higher scores, or is it just a coincidence? In this paper, we investigate if self-recognition capability contributes to self-preference. We discover that, out of the box, LLMs such as GPT-4 and Llama 2 have non-trivial accuracy at distinguishing themselves from other LLMs and humans. By fine-tuning LLMs, we discover a linear correlation between self-recognition capability and the strength of self-preference bias; using controlled experiments, we show that the causal explanation resists straightforward confounders. We discuss how self-recognition can interfere with unbiased evaluations and AI safety more generally."
    },
    {
        "paperId": "c29aa2e58d91e733685914b40eadb83d719c59dd",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.13207, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-19",
        "authors": [
            {
                "authorId": "2188774538",
                "name": "Shirley Wu"
            },
            {
                "authorId": "2297830746",
                "name": "Shiyu Zhao"
            },
            {
                "authorId": "19168196",
                "name": "Michihiro Yasunaga"
            },
            {
                "authorId": "2257213179",
                "name": "Kexin Huang"
            },
            {
                "authorId": "48865984",
                "name": "Kaidi Cao"
            },
            {
                "authorId": "2302855404",
                "name": "Qian Huang"
            },
            {
                "authorId": "40043851",
                "name": "V. Ioannidis"
            },
            {
                "authorId": "2691095",
                "name": "Karthik Subbian"
            },
            {
                "authorId": "2265619476",
                "name": "James Zou"
            },
            {
                "authorId": "2251205420",
                "name": "J. Leskovec"
            }
        ],
        "abstract": "Answering real-world complex queries, such as complex product search, often requires accurate retrieval from semi-structured knowledge bases that involve blend of unstructured (e.g., textual descriptions of products) and structured (e.g., entity relations of products) information. However, many previous works studied textual and relational retrieval tasks as separate topics. To address the gap, we develop STARK, a large-scale Semi-structure retrieval benchmark on Textual and Relational Knowledge Bases. Our benchmark covers three domains: product search, academic paper search, and queries in precision medicine. We design a novel pipeline to synthesize realistic user queries that integrate diverse relational information and complex textual properties, together with their ground-truth answers (items). We conduct rigorous human evaluation to validate the quality of our synthesized queries. We further enhance the benchmark with high-quality human-generated queries to provide an authentic reference. STARK serves as a comprehensive testbed for evaluating the performance of retrieval systems driven by large language models (LLMs). Our experiments suggest that STARK presents significant challenges to the current retrieval and LLM systems, highlighting the need for more capable semi-structured retrieval systems. The benchmark data and code are available on https://github.com/snap-stanford/STaRK."
    },
    {
        "paperId": "79bbcfc461722e63bd25c80679c8af51bf90359d",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Lost in Space: Probing Fine-grained Spatial Understanding in Vision and Language Resamplers",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.13594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-21",
        "authors": [
            {
                "authorId": "2060170791",
                "name": "Georgios Pantazopoulos"
            },
            {
                "authorId": "3444866",
                "name": "Alessandro Suglia"
            },
            {
                "authorId": "2265580930",
                "name": "Oliver Lemon"
            },
            {
                "authorId": "2634217",
                "name": "Arash Eshghi"
            }
        ],
        "abstract": "An effective method for combining frozen large language models (LLM) and visual encoders involves a resampler module that creates a \u2018visual prompt\u2019 which is provided to the LLM, along with the textual prompt. While this approach has enabled impressive performance across many coarse-grained tasks like image captioning and visual question answering, more fine-grained tasks that require spatial understanding have not been thoroughly examined. In this paper, we use diagnostic classifiers to measure the extent to which the visual prompt produced by the resampler encodes spatial information. Our results show that this information is largely absent from the resampler output when kept frozen during training of the classifiers. However, when the resampler and classifier are trained jointly, we observe a significant performance boost. This shows that the compression achieved by the resamplers can in principle encode the requisite spatial information, but that more object-aware objectives are needed at the pretraining stage to facilitate this capability."
    },
    {
        "paperId": "4055e37ec2f1dd6c883001d12181a7131010882d",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "\u201cA good pun is its own reword\u201d: Can Large Language Models Understand Puns?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.13599, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-21",
        "authors": [
            {
                "authorId": "2297812813",
                "name": "Zhijun Xu"
            },
            {
                "authorId": "2145968425",
                "name": "Siyu Yuan"
            },
            {
                "authorId": "2297793300",
                "name": "Lingjie Chen"
            },
            {
                "authorId": "2279399111",
                "name": "Deqing Yang"
            }
        ],
        "abstract": "Puns play a vital role in academic research due to their distinct structure and clear definition, which aid in the comprehensive analysis of linguistic humor. However, the understanding of puns in large language models (LLMs) has not been thoroughly examined, limiting their use in creative writing and humor creation. In this paper, we leverage three popular tasks, i.e., pun recognition, explanation and generation to systematically evaluate the capabilities of LLMs in pun understanding. In addition to adopting the automated evaluation metrics from prior research, we introduce new evaluation methods and metrics that are better suited to the in-context learning paradigm of LLMs. These new metrics offer a more rigorous assessment of an LLM\u2019s ability to understand puns and align more closely with human cognition than previous metrics. Our findings reveal the \u201clazy pun generation\u201d pattern and identify the primary challenges LLMs encounter in understanding puns."
    },
    {
        "paperId": "eebb45d3d4e122c3d776bff33fd82989c669406f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.13627, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-21",
        "authors": [
            {
                "authorId": "2216598559",
                "name": "Chunkit Chan"
            },
            {
                "authorId": "2109077713",
                "name": "Cheng Jiayang"
            },
            {
                "authorId": "2297769543",
                "name": "Yauwai Yim"
            },
            {
                "authorId": "2260296509",
                "name": "Zheye Deng"
            },
            {
                "authorId": "2262397115",
                "name": "Wei Fan"
            },
            {
                "authorId": "2260286743",
                "name": "Haoran Li"
            },
            {
                "authorId": "2260287837",
                "name": "Xin Liu"
            },
            {
                "authorId": "2260447535",
                "name": "Hongming Zhang"
            },
            {
                "authorId": "1587728690",
                "name": "Weiqi Wang"
            },
            {
                "authorId": "2241325169",
                "name": "Yangqiu Song"
            }
        ],
        "abstract": "Large Language Models (LLMs) have sparked substantial interest and debate concerning their potential emergence of Theory of Mind (ToM) ability. Theory of mind evaluations currently focuses on testing models using machine-generated data or game settings prone to shortcuts and spurious correlations, which lacks evaluation of machine ToM ability in real-world human interaction scenarios. This poses a pressing demand to develop new real-world scenario benchmarks. We introduce NegotiationToM, a new benchmark designed to stress-test machine ToM in real-world negotiation surrounding covered multi-dimensional mental states (i.e., desires, beliefs, and intentions). Our benchmark builds upon the Belief-Desire-Intention (BDI) agent modeling theory and conducts the necessary empirical experiments to evaluate large language models. Our findings demonstrate that NegotiationToM is challenging for state-of-the-art LLMs, as they consistently perform significantly worse than humans, even when employing the chain-of-thought (CoT) method."
    },
    {
        "paperId": "38d9061a2e7d777da6fa96a6962ffbafa0762972",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Information Re-Organization Improves Reasoning in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.13985, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-22",
        "authors": [
            {
                "authorId": "2149478197",
                "name": "Xiaoxia Cheng"
            },
            {
                "authorId": "2092670555",
                "name": "Zeqi Tan"
            },
            {
                "authorId": "2292408874",
                "name": "Weiming Lu"
            }
        ],
        "abstract": "Improving the reasoning capabilities of large language models (LLMs) has attracted considerable interest. Recent approaches primarily focus on improving the reasoning process to yield a more precise final answer. However, in scenarios involving contextually aware reasoning, these methods neglect the importance of first identifying logical relationships from the context before proceeding with the reasoning. This oversight could lead to a superficial understanding and interaction with the context, potentially undermining the quality and reliability of the reasoning outcomes. In this paper, we propose an information re-organization (InfoRE) method before proceeding with the reasoning to enhance the reasoning ability of LLMs. Our re-organization method involves initially extracting logical relationships from the contextual content, such as documents or paragraphs, and subsequently pruning redundant content to minimize noise. Then, we utilize the re-organized information in the reasoning process. This enables LLMs to deeply understand the contextual content by clearly perceiving these logical relationships, while also ensuring high-quality responses by eliminating potential noise. To demonstrate the effectiveness of our approach in improving the reasoning ability, we conduct experiments using Llama2-70B, GPT-3.5, and GPT-4 on various contextually aware multi-hop reasoning tasks. Using only a zero-shot setting, our method achieves an average absolute improvement of 4% across all tasks, highlighting its potential to improve the reasoning performance of LLMs. Our source code is available at https://github.com/hustcxx/InfoRE."
    },
    {
        "paperId": "72ac28fc5d4f6b1da6051927441cedadfda22283",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.15155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-22",
        "authors": [
            {
                "authorId": "2107937893",
                "name": "Y. Kim"
            },
            {
                "authorId": "2298091351",
                "name": "Chanwoo Park"
            },
            {
                "authorId": "2299119896",
                "name": "H. Jeong"
            },
            {
                "authorId": "2298459121",
                "name": "Yik Siu Chan"
            },
            {
                "authorId": "2268804791",
                "name": "Xuhai Xu"
            },
            {
                "authorId": "2266474644",
                "name": "Daniel McDuff"
            },
            {
                "authorId": "2266253563",
                "name": "C. Breazeal"
            },
            {
                "authorId": "2756001",
                "name": "Hae Won Park"
            }
        ],
        "abstract": "Foundation models are becoming valuable tools in medicine. Yet despite their promise, the best way to leverage Large Language Models (LLMs) in complex medical tasks remains an open question. We introduce a novel multi-agent framework, named Medical Decision-making Agents (MDAgents) that helps address this gap by automatically assigning a collaboration structure to a team of LLMs. The assigned solo or group collaboration structure is tailored to the medical task at hand, emulating real-world medical decision-making processes adapted to tasks of varying complexities. We evaluate our framework and baseline methods using state-of-the-art LLMs across a suite of real-world medical knowledge and medical diagnosis benchmarks, including a comparison of LLMs' medical complexity classification against human physicians. MDAgents achieved the best performance in seven out of ten benchmarks on tasks requiring an understanding of medical knowledge and multi-modal reasoning, showing a significant improvement of up to 4.2% (p<0.05) compared to previous methods' best performances. Ablation studies reveal that MDAgents effectively determines medical complexity to optimize for efficiency and accuracy across diverse medical tasks. Notably, the combination of moderator review and external medical knowledge in group collaboration resulted in an average accuracy improvement of 11.8%. Our code can be found at https://github.com/mitmedialab/MDAgents."
    },
    {
        "paperId": "2d76a048c8a20e22efeed3697d684eb5a20fd7ac",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Student Data Paradox and Curious Case of Single Student-Tutor Model: Regressive Side Effects of Training LLMs for Personalized Learning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.15156, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-23",
        "authors": [
            {
                "authorId": "1720691070",
                "name": "Shashank Sonkar"
            },
            {
                "authorId": "2143066732",
                "name": "Naiming Liu"
            },
            {
                "authorId": "2249763597",
                "name": "R. Baraniuk"
            }
        ],
        "abstract": "The pursuit of personalized education has led to the integration of Large Language Models (LLMs) in developing intelligent tutoring systems. To better understand and adapt to individual student needs, including their misconceptions, LLMs need to be trained on extensive datasets of student-tutor dialogues. Our research uncovers a fundamental challenge in this approach: the ``Student Data Paradox.'' This paradox emerges when LLMs, trained on student data to understand learner behavior, inadvertently compromise their own factual knowledge and reasoning abilities. We investigate this paradox by training state-of-the-art language models on student-tutor dialogue datasets and evaluating their performance across multiple benchmarks. These benchmarks assess various aspects of language model capabilities, including reasoning, truthfulness, and common sense understanding. Our findings reveal significant declines in the models' performance across these diverse benchmarks, indicating a broad impact on their capabilities when trained to model student behavior. Our research makes two primary contributions: (1) empirical demonstration of the Student Data Paradox through quantitative analysis of model performance, and (2) introduction of ``hallucination tokens'' as a mitigation strategy. These tokens, while improving performance, highlight the persistent challenge of balancing accurate student behavior modeling with maintaining the LLM's integrity as an educational tool. This study emphasizes the need for innovative solutions to reconcile the conflicting goals of faithfully understanding diverse student cognition while preserving the model's ability to provide accurate information and guidance."
    },
    {
        "paperId": "a670b9eeec59421e096056dd7eb48ced1ab83be1",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.15238, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-23",
        "authors": [
            {
                "authorId": "2266973626",
                "name": "Weiyan Shi"
            },
            {
                "authorId": "2327163089",
                "name": "Ryan Li"
            },
            {
                "authorId": "2298017529",
                "name": "Yutong Zhang"
            },
            {
                "authorId": "1399135100",
                "name": "Caleb Ziems"
            },
            {
                "authorId": "2297996303",
                "name": "Chunhua yu"
            },
            {
                "authorId": "2539798",
                "name": "R. Horesh"
            },
            {
                "authorId": "2290804169",
                "name": "Rog'erio Abreu de Paula"
            },
            {
                "authorId": "2261686760",
                "name": "Diyi Yang"
            }
        ],
        "abstract": "To enhance language models' cultural awareness, we design a generalizable pipeline to construct cultural knowledge bases from different online communities on a massive scale. With the pipeline, we construct CultureBank, a knowledge base built upon users' self-narratives with 12K cultural descriptors sourced from TikTok and 11K from Reddit. Unlike previous cultural knowledge resources, CultureBank contains diverse views on cultural descriptors to allow flexible interpretation of cultural knowledge, and contextualized cultural scenarios to help grounded evaluation. With CultureBank, we evaluate different LLMs' cultural awareness, and identify areas for improvement. We also fine-tune a language model on CultureBank: experiments show that it achieves better performances on two downstream cultural tasks in a zero-shot setting. Finally, we offer recommendations based on our findings for future culturally aware language technologies. The project page is https://culturebank.github.io . The code and model is at https://github.com/SALT-NLP/CultureBank . The released CultureBank dataset is at https://huggingface.co/datasets/SALT-NLP/CultureBank ."
    },
    {
        "paperId": "7b508f5a3168e2ecfcb821752dcae576905a50c5",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.16698, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-25",
        "authors": [
            {
                "authorId": "2298275529",
                "name": "Giorgio Piatti"
            },
            {
                "authorId": "2111472502",
                "name": "Zhijing Jin"
            },
            {
                "authorId": "2272856326",
                "name": "Max Kleiman-Weiner"
            },
            {
                "authorId": "2261483511",
                "name": "Bernhard Sch\u00f6lkopf"
            },
            {
                "authorId": "2790926",
                "name": "Mrinmaya Sachan"
            },
            {
                "authorId": "2105984203",
                "name": "Rada Mihalcea"
            }
        ],
        "abstract": "As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge. We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes. We develop an LLM-based agent architecture and test it with the leading open and closed LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54%. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage\"Universalization\"-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface."
    },
    {
        "paperId": "59516436839bb0dd90eee34e913bb9306f383619",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "REBEL: Reinforcement Learning via Regressing Relative Rewards",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.16767, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-25",
        "authors": [
            {
                "authorId": "2298394607",
                "name": "Zhaolin Gao"
            },
            {
                "authorId": "2295558747",
                "name": "Jonathan D. Chang"
            },
            {
                "authorId": "1858443952",
                "name": "Wenhao Zhan"
            },
            {
                "authorId": "2283841578",
                "name": "Owen Oertell"
            },
            {
                "authorId": "2073365488",
                "name": "Gokul Swamy"
            },
            {
                "authorId": "11963742",
                "name": "Kiant\u00e9 Brantley"
            },
            {
                "authorId": "2243190230",
                "name": "Thorsten Joachims"
            },
            {
                "authorId": "1756566",
                "name": "J. Bagnell"
            },
            {
                "authorId": "2257354792",
                "name": "Jason D. Lee"
            },
            {
                "authorId": "2295610342",
                "name": "Wen Sun"
            }
        ],
        "abstract": "While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications, including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping), and is notorious for its sensitivity to the precise implementation of these components. In response, we take a step back and ask what a minimalist RL algorithm for the era of generative models would look like. We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the relative reward between two completions to a prompt in terms of the policy, enabling strikingly lightweight implementation. In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature. REBEL can also cleanly incorporate offline data and be extended to handle the intransitive preferences we frequently see in practice. Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally efficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong performance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard."
    },
    {
        "paperId": "82460f7995f66f3f035f34ecbd2c82b024282529",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Make Your LLM Fully Utilize the Context",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.16811, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-25",
        "authors": [
            {
                "authorId": "2119217081",
                "name": "Shengnan An"
            },
            {
                "authorId": "2264290103",
                "name": "Zexiong Ma"
            },
            {
                "authorId": "2277149729",
                "name": "Zeqi Lin"
            },
            {
                "authorId": "2263567225",
                "name": "Nanning Zheng"
            },
            {
                "authorId": "153249455",
                "name": "Jian-Guang Lou"
            }
        ],
        "abstract": "While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge. We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information. Based on this intuition, our study presents information-intensive (IN2) training, a purely data-driven solution to overcome lost-in-the-middle. Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) fine-grained information awareness on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the integration and reasoning of information from two or more short segments. Through applying this information-intensive training on Mistral-7B, we present FILM-7B (FILl-in-the-Middle). To thoroughly assess the ability of FILM-7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window. Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3->59.2 accuracy on MMLU). Github Link: https://github.com/microsoft/FILM."
    },
    {
        "paperId": "4095e273f5efc419d3b8bb36a2e3c564c346fc33",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.18239, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-28",
        "authors": [
            {
                "authorId": "2051552791",
                "name": "Jinghan Jia"
            },
            {
                "authorId": "2155369380",
                "name": "Yihua Zhang"
            },
            {
                "authorId": "2108441128",
                "name": "Yimeng Zhang"
            },
            {
                "authorId": "2168533640",
                "name": "Jiancheng Liu"
            },
            {
                "authorId": "2180166734",
                "name": "Bharat Runwal"
            },
            {
                "authorId": "7753616",
                "name": "James Diffenderfer"
            },
            {
                "authorId": "1749353",
                "name": "B. Kailkhura"
            },
            {
                "authorId": "2254478722",
                "name": "Sijia Liu"
            }
        ],
        "abstract": "Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility beyond the scope of unlearning. While interest in studying LLM unlearning is growing, the impact of the optimizer choice for LLM unlearning remains unexplored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between second-order optimization and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order optimization-based LLM unlearning framework, termed Second-Order UnLearning (SOUL), which extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, indicating that second-order optimization offers an effective and broadly applicable solution for LLM unlearning."
    },
    {
        "paperId": "3e46e2a679d5220169ac4f3ae1111e8a7052aee2",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Evaluating Readability and Faithfulness of Concept-based Explanations",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.18533, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-29",
        "authors": [
            {
                "authorId": "2299066885",
                "name": "Meng Li"
            },
            {
                "authorId": "2223875821",
                "name": "Haoran Jin"
            },
            {
                "authorId": "2297308396",
                "name": "Ruixuan Huang"
            },
            {
                "authorId": "2297195029",
                "name": "Zhihao Xu"
            },
            {
                "authorId": "2267335798",
                "name": "Defu Lian"
            },
            {
                "authorId": "2298932831",
                "name": "Zijia Lin"
            },
            {
                "authorId": "2299285372",
                "name": "Di Zhang"
            },
            {
                "authorId": "2298928922",
                "name": "Xiting Wang"
            }
        ],
        "abstract": "With the growing popularity of general-purpose Large Language Models (LLMs), comes a need for more global explanations of model behaviors. Concept-based explanations arise as a promising avenue for explaining high-level patterns learned by LLMs. Yet their evaluation poses unique challenges, especially due to their non-local nature and high dimensional representation in a model\u2019s hidden space. Current methods approach concepts from different perspectives, lacking a unified formalization. This makes evaluating the core measures of concepts, namely faithfulness or readability, challenging. To bridge the gap, we introduce a formal definition of concepts generalizing to diverse concept-based explanations\u2019 settings. Based on this, we quantify the faithfulness of a concept explanation via perturbation. We ensure adequate perturbation in the high-dimensional space for different concepts via an optimization problem. Readability is approximated via an automatic and deterministic measure, quantifying the coherence of patterns that maximally activate a concept while aligning with human understanding. Finally, based on measurement theory, we apply a meta-evaluation method for evaluating these measures, generalizable to other types of explanations or tasks as well. Extensive experimental analysis has been conducted to inform the selection of explanation evaluation measures."
    },
    {
        "paperId": "c85c3ba2c34543d906c349ccca5bfc243f2ac3f4",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "RepEval: Effective Text Evaluation with LLM Representation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.19563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-04-30",
        "authors": [
            {
                "authorId": "2125657739",
                "name": "Shuqian Sheng"
            },
            {
                "authorId": "2110289458",
                "name": "Yi Xu"
            },
            {
                "authorId": "2257868315",
                "name": "Tianhang Zhang"
            },
            {
                "authorId": "2284280591",
                "name": "Zanwei Shen"
            },
            {
                "authorId": "1922573",
                "name": "Luoyi Fu"
            },
            {
                "authorId": "2111184548",
                "name": "Jiaxin Ding"
            },
            {
                "authorId": "2215516998",
                "name": "Lei Zhou"
            },
            {
                "authorId": "2259880643",
                "name": "Xinbing Wang"
            },
            {
                "authorId": "2111168706",
                "name": "Cheng Zhou"
            }
        ],
        "abstract": "The era of Large Language Models (LLMs) raises new demands for automatic evaluation metrics, which should be adaptable to various application scenarios while maintaining low cost and effectiveness. Traditional metrics for automatic text evaluation are often tailored to specific scenarios, while LLM-based evaluation metrics are costly, requiring fine-tuning or rely heavily on the generation capabilities of LLMs. Besides, previous LLM-based metrics ignore the fact that, within the space of LLM representations, there exist direction vectors that indicate the estimation of text quality. To this end, we introduce RepEval, a metric that leverages the projection of LLM representations for evaluation. Through simple prompt modifications, RepEval can easily transition to various tasks, requiring only minimal sample pairs for direction vector construction. Results on fourteen datasets across two evaluation tasks demonstrate the high effectiveness of our method, which exhibits a higher correlation with human judgments than previous methods, even in complex evaluation scenarios involving pair-wise selection under nuanced aspects. Our work underscores the richness of information regarding text quality embedded within LLM representations, offering insights for the development of new metrics."
    },
    {
        "paperId": "1a6d55fc6525eee6734b029cc4a3a07f240d75f1",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Creative Problem Solving in Large Language and Vision Models - What Would it Take?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.01453, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-02",
        "authors": [
            {
                "authorId": "2299325823",
                "name": "Lakshmi Nair"
            },
            {
                "authorId": "51512973",
                "name": "Evana Gizzi"
            },
            {
                "authorId": "1715858",
                "name": "Jivko Sinapov"
            }
        ],
        "abstract": "We advocate for a strong integration of Computational Creativity (CC) with research in large language and vision models (LLVMs) to address a key limitation of these models, i.e., creative problem solving. We present preliminary experiments showing how CC principles can be applied to address this limitation. Our goal is to foster discussions on creative problem solving in LLVMs and CC at prestigious ML venues. Our code is available at: https://github.com/lnairGT/creative-problem-solving-LLMs"
    },
    {
        "paperId": "83d49eaa89c4d06ad25ffc847ed8210033f62419",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Analyzing the Role of Semantic Representations in the Era of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.01502, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-02",
        "authors": [
            {
                "authorId": "2111472502",
                "name": "Zhijing Jin"
            },
            {
                "authorId": "2265625277",
                "name": "Yuen Chen"
            },
            {
                "authorId": "2186876774",
                "name": "Fernando Gonzalez"
            },
            {
                "authorId": "2299327550",
                "name": "Jiarui Liu"
            },
            {
                "authorId": "2299486043",
                "name": "Jiayi Zhang"
            },
            {
                "authorId": "2299328790",
                "name": "Julian Michael"
            },
            {
                "authorId": "2261483511",
                "name": "Bernhard Sch\u00f6lkopf"
            },
            {
                "authorId": "2299327308",
                "name": "Mona T. Diab"
            }
        ],
        "abstract": "Traditionally, natural language processing (NLP) models often use a rich set of features created by linguistic expertise, such as semantic representations. However, in the era of large language models (LLMs), more and more tasks are turned into generic, end-to-end sequence generation problems. In this paper, we investigate the question: what is the role of semantic representations in the era of LLMs? Specifically, we investigate the effect of Abstract Meaning Representation (AMR) across five diverse NLP tasks. We propose an AMR-driven chain-of-thought prompting method, which we call AMRCOT, and find that it generally hurts performance more than it helps. To investigate what AMR may have to offer on these tasks, we conduct a series of analysis experiments. We find that it is difficult to predict which input examples AMR may help or hurt on, but errors tend to arise with multi-word expressions, named entities, and in the final inference step where the LLM must connect its reasoning over the AMR to its prediction. We recommend focusing on these areas for future work in semantic representations for LLMs. Our code: https://github.com/causalNLP/amr_llm"
    },
    {
        "paperId": "6002f0d76189ad99ed3cb2d9945e8be869b96244",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Understanding LLMs Requires More Than Statistical Generalization",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.01964, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-03",
        "authors": [
            {
                "authorId": "1382657853",
                "name": "Patrik Reizinger"
            },
            {
                "authorId": "2299943014",
                "name": "Szilvia Ujv'ary"
            },
            {
                "authorId": "2214094238",
                "name": "Anna M'esz'aros"
            },
            {
                "authorId": "2075495811",
                "name": "A. Kerekes"
            },
            {
                "authorId": "40634590",
                "name": "Wieland Brendel"
            },
            {
                "authorId": "2188240184",
                "name": "Ferenc Husz'ar"
            }
        ],
        "abstract": "The last decade has seen blossoming research in deep learning theory attempting to answer,\"Why does deep learning generalize?\"A powerful shift in perspective precipitated this progress: the study of overparametrized models in the interpolation regime. In this paper, we argue that another perspective shift is due, since some of the desirable qualities of LLMs are not a consequence of good statistical generalization and require a separate theoretical explanation. Our core argument relies on the observation that AR probabilistic models are inherently non-identifiable: models zero or near-zero KL divergence apart -- thus, equivalent test loss -- can exhibit markedly different behaviors. We support our position with mathematical examples and empirical observations, illustrating why non-identifiability has practical relevance through three case studies: (1) the non-identifiability of zero-shot rule extrapolation; (2) the approximate non-identifiability of in-context learning; and (3) the non-identifiability of fine-tunability. We review promising research directions focusing on LLM-relevant generalization measures, transferability, and inductive biases."
    },
    {
        "paperId": "be5e9c1c3ae0a2938328da044448164309ddaf96",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "COPAL: Continual Pruning in Large Language Generative Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.02347, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-02",
        "authors": [
            {
                "authorId": "51128743",
                "name": "Srikanth Malla"
            },
            {
                "authorId": "2300246153",
                "name": "Joon Hee Choi"
            },
            {
                "authorId": "2301128189",
                "name": "Chiho Choi"
            }
        ],
        "abstract": "Adapting pre-trained large language models to different domains in natural language processing requires two key considerations: high computational demands and model's inability to continual adaptation. To simultaneously address both issues, this paper presents COPAL (COntinual Pruning in Adaptive Language settings), an algorithm developed for pruning large language generative models under a continual model adaptation setting. While avoiding resource-heavy finetuning or retraining, our pruning process is guided by the proposed sensitivity analysis. The sensitivity effectively measures model's ability to withstand perturbations introduced by the new dataset and finds model's weights that are relevant for all encountered datasets. As a result, COPAL allows seamless model adaptation to new domains while enhancing the resource efficiency. Our empirical evaluation on a various size of LLMs show that COPAL outperforms baseline models, demonstrating its efficacy in efficiency and adaptability."
    },
    {
        "paperId": "dd5ef6ceed4e77eb2702b40ba4afece05c1b0af8",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.02501, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-03",
        "authors": [
            {
                "authorId": "2162122419",
                "name": "Hyeong Kyu Choi"
            },
            {
                "authorId": "2300129169",
                "name": "Yixuan Li"
            }
        ],
        "abstract": "Large Language Models (LLMs) are trained on massive text corpora, which are encoded with diverse personality traits. This triggers an interesting goal of eliciting a desired personality trait from the LLM, and probing its behavioral preferences. Accordingly, we formalize the persona elicitation task, aiming to customize LLM behaviors to align with a target persona. We present Persona In-Context Learning (PICLe), a novel persona elicitation framework grounded in Bayesian inference. At the core, PICLe introduces a new ICL example selection criterion based on likelihood ratio, which is designed to optimally guide the model in eliciting a specific target persona. We demonstrate the effectiveness of PICLe through extensive comparisons against baseline methods across three contemporary LLMs. Code is available at https://github.com/deeplearning-wisc/picle."
    },
    {
        "paperId": "afff235c6d8e196be4d2dafaee01a03595a66de1",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Learning from Students: Applying t-Distributions to Explore Accurate and Efficient Formats for LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.03103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-06",
        "authors": [
            {
                "authorId": "52022655",
                "name": "Jordan Dotzel"
            },
            {
                "authorId": "2265517647",
                "name": "Yuzong Chen"
            },
            {
                "authorId": "2284864129",
                "name": "Bahaa Kotb"
            },
            {
                "authorId": "2300142689",
                "name": "Sushma Prasad"
            },
            {
                "authorId": "2300129355",
                "name": "Gang Wu"
            },
            {
                "authorId": "2300312440",
                "name": "Sheng Li"
            },
            {
                "authorId": "2295227476",
                "name": "Mohamed S. Abdelfattah"
            },
            {
                "authorId": "2284934481",
                "name": "Zhiru Zhang"
            }
        ],
        "abstract": "The increasing size of large language models (LLMs) traditionally requires low-precision integer formats to meet strict latency and power demands. Yet recently, alternative formats such as Normal Float (NF4) have increased model accuracy at the cost of increased chip area. In this work, we first conduct a large-scale analysis of LLM weights and activations across 30 networks and conclude that most distributions follow a Student's t-distribution. We then derive a new theoretically optimal format, Student Float (SF4), that improves over NF4 across modern LLMs, for example increasing the average accuracy on LLaMA2-7B by 0.76% across tasks. Using this format as a high-accuracy reference, we then propose augmenting E2M1 with two variants of supernormal support for higher model accuracy. Finally, we explore the quality and efficiency frontier across 11 datatypes by evaluating their model accuracy and hardware complexity. We discover a Pareto curve composed of INT4, E2M1, and E2M1 with supernormal support, which offers a continuous tradeoff between model accuracy and chip area. For example, E2M1 with supernormal support increases the accuracy of Phi-2 by up to 2.19% with 1.22% area overhead, enabling more LLM-based applications to be run at four bits. The supporting code is hosted at https://github.com/cornell-zhang/llm-datatypes."
    },
    {
        "paperId": "8a6cf40cf355eca89d39e5532206b2e7fff028d8",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Collage: Light-Weight Low-Precision Strategy for LLM Training",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.03637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-06",
        "authors": [
            {
                "authorId": "2300996598",
                "name": "Tao Yu"
            },
            {
                "authorId": "2300099354",
                "name": "Gaurav Gupta"
            },
            {
                "authorId": "31987844",
                "name": "Karthick Gopalswamy"
            },
            {
                "authorId": "1783716",
                "name": "Amith R. Mamidala"
            },
            {
                "authorId": "2296821955",
                "name": "Hao Zhou"
            },
            {
                "authorId": "2300098293",
                "name": "Jeffrey Huynh"
            },
            {
                "authorId": "2296621337",
                "name": "Youngsuk Park"
            },
            {
                "authorId": "2330935615",
                "name": "Ron Diamant"
            },
            {
                "authorId": "1713801",
                "name": "Anoop Deoras"
            },
            {
                "authorId": "2296789669",
                "name": "Jun Huan"
            }
        ],
        "abstract": "Large models training is plagued by the intense compute cost and limited hardware memory. A practical solution is low-precision representation but is troubled by loss in numerical accuracy and unstable training rendering the model less useful. We argue that low-precision floating points can perform well provided the error is properly compensated at the critical locations in the training process. We propose Collage which utilizes multi-component float representation in low-precision to accurately perform operations with numerical errors accounted. To understand the impact of imprecision to training, we propose a simple and novel metric which tracks the lost information during training as well as differentiates various precision strategies. Our method works with commonly used low-precision such as half-precision ($16$-bit floating points) and can be naturally extended to work with even lower precision such as $8$-bit. Experimental results show that pre-training using Collage removes the requirement of using $32$-bit floating-point copies of the model and attains similar/better training performance compared to $(16, 32)$-bit mixed-precision strategy, with up to $3.7\\times$ speedup and $\\sim 15\\%$ to $23\\%$ less memory usage in practice."
    },
    {
        "paperId": "fb5cd1a3d206eb8b96e87b3a7af59dac73ff1bc9",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Toward In-Context Teaching: Adapting Examples to Students' Misconceptions",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.04495, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-07",
        "authors": [
            {
                "authorId": "32739287",
                "name": "Alexis Ross"
            },
            {
                "authorId": "2300172860",
                "name": "Jacob Andreas"
            }
        ],
        "abstract": "When a teacher provides examples for a student to study, these examples must be informative, enabling a student to progress from their current state toward a target concept or skill. Good teachers must therefore simultaneously infer what students already know and adapt their teaching to students' changing state of knowledge. There is increasing interest in using computational models, particularly large language models, as pedagogical tools. As students, language models in particular have shown a remarkable ability to adapt to new tasks given small numbers of examples. But how effectively can these models adapt as teachers to students of different types? To study this question, we introduce a suite of models and evaluation methods we call AdapT. AdapT has two components: (1) a collection of simulated Bayesian student models that can be used for evaluation of automated teaching methods; (2) a platform for evaluation with human students, to characterize the real-world effectiveness of these methods. We additionally introduce (3) AToM, a new probabilistic model for adaptive teaching that jointly infers students' past beliefs and optimizes for the correctness of future beliefs. In evaluations of simulated students across three learning domains (fraction arithmetic, English morphology, function learning), AToM systematically outperforms LLM-based and standard Bayesian teaching models. In human experiments, both AToM and LLMs outperform non-adaptive random example selection. Our results highlight both the difficulty of the adaptive teaching task and the potential of learned adaptive models for solving it."
    },
    {
        "paperId": "f17f99b8563d1298c330020212310cf1c994bfad",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.04655, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-07",
        "authors": [
            {
                "authorId": "2072820796",
                "name": "Siqi Shen"
            },
            {
                "authorId": "2876316",
                "name": "Lajanugen Logeswaran"
            },
            {
                "authorId": "3056520",
                "name": "Moontae Lee"
            },
            {
                "authorId": "2261393055",
                "name": "Honglak Lee"
            },
            {
                "authorId": "1746416",
                "name": "Soujanya Poria"
            },
            {
                "authorId": "2105984203",
                "name": "Rada Mihalcea"
            }
        ],
        "abstract": "Large language models (LLMs) have demonstrated substantial commonsense understanding through numerous benchmark evaluations. However, their understanding of cultural commonsense remains largely unexamined. In this paper, we conduct a comprehensive examination of the capabilities and limitations of several state-of-the-art LLMs in the context of cultural commonsense tasks. Using several general and cultural commonsense benchmarks, we find that (1) LLMs have a significant discrepancy in performance when tested on culture-specific commonsense knowledge for different cultures; (2) LLMs\u2019 general commonsense capability is affected by cultural context; and (3) The language used to query the LLMs can impact their performance on cultural-related tasks.Our study points to the inherent bias in the cultural understanding of LLMs and provides insights that can help develop culturally-aware language models."
    },
    {
        "paperId": "9ec7509cd2011fa2d11f88c8515e018fd6bc27a4",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "\u201cThey are uncultured\u201d: Unveiling Covert Harms and Social Threats in LLM Generated Conversations",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.05378, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-08",
        "authors": [
            {
                "authorId": "1434554495",
                "name": "Preetam Prabhu Srikar Dammu"
            },
            {
                "authorId": "2300424756",
                "name": "Hayoung Jung"
            },
            {
                "authorId": "2300422086",
                "name": "Anjali Singh"
            },
            {
                "authorId": "2300368577",
                "name": "Monojit Choudhury"
            },
            {
                "authorId": "2300368790",
                "name": "Tanushree Mitra"
            }
        ],
        "abstract": "Large language models (LLMs) have emerged as an integral part of modern societies, powering user-facing applications such as personal assistants and enterprise applications like recruitment tools. Despite their utility, research indicates that LLMs perpetuate systemic biases. Yet, prior works on LLM harms predominantly focus on Western concepts like race and gender, often overlooking cultural concepts from other parts of the world. Additionally, these studies typically investigate \u201charm\u201d as a singular dimension, ignoring the various and subtle forms in which harms manifest. To address this gap, we introduce the Covert Harms and Social Threats (CHAST), a set of seven metrics grounded in social science literature. We utilize evaluation models aligned with human assessments to examine the presence of covert harms in LLM-generated conversations, particularly in the context of recruitment. Our experiments reveal that seven out of the eight LLMs included in this study generated conversations riddled with CHAST, characterized by malign views expressed in seemingly neutral language unlikely to be detected by existing methods. Notably, these LLMs manifested more extreme views and opinions when dealing with non-Western concepts like caste, compared to Western ones such as race."
    },
    {
        "paperId": "b50a0752e812f75cec35225ffa7649356094e5b9",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Automatic Generation of Model and Data Cards: A Step Towards Responsible AI",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.06258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-10",
        "authors": [
            {
                "authorId": "2299327550",
                "name": "Jiarui Liu"
            },
            {
                "authorId": "2301076518",
                "name": "Wenkai Li"
            },
            {
                "authorId": "2111472502",
                "name": "Zhijing Jin"
            },
            {
                "authorId": "2299327308",
                "name": "Mona T. Diab"
            }
        ],
        "abstract": "In an era of model and data proliferation in machine learning/AI especially marked by the rapid advancement of open-sourced technologies, there arises a critical need for standardized consistent documentation. Our work addresses the information incompleteness in current human-written model and data cards. We propose an automated generation approach using Large Language Models (LLMs). Our key contributions include the establishment of CardBench, a comprehensive dataset aggregated from over 4.8k model cards and 1.4k data cards, coupled with the development of the CardGen pipeline comprising a two-step retrieval process. Our approach exhibits enhanced completeness, objectivity, and faithfulness in generated model and data cards, a significant step in responsible AI documentation practices ensuring better accountability and traceability."
    },
    {
        "paperId": "ed502eae116b7ce63147d210d86bfde6bb04ba68",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.06680, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-05",
        "authors": [
            {
                "authorId": "2257315251",
                "name": "Jun Zhao"
            },
            {
                "authorId": "2301150359",
                "name": "Jingqi Tong"
            },
            {
                "authorId": "2281640548",
                "name": "Yurong Mou"
            },
            {
                "authorId": "2240574799",
                "name": "Ming Zhang"
            },
            {
                "authorId": "2291900515",
                "name": "Qi Zhang"
            },
            {
                "authorId": "2257129989",
                "name": "Xuanjing Huang"
            }
        ],
        "abstract": "Human cognition exhibits systematic compositionality, the algebraic ability to generate infinite novel combinations from finite learned components, which is the key to understanding and reasoning about complex logic. In this work, we investigate the compositionality of large language models (LLMs) in mathematical reasoning. Specifically, we construct a new dataset MathTrap by introducing carefully designed logical traps into the problem descriptions of MATH and GSM8K. Since problems with logical flaws are quite rare in the real world, these represent \u201cunseen\u201d cases to LLMs. Solving these requires the models to systematically compose (1) the mathematical knowledge involved in the original problems with (2) knowledge related to the introduced traps. Our experiments show that while LLMs possess both components of requisite knowledge, they do not spontaneously combine them to handle these novel cases. We explore several methods to mitigate this deficiency, such as natural language prompts, few-shot demonstrations, and fine-tuning. We find that LLMs\u2019 performance can be improved through the above external intervention. Overall, systematic compositionality remains an open challenge for large language models."
    },
    {
        "paperId": "8cffdafde24bdc852726ff29b4b2984f976d61c6",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Intent Resolution in LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.08760, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-14",
        "authors": [
            {
                "authorId": "1388021166",
                "name": "Akhila Yerukola"
            },
            {
                "authorId": "2111868853",
                "name": "Saujas Vaduguru"
            },
            {
                "authorId": "2238210389",
                "name": "Daniel Fried"
            },
            {
                "authorId": "2729164",
                "name": "Maarten Sap"
            }
        ],
        "abstract": "Humans often express their communicative intents indirectly or non-literally, which requires their interlocutors -- human or AI -- to understand beyond the literal meaning of words. While most existing work has focused on discriminative evaluations, we present a new approach to generatively evaluate large language models' (LLMs') intention understanding by examining their responses to non-literal utterances. Ideally, an LLM should respond in line with the true intention of a non-literal utterance, not its literal interpretation. Our findings show that LLMs struggle to generate pragmatically relevant responses to non-literal language, achieving only 50-55% accuracy on average. While explicitly providing oracle intentions significantly improves performance (e.g., 75% for Mistral-Instruct), this still indicates challenges in leveraging given intentions to produce appropriate responses. Using chain-of-thought to make models spell out intentions yields much smaller gains (60% for Mistral-Instruct). These findings suggest that LLMs are not yet effective pragmatic interlocutors, highlighting the need for better approaches for modeling intentions and utilizing them for pragmatic generation."
    },
    {
        "paperId": "87da4349c42fef4d1f4926a68c09a486661e9c2d",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "DEBATE: Devil's Advocate-Based Assessment and Text Evaluation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.09935, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-16",
        "authors": [
            {
                "authorId": "2301536519",
                "name": "Alex G. Kim"
            },
            {
                "authorId": "2301542433",
                "name": "Keonwoo Kim"
            },
            {
                "authorId": "2337776253",
                "name": "Sangwon Yoon"
            }
        ],
        "abstract": "As natural language generation (NLG) models have become prevalent, systematically assessing the quality of machine-generated texts has become increasingly important. Recent studies introduce LLM-based evaluators that operate as reference-free metrics, demonstrating their capability to adeptly handle novel tasks. However, these models generally rely on a single-agent approach, which, we argue, introduces an inherent limit to their performance. This is because there exist biases in LLM agent's responses, including preferences for certain text structure or content. In this work, we propose DEBATE, an NLG evaluation framework based on multi-agent scoring system augmented with a concept of Devil's Advocate. Within the framework, one agent is instructed to criticize other agents' arguments, potentially resolving the bias in LLM agent's answers. DEBATE substantially outperforms the previous state-of-the-art methods in two meta-evaluation benchmarks in NLG evaluation, SummEval and TopicalChat. We also show that the extensiveness of debates among agents and the persona of an agent can influence the performance of evaluators."
    },
    {
        "paperId": "f0c32c08af45634e5dc977ab993bb0e9c6c0bb45",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Speaker Verification in Agent-Generated Conversations",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.10150, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-16",
        "authors": [
            {
                "authorId": "2261393344",
                "name": "Yizhe Yang"
            },
            {
                "authorId": "2261394092",
                "name": "Heyan Huang"
            },
            {
                "authorId": "2181025",
                "name": "Palakorn Achananuparp"
            },
            {
                "authorId": "2285228529",
                "name": "Jing Jiang"
            },
            {
                "authorId": "2285118782",
                "name": "Ee-Peng Lim"
            }
        ],
        "abstract": "The recent success of large language models (LLMs) has attracted widespread interest to develop role-playing conversational agents personalized to the characteristics and styles of different speakers to enhance their abilities to perform both general and special purpose dialogue tasks. However, the ability to personalize the generated utterances to speakers, whether conducted by human or LLM, has not been well studied. To bridge this gap, our study introduces a novel evaluation challenge: speaker verification in agent-generated conversations, which aimed to verify whether two sets of utterances originate from the same speaker. To this end, we assemble a large dataset collection encompassing thousands of speakers and their utterances. We also develop and evaluate speaker verification models under experiment setups. We further utilize the speaker verification models to evaluate the personalization abilities of LLM-based role-playing models. Comprehensive experiments suggest that the current role-playing models fail in accurately mimicking speakers, primarily due to their inherent linguistic characteristics."
    },
    {
        "paperId": "af4328bc1eb2d80e4fc035b143cecba5580d39ef",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.10443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-16",
        "authors": [
            {
                "authorId": "2221128059",
                "name": "Matthew Raffel"
            },
            {
                "authorId": "2214580025",
                "name": "Victor Agostinelli"
            },
            {
                "authorId": "2272959539",
                "name": "Lizhong Chen"
            }
        ],
        "abstract": "Large language models (LLMs) have achieved state-of-the-art performance in various language processing tasks, motivating their adoption in simultaneous translation. Current fine-tuning methods to adapt LLMs for simultaneous translation focus on prompting optimization strategies using either data augmentation or prompt structure modifications. However, these methods suffer from several issues, such as unnecessarily expanded training sets, computational inefficiency from dumping the key and value cache, increased prompt sizes, or restriction to a single decision policy. To eliminate these issues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs for simultaneous translation. It utilizes a novel attention mask approach that models simultaneous translation during fine-tuning by masking attention for a desired decision policy. Applying the proposed SimulMask on a Falcon LLM for the IWSLT 2017 dataset, we have observed a significant translation quality improvement compared to state-of-the-art prompting optimization strategies on five language pairs while reducing the computational cost."
    },
    {
        "paperId": "093f9b0cd66a2356f18ecac15cf4209edae1ca4c",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.10548, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-17",
        "authors": [
            {
                "authorId": "2302155945",
                "name": "Anwoy Chatterjee"
            },
            {
                "authorId": "2210797473",
                "name": "Eshaan Tanwar"
            },
            {
                "authorId": "50757931",
                "name": "Subhabrata Dutta"
            },
            {
                "authorId": "2256999352",
                "name": "Tanmoy Chakraborty"
            }
        ],
        "abstract": "Large Language Models (LLMs) have transformed NLP with their remarkable In-context Learning (ICL) capabilities. Automated assistants based on LLMs are gaining popularity; however, adapting them to novel tasks is still challenging. While colossal models excel in zero-shot performance, their computational demands limit widespread use, and smaller language models struggle without context. This paper investigates whether LLMs can generalize from labeled examples of predefined tasks to novel tasks. Drawing inspiration from biological neurons and the mechanistic interpretation of the Transformer architecture, we explore the potential for information sharing across tasks. We design a cross-task prompting setup with three LLMs and show that LLMs achieve significant performance improvements despite no examples from the target task in the context. Cross-task prompting leads to a remarkable performance boost of 107% for LLaMA-2 7B, 18.6% for LLaMA-2 13B, and 3.2% for GPT 3.5 on average over zero-shot prompting, and performs comparable to standard in-context learning. The effectiveness of generating pseudo-labels for in-task examples is demonstrated, and our analyses reveal a strong correlation between the effect of cross-task examples and model activation similarities in source and target input tokens. This paper offers a first-of-its-kind exploration of LLMs' ability to solve novel tasks based on contextual signals from different task examples."
    },
    {
        "paperId": "6907de99f81d7bff76571968d44ea7349684aa38",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Can AI Relate: Testing Large Language Model Response for Mental Health Support",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.12021, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-20",
        "authors": [
            {
                "authorId": "2278633302",
                "name": "Saadia Gabriel"
            },
            {
                "authorId": "2294116164",
                "name": "Isha Puri"
            },
            {
                "authorId": "2268804791",
                "name": "Xuhai Xu"
            },
            {
                "authorId": "2339161433",
                "name": "Matteo Malgaroli"
            },
            {
                "authorId": "2294092232",
                "name": "Marzyeh Ghassemi"
            }
        ],
        "abstract": "Large language models (LLMs) are already being piloted for clinical use in hospital systems like NYU Langone, Dana-Farber and the NHS. A proposed deployment use case is psychotherapy, where a LLM-powered chatbot can treat a patient undergoing a mental health crisis. Deployment of LLMs for mental health response could hypothetically broaden access to psychotherapy and provide new possibilities for personalizing care. However, recent high-profile failures, like damaging dieting advice offered by the Tessa chatbot to patients with eating disorders, have led to doubt about their reliability in high-stakes and safety-critical settings. In this work, we develop an evaluation framework for determining whether LLM response is a viable and ethical path forward for the automation of mental health treatment. Our framework measures equity in empathy and adherence of LLM responses to motivational interviewing theory. Using human evaluation with trained clinicians and automatic quality-of-care metrics grounded in psychology research, we compare the responses provided by peer-to-peer responders to those provided by a state-of-the-art LLM. We show that LLMs like GPT-4 use implicit and explicit cues to infer patient demographics like race. We then show that there are statistically significant discrepancies between patient subgroups: Responses to Black posters consistently have lower empathy than for any other demographic group (2%-13% lower than the control group). Promisingly, we do find that the manner in which responses are generated significantly impacts the quality of the response. We conclude by proposing safety guidelines for the potential deployment of LLMs for mental health response."
    },
    {
        "paperId": "011193852a9f1ff60e36b281686aadb6f59bc6d3",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.12063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-20",
        "authors": [
            {
                "authorId": "2270989965",
                "name": "Tong Zhang"
            },
            {
                "authorId": "2273352723",
                "name": "Peixin Qin"
            },
            {
                "authorId": "145843537",
                "name": "Yang Deng"
            },
            {
                "authorId": "2270744273",
                "name": "Chen Huang"
            },
            {
                "authorId": "2269811383",
                "name": "Wenqiang Lei"
            },
            {
                "authorId": "2271280216",
                "name": "Junhong Liu"
            },
            {
                "authorId": "2273538061",
                "name": "Dingnan Jin"
            },
            {
                "authorId": "2272276724",
                "name": "Hongru Liang"
            },
            {
                "authorId": "2257036129",
                "name": "Tat-Seng Chua"
            }
        ],
        "abstract": "Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct ~12K high-quality data to assess the strengths, weaknesses, and potential risks of various off-the-shelf LLMs. Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting. These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity. Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge. In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs. Our dataset is available at https://github.com/zt991211/CLAMBER"
    },
    {
        "paperId": "79846e748a9e6324742c300453ec293661e90884",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.12205, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-20",
        "authors": [
            {
                "authorId": "150010392",
                "name": "A. Didolkar"
            },
            {
                "authorId": "1996705",
                "name": "Anirudh Goyal"
            },
            {
                "authorId": "145604319",
                "name": "Nan Rosemary Ke"
            },
            {
                "authorId": "2302363710",
                "name": "Siyuan Guo"
            },
            {
                "authorId": "2259912893",
                "name": "Michal Valko"
            },
            {
                "authorId": "2302799561",
                "name": "Timothy Lillicrap"
            },
            {
                "authorId": "2302320004",
                "name": "D. Rezende"
            },
            {
                "authorId": "1865800402",
                "name": "Y. Bengio"
            },
            {
                "authorId": "2114809026",
                "name": "M. Mozer"
            },
            {
                "authorId": "2261737783",
                "name": "Sanjeev Arora"
            }
        ],
        "abstract": "Metacognitive knowledge refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans. To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems."
    },
    {
        "paperId": "d1f769b04c53e7197654559fa386259467d30ab5",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.12933, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-21",
        "authors": [
            {
                "authorId": "2090055589",
                "name": "Bilgehan Sel"
            },
            {
                "authorId": "2302410667",
                "name": "Priya Shanmugasundaram"
            },
            {
                "authorId": "2302410553",
                "name": "Mohammad Kachuee"
            },
            {
                "authorId": "2302698347",
                "name": "Kun Zhou"
            },
            {
                "authorId": "2249761788",
                "name": "Ruoxi Jia"
            },
            {
                "authorId": "2243010174",
                "name": "Ming Jin"
            }
        ],
        "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in tasks such as summarization, arithmetic reasoning, and question answering. However, they encounter significant challenges in the domain of moral reasoning and ethical decision-making, especially in complex scenarios with multiple stakeholders. This paper introduces the Skin-in-the-Game (SKIG) framework, aimed at enhancing moral reasoning in LLMs by exploring decisions' consequences from multiple stakeholder perspectives. Central to SKIG's mechanism is simulating accountability for actions, which, alongside empathy exercises and risk assessment, is pivotal to its effectiveness. We validate SKIG's performance across various moral reasoning benchmarks with proprietary and opensource LLMs, and investigate its crucial components through extensive ablation analyses."
    },
    {
        "paperId": "a14648abc56c602396634609e911f0ec071e43c1",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.13226, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-21",
        "authors": [
            {
                "authorId": "2239094185",
                "name": "Hadi Pouransari"
            },
            {
                "authorId": "2302812706",
                "name": "Chun-Liang Li"
            },
            {
                "authorId": "2148969927",
                "name": "Jen-Hao Rick Chang"
            },
            {
                "authorId": "2111681666",
                "name": "Pavan Kumar Anasosalu Vasu"
            },
            {
                "authorId": "2302802949",
                "name": "Cem Koc"
            },
            {
                "authorId": "34961417",
                "name": "Vaishaal Shankar"
            },
            {
                "authorId": "2577513",
                "name": "Oncel Tuzel"
            }
        ],
        "abstract": "Large language models (LLMs) are commonly trained on datasets consisting of fixed-length token sequences. These datasets are created by randomly concatenating documents of various lengths and then chunking them into sequences of a predetermined target length (concat-and-chunk). Recent attention implementations mask cross-document attention, reducing the effective length of a chunk of tokens. Additionally, training on long sequences becomes computationally prohibitive due to the quadratic cost of attention. In this study, we introduce dataset decomposition, a novel variable sequence length training technique, to tackle these challenges. We decompose a dataset into a union of buckets, each containing sequences of the same size extracted from a unique document. During training, we use variable sequence length and batch-size, sampling simultaneously from all buckets with a curriculum. In contrast to the concat-and-chunk baseline, which incurs a fixed attention cost at every step of training, our proposed method incurs a computational cost proportional to the actual document lengths at each step, resulting in significant savings in training time. We train an 8k context-length 1B model at the same cost as a 2k context-length model trained with the baseline approach. Experiments on a web-scale corpus demonstrate that our approach significantly enhances performance on standard language evaluations and long-context benchmarks, reaching target accuracy with up to 6x faster training compared to the baseline. Our method not only enables efficient pretraining on long sequences but also scales effectively with dataset size. Lastly, we shed light on a critical yet less studied aspect of training large language models: the distribution and curriculum of sequence lengths, which results in a non-negligible difference in performance."
    },
    {
        "paperId": "11dda1081d32b607ea89b7e1028052af5583b25d",
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation",
        "openAccessPdf": {
            "url": "https://doi.org/10.1162/tacl_a_00689",
            "status": "GOLD",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.13769, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-22",
        "authors": [
            {
                "authorId": "2182520819",
                "name": "Cyril Chhun"
            },
            {
                "authorId": "2266842840",
                "name": "Fabian M. Suchanek"
            },
            {
                "authorId": "2266841646",
                "name": "Chlo\u00e9 Clavel"
            }
        ],
        "abstract": "Abstract Storytelling is an integral part of human experience and plays a crucial role in social interactions. Thus, Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways, but they are challenging tasks which require high-level human abilities such as creativity, reasoning, and deep understanding. Meanwhile, Large Language Models (LLMs) now achieve state-of-the-art performance on many NLP tasks. In this paper, we study whether LLMs can be used as substitutes for human annotators for ASE. We perform an extensive analysis of the correlations between LLM ratings, other automatic measures, and human annotations, and we explore the influence of prompting on the results and the explainability of LLM behaviour. Most notably, we find that LLMs outperform current automatic measures for system-level evaluation but still struggle at providing satisfactory explanations for their answers."
    },
    {
        "paperId": "844ce36b30b50a3f611e4b670eeec60aae5b1878",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Agent Planning with World Knowledge Model",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.14205, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-23",
        "authors": [
            {
                "authorId": "2190751119",
                "name": "Shuofei Qiao"
            },
            {
                "authorId": "2278840339",
                "name": "Runnan Fang"
            },
            {
                "authorId": "2153010067",
                "name": "Ningyu Zhang"
            },
            {
                "authorId": "2283182145",
                "name": "Yuqi Zhu"
            },
            {
                "authorId": "2143735911",
                "name": "Xiang Chen"
            },
            {
                "authorId": "152931849",
                "name": "Shumin Deng"
            },
            {
                "authorId": "2256747040",
                "name": "Yong Jiang"
            },
            {
                "authorId": "35930962",
                "name": "Pengjun Xie"
            },
            {
                "authorId": "2276428076",
                "name": "Fei Huang"
            },
            {
                "authorId": "2144200945",
                "name": "Huajun Chen"
            }
        ],
        "abstract": "Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the ``real'' physical world. Imitating humans' mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three complex real-world simulated datasets with three state-of-the-art open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent's understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development. The code is available at https://github.com/zjunlp/WKM."
    },
    {
        "paperId": "f274f97ca7445b6ea3cdf9c398ea44449c627e02",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.14555, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-23",
        "authors": [
            {
                "authorId": "2302786514",
                "name": "Abhishek Kumar"
            },
            {
                "authorId": "2302806171",
                "name": "Sarfaroz Yunusov"
            },
            {
                "authorId": "2302806420",
                "name": "Ali Emami"
            }
        ],
        "abstract": "Research on Large Language Models (LLMs) has often neglected subtle biases that, although less apparent, can significantly influence the models' outputs toward particular social narratives. This study addresses two such biases within LLMs: representative bias, which denotes a tendency of LLMs to generate outputs that mirror the experiences of certain identity groups, and affinity bias, reflecting the models' evaluative preferences for specific narratives or viewpoints. We introduce two novel metrics to measure these biases: the Representative Bias Score (RBS) and the Affinity Bias Score (ABS), and present the Creativity-Oriented Generation Suite (CoGS), a collection of open-ended tasks such as short story writing and poetry composition, designed with customized rubrics to detect these subtle biases. Our analysis uncovers marked representative biases in prominent LLMs, with a preference for identities associated with being white, straight, and men. Furthermore, our investigation of affinity bias reveals distinctive evaluative patterns within each model, akin to `bias fingerprints'. This trend is also seen in human evaluators, highlighting a complex interplay between human and machine bias perceptions."
    },
    {
        "paperId": "82a8b7956661a077d41f58bee59aadd7ff9c79a0",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.14751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-23",
        "authors": [
            {
                "authorId": "2302773746",
                "name": "Peiyuan Feng"
            },
            {
                "authorId": "2302988027",
                "name": "Yichen He"
            },
            {
                "authorId": "2303544894",
                "name": "Guanhua Huang"
            },
            {
                "authorId": "2268547458",
                "name": "Yuan Lin"
            },
            {
                "authorId": "2303077300",
                "name": "Hanchong Zhang"
            },
            {
                "authorId": "2268418250",
                "name": "Yuchen Zhang"
            },
            {
                "authorId": "2267425733",
                "name": "Hang Li"
            }
        ],
        "abstract": "We introduce a novel reinforcement learning framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments) designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts. The agent possesses capabilities beyond conversation, including reflection, tool usage, and expert consultation. We formulate the construction of such an LLM agent as a reinforcement learning (RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm. We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping. Our extensive experiments on ProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs trained with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance. Datasets and code are available at https://github.com/bytarnish/AGILE."
    },
    {
        "paperId": "8ce5f6e28d49e1bc00804fa2fa3e917deb203388",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.14768, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-23",
        "authors": [
            {
                "authorId": "2277701209",
                "name": "Peng Wang"
            },
            {
                "authorId": "2302767435",
                "name": "Zexi Li"
            },
            {
                "authorId": "2153010067",
                "name": "Ningyu Zhang"
            },
            {
                "authorId": "2277568426",
                "name": "Ziwen Xu"
            },
            {
                "authorId": "4841460",
                "name": "Yunzhi Yao"
            },
            {
                "authorId": "2256747040",
                "name": "Yong Jiang"
            },
            {
                "authorId": "35930962",
                "name": "Pengjun Xie"
            },
            {
                "authorId": "2276428076",
                "name": "Fei Huang"
            },
            {
                "authorId": "2144200945",
                "name": "Huajun Chen"
            }
        ],
        "abstract": "Large language models (LLMs) need knowledge updates to meet the ever-growing world facts and correct the hallucinated responses, facilitating the methods of lifelong model editing. Where the updated knowledge resides in memories is a fundamental question for model editing. In this paper, we find that editing either long-term memory (direct model parameters) or working memory (non-parametric knowledge of neural network activations/representations by retrieval) will result in an impossible triangle -- reliability, generalization, and locality can not be realized together in the lifelong editing settings. For long-term memory, directly editing the parameters will cause conflicts with irrelevant pretrained knowledge or previous edits (poor reliability and locality). For working memory, retrieval-based activations can hardly make the model understand the edits and generalize (poor generalization). Therefore, we propose WISE to bridge the gap between memories. In WISE, we design a dual parametric memory scheme, which consists of the main memory for the pretrained knowledge and a side memory for the edited knowledge. We only edit the knowledge in the side memory and train a router to decide which memory to go through when given a query. For continual editing, we devise a knowledge-sharding mechanism where different sets of edits reside in distinct subspaces of parameters, and are subsequently merged into a shared memory without conflicts. Extensive experiments show that WISE can outperform previous model editing methods and overcome the impossible triangle under lifelong model editing of question answering, hallucination, and out-of-distribution settings across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is available at https://github.com/zjunlp/EasyEdit."
    },
    {
        "paperId": "4308208fac24626e0c927ee728038aadc4e87266",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.14831, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-23",
        "authors": [
            {
                "authorId": "1666169546",
                "name": "Bernal Jimenez Gutierrez"
            },
            {
                "authorId": "1406331721",
                "name": "Yiheng Shu"
            },
            {
                "authorId": "2022231256",
                "name": "Yu Gu"
            },
            {
                "authorId": "19168196",
                "name": "Michihiro Yasunaga"
            },
            {
                "authorId": "1758652",
                "name": "Yu Su"
            }
        ],
        "abstract": "In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering and show that our method outperforms the state-of-the-art methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG."
    },
    {
        "paperId": "d04a7906161835440daaff90265dacca01ed3ba3",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Linking In-context Learning in Transformers to Human Episodic Memory",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.14992, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-23",
        "authors": [
            {
                "authorId": "2281647318",
                "name": "Ji-An Li"
            },
            {
                "authorId": "2216730340",
                "name": "Corey Zhou"
            },
            {
                "authorId": "47627582",
                "name": "M. Benna"
            },
            {
                "authorId": "2238260970",
                "name": "Marcelo G. Mattar"
            }
        ],
        "abstract": "Understanding connections between artificial and biological intelligent systems can reveal fundamental principles of general intelligence. While many artificial intelligence models have a neuroscience counterpart, such connections are largely missing in Transformer models and the self-attention mechanism. Here, we examine the relationship between interacting attention heads and human episodic memory. We focus on induction heads, which contribute to in-context learning in Transformer-based large language models (LLMs). We demonstrate that induction heads are behaviorally, functionally, and mechanistically similar to the contextual maintenance and retrieval (CMR) model of human episodic memory. Our analyses of LLMs pre-trained on extensive text data show that CMR-like heads often emerge in the intermediate and late layers, qualitatively mirroring human memory biases. The ablation of CMR-like heads suggests their causal role in in-context learning. Our findings uncover a parallel between the computational mechanisms of LLMs and human memory, offering valuable insights into both research fields."
    },
    {
        "paperId": "32591fbbf04c14483867159a787d8d095c780570",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Promoting Constructive Deliberation: Reframing for Receptiveness",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.15067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-23",
        "authors": [
            {
                "authorId": "1453874533",
                "name": "Gauri Kambhatla"
            },
            {
                "authorId": "2294362805",
                "name": "Matthew Lease"
            },
            {
                "authorId": "2116525",
                "name": "Ashwin Rajadesingan"
            }
        ],
        "abstract": "To promote constructive discussion of controversial topics online, we propose automatic reframing of disagreeing responses to signal receptiveness to a preceding comment. Drawing on research from psychology, communications, and linguistics, we identify six strategies for reframing. We automatically reframe replies to comments according to each strategy, using a Reddit dataset. Through human-centered experiments, we find that the replies generated with our framework are perceived to be significantly more receptive than the original replies and a generic receptiveness baseline. We illustrate how transforming receptiveness, a particular social science construct, into a computational framework, can make LLM generations more aligned with human perceptions. We analyze and discuss the implications of our results, and highlight how a tool based on our framework might be used for more teachable and creative content moderation."
    },
    {
        "paperId": "4c105e888f749201b5127f60f1d9aa8ed451a19c",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "CulturePark: Boosting Cross-cultural Understanding in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.15145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-24",
        "authors": [
            {
                "authorId": "2258600474",
                "name": "Cheng Li"
            },
            {
                "authorId": "2253463627",
                "name": "Damien Teney"
            },
            {
                "authorId": "2292413132",
                "name": "Linyi Yang"
            },
            {
                "authorId": "2303259263",
                "name": "Qingsong Wen"
            },
            {
                "authorId": "2249681654",
                "name": "Xing Xie"
            },
            {
                "authorId": "2300128890",
                "name": "Jindong Wang"
            }
        ],
        "abstract": "Cultural bias is pervasive in many large language models (LLMs), largely due to the deficiency of data representative of different cultures. Typically, cultural datasets and benchmarks are constructed either by extracting subsets of existing datasets or by aggregating from platforms such as Wikipedia and social media. However, these approaches are highly dependent on real-world data and human annotations, making them costly and difficult to scale. Inspired by cognitive theories on social communication, this paper introduces CulturePark, an LLM-powered multi-agent communication framework for cultural data collection. CulturePark simulates cross-cultural human communication with LLM-based agents playing roles in different cultures. It generates high-quality cross-cultural dialogues encapsulating human beliefs, norms, and customs. Using CulturePark, we generated 41,000 cultural samples to fine-tune eight culture-specific LLMs. We evaluated these models across three downstream tasks: content moderation, cultural alignment, and cultural education. Results show that for content moderation, our GPT-3.5-based models either match or outperform GPT-4 on datasets. Regarding cultural alignment, our models surpass GPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of human participants, our models demonstrate superior outcomes in both learning efficacy and user experience compared to GPT-4. CulturePark proves an important step in addressing cultural bias and advancing the democratization of AI, highlighting the critical role of culturally inclusive data in model training. Code is released at https://github.com/Scarelette/CulturePark."
    },
    {
        "paperId": "ca6e4f2be5d595623298d62682504571d8882b5d",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "EffiLearner: Enhancing Efficiency of Generated Code via Self-Optimization",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.15189, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-24",
        "authors": [
            {
                "authorId": "2148911355",
                "name": "Dong Huang"
            },
            {
                "authorId": "2302368830",
                "name": "Jianbo Dai"
            },
            {
                "authorId": "2303256169",
                "name": "Han Weng"
            },
            {
                "authorId": "2203250528",
                "name": "Puzhen Wu"
            },
            {
                "authorId": "2066367605",
                "name": "Yuhao Qing"
            },
            {
                "authorId": "2303256339",
                "name": "Jie M.Zhang"
            },
            {
                "authorId": "2275279817",
                "name": "Heming Cui"
            },
            {
                "authorId": "2681038",
                "name": "Zhijiang Guo"
            }
        ],
        "abstract": "Large language models (LLMs) have shown remarkable progress in code generation, but their generated code often suffers from inefficiency, resulting in longer execution times and higher memory consumption. To address this issue, we propose \\textbf{EffiLearner}, a self-optimization framework that utilizes execution overhead profiles to improve the efficiency of LLM-generated code. EffiLearner first generates code using an LLM, then executes it locally to capture execution time and memory usage profiles. These profiles are fed back to the LLM, which then revises the code to reduce overhead. To evaluate the effectiveness of EffiLearner, we conduct extensive experiments on the EffiBench, HumanEval, and MBPP with 16 open-source and 6 closed-source models. Our evaluation results demonstrate that through iterative self-optimization, EffiLearner significantly enhances the efficiency of LLM-generated code. For example, the execution time (ET) of StarCoder2-15B for the EffiBench decreases from 0.93 (s) to 0.12 (s) which reduces 87.1% the execution time requirement compared with the initial code. The total memory usage (TMU) of StarCoder2-15B also decreases from 22.02 (Mb*s) to 2.03 (Mb*s), which decreases 90.8% of total memory consumption during the execution process. The source code of EffiLearner was released in https://github.com/huangd1999/EffiLearner"
    },
    {
        "paperId": "d4dcd6eaab30e47ed3ba526663bdaa99e74a16e4",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.15585, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-24",
        "authors": [
            {
                "authorId": "2218411692",
                "name": "Vishal Vivek Saley"
            },
            {
                "authorId": "2211732585",
                "name": "Rocktim Jyoti Das"
            },
            {
                "authorId": "1916865",
                "name": "Dinesh Raghu"
            },
            {
                "authorId": "2303255890",
                "name": "Mausam"
            }
        ],
        "abstract": "End-to-end Task-Oriented Dialog (TOD) systems typically require extensive training datasets to perform well. In contrast, large language model (LLM) based TOD systems can excel even with limited data due to their ability to learn tasks through in-context exemplars. However, these models lack alignment with the style of responses in training data and often generate comprehensive responses, making it difficult for users to grasp the information quickly. In response, we propose SyncTOD that synergizes LLMs with task-specific hints to improve alignment in low-data settings. SyncTOD employs small auxiliary models to provide hints and select exemplars for in-context prompts. With ChatGPT, SyncTOD achieves superior performance compared to LLM-based baselines and SoTA models in low-data settings, while retaining competitive performance in full-data settings."
    },
    {
        "paperId": "723ae07c5c8921eb94ac52528c8a53bec837c5a9",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Efficient Adversarial Training in LLMs with Continuous Attacks",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.15589, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-24",
        "authors": [
            {
                "authorId": "2204462616",
                "name": "Sophie Xhonneux"
            },
            {
                "authorId": "2041695",
                "name": "Alessandro Sordoni"
            },
            {
                "authorId": "2241501445",
                "name": "Stephan G\u00fcnnemann"
            },
            {
                "authorId": "8150760",
                "name": "G. Gidel"
            },
            {
                "authorId": "1502273680",
                "name": "Leo Schwinn"
            }
        ],
        "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on five models from different families (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs."
    },
    {
        "paperId": "6718aa9d76b8282cbf9eac834b1430df83b0b8c7",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.15760, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-24",
        "authors": [
            {
                "authorId": "2140292594",
                "name": "Virginia K. Felkner"
            },
            {
                "authorId": "2303324284",
                "name": "Jennifer A. Thompson"
            },
            {
                "authorId": "2303258923",
                "name": "Jonathan May"
            }
        ],
        "abstract": "Social biases in LLMs are usually measured via bias benchmark datasets. Current benchmarks have limitations in scope, grounding, quality, and human effort required. Previous work has shown success with a community-sourced, rather than crowd-sourced, approach to benchmark development. However, this work still required considerable effort from annotators with relevant lived experience. This paper explores whether an LLM (specifically, GPT-3.5-Turbo) can assist with the task of developing a bias benchmark dataset from responses to an open-ended community survey. We also extend the previous work to a new community and set of biases: the Jewish community and antisemitism. Our analysis shows that GPT-3.5-Turbo has poor performance on this annotation task and produces unacceptable quality issues in its output. Thus, we conclude that GPT-3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use actually negates many of the benefits of community-sourcing bias benchmarks."
    },
    {
        "paperId": "5f8b4e2e8c337447bfbcf47044af4a1d5f75f41e",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.15880, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-24",
        "authors": [
            {
                "authorId": "115508430",
                "name": "Shraddha Barke"
            },
            {
                "authorId": "2303438153",
                "name": "Emmanuel Anaya Gonzalez"
            },
            {
                "authorId": "73097539",
                "name": "Saketh Ram Kasibatla"
            },
            {
                "authorId": "2303399361",
                "name": "Taylor Berg-Kirkpatrick"
            },
            {
                "authorId": "2258443511",
                "name": "Nadia Polikarpova"
            }
        ],
        "abstract": "Many structured prediction and reasoning tasks can be framed as program synthesis problems, where the goal is to generate a program in a domain-specific language (DSL) that transforms input data into the desired output. Unfortunately, purely neural approaches, such as large language models (LLMs), often fail to produce fully correct programs in unfamiliar DSLs, while purely symbolic methods based on combinatorial search scale poorly to complex problems. Motivated by these limitations, we introduce a hybrid approach, where LLM completions for a given task are used to learn a task-specific, context-free surrogate model, which is then used to guide program synthesis. We evaluate this hybrid approach on three domains, and show that it outperforms both unguided search and direct sampling from LLMs, as well as existing program synthesizers."
    },
    {
        "paperId": "b9a941c9b2c9becb07510f2cf1274678ef0cfcab",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "SLIDE: A Framework Integrating Small and Large Language Models for Open-Domain Dialogues Evaluation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.15924, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-24",
        "authors": [
            {
                "authorId": "2283546822",
                "name": "Kun Zhao"
            },
            {
                "authorId": "84537195",
                "name": "Bohao Yang"
            },
            {
                "authorId": "1488944173",
                "name": "Chen Tang"
            },
            {
                "authorId": "2244126309",
                "name": "Chenghua Lin"
            },
            {
                "authorId": "2277600293",
                "name": "Liang Zhan"
            }
        ],
        "abstract": "The long-standing one-to-many problem of gold standard responses in open-domain dialogue systems presents challenges for automatic evaluation metrics. Though prior works have demonstrated some success by applying powerful Large Language Models (LLMs), existing approaches still struggle with the one-to-many problem, and exhibit subpar performance in domain-specific scenarios. We assume the commonsense reasoning biases within LLMs may hinder their performance in domainspecific evaluations. To address both issues, we propose a novel framework SLIDE (Small and Large Integrated for Dialogue Evaluation), that leverages both a small, specialised model (SLM), and LLMs for the evaluation of open domain dialogues. Our approach introduces several techniques: (1) Contrastive learning to differentiate between robust and non-robust response embeddings; (2) A novel metric for semantic sensitivity that combines embedding cosine distances with similarity learned through neural networks, and (3) a strategy for incorporating the evaluation results from both the SLM and LLMs. Our empirical results demonstrate that our approach achieves state-of-the-art performance in both the classification and evaluation tasks, and additionally the SLIDE evaluator exhibits better correlation with human judgements. Our code is available at https:// github.com/hegehongcha/SLIDE-ACL2024."
    },
    {
        "paperId": "df4a51fd8fae13cac60c5f3a71b589c92a1a58c0",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.16057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-25",
        "authors": [
            {
                "authorId": "2238395310",
                "name": "Xudong Lu"
            },
            {
                "authorId": "2276425017",
                "name": "Aojun Zhou"
            },
            {
                "authorId": "48615640",
                "name": "Yuhui Xu"
            },
            {
                "authorId": "2274929565",
                "name": "Renrui Zhang"
            },
            {
                "authorId": "2279706151",
                "name": "Peng Gao"
            },
            {
                "authorId": "2285017444",
                "name": "Hongsheng Li"
            }
        ],
        "abstract": "Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine-tuning and deployment. Current post-training pruning methods, while reducing the sizes of LLMs, often fail to maintain their original performance. To address these challenges, this paper introduces SPP, a Sparsity-Preserved Parameter-efficient fine-tuning method. Different from existing post-training pruning approaches that struggle with performance retention, SPP proposes to employ lightweight learnable column and row matrices to optimize sparse LLM weights, keeping the structure and sparsity of pruned pre-trained models intact. By element-wise multiplication and residual addition, SPP ensures the consistency of model sparsity pattern and ratio during both training and weight-merging processes. We demonstrate the effectiveness of SPP by applying it to the LLaMA and LLaMA-2 model families with recent post-training pruning methods. Our results show that SPP significantly enhances the performance of models with different sparsity patterns (i.e. unstructured and N:M sparsity), especially for those with high sparsity ratios (e.g. 75%), making it a promising solution for the efficient fine-tuning of sparse LLMs. Code will be made available at https://github.com/Lucky-Lance/SPP."
    },
    {
        "paperId": "3893c0516278a602c296242fb6755914b0a782c1",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Keypoint-based Progressive Chain-of-Thought Distillation for LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.16064, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-25",
        "authors": [
            {
                "authorId": "2170165151",
                "name": "Kaituo Feng"
            },
            {
                "authorId": "2260925375",
                "name": "Changsheng Li"
            },
            {
                "authorId": "2260823600",
                "name": "Xiaolu Zhang"
            },
            {
                "authorId": "2151551686",
                "name": "Jun Zhou"
            },
            {
                "authorId": "2287785713",
                "name": "Ye Yuan"
            },
            {
                "authorId": "2168989170",
                "name": "Guoren Wang"
            }
        ],
        "abstract": "Chain-of-thought distillation is a powerful technique for transferring reasoning abilities from large language models (LLMs) to smaller student models. Previous methods typically require the student to mimic the step-by-step rationale produced by LLMs, often facing the following challenges: (i) Tokens within a rationale vary in significance, and treating them equally may fail to accurately mimic keypoint tokens, leading to reasoning errors. (ii) They usually distill knowledge by consistently predicting all the steps in a rationale, which falls short in distinguishing the learning order of step generation. This diverges from the human cognitive progression of starting with easy tasks and advancing to harder ones, resulting in sub-optimal outcomes. To this end, we propose a unified framework, called KPOD, to address these issues. Specifically, we propose a token weighting module utilizing mask learning to encourage accurate mimicry of keypoint tokens by the student during distillation. Besides, we develop an in-rationale progressive distillation strategy, starting with training the student to generate the final reasoning steps and gradually extending to cover the entire rationale. To accomplish this, a weighted token generation loss is proposed to assess step reasoning difficulty, and a value function is devised to schedule the progressive distillation by considering both step difficulty and question diversity. Extensive experiments on four reasoning benchmarks illustrate our KPOD outperforms previous methods by a large margin."
    },
    {
        "paperId": "5716985a55d5a2dae451c4347707ac89c24175b9",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.16282, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-25",
        "authors": [
            {
                "authorId": "2302786514",
                "name": "Abhishek Kumar"
            },
            {
                "authorId": "93972200",
                "name": "Robert D Morabito"
            },
            {
                "authorId": "2303401314",
                "name": "Sanzhar Umbet"
            },
            {
                "authorId": "2631301",
                "name": "Jad Kabbara"
            },
            {
                "authorId": "2302806420",
                "name": "Ali Emami"
            }
        ],
        "abstract": "As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models. We introduce the concept of Confidence-Probability Alignment, that connects an LLM's internal confidence, quantified by token probabilities, to the confidence conveyed in the model's response when explicitly asked about its certainty. Using various datasets and prompting techniques that encourage model introspection, we probe the alignment between models' internal and expressed confidence. These techniques encompass using structured evaluation scales to rate confidence, including answer options when prompting, and eliciting the model's confidence level for outputs it does not recognize as its own. Notably, among the models analyzed, OpenAI's GPT-4 showed the strongest confidence-probability alignment, with an average Spearman's $\\hat{\\rho}$ of 0.42, across a wide range of tasks. Our work contributes to the ongoing efforts to facilitate risk assessment in the application of LLMs and to further our understanding of model trustworthiness."
    },
    {
        "paperId": "e399fb6a97130fb3ec6db57d583a62f5db45fe18",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.16908, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-27",
        "authors": [
            {
                "authorId": "36825265",
                "name": "G. Yona"
            },
            {
                "authorId": "2335771",
                "name": "Roee Aharoni"
            },
            {
                "authorId": "22245981",
                "name": "Mor Geva"
            }
        ],
        "abstract": "We posit that large language models (LLMs) should be capable of expressing their intrinsic uncertainty in natural language. For example, if the LLM is equally likely to output two contradicting answers to the same question, then its generated response should reflect this uncertainty by hedging its answer (e.g., \u201cI\u2019m not sure, but I think...\u201d). We formalize faithful response uncertainty based on the gap between the model\u2019s intrinsic confidence in the assertions it makes and the decisiveness by which they are conveyed. This example-level metric reliably indicates whether the model reflects its uncertainty, as it penalizes both excessive and insufficient hedging. We evaluate a variety of aligned LLMs at faithfully conveying uncertainty on several knowledge-intensive question answering tasks. Our results provide strong evidence that modern LLMs are poor at faithfully conveying their uncertainty, and that better alignment is necessary to improve their trustworthiness."
    },
    {
        "paperId": "39fe0048180b0a4e5b2bc54dac4af152a33cddb3",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Position: Foundation Agents as the Paradigm Shift for Decision Making",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.17009, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-27",
        "authors": [
            {
                "authorId": "2271396786",
                "name": "Xiaoqian Liu"
            },
            {
                "authorId": "2181368842",
                "name": "Xingzhou Lou"
            },
            {
                "authorId": "2275191645",
                "name": "Jianbin Jiao"
            },
            {
                "authorId": "2275271323",
                "name": "Junge Zhang"
            }
        ],
        "abstract": "Decision making demands intricate interplay between perception, memory, and reasoning to discern optimal policies. Conventional approaches to decision making face challenges related to low sample efficiency and poor generalization. In contrast, foundation models in language and vision have showcased rapid adaptation to diverse new tasks. Therefore, we advocate for the construction of foundation agents as a transformative shift in the learning paradigm of agents. This proposal is underpinned by the formulation of foundation agents with their fundamental characteristics and challenges motivated by the success of large language models (LLMs). Moreover, we specify the roadmap of foundation agents from large interactive data collection or generation, to self-supervised pretraining and adaptation, and knowledge and value alignment with LLMs. Lastly, we pinpoint critical research questions derived from the formulation and delineate trends for foundation agents supported by real-world use cases, addressing both technical and theoretical aspects to propel the field towards a more comprehensive and impactful future."
    },
    {
        "paperId": "a1e2557fa6d5373c8f89b8c4d426168cdf31d7d5",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Efficient multi-prompt evaluation of LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.17202, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-27",
        "authors": [
            {
                "authorId": "1490941219",
                "name": "Felipe Maia Polo"
            },
            {
                "authorId": "2304142327",
                "name": "Ronald Xu"
            },
            {
                "authorId": "2286343827",
                "name": "Lucas Weber"
            },
            {
                "authorId": "2247874639",
                "name": "M'irian Silva"
            },
            {
                "authorId": "2303400299",
                "name": "Onkar Bhardwaj"
            },
            {
                "authorId": "2283849613",
                "name": "Leshem Choshen"
            },
            {
                "authorId": "2303415785",
                "name": "Allysson Flavio Melo de Oliveira"
            },
            {
                "authorId": "2247880508",
                "name": "Yuekai Sun"
            },
            {
                "authorId": "8202372",
                "name": "M. Yurochkin"
            }
        ],
        "abstract": "Most popular benchmarks for comparing LLMs rely on a limited set of prompt templates, which may not fully capture the LLMs' abilities and can affect the reproducibility of results on leaderboards. Many recent works empirically verify prompt sensitivity and advocate for changes in LLM evaluation. In this paper, we consider the problem of estimating the performance distribution across many prompt variants instead of finding a single prompt to evaluate with. We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets. The resulting distribution can be used to obtain performance quantiles to construct various robust performance metrics (e.g., top 95% quantile or median). We prove that PromptEval consistently estimates the performance distribution and demonstrate its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry; for example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations. Moreover, we show how PromptEval can be useful in LLM-as-a-judge and best prompt identification applications."
    },
    {
        "paperId": "a544f18fdb3a1adfd47bb9d2892b37e229193516",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Code Repair with LLMs gives an Exploration-Exploitation Tradeoff",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.17503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-26",
        "authors": [
            {
                "authorId": "2109238481",
                "name": "Hao Tang"
            },
            {
                "authorId": "2303624656",
                "name": "Keya Hu"
            },
            {
                "authorId": "2303726151",
                "name": "Jin Peng Zhou"
            },
            {
                "authorId": "2303463837",
                "name": "Sicheng Zhong"
            },
            {
                "authorId": "2303969843",
                "name": "Wei-Long Zheng"
            },
            {
                "authorId": "2303462714",
                "name": "Xujie Si"
            },
            {
                "authorId": "2284673917",
                "name": "Kevin Ellis"
            }
        ],
        "abstract": "Iteratively improving and repairing source code with large language models (LLMs), known as refinement, has emerged as a popular way of generating programs that would be too complex to construct in one shot. Given a bank of test cases, together with a candidate program, an LLM can improve that program by being prompted with failed test cases. But it remains an open question how to best iteratively refine code, with prior work employing simple greedy or breadth-first strategies. We show here that refinement exposes an explore-exploit tradeoff: exploit by refining the program that passes the most test cases, or explore by refining a lesser considered program. We frame this as an arm-acquiring bandit problem, which we solve with Thompson Sampling. The resulting LLM-based program synthesis algorithm is broadly applicable: Across loop invariant synthesis, visual reasoning puzzles, and competition programming problems, we find that our new method can solve more problems using fewer language model calls."
    },
    {
        "paperId": "6f38dc421b3f42eb737905262c867c6bcf6a77a4",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.17633, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-27",
        "authors": [
            {
                "authorId": "2115509633",
                "name": "Jocelyn Jiayue Shen"
            },
            {
                "authorId": "2266840684",
                "name": "Joel Mire"
            },
            {
                "authorId": "2756001",
                "name": "Hae Won Park"
            },
            {
                "authorId": "2266253563",
                "name": "C. Breazeal"
            },
            {
                "authorId": "2729164",
                "name": "Maarten Sap"
            }
        ],
        "abstract": "Empathy serves as a cornerstone in enabling prosocial behaviors, and can be evoked through sharing of personal experiences in stories. While empathy is influenced by narrative content, intuitively, people respond to the way a story is told as well, through narrative style. Yet the relationship between empathy and narrative style is not fully understood. In this work, we empirically examine and quantify this relationship between style and empathy using LLMs and large-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy, HEART (Human Empathy and Narrative Taxonomy) that delineates elements of narrative style that can lead to empathy with the narrator of a story. We establish the performance of LLMs in extracting narrative elements from HEART, showing that prompting with our taxonomy leads to reasonable, human-level annotations beyond what prior lexicon-based methods can do. To show empirical use of our taxonomy, we collect a dataset of empathy judgments of stories via a large-scale crowdsourcing study with N=2,624 participants. We show that narrative elements extracted via LLMs, in particular, vividness of emotions and plot volume, can elucidate the pathways by which narrative style cultivates empathy towards personal stories. Our work suggests that such models can be used for narrative analyses that lead to human-centered social and behavioral insights."
    },
    {
        "paperId": "d409053ed94ec725d72c812e7c8bd71b87278b96",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.18027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-28",
        "authors": [
            {
                "authorId": "2111076389",
                "name": "Jaewoo Ahn"
            },
            {
                "authorId": "152988918",
                "name": "Taehyun Lee"
            },
            {
                "authorId": "2303525876",
                "name": "Junyoung Lim"
            },
            {
                "authorId": "2303499674",
                "name": "Jin-Hwa Kim"
            },
            {
                "authorId": "2269993531",
                "name": "Sangdoo Yun"
            },
            {
                "authorId": "2271303845",
                "name": "Hwaran Lee"
            },
            {
                "authorId": "2201777220",
                "name": "Gunhee Kim"
            }
        ],
        "abstract": "While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e., role-playing agents), we emphasize the importance of point-in-time role-playing. This situates characters at specific moments in the narrative progression for three main reasons: (i) enhancing users' narrative immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom role-playing. To accurately represent characters at specific time points, agents must avoid character hallucination, where they display knowledge that contradicts their characters' identities and historical timelines. We introduce TimeChara, a new benchmark designed to evaluate point-in-time character hallucination in role-playing LLMs. Comprising 10,895 instances generated through an automated pipeline, this benchmark reveals significant hallucination issues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this challenge, we propose Narrative-Experts, a method that decomposes the reasoning steps and utilizes narrative experts to reduce point-in-time character hallucinations effectively. Still, our findings with TimeChara highlight the ongoing challenges of point-in-time character hallucination, calling for further study."
    },
    {
        "paperId": "1b1265a7fc7debcdd0cd92fc1d9b51fc55d57fdc",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Faithful Logical Reasoning via Symbolic Chain-of-Thought",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.18357, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-28",
        "authors": [
            {
                "authorId": "2304023176",
                "name": "Jundong Xu"
            },
            {
                "authorId": "2303470150",
                "name": "Hao Fei"
            },
            {
                "authorId": "2304300631",
                "name": "Liangming Pan"
            },
            {
                "authorId": "2303474809",
                "name": "Qian Liu"
            },
            {
                "authorId": "2237598359",
                "name": "M. Lee"
            },
            {
                "authorId": "144793401",
                "name": "W. Hsu"
            }
        ],
        "abstract": "While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs. Code is open at https://github.com/Aiden0526/SymbCoT."
    },
    {
        "paperId": "50d40517ede17d332bc3d590da5dc44999bf2490",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models",
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2405.18638",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.18638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-28",
        "authors": [
            {
                "authorId": "26940961",
                "name": "Aparna Elangovan"
            },
            {
                "authorId": "2303853477",
                "name": "Ling Liu"
            },
            {
                "authorId": "2303797447",
                "name": "Lei Xu"
            },
            {
                "authorId": "40696276",
                "name": "S. Bodapati"
            },
            {
                "authorId": "2303654944",
                "name": "Dan Roth"
            }
        ],
        "abstract": "In this position paper, we argue that human evaluation of generative large language models (LLMs) should be a multidisciplinary undertaking that draws upon insights from disciplines such as user experience research and human behavioral psychology to ensure that the experimental design and results are reliable. The conclusions from these evaluations, thus, must consider factors such as usability, aesthetics, and cognitive biases. We highlight how cognitive biases can conflate fluent information and truthfulness, and how cognitive uncertainty affects the reliability of rating scores such as Likert. Furthermore, the evaluation should differentiate the capabilities and weaknesses of increasingly powerful large language models -- which requires effective test sets. The scalability of human evaluation is also crucial to wider adoption. Hence, to design an effective human evaluation system in the age of generative NLP, we propose the ConSiDERS-The-Human evaluation framework consisting of 6 pillars -- Consistency, Scoring Criteria, Differentiating, User Experience, Responsible, and Scalability."
    },
    {
        "paperId": "cff0f8be64b0edfcaa2cae956851380ae04294be",
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "title": "Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.18765, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-29",
        "authors": [
            {
                "authorId": "2152123927",
                "name": "Wei-Bang Jiang"
            },
            {
                "authorId": "152124888",
                "name": "Li-Ming Zhao"
            },
            {
                "authorId": "2259969676",
                "name": "Bao-Liang Lu"
            }
        ],
        "abstract": "The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs). We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training. Then the models can be fine-tuned for different downstream tasks. However, compared to text data, the volume of EEG datasets is generally small and the format varies widely. For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. To overcome these challenges, we propose a unified foundation model for EEG called Large Brain Model (LaBraM). LaBraM enables cross-dataset learning by segmenting the EEG signals into EEG channel patches. Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes. We then pre-train neural Transformers by predicting the original neural codes for the masked EEG channel patches. The LaBraMs were pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple different types of downstream tasks. Experiments on abnormal detection, event type classification, emotion recognition, and gait prediction show that our LaBraM outperforms all compared SOTA methods in their respective fields. Our code is available at https://github.com/935963004/LaBraM."
    },
    {
        "paperId": "9d376ed1107150a5ccc559e95f46beb33c57e2e2",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Toxicity Detection for Free",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.18822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-29",
        "authors": [
            {
                "authorId": "2303804408",
                "name": "Zhanhao Hu"
            },
            {
                "authorId": "83755654",
                "name": "Julien Piet"
            },
            {
                "authorId": "2111098411",
                "name": "Geng Zhao"
            },
            {
                "authorId": "2258657022",
                "name": "Jiantao Jiao"
            },
            {
                "authorId": "2303655045",
                "name": "David Wagner"
            }
        ],
        "abstract": "Current LLMs are generally aligned to follow safety requirements and tend to refuse toxic prompts. However, LLMs can fail to refuse toxic prompts or be overcautious and refuse benign examples. In addition, state-of-the-art toxicity detectors have low TPRs at low FPR, incurring high costs in real-world applications where toxic examples are rare. In this paper, we introduce Moderation Using LLM Introspection (MULI), which detects toxic prompts using the information extracted directly from LLMs themselves. We found we can distinguish between benign and toxic prompts from the distribution of the first response token's logits. Using this idea, we build a robust detector of toxic prompts using a sparse logistic regression model on the first response token logits. Our scheme outperforms SOTA detectors under multiple metrics."
    },
    {
        "paperId": "590088c7bc699fe26507cb1e9f70aac531d1f069",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Language Generation with Strictly Proper Scoring Rules",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.18906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-29",
        "authors": [
            {
                "authorId": "81050636",
                "name": "Chenze Shao"
            },
            {
                "authorId": "33427918",
                "name": "Fandong Meng"
            },
            {
                "authorId": "2108060455",
                "name": "Yijin Liu"
            },
            {
                "authorId": "2265517237",
                "name": "Jie Zhou"
            }
        ],
        "abstract": "Language generation based on maximum likelihood estimation (MLE) has become the fundamental approach for text generation. Maximum likelihood estimation is typically performed by minimizing the log-likelihood loss, also known as the logarithmic score in statistical decision theory. The logarithmic score is strictly proper in the sense that it encourages honest forecasts, where the expected score is maximized only when the model reports true probabilities. Although many strictly proper scoring rules exist, the logarithmic score is the only local scoring rule among them that depends exclusively on the probability of the observed sample, making it capable of handling the exponentially large sample space of natural text. In this work, we propose a straightforward strategy for adapting scoring rules to language generation, allowing for language modeling with any non-local scoring rules. Leveraging this strategy, we train language generation models using two classic strictly proper scoring rules, the Brier score and the Spherical score, as alternatives to the logarithmic score. Experimental results indicate that simply substituting the loss function, without adjusting other hyperparameters, can yield substantial improvements in model's generation capabilities. Moreover, these improvements can scale up to large language models (LLMs) such as LLaMA-7B and LLaMA-13B. Source code: \\url{https://github.com/shaochenze/ScoringRulesLM}."
    },
    {
        "paperId": "ec6023c6af6087ae9ed76efff448e0f9dc0254cc",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Why Larger Language Models Do In-context Learning Differently?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.19592, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-30",
        "authors": [
            {
                "authorId": "113515522",
                "name": "Zhenmei Shi"
            },
            {
                "authorId": "2260597901",
                "name": "Junyi Wei"
            },
            {
                "authorId": "2286360492",
                "name": "Zhuoyan Xu"
            },
            {
                "authorId": "2260827689",
                "name": "Yingyu Liang"
            }
        ],
        "abstract": "Large language models (LLM) have emerged as a powerful tool for AI, with the key ability of in-context learning (ICL), where they can perform well on unseen tasks based on a brief series of task examples without necessitating any adjustments to the model parameters. One recent interesting mysterious observation is that models of different scales may have different ICL behaviors: larger models tend to be more sensitive to noise in the test context. This work studies this observation theoretically aiming to improve the understanding of LLM and ICL. We analyze two stylized settings: (1) linear regression with one-layer single-head linear transformers and (2) parity classification with two-layer multiple attention heads transformers (non-linear data and non-linear model). In both settings, we give closed-form optimal solutions and find that smaller models emphasize important hidden features while larger ones cover more hidden features; thus, smaller models are more robust to noise while larger ones are more easily distracted, leading to different ICL behaviors. This sheds light on where transformers pay attention to and how that affects ICL. Preliminary experimental results on large base and chat models provide positive support for our analysis."
    },
    {
        "paperId": "07d46677c31a450fecbecd3e96b096fe250031b0",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "GKT: A Novel Guidance-Based Knowledge Transfer Framework For Efficient Cloud-edge Collaboration LLM Deployment",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.19635, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-30",
        "authors": [
            {
                "authorId": "2223290826",
                "name": "Yao Yao"
            },
            {
                "authorId": "30658665",
                "name": "Z. Li"
            },
            {
                "authorId": "2274113564",
                "name": "Hai Zhao"
            }
        ],
        "abstract": "The burgeoning size of Large Language Models (LLMs) has led to enhanced capabilities in generating responses, albeit at the expense of increased inference times and elevated resource demands. Existing methods of acceleration, predominantly hinged on knowledge distillation, generally necessitate fine-tuning of considerably large models, such as Llama-7B, posing a challenge for average users. Furthermore, present techniques for expediting inference and reducing costs operate independently. To address these issues, we introduce a novel and intuitive Guidance-based Knowledge Transfer (GKT) framework. This approach leverages a larger LLM as a ''teacher'' to create guidance prompts, paired with a smaller ''student'' model to finalize responses. Remarkably, GKT requires no fine-tuning and doesn't necessitate the teacher and student models to have the same vocabulary, allowing for extensive batch generation to accelerate the process while ensuring user customization. GKT can be seamlessly integrated into cloud-edge collaboration architectures, and is versatile enough for plug-and-play application across various models. It excels in both efficiency and affordability, epitomizing a ''cheap and cheerful'' solution. GKT achieves a maximum accuracy improvement of 14.18%, along with a 10.72 times speed-up on GSM8K and an accuracy improvement of 14.00 % along with a 7.73 times speed-up in CSQA. When utilizing ChatGPT as teacher model and Llama2-70B as the student model, we can achieve 95.00% of ChatGPT's performance at 52% of the cost. The results highlight substantial enhancements in accuracy and processing speed on the GSM8K and CSQA datasets, surpassing the performance of using either the student or teacher models in isolation."
    },
    {
        "paperId": "4daff69178cdc5b3c0ba3caf56232ed3500dea92",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "PATIENT-\\psi: Using Large Language Models to Simulate Patients for Training Mental Health Professionals",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.19660, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-30",
        "authors": [
            {
                "authorId": "2295767738",
                "name": "Ruiyi Wang"
            },
            {
                "authorId": "2310824103",
                "name": "Stephanie Milani"
            },
            {
                "authorId": "2303842446",
                "name": "Jamie C. Chiu"
            },
            {
                "authorId": "5539386",
                "name": "S. Eack"
            },
            {
                "authorId": "2298548526",
                "name": "Travis Labrum"
            },
            {
                "authorId": "2281448153",
                "name": "Samuel M. Murphy"
            },
            {
                "authorId": "2303844238",
                "name": "Nev Jones"
            },
            {
                "authorId": "2303842928",
                "name": "Kate Hardy"
            },
            {
                "authorId": "2304890464",
                "name": "Hong Shen"
            },
            {
                "authorId": "2310827411",
                "name": "Fei Fang"
            },
            {
                "authorId": "2257096162",
                "name": "Z. Chen"
            }
        ],
        "abstract": "Mental illness remains one of the most critical public health issues. Despite its importance, many mental health professionals highlight a disconnect between their training and actual real-world patient practice. To help bridge this gap, we propose PATIENT-\\psi, a novel patient simulation framework for cognitive behavior therapy (CBT) training. To build PATIENT-\\psi, we construct diverse patient cognitive models based on CBT principles and use large language models (LLMs) programmed with these cognitive models to act as a simulated therapy patient. We propose an interactive training scheme, PATIENT-\\psi-TRAINER, for mental health trainees to practice a key skill in CBT \u2013 formulating the cognitive model of the patient \u2013 through role-playing a therapy session with PATIENT-\\psi. To evaluate PATIENT-\\psi, we conducted a comprehensive user study of 13 mental health trainees and 20 experts. The results demonstrate that practice using PATIENT-\\psi-TRAINER enhances the perceived skill acquisition and confidence of the trainees beyond existing forms of training such as textbooks, videos, and role-play with non-patients. Based on the experts\u2019 perceptions, PATIENT-\\psi is perceived to be closer to real patient interactions than GPT-4, and PATIENT-\\psi-TRAINER holds strong promise to improve trainee competencies. Our code and data are released at https://github.com/ruiyiw/patient-psi."
    },
    {
        "paperId": "fee64692cf04ee22177726f6785f51ea814fba3b",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.19740, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-30",
        "authors": [
            {
                "authorId": "2237588755",
                "name": "Jiatong Li"
            },
            {
                "authorId": "2283293099",
                "name": "Renjun Hu"
            },
            {
                "authorId": "2257213154",
                "name": "Kunzhe Huang"
            },
            {
                "authorId": "2261108632",
                "name": "Zhuang Yan"
            },
            {
                "authorId": "2303974389",
                "name": "Qi Liu"
            },
            {
                "authorId": "2294616638",
                "name": "Mengxiao Zhu"
            },
            {
                "authorId": "2283143514",
                "name": "Xing Shi"
            },
            {
                "authorId": "2282533964",
                "name": "Wei Lin"
            }
        ],
        "abstract": "Expert-designed close-ended benchmarks are indispensable in assessing the knowledge capacity of large language models (LLMs). Despite their widespread use, concerns have mounted regarding their reliability due to limited test scenarios and an unavoidable risk of data contamination. To rectify this, we present PertEval, a toolkit devised for in-depth probing of LLMs' knowledge capacity through \\textbf{knowledge-invariant perturbations}. These perturbations employ human-like restatement techniques to generate on-the-fly test samples from static benchmarks, meticulously retaining knowledge-critical content while altering irrelevant details. Our toolkit further includes a suite of \\textbf{response consistency analyses} that compare performance on raw vs. perturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six representative LLMs are re-evaluated using PertEval. Results reveal significantly inflated performance of the LLMs on raw benchmarks, including an absolute 25.8% overestimation for GPT-4. Additionally, through a nuanced response pattern analysis, we discover that PertEval retains LLMs' uncertainty to specious knowledge, and reveals their potential rote memorization to correct options which leads to overestimated performance. We also find that the detailed response consistency analyses by PertEval could illuminate various weaknesses in existing LLMs' knowledge mastery and guide the development of refinement. Our findings provide insights for advancing more robust and genuinely knowledgeable LLMs. Our code is available at \\url{https://github.com/aigc-apps/PertEval}."
    },
    {
        "paperId": "b81bc9208c088dfa925375368f249c5e76fddd9e",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Learning to Discuss Strategically: A Case Study on One Night Ultimate Werewolf",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.19946, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-30",
        "authors": [
            {
                "authorId": "2303980025",
                "name": "Xuanfa Jin"
            },
            {
                "authorId": "2142663126",
                "name": "Ziyan Wang"
            },
            {
                "authorId": "2269828122",
                "name": "Yali Du"
            },
            {
                "authorId": "2257271855",
                "name": "Meng Fang"
            },
            {
                "authorId": "2247479851",
                "name": "Haifeng Zhang"
            },
            {
                "authorId": "2257572528",
                "name": "Jun Wang"
            }
        ],
        "abstract": "Communication is a fundamental aspect of human society, facilitating the exchange of information and beliefs among people. Despite the advancements in large language models (LLMs), recent agents built with these often neglect the control over discussion tactics, which are essential in communication scenarios and games. As a variant of the famous communication game Werewolf, One Night Ultimate Werewolf (ONUW) requires players to develop strategic discussion policies due to the potential role changes that increase the uncertainty and complexity of the game. In this work, we first present the existence of the Perfect Bayesian Equilibria (PBEs) in two scenarios of the ONUW game: one with discussion and one without. The results showcase that the discussion greatly changes players' utilities by affecting their beliefs, emphasizing the significance of discussion tactics. Based on the insights obtained from the analyses, we propose an RL-instructed language agent framework, where a discussion policy trained by reinforcement learning (RL) is employed to determine appropriate discussion tactics to adopt. Our experimental results on several ONUW game settings demonstrate the effectiveness and generalizability of our proposed framework. The project page of our paper: $\\href{https://one-night-ultimate-werewolf.github.io}{one-night-ultimate-werewolf.github.io}$."
    },
    {
        "paperId": "ae03f10729959435ecefc0e90cba4cbe8438a10b",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Evaluating Large Language Model Biases in Persona-Steered Generation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.20253, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-30",
        "authors": [
            {
                "authorId": "1998678399",
                "name": "Andy Liu"
            },
            {
                "authorId": "2299327308",
                "name": "Mona T. Diab"
            },
            {
                "authorId": "2303850390",
                "name": "Daniel Fried"
            }
        ],
        "abstract": "The task of persona-steered text generation requires large language models (LLMs) to generate text that reflects the distribution of views that an individual fitting a persona could have. People have multifaceted personas, but prior work on bias in LLM-generated opinions has only explored multiple-choice settings or one-dimensional personas. We define an incongruous persona as a persona with multiple traits where one trait makes its other traits less likely in human survey data, e.g. political liberals who support increased military spending. We find that LLMs are 9.7% less steerable towards incongruous personas than congruous ones, sometimes generating the stereotypical stance associated with its demographic rather than the target stance. Models that we evaluate that are fine-tuned with Reinforcement Learning from Human Feedback (RLHF) are more steerable, especially towards stances associated with political liberals and women, but present significantly less diverse views of personas. We also find variance in LLM steerability that cannot be predicted from multiple-choice opinion evaluation. Our results show the importance of evaluating models in open-ended text generation, as it can surface new LLM opinion biases. Moreover, such a setup can shed light on our ability to steer models toward a richer and more diverse range of viewpoints."
    },
    {
        "paperId": "2d77b7203824e617206634277bce7eec2b71a2bd",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.20701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-31",
        "authors": [
            {
                "authorId": "2105515515",
                "name": "Pengwei Zhan"
            },
            {
                "authorId": "2294783971",
                "name": "Zhen Xu"
            },
            {
                "authorId": "2149324430",
                "name": "Qian Tan"
            },
            {
                "authorId": "2304366173",
                "name": "Jie Song"
            },
            {
                "authorId": "2304326919",
                "name": "Ru Xie"
            }
        ],
        "abstract": "Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks. Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions. In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans. By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different. Following this property, we propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE). COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence. Experiments show that even widely-used human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks."
    },
    {
        "paperId": "646974c82e68b600316206442fc21b9a31659ff4",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "That's Optional: A Contemporary Exploration of \"that\" Omission in English Subordinate Clauses",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.20833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-31",
        "authors": [
            {
                "authorId": "2304322274",
                "name": "Ella Rabinovich"
            }
        ],
        "abstract": "The Uniform Information Density (UID) hypothesis posits that speakers optimize the communicative properties of their utterances by avoiding spikes in information, thereby maintaining a relatively uniform information profile over time. This paper investigates the impact of UID principles on syntactic reduction, specifically focusing on the optional omission of the connector\"that\"in English subordinate clauses. Building upon previous research, we extend our investigation to a larger corpus of written English, utilize contemporary large language models (LLMs) and extend the information-uniformity principles by the notion of entropy, to estimate the UID manifestations in the usecase of syntactic reduction choices."
    },
    {
        "paperId": "d8ba6085c7c32cc367ab7f50d18744de846fcebd",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Preemptive Answer \"Attacks\" on Chain-of-Thought Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.20902, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-05-31",
        "authors": [
            {
                "authorId": "2158524037",
                "name": "Rongwu Xu"
            },
            {
                "authorId": "2257044451",
                "name": "Zehan Qi"
            },
            {
                "authorId": "2304324554",
                "name": "Wei Xu"
            }
        ],
        "abstract": "Large language models (LLMs) showcase impressive reasoning capabilities when coupled with Chain-of-Thought (CoT) prompting. However, the robustness of this approach warrants further investigation. In this paper, we introduce a novel scenario termed preemptive answers, where the LLM obtains an answer before engaging in reasoning. This situation can arise inadvertently or induced by malicious users by prompt injection attacks. Experiments reveal that preemptive answers significantly impair the model's reasoning capability across various CoT methods and a broad spectrum of datasets. To bolster the robustness of reasoning, we propose two measures aimed at mitigating this issue to some extent."
    },
    {
        "paperId": "5c8245c04c902b0dd631b8633ca835967f395ad0",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "HonestLLM: Toward an Honest and Helpful Large Language Model",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.00380, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-01",
        "authors": [
            {
                "authorId": "2279094112",
                "name": "Chujie Gao"
            },
            {
                "authorId": "2254867423",
                "name": "Siyuan Wu"
            },
            {
                "authorId": "2257084278",
                "name": "Yue Huang"
            },
            {
                "authorId": "2279219833",
                "name": "Dongping Chen"
            },
            {
                "authorId": "46324457",
                "name": "Qihui Zhang"
            },
            {
                "authorId": "2294870270",
                "name": "Zhengyan Fu"
            },
            {
                "authorId": "2254266993",
                "name": "Yao Wan"
            },
            {
                "authorId": "2257131651",
                "name": "Lichao Sun"
            },
            {
                "authorId": "2307963162",
                "name": "Xiangliang Zhang"
            }
        ],
        "abstract": "Large Language Models (LLMs) have achieved remarkable success across various industries due to their exceptional generative capabilities. However, for safe and effective real-world deployments, ensuring honesty and helpfulness is critical. This paper addresses the question: Can we prioritize the helpfulness of LLMs while preserving their honesty? To begin with, we establish exhaustive principles aimed at guaranteeing the honesty of LLM. Additionally, we introduce a novel dataset, referred to as HoneSet, comprising 930 queries spanning six categories meticulously crafted to assess an LLM's capacity for maintaining honesty. Subsequently, we present two approaches to augmenting honesty and helpfulness in LLMs: a training-free enhancement and a fine-tuning-based improvement. The training-free approach, which is based on curiosity-driven prompting, empowers LLMs to articulate internal confusion and uncertainty regarding queries, thereby optimizing their responses. Conversely, the fine-tuning-based method employs a two-stage process inspired by curriculum learning: initially instructing LLMs to discern between honest and dishonest responses, then refining their training to enhance helpfulness. Experiments conducted on nine prominent LLMs demonstrate a significant improvement in alignment with honesty across all models through the implementation of our proposed enhancements. Particularly noteworthy is the 65.3% enhancement observed in Llama3-8b and the remarkable 124.7% improvement in Mistral-7b, as measured by the H$^{2}$ (honest and helpful) assessment. We believe that our work can pave the way for developing more trustworthy LLMs for real-world applications."
    },
    {
        "paperId": "8d49a00ac9829942e9b4dd2cf7683490a9161a87",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Is In-Context Learning in Large Language Models Bayesian? A Martingale Perspective",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.00793, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-02",
        "authors": [
            {
                "authorId": "2065051911",
                "name": "Fabian Falck"
            },
            {
                "authorId": "2293764317",
                "name": "Ziyu Wang"
            },
            {
                "authorId": "2293724535",
                "name": "Chris Holmes"
            }
        ],
        "abstract": "In-context learning (ICL) has emerged as a particularly remarkable characteristic of Large Language Models (LLM): given a pretrained LLM and an observed dataset, LLMs can make predictions for new data points from the same distribution without fine-tuning. Numerous works have postulated ICL as approximately Bayesian inference, rendering this a natural hypothesis. In this work, we analyse this hypothesis from a new angle through the martingale property, a fundamental requirement of a Bayesian learning system for exchangeable data. We show that the martingale property is a necessary condition for unambiguous predictions in such scenarios, and enables a principled, decomposed notion of uncertainty vital in trustworthy, safety-critical systems. We derive actionable checks with corresponding theory and test statistics which must hold if the martingale property is satisfied. We also examine if uncertainty in LLMs decreases as expected in Bayesian learning when more data is observed. In three experiments, we provide evidence for violations of the martingale property, and deviations from a Bayesian scaling behaviour of uncertainty, falsifying the hypothesis that ICL is Bayesian."
    },
    {
        "paperId": "be2f3ad5d46ab7fe22e66301ec7149abe40b9da6",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Using RL to Identify Divisive Perspectives Improves LLMs Abilities to Identify Communities on Social Media",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.00969, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-03",
        "authors": [
            {
                "authorId": "144370236",
                "name": "Nikhil Mehta"
            },
            {
                "authorId": "2240520676",
                "name": "Dan Goldwasser"
            }
        ],
        "abstract": "The large scale usage of social media, combined with its significant impact, has made it increasingly important to understand it. In particular, identifying user communities, can be helpful for many downstream tasks. However, particularly when models are trained on past data and tested on future, doing this is difficult. In this paper, we hypothesize to take advantage of Large Language Models (LLMs), to better identify user communities. Due to the fact that many LLMs, such as ChatGPT, are fixed and must be treated as black-boxes, we propose an approach to better prompt them, by training a smaller LLM to do this. We devise strategies to train this smaller model, showing how it can improve the larger LLMs ability to detect communities. Experimental results show improvements on Reddit and Twitter data, on the tasks of community detection, bot detection, and news media profiling."
    },
    {
        "paperId": "fd80d2ca2bb584875a4dfbf5345b8c705a9a3a26",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Latent Logic Tree Extraction for Event Sequence Explanation from LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.01124, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-03",
        "authors": [
            {
                "authorId": "2257123096",
                "name": "Zitao Song"
            },
            {
                "authorId": "2220433094",
                "name": "Chao Yang"
            },
            {
                "authorId": "2304610570",
                "name": "Chaojie Wang"
            },
            {
                "authorId": "2304429498",
                "name": "Bo An"
            },
            {
                "authorId": "2228313627",
                "name": "Shuang Li"
            }
        ],
        "abstract": "Modern high-stakes systems, such as healthcare or robotics, often generate vast streaming event sequences. Our goal is to design an efficient, plug-and-play tool to elicit logic tree-based explanations from Large Language Models (LLMs) to provide customized insights into each observed event sequence. Built on the temporal point process model for events, our method employs the likelihood function as a score to evaluate generated logic trees. We propose an amortized Expectation-Maximization (EM) learning framework and treat the logic tree as latent variables. In the E-step, we evaluate the posterior distribution over the latent logic trees using an LLM prior and the likelihood of the observed event sequences. LLM provides a high-quality prior for the latent logic trees, however, since the posterior is built over a discrete combinatorial space, we cannot get the closed-form solution. We propose to generate logic tree samples from the posterior using a learnable GFlowNet, which is a diversity-seeking generator for structured discrete variables. The M-step employs the generated logic rules to approximate marginalization over the posterior, facilitating the learning of model parameters and refining the tunable LLM prior parameters. In the online setting, our locally built, lightweight model will iteratively extract the most relevant rules from LLMs for each sequence using only a few iterations. Empirical demonstrations showcase the promising performance and adaptability of our framework."
    },
    {
        "paperId": "394c1272a672ddbfdde581c677617b960029c253",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.01171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-03",
        "authors": [
            {
                "authorId": "2304446881",
                "name": "Yu-Min Tseng"
            },
            {
                "authorId": "2295597018",
                "name": "Yu-Chao Huang"
            },
            {
                "authorId": "2304467267",
                "name": "Teng-Yun Hsiao"
            },
            {
                "authorId": "2304519505",
                "name": "Yu-Ching Hsu"
            },
            {
                "authorId": "2304424513",
                "name": "Jia-Yin Foo"
            },
            {
                "authorId": "47396497",
                "name": "Chao-Wei Huang"
            },
            {
                "authorId": "2286884447",
                "name": "Yun-Nung Chen"
            }
        ],
        "abstract": "The concept of persona, originally adopted in dialogue literature, has re-surged as a promising framework for tailoring large language models (LLMs) to specific context (e.g., personalized search, LLM-as-a-judge). However, the growing research on leveraging persona in LLMs is relatively disorganized and lacks a systematic taxonomy. To close the gap, we present a comprehensive survey to categorize the current state of the field. We identify two lines of research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs take care of user personas. Additionally, we introduce existing methods for LLM personality evaluation. To the best of our knowledge, we present the first survey for role-playing and personalization in LLMs under the unified view of persona. We continuously maintain a paper collection to foster future endeavors: https://github.com/MiuLab/PersonaLLM-Survey"
    },
    {
        "paperId": "face23fe92eb8f547259d8e4f99cc37379342b39",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.01382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-03",
        "authors": [
            {
                "authorId": "70025184",
                "name": "Keyon Vafa"
            },
            {
                "authorId": "88738815",
                "name": "Ashesh Rambachan"
            },
            {
                "authorId": "2253742235",
                "name": "S. Mullainathan"
            }
        ],
        "abstract": "What makes large language models (LLMs) impressive is also what makes them hard to evaluate: their diversity of uses. To evaluate these models, we must understand the purposes they will be used for. We consider a setting where these deployment decisions are made by people, and in particular, people's beliefs about where an LLM will perform well. We model such beliefs as the consequence of a human generalization function: having seen what an LLM gets right or wrong, people generalize to where else it might succeed. We collect a dataset of 19K examples of how humans make generalizations across 79 tasks from the MMLU and BIG-Bench benchmarks. We show that the human generalization function can be predicted using NLP methods: people have consistent structured ways to generalize. We then evaluate LLM alignment with the human generalization function. Our results show that -- especially for cases where the cost of mistakes is high -- more capable models (e.g. GPT-4) can do worse on the instances people choose to use them for, exactly because they are not aligned with the human generalization function."
    },
    {
        "paperId": "b1b6948ea473605e7272970c77cbbcabd4569577",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.01806, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-03",
        "authors": [
            {
                "authorId": "2046763041",
                "name": "Zhen Lin"
            },
            {
                "authorId": "145927896",
                "name": "Shubhendu Trivedi"
            },
            {
                "authorId": "2290028416",
                "name": "Jimeng Sun"
            }
        ],
        "abstract": "The advent of large language models (LLMs) has dramatically advanced the state-of-the-art in numerous natural language generation tasks. For LLMs to be applied reliably, it is essential to have an accurate measure of their confidence. Currently, the most commonly used confidence score function is the likelihood of the generated sequence, which, however, conflates semantic and syntactic components. For instance, in question-answering (QA) tasks, an awkward phrasing of the correct answer might result in a lower probability prediction. Additionally, different tokens should be weighted differently depending on the context. In this work, we propose enhancing the predicted sequence probability by assigning different weights to various tokens using attention values elicited from the base LLM. By employing a validation set, we can identify the relevant attention heads, thereby significantly improving the reliability of the vanilla sequence probability confidence measure. We refer to this new score as the Contextualized Sequence Likelihood (CSL). CSL is easy to implement, fast to compute, and offers considerable potential for further improvement with task-specific prompts. Across several QA datasets and a diverse array of LLMs, CSL has demonstrated significantly higher reliability than state-of-the-art baselines in predicting generation quality, as measured by the AUROC or AUARC."
    },
    {
        "paperId": "637357eb041ea0f1e113f7f37e934c8e40103508",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Reinforcement Tuning for Detecting Stances and Debunking Rumors Jointly with Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.02143, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-04",
        "authors": [
            {
                "authorId": "2152903772",
                "name": "Ruichao Yang"
            },
            {
                "authorId": "2261685863",
                "name": "Wei Gao"
            },
            {
                "authorId": "2157404600",
                "name": "Jing Ma"
            },
            {
                "authorId": "2109380683",
                "name": "Hongzhan Lin"
            },
            {
                "authorId": "2153211826",
                "name": "Bo Wang"
            }
        ],
        "abstract": "Learning multi-task models for jointly detecting stance and verifying rumors poses challenges due to the need for training data of stance at post level and rumor veracity at claim level, which are difficult to obtain. To address this issue, we leverage large language models (LLMs) as the foundation annotators for the joint stance detection (SD) and rumor verification (RV) tasks, dubbed as JSDRV. We introduce a novel reinforcement tuning framework to enhance the joint predictive capabilities of LLM-based SD and RV components. Specifically, we devise a policy for selecting LLM-annotated data at the two levels, employing a hybrid reward mechanism to choose high-quality labels for effective LLM fine-tuning on both tasks. Results demonstrate that JSDRV improves the capabilities of LLMs in the joint tasks, not only outperforming state-of-the-art methods but also generalizing to non-LLMs accommodated as task models."
    },
    {
        "paperId": "5aad568d4f02b09af3c282b1f4c20ee0993bc2e6",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.02472, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-04",
        "authors": [
            {
                "authorId": "2304613417",
                "name": "Zhihan Zhang"
            },
            {
                "authorId": "2258806194",
                "name": "Yixin Cao"
            },
            {
                "authorId": "2269465569",
                "name": "Chenchen Ye"
            },
            {
                "authorId": "51487414",
                "name": "Yunshan Ma"
            },
            {
                "authorId": "32781973",
                "name": "Lizi Liao"
            },
            {
                "authorId": "2270722858",
                "name": "Tat-Seng Chua"
            }
        ],
        "abstract": "The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events. We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window."
    },
    {
        "paperId": "5322cd631e69ae484038b13ac320194afaccdc3b",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "To Believe or Not to Believe Your LLM",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.02543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-04",
        "authors": [
            {
                "authorId": "1388837087",
                "name": "Yasin Abbasi-Yadkori"
            },
            {
                "authorId": "3150458",
                "name": "Ilja Kuzborskij"
            },
            {
                "authorId": "2305592785",
                "name": "Andr\u00e1s Gy\u00f6rgy"
            },
            {
                "authorId": "2257346986",
                "name": "Csaba Szepesv'ari"
            }
        ],
        "abstract": "We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest."
    },
    {
        "paperId": "1b0fa09f097591d697162300cc6ecb3ee425fd8d",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Chain of Agents: Large Language Models Collaborating on Long-Context Tasks",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.02818, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-04",
        "authors": [
            {
                "authorId": "2108051142",
                "name": "Yusen Zhang"
            },
            {
                "authorId": "2068169921",
                "name": "Ruoxi Sun"
            },
            {
                "authorId": "2258603407",
                "name": "Yanfei Chen"
            },
            {
                "authorId": "2264567300",
                "name": "Tomas Pfister"
            },
            {
                "authorId": "2305152311",
                "name": "Rui Zhang"
            },
            {
                "authorId": "2676352",
                "name": "Sercan \u00d6. Arik"
            }
        ],
        "abstract": "Addressing the challenge of effectively processing long contexts has become a critical issue for Large Language Models (LLMs). Two common strategies have emerged: 1) reducing the input length, such as retrieving relevant chunks by Retrieval-Augmented Generation (RAG), and 2) expanding the context window limit of LLMs. However, both strategies have drawbacks: input reduction has no guarantee of covering the part with needed information, while window extension struggles with focusing on the pertinent information for solving the task. To mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework that harnesses multi-agent collaboration through natural language to enable information aggregation and context reasoning across various LLMs over long-context tasks. CoA consists of multiple worker agents who sequentially communicate to handle different segmented portions of the text, followed by a manager agent who synthesizes these contributions into a coherent final output. CoA processes the entire input by interleaving reading and reasoning, and it mitigates long context focus issues by assigning each agent a short context. We perform comprehensive evaluation of CoA on a wide range of long-context tasks in question answering, summarization, and code completion, demonstrating significant improvements by up to 10% over strong baselines of RAG, Full-Context, and multi-agent LLMs."
    },
    {
        "paperId": "5fda196ca0039089181e7d8e22cc500cbea4c877",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "NUMCoT: Numerals and Units of Measurement in Chain-of-Thought Reasoning using Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.02864, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-05",
        "authors": [
            {
                "authorId": "2291366467",
                "name": "Ancheng Xu"
            },
            {
                "authorId": "2007546355",
                "name": "Minghuan Tan"
            },
            {
                "authorId": "2304755670",
                "name": "Lei Wang"
            },
            {
                "authorId": "2291892150",
                "name": "Min Yang"
            },
            {
                "authorId": "2266809482",
                "name": "Ruifeng Xu"
            }
        ],
        "abstract": "Numeral systems and units of measurement are two conjoined topics in activities of human beings and have mutual effects with the languages expressing them. Currently, the evaluation of Large Language Models (LLMs) often involves mathematical reasoning, yet little attention is given to how minor changes in numbers or units can drastically alter the complexity of problems and the performance of LLMs. In this paper, we scrutinize existing LLMs on processing of numerals and units of measurement by constructing datasets with perturbations. We first anatomize the reasoning of math word problems to different sub-procedures like numeral conversions from language to numbers and measurement conversions based on units. Then we further annotate math word problems from ancient Chinese arithmetic works which are challenging in numerals and units of measurement. Experiments on perturbed datasets demonstrate that LLMs still encounter difficulties in handling numeral and measurement conversions."
    },
    {
        "paperId": "4acab93bf43857eb834f5401aab83781657cf30f",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Improving In-Context Learning with Prediction Feedback for Sentiment Analysis",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.02911, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-05",
        "authors": [
            {
                "authorId": "2267313691",
                "name": "Hongling Xu"
            },
            {
                "authorId": "2267013210",
                "name": "Qianlong Wang"
            },
            {
                "authorId": "51213827",
                "name": "Yice Zhang"
            },
            {
                "authorId": "2144399430",
                "name": "Min Yang"
            },
            {
                "authorId": "2307083414",
                "name": "Xi Zeng"
            },
            {
                "authorId": "2257014309",
                "name": "Bing Qin"
            },
            {
                "authorId": "2266809482",
                "name": "Ruifeng Xu"
            }
        ],
        "abstract": "Large language models (LLMs) have achieved promising results in sentiment analysis through the in-context learning (ICL) paradigm. However, their ability to distinguish subtle sentiments still remains a challenge. Inspired by the human ability to adjust understanding via feedback, this paper enhances ICL by incorporating prior predictions and feedback, aiming to rectify sentiment misinterpretation of LLMs. Specifically, the proposed framework consists of three steps: (1) acquiring prior predictions of LLMs, (2) devising predictive feedback based on correctness, and (3) leveraging a feedback-driven prompt to refine sentiment understanding. Experimental results across nine sentiment analysis datasets demonstrate the superiority of our framework over conventional ICL methods, with an average F1 improvement of 5.95%."
    },
    {
        "paperId": "a7919a3c6dbdcc524776a3102110d637836ad2e0",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.02924, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-05",
        "authors": [
            {
                "authorId": "2265581222",
                "name": "Peijie Dong"
            },
            {
                "authorId": "2240002722",
                "name": "Lujun Li"
            },
            {
                "authorId": "66873962",
                "name": "Zhenheng Tang"
            },
            {
                "authorId": "2265743820",
                "name": "Xiang Liu"
            },
            {
                "authorId": "2237187329",
                "name": "Xinglin Pan"
            },
            {
                "authorId": "2183630484",
                "name": "Qiang Wang"
            },
            {
                "authorId": "2259315629",
                "name": "Xiaowen Chu"
            }
        ],
        "abstract": "Despite the remarkable capabilities, Large Language Models (LLMs) face deployment challenges due to their extensive size. Pruning methods drop a subset of weights to accelerate, but many of them require retraining, which is prohibitively expensive and computationally demanding. Recently, post-training pruning approaches introduced novel metrics, enabling the pruning of LLMs without retraining. However, these metrics require the involvement of human experts and tedious trial and error. To efficiently identify superior pruning metrics, we develop an automatic framework for searching symbolic pruning metrics using genetic programming. In particular, we devise an elaborate search space encompassing the existing pruning metrics to discover the potential symbolic pruning metric. We propose an opposing operation simplification strategy to increase the diversity of the population. In this way, Pruner-Zero allows auto-generation of symbolic pruning metrics. Based on the searched results, we explore the correlation between pruning metrics and performance after pruning and summarize some principles. Extensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot tasks demonstrate that our Pruner-Zero obtains superior performance than SOTA post-training pruning methods. Code at: \\url{https://github.com/pprp/Pruner-Zero}."
    },
    {
        "paperId": "97f7a0c4be425f0019f7fca603d3dfd522da025a",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.02958, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-05",
        "authors": [
            {
                "authorId": "1443436264",
                "name": "Charlie Hou"
            },
            {
                "authorId": "1519979046",
                "name": "Akshat Shrivastava"
            },
            {
                "authorId": "2304748832",
                "name": "Hongyuan Zhan"
            },
            {
                "authorId": "2304748670",
                "name": "Rylan Conway"
            },
            {
                "authorId": "2241614580",
                "name": "Trang Le"
            },
            {
                "authorId": "2063995456",
                "name": "Adithya Sagar"
            },
            {
                "authorId": "2288256947",
                "name": "Giulia Fanti"
            },
            {
                "authorId": "2178345339",
                "name": "Daniel Lazar"
            }
        ],
        "abstract": "On-device training is currently the most common approach for training machine learning (ML) models on private, distributed user data. Despite this, on-device training has several drawbacks: (1) most user devices are too small to train large models on-device, (2) on-device training is communication- and computation-intensive, and (3) on-device training can be difficult to debug and deploy. To address these problems, we propose Private Evolution-Text (PrE-Text), a method for generating differentially private (DP) synthetic textual data. First, we show that across multiple datasets, training small models (models that fit on user devices) with PrE-Text synthetic data outperforms small models trained on-device under practical privacy regimes ($\\epsilon=1.29$, $\\epsilon=7.58$). We achieve these results while using 9$\\times$ fewer rounds, 6$\\times$ less client computation per round, and 100$\\times$ less communication per round. Second, finetuning large models on PrE-Text's DP synthetic data improves large language model (LLM) performance on private data across the same range of privacy budgets. Altogether, these results suggest that training on DP synthetic data can be a better option than training a model on-device on private distributed data. Code is available at https://github.com/houcharlie/PrE-Text."
    },
    {
        "paperId": "62b5568279360b8fda2d61af2bdbe575f02975b4",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Which Side Are You On? A Multi-task Dataset for End-to-End Argument Summarisation and Evaluation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.03151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-05",
        "authors": [
            {
                "authorId": "2274084219",
                "name": "Hao Li"
            },
            {
                "authorId": "2304897005",
                "name": "Yuping Wu"
            },
            {
                "authorId": "71034258",
                "name": "Viktor Schlegel"
            },
            {
                "authorId": "1400900759",
                "name": "R. Batista-Navarro"
            },
            {
                "authorId": "2091992333",
                "name": "Tharindu Madusanka"
            },
            {
                "authorId": "90788907",
                "name": "Iqra Zahid"
            },
            {
                "authorId": "2305548589",
                "name": "Jiayan Zeng"
            },
            {
                "authorId": "2304894943",
                "name": "Xiaochi Wang"
            },
            {
                "authorId": "2305040475",
                "name": "Xinran He"
            },
            {
                "authorId": "2304898804",
                "name": "Yizhi Li"
            },
            {
                "authorId": "2253451328",
                "name": "Goran Nenadic"
            }
        ],
        "abstract": "With the recent advances of large language models (LLMs), it is no longer infeasible to build an automated debate system that helps people to synthesise persuasive arguments. Previous work attempted this task by integrating multiple components. In our work, we introduce an argument mining dataset that captures the end-to-end process of preparing an argumentative essay for a debate, which covers the tasks of claim and evidence identification (Task 1 ED), evidence convincingness ranking (Task 2 ECR), argumentative essay summarisation and human preference ranking (Task 3 ASR) and metric learning for automated evaluation of resulting essays, based on human feedback along argument quality dimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are fully annotated with the various properties supporting the aforementioned tasks. We evaluate multiple generative baselines for each of these tasks, including representative LLMs. We find, that while they show promising results on individual tasks in our benchmark, their end-to-end performance on all four tasks in succession deteriorates significantly, both in automated measures as well as in human-centred evaluation. This challenge presented by our proposed dataset motivates future research on end-to-end argument mining and summarisation. The repository of this project is available at https://github.com/HaoBytes/ArgSum-Datatset"
    },
    {
        "paperId": "beb5794995196c84d9325f0e889c23c14f9b1c9c",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Missci: Reconstructing Fallacies in Misrepresented Science",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.03181, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-05",
        "authors": [
            {
                "authorId": "46179620",
                "name": "Max Glockner"
            },
            {
                "authorId": "2282504397",
                "name": "Yufang Hou"
            },
            {
                "authorId": "2026545715",
                "name": "Preslav Nakov"
            },
            {
                "authorId": "2260340390",
                "name": "Iryna Gurevych"
            }
        ],
        "abstract": "Health-related misinformation on social networks can lead to poor decision-making and real-world dangers. Such misinformation often misrepresents scientific publications and cites them as\"proof\"to gain perceived credibility. To effectively counter such claims automatically, a system must explain how the claim was falsely derived from the cited publication. Current methods for automated fact-checking or fallacy detection neglect to assess the (mis)used evidence in relation to misinformation claims, which is required to detect the mismatch between them. To address this gap, we introduce Missci, a novel argumentation theoretical model for fallacious reasoning together with a new dataset for real-world misinformation detection that misrepresents biomedical publications. Unlike previous fallacy detection datasets, Missci (i) focuses on implicit fallacies between the relevant content of the cited publication and the inaccurate claim, and (ii) requires models to verbalize the fallacious reasoning in addition to classifying it. We present Missci as a dataset to test the critical reasoning abilities of large language models (LLMs), that are required to reconstruct real-world fallacious arguments, in a zero-shot setting. We evaluate two representative LLMs and the impact of different levels of detail about the fallacy classes provided to the LLM via prompts. Our experiments and human evaluation show promising results for GPT 4, while also demonstrating the difficulty of this task."
    },
    {
        "paperId": "2ba56b65c0ccd82ce7ef96a5008194932d934861",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "LLM-based Rewriting of Inappropriate Argumentation using Reinforcement Learning from Machine Feedback",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.03363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-05",
        "authors": [
            {
                "authorId": "2047307140",
                "name": "Timon Ziegenbein"
            },
            {
                "authorId": "2478960",
                "name": "Gabriella Skitalinskaya"
            },
            {
                "authorId": "2211207204",
                "name": "Alireza Bayat Makou"
            },
            {
                "authorId": "2293312008",
                "name": "Henning Wachsmuth"
            }
        ],
        "abstract": "Ensuring that online discussions are civil and productive is a major challenge for social media platforms. Such platforms usually rely both on users and on automated detection tools to flag inappropriate arguments of other users, which moderators then review. However, this kind of post-hoc moderation is expensive and time-consuming, and moderators are often overwhelmed by the amount and severity of flagged content. Instead, a promising alternative is to prevent negative behavior during content creation. This paper studies how inappropriate language in arguments can be computationally mitigated. We propose a reinforcement learning-based rewriting approach that balances content preservation and appropriateness based on existing classifiers, prompting an instruction-finetuned large language model (LLM) as our initial policy. Unlike related style transfer tasks, rewriting inappropriate arguments allows deleting and adding content permanently. It is therefore tackled on document level rather than sentence level. We evaluate different weighting schemes for the reward function in both absolute and relative human assessment studies. Systematic experiments on non-parallel data provide evidence that our approach can mitigate the inappropriateness of arguments while largely preserving their content. It significantly outperforms competitive baselines, including few-shot learning, prompting, and humans."
    },
    {
        "paperId": "a95ca00e71eaadb1bb86015badf08e54bb55134b",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "BIPED: Pedagogically Informed Tutoring System for ESL Education",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.03486, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-05",
        "authors": [
            {
                "authorId": "2292676489",
                "name": "Soonwoo Kwon"
            },
            {
                "authorId": "2292454026",
                "name": "Sojung Kim"
            },
            {
                "authorId": "2292398858",
                "name": "Minju Park"
            },
            {
                "authorId": "2292405500",
                "name": "Seunghyun Lee"
            },
            {
                "authorId": "2233125631",
                "name": "Kyuseok Kim"
            }
        ],
        "abstract": "Large Language Models (LLMs) have a great potential to serve as readily available and cost-efficient Conversational Intelligent Tutoring Systems (CITS) for teaching L2 learners of English. Existing CITS, however, are designed to teach only simple concepts or lack the pedagogical depth necessary to address diverse learning strategies. To develop a more pedagogically informed CITS capable of teaching complex concepts, we construct a BIlingual PEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human English tutoring interactions. Through post-hoc analysis of the tutoring interactions, we come up with a lexicon of dialogue acts (34 tutor acts and 9 student acts), which we use to further annotate the collected dataset. Based on a two-step framework of first predicting the appropriate tutor act then generating the corresponding response, we implemented two CITS models using GPT-4 and SOLAR-KO, respectively. We experimentally demonstrate that the implemented models not only replicate the style of human teachers but also employ diverse and contextually appropriate pedagogical strategies."
    },
    {
        "paperId": "72c5a61c034c3cce735d6280c43e084091c0edc3",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "TACT: Advancing Complex Aggregative Reasoning with Information Extraction Tools",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.03618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-05",
        "authors": [
            {
                "authorId": "2288816486",
                "name": "Avi Caciularu"
            },
            {
                "authorId": "41016275",
                "name": "Alon Jacovi"
            },
            {
                "authorId": "2304952646",
                "name": "Eyal Ben-David"
            },
            {
                "authorId": "35540270",
                "name": "S. Goldshtein"
            },
            {
                "authorId": "32303439",
                "name": "Tal Schuster"
            },
            {
                "authorId": "2253566854",
                "name": "Jonathan Herzig"
            },
            {
                "authorId": "1684677",
                "name": "G. Elidan"
            },
            {
                "authorId": "2304952308",
                "name": "Amir Globerson"
            }
        ],
        "abstract": "Large Language Models (LLMs) often do not perform well on queries that require the aggregation of information across texts. To better evaluate this setting and facilitate modeling efforts, we introduce TACT - Text And Calculations through Tables, a dataset crafted to evaluate LLMs' reasoning and computational abilities using complex instructions. TACT contains challenging instructions that demand stitching information scattered across one or more texts, and performing complex integration on this information to generate the answer. We construct this dataset by leveraging an existing dataset of texts and their associated tables. For each such tables, we formulate new queries, and gather their respective answers. We demonstrate that all contemporary LLMs perform poorly on this dataset, achieving an accuracy below 38%. To pinpoint the difficulties and thoroughly dissect the problem, we analyze model performance across three components: table-generation, Pandas command-generation, and execution. Unexpectedly, we discover that each component presents substantial challenges for current LLMs. These insights lead us to propose a focused modeling framework, which we refer to as IE as a tool. Specifically, we propose to add\"tools\"for each of the above steps, and implement each such tool with few-shot prompting. This approach shows an improvement over existing prompting techniques, offering a promising direction for enhancing model capabilities in these tasks."
    },
    {
        "paperId": "7009263033f6fdbad688c1cb010a24be3356ba25",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "M-QALM: A Benchmark to Assess Clinical Reading Comprehension and Knowledge Recall in Large Language Models via Question Answering",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.03699, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-06",
        "authors": [
            {
                "authorId": "2304955248",
                "name": "Anand Subramanian"
            },
            {
                "authorId": "71034258",
                "name": "Viktor Schlegel"
            },
            {
                "authorId": "41124383",
                "name": "Abhinav Ramesh Kashyap"
            },
            {
                "authorId": "2117824172",
                "name": "Thanh-Tung Nguyen"
            },
            {
                "authorId": "2275599629",
                "name": "Vijay Prakash Dwivedi"
            },
            {
                "authorId": "2057271731",
                "name": "Stefan Winkler"
            }
        ],
        "abstract": "There is vivid research on adapting Large Language Models (LLMs) to perform a variety of tasks in high-stakes domains such as healthcare. Despite their popularity, there is a lack of understanding of the extent and contributing factors that allow LLMs to recall relevant knowledge and combine it with presented information in the clinical and biomedical domain: a fundamental pre-requisite for success on down-stream tasks. Addressing this gap, we use Multiple Choice and Abstractive Question Answering to conduct a large-scale empirical study on 22 datasets in three generalist and three specialist biomedical sub-domains. Our multifaceted analysis of the performance of 15 LLMs, further broken down by sub-domain, source of knowledge and model architecture, uncovers success factors such as instruction tuning that lead to improved recall and comprehension. We further show that while recently proposed domain-adapted models may lack adequate knowledge, directly fine-tuning on our collected medical knowledge datasets shows encouraging results, even generalising to unseen specialist sub-domains. We complement the quantitative results with a skill-oriented manual error analysis, which reveals a significant gap between the models' capabilities to simply recall necessary knowledge and to integrate it with the presented context. To foster research and collaboration in this field we share M-QALM, our resources, standardised methodology, and evaluation results, with the research community to facilitate further advancements in clinical knowledge representation learning within language models."
    },
    {
        "paperId": "110be9d98a8175028bc8360e6bfbff84d3fd96f2",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Ask LLMs Directly, \"What shapes your bias?\": Measuring Social Bias in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04064, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-06",
        "authors": [
            {
                "authorId": "2180054773",
                "name": "Jisu Shin"
            },
            {
                "authorId": "1488736131",
                "name": "Hoyun Song"
            },
            {
                "authorId": "2150564118",
                "name": "Huije Lee"
            },
            {
                "authorId": "8599185",
                "name": "Soyeong Jeong"
            },
            {
                "authorId": "2152282232",
                "name": "Jong C. Park"
            }
        ],
        "abstract": "Social bias is shaped by the accumulation of social perceptions towards targets across various demographic identities. To fully understand such social bias in large language models (LLMs), it is essential to consider the composite of social perceptions from diverse perspectives among identities. Previous studies have either evaluated biases in LLMs by indirectly assessing the presence of sentiments towards demographic identities in the generated text or measuring the degree of alignment with given stereotypes. These methods have limitations in directly quantifying social biases at the level of distinct perspectives among identities. In this paper, we aim to investigate how social perceptions from various viewpoints contribute to the development of social bias in LLMs. To this end, we propose a novel strategy to intuitively quantify these social perceptions and suggest metrics that can evaluate the social biases within LLMs by aggregating diverse social perceptions. The experimental results show the quantitative demonstration of the social attitude in LLMs by examining social perception. The analysis we conducted shows that our proposed metrics capture the multi-dimensional aspects of social bias, enabling a fine-grained and comprehensive investigation of bias in LLMs."
    },
    {
        "paperId": "bfbe8a37906749f09c4ad38e452ce1adb10a29db",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Confabulation: The Surprising Value of Large Language Model Hallucinations",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-06",
        "authors": [
            {
                "authorId": "2304952629",
                "name": "Peiqi Sui"
            },
            {
                "authorId": "2193109",
                "name": "Eamon Duede"
            },
            {
                "authorId": "2305033132",
                "name": "Sophie Wu"
            },
            {
                "authorId": "2304954202",
                "name": "Richard Jean So"
            }
        ],
        "abstract": "This paper presents a systematic defense of large language model (LLM) hallucinations or 'confabulations' as a potential resource instead of a categorically negative pitfall. The standard view is that confabulations are inherently problematic and AI research should eliminate this flaw. In this paper, we argue and empirically demonstrate that measurable semantic characteristics of LLM confabulations mirror a human propensity to utilize increased narrativity as a cognitive resource for sense-making and communication. In other words, it has potential value. Specifically, we analyze popular hallucination benchmarks and reveal that hallucinated outputs display increased levels of narrativity and semantic coherence relative to veridical outputs. This finding reveals a tension in our usually dismissive understandings of confabulation. It suggests, counter-intuitively, that the tendency for LLMs to confabulate may be intimately associated with a positive capacity for coherent narrative-text generation."
    },
    {
        "paperId": "d0d229d4308eeeb4b67af6c43893bbdfb7191a80",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-06",
        "authors": [
            {
                "authorId": "2159546783",
                "name": "Yuanyi Ren"
            },
            {
                "authorId": "2305525473",
                "name": "Haoran Ye"
            },
            {
                "authorId": "2306186004",
                "name": "Hanjun Fang"
            },
            {
                "authorId": "2304969090",
                "name": "Xin Zhang"
            },
            {
                "authorId": "2282505480",
                "name": "Guojie Song"
            }
        ],
        "abstract": "Large Language Models (LLMs) are transforming diverse fields and gaining increasing influence as human proxies. This development underscores the urgent need for evaluating value orientations and understanding of LLMs to ensure their responsible integration into public-facing applications. This work introduces ValueBench, the first comprehensive psychometric benchmark for evaluating value orientations and value understanding in LLMs. ValueBench collects data from 44 established psychometric inventories, encompassing 453 multifaceted value dimensions. We propose an evaluation pipeline grounded in realistic human-AI interactions to probe value orientations, along with novel tasks for evaluating value understanding in an open-ended value space. With extensive experiments conducted on six representative LLMs, we unveil their shared and distinctive value orientations and exhibit their ability to approximate expert conclusions in value-related extraction and generation tasks. ValueBench is openly accessible at https://github.com/Value4AI/ValueBench."
    },
    {
        "paperId": "32d4a2e67966380251cd44170ad8887a84561c0c",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04278, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-06",
        "authors": [
            {
                "authorId": "2305323121",
                "name": "Dun-Ming Huang"
            },
            {
                "authorId": "1856121554",
                "name": "Pol van Rijn"
            },
            {
                "authorId": "2226897111",
                "name": "Ilia Sucholutsky"
            },
            {
                "authorId": "2075275038",
                "name": "Raja Marjieh"
            },
            {
                "authorId": "48771852",
                "name": "Nori Jacoby"
            }
        ],
        "abstract": "Conversational tones -- the manners and attitudes in which speakers communicate -- are essential to effective communication. Amidst the increasing popularization of Large Language Models (LLMs) over recent years, it becomes necessary to characterize the divergences in their conversational tones relative to humans. However, existing investigations of conversational modalities rely on pre-existing taxonomies or text corpora, which suffer from experimenter bias and may not be representative of real-world distributions for the studies' psycholinguistic domains. Inspired by methods from cognitive science, we propose an iterative method for simultaneously eliciting conversational tones and sentences, where participants alternate between two tasks: (1) one participant identifies the tone of a given sentence and (2) a different participant generates a sentence based on that tone. We run 100 iterations of this process with human participants and GPT-4, then obtain a dataset of sentences and frequent conversational tones. In an additional experiment, humans and GPT-4 annotated all sentences with all tones. With data from 1,339 human participants, 33,370 human judgments, and 29,900 GPT-4 queries, we show how our approach can be used to create an interpretable geometric representation of relations between conversational tones in humans and GPT-4. This work demonstrates how combining ideas from machine learning and cognitive science can address challenges in human-computer interactions."
    },
    {
        "paperId": "ffe71c6035e7e5b8f988dc131224e32f64a90f1d",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "PaCE: Parsimonious Concept Engineering for Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04331, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-06",
        "authors": [
            {
                "authorId": "32020492",
                "name": "Jinqi Luo"
            },
            {
                "authorId": "4607942",
                "name": "Tianjiao Ding"
            },
            {
                "authorId": "2275849013",
                "name": "Kwan Ho Ryan Chan"
            },
            {
                "authorId": "40494989",
                "name": "D. Thaker"
            },
            {
                "authorId": "2275201091",
                "name": "Aditya Chattopadhyay"
            },
            {
                "authorId": "1763608",
                "name": "Chris Callison-Burch"
            },
            {
                "authorId": "2268674509",
                "name": "Ren'e Vidal"
            }
        ],
        "abstract": "Large Language Models (LLMs) are being used for a wide variety of tasks. While they are capable of generating human-like responses, they can also produce undesirable output including potentially harmful information, racist or sexist language, and hallucinations. Alignment methods are designed to reduce such undesirable outputs via techniques such as fine-tuning, prompt engineering, and representation engineering. However, existing methods face several challenges: some require costly fine-tuning for every alignment task; some do not adequately remove undesirable concepts, failing alignment; some remove benign concepts, lowering the linguistic capabilities of LLMs. To address these issues, we propose Parsimonious Concept Engineering (PaCE), a novel activation engineering framework for alignment. First, to sufficiently model the concepts, we construct a large-scale concept dictionary in the activation space, in which each atom corresponds to a semantic concept. Given any alignment task, we instruct a concept partitioner to efficiently annotate the concepts as benign or undesirable. Then, at inference time, we decompose the LLM activations along the concept dictionary via sparse coding, to accurately represent the activations as linear combinations of benign and undesirable components. By removing the latter ones from the activations, we reorient the behavior of the LLM towards the alignment goal. We conduct experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising, and show that PaCE achieves state-of-the-art alignment performance while maintaining linguistic capabilities."
    },
    {
        "paperId": "fa83074f19beab66f6e1b1f3e428fdf7730b33af",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "StackSight: Unveiling WebAssembly through Large Language Models and Neurosymbolic Chain-of-Thought Decompilation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04568, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-07",
        "authors": [
            {
                "authorId": "2284438658",
                "name": "Weike Fang"
            },
            {
                "authorId": "2157944555",
                "name": "Zhejian Zhou"
            },
            {
                "authorId": "2305584718",
                "name": "Junzhou He"
            },
            {
                "authorId": "2337967602",
                "name": "Weihang Wang"
            }
        ],
        "abstract": "WebAssembly enables near-native execution in web applications and is increasingly adopted for tasks that demand high performance and robust security. However, its assembly-like syntax, implicit stack machine, and low-level data types make it extremely difficult for human developers to understand, spurring the need for effective WebAssembly reverse engineering techniques. In this paper, we propose StackSight, a novel neurosymbolic approach that combines Large Language Models (LLMs) with advanced program analysis to decompile complex WebAssembly code into readable C++ snippets. StackSight visualizes and tracks virtual stack alterations via a static analysis algorithm and then applies chain-of-thought prompting to harness LLM's complex reasoning capabilities. Evaluation results show that StackSight significantly improves WebAssembly decompilation. Our user study also demonstrates that code snippets generated by StackSight have significantly higher win rates and enable a better grasp of code semantics."
    },
    {
        "paperId": "ab6b11235338ca66328be4d34e9ac3ffdbb2d5a1",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Helpful or Harmful Data? Fine-tuning-free Shapley Attribution for Explaining Language Model Predictions",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04606, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-07",
        "authors": [
            {
                "authorId": "2249766748",
                "name": "Jingtan Wang"
            },
            {
                "authorId": "2220591847",
                "name": "Xiaoqiang Lin"
            },
            {
                "authorId": "2305481061",
                "name": "Rui Qiao"
            },
            {
                "authorId": "2305481019",
                "name": "Chuan-Sheng Foo"
            },
            {
                "authorId": "145454065",
                "name": "K. H. Low"
            }
        ],
        "abstract": "The increasing complexity of foundational models underscores the necessity for explainability, particularly for fine-tuning, the most widely used training method for adapting models to downstream tasks. Instance attribution, one type of explanation, attributes the model prediction to each training example by an instance score. However, the robustness of instance scores, specifically towards dataset resampling, has been overlooked. To bridge this gap, we propose a notion of robustness on the sign of the instance score. We theoretically and empirically demonstrate that the popular leave-one-out-based methods lack robustness, while the Shapley value behaves significantly better, but at a higher computational cost. Accordingly, we introduce an efficient fine-tuning-free approximation of the Shapley value (FreeShap) for instance attribution based on the neural tangent kernel. We empirically demonstrate that FreeShap outperforms other methods for instance attribution and other data-centric applications such as data removal, data selection, and wrong label detection, and further generalize our scale to large language models (LLMs). Our code is available at https://github.com/JTWang2000/FreeShap."
    },
    {
        "paperId": "efd5b4f9be1907bd9eebf3345649e2c63192130f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "DiNeR: A Large Realistic Dataset for Evaluating Compositional Generalization",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.924.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-07",
        "authors": [
            {
                "authorId": "2188573668",
                "name": "ChenGang Hu"
            },
            {
                "authorId": "2288030514",
                "name": "Xiao Liu"
            },
            {
                "authorId": "2273532365",
                "name": "Yansong Feng"
            }
        ],
        "abstract": "Most of the existing compositional generalization datasets are synthetically-generated, resulting in a lack of natural language variation. While there have been recent attempts to introduce non-synthetic datasets for compositional generalization, they suffer from either limited data scale or a lack of diversity in the forms of combinations. To better investigate compositional generalization with more linguistic phenomena and compositional diversity, we propose the DIsh NamE Recognition (DiNeR) task and create a large realistic Chinese dataset. Given a recipe instruction, models are required to recognize the dish name composed of diverse combinations of food, actions, and flavors. Our dataset consists of 3,811 dishes and 228,114 recipes, and involves plenty of linguistic phenomena such as anaphora, omission and ambiguity. We provide two strong baselines based on T5 and large language models (LLMs). This work contributes a challenging task, baseline methods to tackle the task, and insights into compositional generalization in the context of dish name recognition. Code and data are available at https://github.com/Jumpy-pku/DiNeR."
    },
    {
        "paperId": "be75e2fceed00e1f98bde055ec07131fb2c1d6a2",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "MemeGuard: An LLM and VLM-based Framework for Advancing Content Moderation via Meme Intervention",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.05344, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-08",
        "authors": [
            {
                "authorId": "2165947844",
                "name": "Prince Jha"
            },
            {
                "authorId": "2088137695",
                "name": "Raghav Jain"
            },
            {
                "authorId": "2305618674",
                "name": "Konika Mandal"
            },
            {
                "authorId": "2275226689",
                "name": "Aman Chadha"
            },
            {
                "authorId": "2243463123",
                "name": "Sriparna Saha"
            },
            {
                "authorId": "145532184",
                "name": "P. Bhattacharyya"
            }
        ],
        "abstract": "In the digital world, memes present a unique challenge for content moderation due to their potential to spread harmful content. Although detection methods have improved, proactive solutions such as intervention are still limited, with current research focusing mostly on text-based content, neglecting the widespread influence of multimodal content like memes. Addressing this gap, we present \\textit{MemeGuard}, a comprehensive framework leveraging Large Language Models (LLMs) and Visual Language Models (VLMs) for meme intervention. \\textit{MemeGuard} harnesses a specially fine-tuned VLM, \\textit{VLMeme}, for meme interpretation, and a multimodal knowledge selection and ranking mechanism (\\textit{MKS}) for distilling relevant knowledge. This knowledge is then employed by a general-purpose LLM to generate contextually appropriate interventions. Another key contribution of this work is the \\textit{\\textbf{I}ntervening} \\textit{\\textbf{C}yberbullying in \\textbf{M}ultimodal \\textbf{M}emes (ICMM)} dataset, a high-quality, labeled dataset featuring toxic memes and their corresponding human-annotated interventions. We leverage \\textit{ICMM} to test \\textit{MemeGuard}, demonstrating its proficiency in generating relevant and effective responses to toxic memes."
    },
    {
        "paperId": "a454f660e43ff3133dd73025fa28072c7e38f98a",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Planning Like Human: A Dual-process Framework for Dialogue Planning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.05374, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-08",
        "authors": [
            {
                "authorId": "2247838930",
                "name": "Tao He"
            },
            {
                "authorId": "2273335137",
                "name": "Lizi Liao"
            },
            {
                "authorId": "2283537714",
                "name": "Yixin Cao"
            },
            {
                "authorId": "2307211316",
                "name": "Yuanxing Liu"
            },
            {
                "authorId": "2112748107",
                "name": "Ming Liu"
            },
            {
                "authorId": "2305640956",
                "name": "Zerui Chen"
            },
            {
                "authorId": null,
                "name": "Bing Qin"
            }
        ],
        "abstract": "In proactive dialogue, the challenge lies not just in generating responses but in steering conversations toward predetermined goals, a task where Large Language Models (LLMs) typically struggle due to their reactive nature. Traditional approaches to enhance dialogue planning in LLMs, ranging from elaborate prompt engineering to the integration of policy networks, either face efficiency issues or deliver suboptimal performance. Inspired by the dualprocess theory in psychology, which identifies two distinct modes of thinking - intuitive (fast) and analytical (slow), we propose the Dual-Process Dialogue Planning (DPDP) framework. DPDP embodies this theory through two complementary planning systems: an instinctive policy model for familiar contexts and a deliberative Monte Carlo Tree Search (MCTS) mechanism for complex, novel scenarios. This dual strategy is further coupled with a novel two-stage training regimen: offline Reinforcement Learning for robust initial policy model formation followed by MCTS-enhanced on-the-fly learning, which ensures a dynamic balance between efficiency and strategic depth. Our empirical evaluations across diverse dialogue tasks affirm DPDP's superiority in achieving both high-quality dialogues and operational efficiency, outpacing existing methods."
    },
    {
        "paperId": "a5a3dd4af6e754ac82670751f6789c389e91d533",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.05972, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-10",
        "authors": [
            {
                "authorId": "2305628478",
                "name": "Jingru Jia"
            },
            {
                "authorId": "2204470572",
                "name": "Zehua Yuan"
            },
            {
                "authorId": "2047460825",
                "name": "Junhao Pan"
            },
            {
                "authorId": "2305606743",
                "name": "Paul E. McNamara"
            },
            {
                "authorId": "2293560900",
                "name": "Deming Chen"
            }
        ],
        "abstract": "When making decisions under uncertainty, individuals often deviate from rational behavior, which can be evaluated across three dimensions: risk preference, probability weighting, and loss aversion. Given the widespread use of large language models (LLMs) in decision-making processes, it is crucial to assess whether their behavior aligns with human norms and ethical expectations or exhibits potential biases. Several empirical studies have investigated the rationality and social behavior performance of LLMs, yet their internal decision-making tendencies and capabilities remain inadequately understood. This paper proposes a framework, grounded in behavioral economics, to evaluate the decision-making behaviors of LLMs. Through a multiple-choice-list experiment, we estimate the degree of risk preference, probability weighting, and loss aversion in a context-free setting for three commercial LLMs: ChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that LLMs generally exhibit patterns similar to humans, such as risk aversion and loss aversion, with a tendency to overweight small probabilities. However, there are significant variations in the degree to which these behaviors are expressed across different LLMs. We also explore their behavior when embedded with socio-demographic features, uncovering significant disparities. For instance, when modeled with attributes of sexual minority groups or physical disabilities, Claude-3-Opus displays increased risk aversion, leading to more conservative choices. These findings underscore the need for careful consideration of the ethical implications and potential biases in deploying LLMs in decision-making scenarios. Therefore, this study advocates for developing standards and guidelines to ensure that LLMs operate within ethical boundaries while enhancing their utility in complex decision-making environments."
    },
    {
        "paperId": "299cbb7e4c7e676e81d87d28efb8701df8892ec4",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.06196, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-10",
        "authors": [
            {
                "authorId": "2242554313",
                "name": "Andrew M. Bean"
            },
            {
                "authorId": "2305621214",
                "name": "Simi Hellsten"
            },
            {
                "authorId": "2290011658",
                "name": "Harry Mayne"
            },
            {
                "authorId": "2305622172",
                "name": "Jabez Magomere"
            },
            {
                "authorId": "2253469028",
                "name": "Ethan A. Chi"
            },
            {
                "authorId": "2305623222",
                "name": "Ryan Chi"
            },
            {
                "authorId": "1741886127",
                "name": "Scott A. Hale"
            },
            {
                "authorId": "90729626",
                "name": "Hannah Rose Kirk"
            }
        ],
        "abstract": "In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions. The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 38.7% accuracy, a 24.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models."
    },
    {
        "paperId": "8fe6b36b36d07e14f691e3bc5cdd9301cded222a",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.06435, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-10",
        "authors": [
            {
                "authorId": "2305629869",
                "name": "Brian Hu"
            },
            {
                "authorId": "2305614680",
                "name": "Bill Ray"
            },
            {
                "authorId": "2305620668",
                "name": "Alice Leung"
            },
            {
                "authorId": "2305620656",
                "name": "Amy Summerville"
            },
            {
                "authorId": "2305620742",
                "name": "David Joy"
            },
            {
                "authorId": "2305620699",
                "name": "Christopher Funk"
            },
            {
                "authorId": "32865856",
                "name": "Arslan Basharat"
            }
        ],
        "abstract": "In difficult decision-making scenarios, it is common to have conflicting opinions among expert human decision-makers as there may not be a single right answer. Such decisions may be guided by different attributes that can be used to characterize an individual's decision. We introduce a novel dataset for medical triage decision-making, labeled with a set of decision-maker attributes (DMAs). This dataset consists of 62 scenarios, covering six different DMAs, including ethical principles such as fairness and moral desert. We present a novel software framework for human-aligned decision-making by utilizing these DMAs, paving the way for trustworthy AI with better guardrails. Specifically, we demonstrate how large language models (LLMs) can serve as ethical decision-makers, and how their decisions can be aligned to different DMAs using zero-shot prompting. Our experiments focus on different open-source models with varying sizes and training techniques, such as Falcon, Mistral, and Llama 2. Finally, we also introduce a new form of weighted self-consistency that improves the overall quantified performance. Our results provide new research directions in the use of LLMs as alignable decision-makers. The dataset and open-source software are publicly available at: https://github.com/ITM-Kitware/llm-alignable-dm."
    },
    {
        "paperId": "9dabb370d4e8e12f3753ac228bb9ec228d65e659",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "When is Multicalibration Post-Processing Necessary?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.06487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-10",
        "authors": [
            {
                "authorId": "2305623214",
                "name": "Dutch Hansen"
            },
            {
                "authorId": "117403158",
                "name": "Siddartha Devic"
            },
            {
                "authorId": "2181918",
                "name": "Preetum Nakkiran"
            },
            {
                "authorId": "2798845",
                "name": "Vatsal Sharan"
            }
        ],
        "abstract": "Calibration is a well-studied property of predictors which guarantees meaningful uncertainty estimates. Multicalibration is a related notion -- originating in algorithmic fairness -- which requires predictors to be simultaneously calibrated over a potentially complex and overlapping collection of protected subpopulations (such as groups defined by ethnicity, race, or income). We conduct the first comprehensive study evaluating the usefulness of multicalibration post-processing across a broad set of tabular, image, and language datasets for models spanning from simple decision trees to 90 million parameter fine-tuned LLMs. Our findings can be summarized as follows: (1) models which are calibrated out of the box tend to be relatively multicalibrated without any additional post-processing; (2) multicalibration post-processing can help inherently uncalibrated models and large vision and language models; and (3) traditional calibration measures may sometimes provide multicalibration implicitly. More generally, we also distill many independent observations which may be useful for practical and effective applications of multicalibration post-processing in real-world contexts. We also release a python package implementing multicalibration algorithms, available via `pip install multicalibration'."
    },
    {
        "paperId": "924ea5bed43a9856d5ffae7073ced95bb90ab548",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.06576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-04",
        "authors": [
            {
                "authorId": "2097881334",
                "name": "Owen Dugan"
            },
            {
                "authorId": "2305685250",
                "name": "Donato Manuel Jimenez Beneto"
            },
            {
                "authorId": "40917646",
                "name": "Charlotte Loh"
            },
            {
                "authorId": "2269204883",
                "name": "Zhuo Chen"
            },
            {
                "authorId": "26916003",
                "name": "Rumen Dangovski"
            },
            {
                "authorId": "2149992752",
                "name": "Marin Solja\u010di\u0107"
            }
        ],
        "abstract": "Despite significant advancements in text generation and reasoning, Large Language Models (LLMs) still face challenges in accurately performing complex arithmetic operations. Language model systems often enable LLMs to generate code for arithmetic operations to achieve accurate calculations. However, this approach compromises speed and security, and fine-tuning risks the language model losing prior capabilities. We propose a framework that enables exact arithmetic in a single autoregressive step, providing faster, more secure, and more interpretable LLM systems with arithmetic capabilities. We use the hidden states of a LLM to control a symbolic architecture that performs arithmetic. Our implementation using Llama 3 with OccamNet as a symbolic model (OccamLlama) achieves 100\\% accuracy on single arithmetic operations ($+,-,\\times,\\div,\\sin{},\\cos{},\\log{},\\exp{},\\sqrt{}$), outperforming GPT 4o with and without a code interpreter. Furthermore, OccamLlama outperforms GPT 4o with and without a code interpreter on average across a range of mathematical problem solving benchmarks, demonstrating that OccamLLMs can excel in arithmetic tasks, even surpassing much larger models. We will make our code public shortly."
    },
    {
        "paperId": "b2145d032360697b95631bfdb8bd694607fab2b5",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Are LLMs classical or nonmonotonic reasoners? Lessons from generics",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.06590, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-05",
        "authors": [
            {
                "authorId": "2262444923",
                "name": "Alina Leidinger"
            },
            {
                "authorId": "2238624162",
                "name": "R. Rooij"
            },
            {
                "authorId": "2262445370",
                "name": "Ekaterina Shutova"
            }
        ],
        "abstract": "Recent scholarship on reasoning in LLMs has supplied evidence of impressive performance and flexible adaptation to machine generated or human feedback. Nonmonotonic reasoning, crucial to human cognition for navigating the real world, remains a challenging, yet understudied task. In this work, we study nonmonotonic reasoning capabilities of seven state-of-the-art LLMs in one abstract and one commonsense reasoning task featuring generics, such as 'Birds fly', and exceptions, 'Penguins don't fly' (see Fig. 1). While LLMs exhibit reasoning patterns in accordance with human nonmonotonic reasoning abilities, they fail to maintain stable beliefs on truth conditions of generics at the addition of supporting examples ('Owls fly') or unrelated information ('Lions have manes'). Our findings highlight pitfalls in attributing human reasoning behaviours to LLMs, as well as assessing general capabilities, while consistent reasoning remains elusive."
    },
    {
        "paperId": "a4694af9214241b6131f2b97563657a7ddd5a1a3",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "LLM Questionnaire Completion for Automatic Psychiatric Assessment",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.06636, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-09",
        "authors": [
            {
                "authorId": "2145152816",
                "name": "Gony Rosenman"
            },
            {
                "authorId": "2284664541",
                "name": "Lior Wolf"
            },
            {
                "authorId": "2249893355",
                "name": "Talma Hendler"
            }
        ],
        "abstract": "We employ a Large Language Model (LLM) to convert unstructured psychological interviews into structured questionnaires spanning various psychiatric and personality domains. The LLM is prompted to answer these questionnaires by impersonating the interviewee. The obtained answers are coded as features, which are used to predict standardized psychiatric measures of depression (PHQ-8) and PTSD (PCL-C), using a Random Forest regressor. Our approach is shown to enhance diagnostic accuracy compared to multiple baselines. It thus establishes a novel framework for interpreting unstructured psychological interviews, bridging the gap between narrative-driven and data-driven approaches for mental health assessment."
    },
    {
        "paperId": "a8468c721ce668e6dcf82c51fe8eb4d4d8d2de4a",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "DUAL-REFLECT: Enhancing Large Language Models for Reflective Translation through Dual Learning Feedback Mechanisms",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.07232, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-11",
        "authors": [
            {
                "authorId": "2309001015",
                "name": "Andong Chen"
            },
            {
                "authorId": "2273684176",
                "name": "Lianzhang Lou"
            },
            {
                "authorId": "2266796043",
                "name": "Kehai Chen"
            },
            {
                "authorId": "2320289516",
                "name": "Xuefeng Bai"
            },
            {
                "authorId": "2273965768",
                "name": "Yang Xiang"
            },
            {
                "authorId": "2105775",
                "name": "Muyun Yang"
            },
            {
                "authorId": "2237773157",
                "name": "Tiejun Zhao"
            },
            {
                "authorId": "2273887691",
                "name": "Min Zhang"
            }
        ],
        "abstract": "Recently, large language models (LLMs) enhanced by self-reflection have achieved promising performance on machine translation. The key idea is guiding LLMs to generate translation with human-like feedback. However, existing self-reflection methods lack effective feedback information, limiting the translation performance. To address this, we introduce a DUAL-REFLECT framework, leveraging the dual learning of translation tasks to provide effective feedback, thereby enhancing the models' self-reflective abilities and improving translation performance. The application of this method across various translation tasks has proven its effectiveness in improving translation accuracy and eliminating ambiguities, especially in translation tasks with low-resource language pairs."
    },
    {
        "paperId": "b72a8884a7a2f93d60d44930dd77d0af85dd32b8",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.07368, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-11",
        "authors": [
            {
                "authorId": "47113848",
                "name": "Haoran You"
            },
            {
                "authorId": "2305648167",
                "name": "Yichao Fu"
            },
            {
                "authorId": "2306259358",
                "name": "Zheng Wang"
            },
            {
                "authorId": "2303406300",
                "name": "Amir Yazdanbakhsh"
            },
            {
                "authorId": "2305737055",
                "name": "Y. Lin"
            }
        ],
        "abstract": "Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM."
    },
    {
        "paperId": "e51063cbff70876a435715f4dd8337767243aeb1",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.07599, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-11",
        "authors": [
            {
                "authorId": "2162218185",
                "name": "Md Tanvirul Alam"
            },
            {
                "authorId": "2070387009",
                "name": "Dipkamal Bhusal"
            },
            {
                "authorId": "2305479716",
                "name": "Le Nguyen"
            },
            {
                "authorId": "2266054488",
                "name": "Nidhi Rastogi"
            }
        ],
        "abstract": "Cyber threat intelligence (CTI) is crucial in today's cybersecurity landscape, providing essential insights to understand and mitigate the ever-evolving cyber threats. The recent rise of Large Language Models (LLMs) have shown potential in this domain, but concerns about their reliability, accuracy, and hallucinations persist. While existing benchmarks provide general evaluations of LLMs, there are no benchmarks that address the practical and applied aspects of CTI-specific tasks. To bridge this gap, we introduce CTIBench, a benchmark designed to assess LLMs' performance in CTI applications. CTIBench includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art models on these tasks provides insights into their strengths and weaknesses in CTI contexts, contributing to a better understanding of LLM capabilities in CTI."
    },
    {
        "paperId": "e96a07e8c4f16d09cace07f175da5c1d8b8db949",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable NLG Evaluation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.07935, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-12",
        "authors": [
            {
                "authorId": "2265758439",
                "name": "Jie Ruan"
            },
            {
                "authorId": "2310390215",
                "name": "Wenqing Wang"
            },
            {
                "authorId": "2262215991",
                "name": "Xiaojun Wan"
            }
        ],
        "abstract": "Human evaluation serves as the gold standard for assessing the quality of Natural Language Generation (NLG) systems. Nevertheless, the evaluation guideline, as a pivotal element ensuring reliable and reproducible human assessment, has received limited attention. Our investigation revealed that only 29.84% of recent papers involving human evaluation at top conferences release their evaluation guidelines, with vulnerabilities identified in 77.09% of these guidelines. Unreliable evaluation guidelines can yield inaccurate assessment outcomes, potentially impeding the advancement of NLG in the right direction. To address these challenges, we take an initial step towards reliable evaluation guidelines and propose the first human evaluation guideline dataset by collecting annotations of guidelines extracted from existing papers as well as generated via Large Language Models (LLMs). We then introduce a taxonomy of eight vulnerabilities and formulate a principle for composing evaluation guidelines. Furthermore, a method for detecting guideline vulnerabilities has been explored using LLMs, and we offer a set of recommendations to enhance reliability in human evaluation. The annotated human evaluation guideline dataset and code for the vulnerability detection method are publicly available online."
    },
    {
        "paperId": "d6a67db584355bd17a12600a78eb1b327e1d138b",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Is Programming by Example solved by LLMs?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.08316, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-12",
        "authors": [
            {
                "authorId": "2108635077",
                "name": "Wen-Ding Li"
            },
            {
                "authorId": "2284673917",
                "name": "Kevin Ellis"
            }
        ],
        "abstract": "Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples. Such systems are practically and theoretically important: from an end-user perspective, they are deployed to millions of people, and from an AI perspective, PBE corresponds to a very general form of few-shot inductive inference. Given the success of Large Language Models (LLMs) in code-generation tasks, we investigate here the extent to which LLMs can be said to have\"solved\"PBE. We experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data. We find that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution. We analyze empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization. Collectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short."
    },
    {
        "paperId": "b3bf4ca8da7fe2ffca43dbd2f7ae227729bf719c",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Large Language Models Must Be Taught to Know What They Don't Know",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.08391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-12",
        "authors": [
            {
                "authorId": "153936584",
                "name": "Sanyam Kapoor"
            },
            {
                "authorId": "13297813",
                "name": "Nate Gruver"
            },
            {
                "authorId": "2179329459",
                "name": "Manley Roberts"
            },
            {
                "authorId": "2055306799",
                "name": "Katherine M. Collins"
            },
            {
                "authorId": "2284763248",
                "name": "Arka Pal"
            },
            {
                "authorId": "32326200",
                "name": "Umang Bhatt"
            },
            {
                "authorId": "2267117825",
                "name": "Adrian Weller"
            },
            {
                "authorId": "2066631240",
                "name": "Samuel Dooley"
            },
            {
                "authorId": "2126058635",
                "name": "Micah Goldblum"
            },
            {
                "authorId": "2291435899",
                "name": "A. Wilson"
            }
        ],
        "abstract": "When using large language models (LLMs) in high-stakes applications, we need to know when we can trust their predictions. Some works argue that prompting high-performance LLMs is sufficient to produce calibrated uncertainties, while others introduce sampling methods that can be prohibitively expensive. In this work, we first argue that prompting on its own is insufficient to achieve good calibration and then show that fine-tuning on a small dataset of correct and incorrect answers can create an uncertainty estimate with good generalization and small computational overhead. We show that a thousand graded examples are sufficient to outperform baseline methods and that training through the features of a model is necessary for good performance and tractable for large open-source models when using LoRA. We also investigate the mechanisms that enable reliable LLM uncertainty estimation, finding that many models can be used as general-purpose uncertainty estimators, applicable not just to their own uncertainties but also the uncertainty of other models. Lastly, we show that uncertainty estimates inform human use of LLMs in human-AI collaborative settings through a user study."
    },
    {
        "paperId": "3392f42d99c42098453e6bfcf2e3f7983d34c8c1",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.08398, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-12",
        "authors": [
            {
                "authorId": "2161133253",
                "name": "Anirudh S. Sundar"
            },
            {
                "authorId": "2306051760",
                "name": "Jin Xu"
            },
            {
                "authorId": "2297670756",
                "name": "William Gay"
            },
            {
                "authorId": "2206117112",
                "name": "Christopher Richardson"
            },
            {
                "authorId": "2257280755",
                "name": "Larry Heck"
            }
        ],
        "abstract": "An emerging area of research in situated and multimodal interactive conversations (SIMMC) includes interactions in scientific papers. Since scientific papers are primarily composed of text, equations, figures, and tables, SIMMC methods must be developed specifically for each component to support the depth of inquiry and interactions required by research scientists. This work introduces Conversational Papers (cPAPERS), a dataset of conversational question-answer pairs from reviews of academic papers grounded in these paper components and their associated references from scientific documents available on arXiv. We present a data collection strategy to collect these question-answer pairs from OpenReview and associate them with contextual information from LaTeX source files. Additionally, we present a series of baseline approaches utilizing Large Language Models (LLMs) in both zero-shot and fine-tuned configurations to address the cPAPERS dataset."
    },
    {
        "paperId": "b07880f7b932c7dc8b1091c6c80c9de3e7d64185",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.10216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-14",
        "authors": [
            {
                "authorId": "145094495",
                "name": "Rui Yang"
            },
            {
                "authorId": "2306781402",
                "name": "Ruomeng Ding"
            },
            {
                "authorId": "2238123947",
                "name": "Yong Lin"
            },
            {
                "authorId": "2303307791",
                "name": "Huan Zhang"
            },
            {
                "authorId": "2306841244",
                "name": "Tong Zhang"
            }
        ],
        "abstract": "Reward models trained on human preference data have been proven to effectively align Large Language Models (LLMs) with human intent within the framework of reinforcement learning from human feedback (RLHF). However, current reward models have limited generalization capabilities to unseen prompts and responses, which can lead to an unexpected phenomenon known as reward over-optimization, resulting in a decline in actual performance due to excessive optimization of rewards. While previous research has advocated for constraining policy optimization, our study introduces a novel approach to enhance the reward model's generalization ability against distribution shifts by regularizing the hidden states. Specifically, we retain the base model's language model head and incorporate a suite of text-generation losses to preserve the hidden states' text-generation capabilities, while concurrently learning a reward head behind the same hidden states. Our experimental results demonstrate that the introduced regularization technique markedly improves the accuracy of learned reward models across a variety of out-of-distribution (OOD) tasks and effectively alleviates the over-optimization issue in RLHF, offering a more reliable and robust preference learning paradigm."
    },
    {
        "paperId": "64dbe50f4bf7b37f395437f13028a06624aa70e8",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Personalized Pieces: Efficient Personalized Large Language Models through Collaborative Efforts",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.10471, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-15",
        "authors": [
            {
                "authorId": "2093186816",
                "name": "Zhaoxuan Tan"
            },
            {
                "authorId": "2122087252",
                "name": "Zheyuan Liu"
            },
            {
                "authorId": "2275403324",
                "name": "Meng Jiang"
            }
        ],
        "abstract": "Personalized large language models (LLMs) aim to tailor interactions, content, and recommendations to individual user preferences. While parameter-efficient fine-tuning (PEFT) methods excel in performance and generalization, they are costly and limit communal benefits when used individually. To this end, we introduce Personalized Pieces (Per-Pcs), a framework that allows users to safely share and assemble personalized PEFT efficiently with collaborative efforts. Per-Pcs involves selecting sharers, breaking their PEFT into pieces, and training gates for each piece. These pieces are added to a pool, from which target users can select and assemble personalized PEFT using their history data. This approach preserves privacy and enables fine-grained user modeling without excessive storage and computation demands. Experimental results show Per-Pcs outperforms non-personalized and PEFT retrieval baselines, offering performance comparable to OPPU with significantly lower resource use across six tasks. Further analysis highlights Per-Pcs\u2019s robustness concerning sharer count and selection strategy, pieces sharing ratio, and scalability in computation time and storage space. Per-Pcs\u2019s modularity promotes safe sharing, making LLM personalization more efficient, effective, and widely accessible through collaborative efforts."
    },
    {
        "paperId": "1ea5864a08437c999cc61d11d7c35ff605886c85",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.10486, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-15",
        "authors": [
            {
                "authorId": "1576552123",
                "name": "Haozhe An"
            },
            {
                "authorId": "2307006593",
                "name": "Christabel Acquaye"
            },
            {
                "authorId": "2307028393",
                "name": "Colin Wang"
            },
            {
                "authorId": "2118208398",
                "name": "Zongxia Li"
            },
            {
                "authorId": "2034613",
                "name": "Rachel Rudinger"
            }
        ],
        "abstract": "We examine whether large language models (LLMs) exhibit race- and gender-based name discrimination in hiring decisions, similar to classic findings in the social sciences (Bertrand and Mullainathan, 2004). We design a series of templatic prompts to LLMs to write an email to a named job applicant informing them of a hiring decision. By manipulating the applicant's first name, we measure the effect of perceived race, ethnicity, and gender on the probability that the LLM generates an acceptance or rejection email. We find that the hiring decisions of LLMs in many settings are more likely to favor White applicants over Hispanic applicants. In aggregate, the groups with the highest and lowest acceptance rates respectively are masculine White names and masculine Hispanic names. However, the comparative acceptance rates by group vary under different templatic settings, suggesting that LLMs' race- and gender-sensitivity may be idiosyncratic and prompt-sensitive."
    },
    {
        "paperId": "00e88a0c006296e53bb7d4cfc90a134883ad34fd",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-16",
        "authors": [
            {
                "authorId": "2258471494",
                "name": "Bowen Jiang"
            },
            {
                "authorId": "2283875421",
                "name": "Yangxinyu Xie"
            },
            {
                "authorId": "2307597941",
                "name": "Zhuoqun Hao"
            },
            {
                "authorId": "2304477321",
                "name": "Xiaomeng Wang"
            },
            {
                "authorId": "2304499386",
                "name": "Tanwi Mallick"
            },
            {
                "authorId": "2283845701",
                "name": "Weijie J. Su"
            },
            {
                "authorId": "2239095982",
                "name": "C. J. Taylor"
            },
            {
                "authorId": "2293173536",
                "name": "Dan Roth"
            }
        ],
        "abstract": "This study introduces a hypothesis-testing framework to assess whether large language models (LLMs) possess genuine reasoning abilities or primarily depend on token bias. We go beyond evaluating LLMs on accuracy; rather, we aim to investigate their token bias in solving logical reasoning tasks. Specifically, we develop carefully controlled synthetic datasets, featuring conjunction fallacy and syllogistic problems. Our framework outlines a list of hypotheses where token biases are readily identifiable, with all null hypotheses assuming genuine reasoning capabilities of LLMs. The findings in this study suggest, with statistical guarantee, that most LLMs still struggle with logical reasoning. While they may perform well on classic problems, their success largely depends on recognizing superficial patterns with strong token bias, thereby raising concerns about their actual reasoning and generalization abilities."
    },
    {
        "paperId": "962c67e5e563e45bb493664941b93822848bb977",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Can LLMs Understand the Implication of Emphasized Sentences in Dialogue?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11065, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-16",
        "authors": [
            {
                "authorId": "5857492",
                "name": "Guan-Ting Lin"
            },
            {
                "authorId": "2276484232",
                "name": "Hung-yi Lee"
            }
        ],
        "abstract": "Emphasis is a crucial component in human communication, which indicates the speaker's intention and implication beyond pure text in dialogue. While Large Language Models (LLMs) have revolutionized natural language processing, their ability to understand emphasis in dialogue remains unclear. This paper introduces Emphasized-Talk, a benchmark with emphasis-annotated dialogue samples capturing the implications of emphasis. We evaluate various LLMs, both open-source and commercial, to measure their performance in understanding emphasis. Additionally, we propose an automatic evaluation pipeline using GPT-4, which achieves a high correlation with human rating. Our findings reveal that although commercial LLMs generally perform better, there is still significant room for improvement in comprehending emphasized sentences."
    },
    {
        "paperId": "af61c4f2c9e4ed58d1cd3daf8cd4dbf916f5f4e7",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11096, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-16",
        "authors": [
            {
                "authorId": "2188764477",
                "name": "Bolei Ma"
            },
            {
                "authorId": "2108254820",
                "name": "Xinpeng Wang"
            },
            {
                "authorId": "2046818614",
                "name": "Tiancheng Hu"
            },
            {
                "authorId": "23107750",
                "name": "Anna Haensch"
            },
            {
                "authorId": "51133383",
                "name": "Michael A. Hedderich"
            },
            {
                "authorId": "2284982105",
                "name": "Barbara Plank"
            },
            {
                "authorId": "2249553408",
                "name": "Frauke Kreuter"
            }
        ],
        "abstract": "Recent advances in Large Language Models (LLMs) have sparked wide interest in validating and comprehending the human-like cognitive-behavioral traits LLMs may capture and convey. These cognitive-behavioral traits include typically Attitudes, Opinions, Values (AOVs). However, measuring AOVs embedded within LLMs remains opaque, and different evaluation methods may yield different results. This has led to a lack of clarity on how different studies are related to each other and how they can be interpreted. This paper aims to bridge this gap by providing a comprehensive overview of recent works on the evaluation of AOVs in LLMs. Moreover, we survey related approaches in different stages of the evaluation pipeline in these works. By doing so, we address the potential and challenges with respect to understanding the model, human-AI alignment, and downstream application in social sciences. Finally, we provide practical insights into evaluation methods, model enhancement, and interdisciplinary collaboration, thereby contributing to the evolving landscape of evaluating AOVs in LLMs."
    },
    {
        "paperId": "06c4eb687408f0c5eeb94f350eadc24a66cced5c",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-17",
        "authors": [
            {
                "authorId": "2824588",
                "name": "Sri Harsha Dumpala"
            },
            {
                "authorId": "2056921114",
                "name": "Aman Jaiswal"
            },
            {
                "authorId": "2284991948",
                "name": "C. Sastry"
            },
            {
                "authorId": "1736641",
                "name": "E. Milios"
            },
            {
                "authorId": "2129122063",
                "name": "Sageev Oore"
            },
            {
                "authorId": "2298268693",
                "name": "Hassan Sajjad"
            }
        ],
        "abstract": "Despite their remarkable successes, state-of-the-art large language models (LLMs), including vision-and-language models (VLMs) and unimodal language models (ULMs), fail to understand precise semantics. For example, semantically equivalent sentences expressed using different lexical compositions elicit diverging representations. The degree of this divergence and its impact on encoded semantics is not very well understood. In this paper, we introduce the SUGARCREPE++ dataset to analyze the sensitivity of VLMs and ULMs to lexical and semantic alterations. Each sample in SUGARCREPE++ dataset consists of an image and a corresponding triplet of captions: a pair of semantically equivalent but lexically different positive captions and one hard negative caption. This poses a 3-way semantic (in)equivalence problem to the language models. We comprehensively evaluate VLMs and ULMs that differ in architecture, pre-training objectives and datasets to benchmark the performance of SUGARCREPE++ dataset. Experimental results highlight the difficulties of VLMs in distinguishing between lexical and semantic variations, particularly in object attributes and spatial relations. Although VLMs with larger pre-training datasets, model sizes, and multiple pre-training objectives achieve better performance on SUGARCREPE++, there is a significant opportunity for improvement. We show that all the models which achieve better performance on compositionality datasets need not perform equally well on SUGARCREPE++, signifying that compositionality alone may not be sufficient for understanding semantic and lexical alterations. Given the importance of the property that the SUGARCREPE++ dataset targets, it serves as a new challenge to the vision-and-language community."
    },
    {
        "paperId": "6dd9a63109f1b5722c9e7c8a2458b4c64731b475",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11341, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-17",
        "authors": [
            {
                "authorId": "2243233537",
                "name": "Leonardo Bertolazzi"
            },
            {
                "authorId": "2307004114",
                "name": "Albert Gatt"
            },
            {
                "authorId": "2243236040",
                "name": "Raffaella Bernardi"
            }
        ],
        "abstract": "The reasoning abilities of Large Language Models (LLMs) are becoming a central focus of study in NLP. In this paper, we consider the case of syllogistic reasoning, an area of deductive reasoning studied extensively in logic and cognitive psychology. Previous research has shown that pre-trained LLMs exhibit reasoning biases, such as content effects, avoid answering that no conclusion follows, align with human difficulties, and struggle with multi-step reasoning. We contribute to this research line by systematically investigating the effects of chain-of-thought reasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on syllogistic reasoning, considering syllogisms with conclusions that support or violate world knowledge and with multiple premises. Crucially, we go beyond the standard focus on accuracy, with an in-depth analysis of the conclusions generated by the models. Our results suggest that the behavior of pre-trained LLMs can be explained by heuristics studied in cognitive science and that both ICL and SFT improve model performance on valid inferences, although only the latter can mitigate most reasoning biases while being consistent."
    },
    {
        "paperId": "ace05d37f23750f16896976c0db17d48e52c4478",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11370, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-17",
        "authors": [
            {
                "authorId": "2111825271",
                "name": "Han Zhou"
            },
            {
                "authorId": "1470501980",
                "name": "Xingchen Wan"
            },
            {
                "authorId": "2108068935",
                "name": "Yinhong Liu"
            },
            {
                "authorId": "2304551831",
                "name": "Nigel Collier"
            },
            {
                "authorId": "2267339029",
                "name": "Ivan Vuli'c"
            },
            {
                "authorId": "2260336540",
                "name": "Anna Korhonen"
            }
        ],
        "abstract": "Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments."
    },
    {
        "paperId": "0eb69035aef6b190f0fd0c3892b9bbd26388a2a0",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "P-TA: Using Proximal Policy Optimization to Enhance Tabular Data Augmentation via Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-17",
        "authors": [
            {
                "authorId": "2155666424",
                "name": "Shuo Yang"
            },
            {
                "authorId": "2307865214",
                "name": "Chenchen Yuan"
            },
            {
                "authorId": "2307007683",
                "name": "Yao Rong"
            },
            {
                "authorId": "2307003068",
                "name": "Felix Steinbauer"
            },
            {
                "authorId": "1686448",
                "name": "Gjergji Kasneci"
            }
        ],
        "abstract": "A multitude of industries depend on accurate and reasonable tabular data augmentation for their business processes. Contemporary methodologies in generating tabular data revolve around utilizing Generative Adversarial Networks (GAN) or fine-tuning Large Language Models (LLM). However, GAN-based approaches are documented to produce samples with common-sense errors attributed to the absence of external knowledge. On the other hand, LLM-based methods exhibit a limited capacity to capture the disparities between synthesized and actual data distribution due to the absence of feedback from a discriminator during training. Furthermore, the decoding of LLM-based generation introduces gradient breakpoints, impeding the backpropagation of loss from a discriminator, thereby complicating the integration of these two approaches. To solve this challenge, we propose using proximal policy optimization (PPO) to apply GANs, guiding LLMs to enhance the probability distribution of tabular features. This approach enables the utilization of LLMs as generators for GANs in synthesizing tabular data. Our experiments demonstrate that PPO leads to an approximately 4\\% improvement in the accuracy of models trained on synthetically generated data over state-of-the-art across three real-world datasets."
    },
    {
        "paperId": "a67c8f49509e81656651a221bac5b1b87a6291f5",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Building Knowledge-Guided Lexica to Model Cultural Variation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-17",
        "authors": [
            {
                "authorId": "151207988",
                "name": "Shreya Havaldar"
            },
            {
                "authorId": "2306632156",
                "name": "Salvatore Giorgi"
            },
            {
                "authorId": "2261734219",
                "name": "Sunny Rai"
            },
            {
                "authorId": "2306632105",
                "name": "Thomas Talhelm"
            },
            {
                "authorId": "2008166098",
                "name": "Sharath Chandra Guntuku"
            },
            {
                "authorId": "143857273",
                "name": "Pallavi V. Kulkarni"
            }
        ],
        "abstract": "Cultural variation exists between nations (e.g., the United States vs. China), but also within regions (e.g., California vs. Texas, Los Angeles vs. San Francisco). Measuring this regional cultural variation can illuminate how and why people think and behave differently. Historically, it has been difficult to computationally model cultural variation due to a lack of training data and scalability constraints. In this work, we introduce a new research problem for the NLP community: How do we measure variation in cultural constructs across regions using language? We then provide a scalable solution: building knowledge-guided lexica to model cultural variation, encouraging future work at the intersection of NLP and cultural understanding. We also highlight modern LLMs\u2019 failure to measure cultural variation or generate culturally varied language."
    },
    {
        "paperId": "e48b5bbd881098b4dcee07e037d243b6177566e6",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11634, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-17",
        "authors": [
            {
                "authorId": "2112423834",
                "name": "Kyle Moore"
            },
            {
                "authorId": "2115904887",
                "name": "Jesse Roberts"
            },
            {
                "authorId": "2309222225",
                "name": "Thao Pham"
            },
            {
                "authorId": "2307002126",
                "name": "Oseremhen Ewaleifoh"
            },
            {
                "authorId": "2306944206",
                "name": "Douglas H. Fisher"
            }
        ],
        "abstract": "Cloze testing is a common method for measuring the behavior of large language models on a number of benchmark tasks. Using the MMLU dataset, we show that the base-rate probability (BRP) differences across answer tokens are significant and affect task performance ie. guess A if uncertain. We find that counterfactual prompting does sufficiently mitigate the BRP effect. The BRP effect is found to have a similar effect to test taking strategies employed by humans leading to the conflation of task performance and test-taking ability. We propose the Nvr-X-MMLU task, a variation of MMLU, which helps to disambiguate test-taking ability from task performance and reports the latter."
    },
    {
        "paperId": "4d9d8635c4a849b84db9d95e3c1b3249c05c1193",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-17",
        "authors": [
            {
                "authorId": "1490933248",
                "name": "Priyanka Kargupta"
            },
            {
                "authorId": "2307000129",
                "name": "Ishika Agarwal"
            },
            {
                "authorId": "2365041900",
                "name": "Dilek Hakkani-Tur"
            },
            {
                "authorId": "2307763214",
                "name": "Jiawei Han"
            }
        ],
        "abstract": "Socratic questioning is an effective teaching strategy, encouraging critical thinking and problem-solving. The conversational capabilities of large language models (LLMs) show great potential for providing scalable, real-time student guidance. However, current LLMs often give away solutions directly, making them ineffective instructors. We tackle this issue in the code debugging domain with TreeInstruct, an Instructor agent guided by a novel state space-based planning algorithm. TreeInstruct asks probing questions to help students independently identify and resolve errors. It estimates a student's conceptual and syntactical knowledge to dynamically construct a question tree based on their responses and current knowledge state, effectively addressing both independent and dependent mistakes concurrently in a multi-turn interaction setting. In addition to using an existing single-bug debugging benchmark, we construct a more challenging multi-bug dataset of 150 coding problems, incorrect solutions, and bug fixes -- all carefully constructed and annotated by experts. Extensive evaluation shows TreeInstruct's state-of-the-art performance on both datasets, proving it to be a more effective instructor than baselines. Furthermore, a real-world case study with five students of varying skill levels further demonstrates TreeInstruct's ability to guide students to debug their code efficiently with minimal turns and highly Socratic questioning."
    },
    {
        "paperId": "5c23d83e1dad580160f58afb84d12e61f369b8ab",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MDCR: A Dataset for Multi-Document Conditional Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11784, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-17",
        "authors": [
            {
                "authorId": "2296798619",
                "name": "P. Chen"
            },
            {
                "authorId": "2296752058",
                "name": "Yi Zhang"
            },
            {
                "authorId": "2308602073",
                "name": "Chunwei Liu"
            },
            {
                "authorId": "2307038743",
                "name": "Sejal Gupta"
            },
            {
                "authorId": "2307201129",
                "name": "Yoon Kim"
            },
            {
                "authorId": "2257271167",
                "name": "Michael J. Cafarella"
            }
        ],
        "abstract": "The same real-life questions posed to different individuals may lead to different answers based on their unique situations. For instance, whether a student is eligible for a scholarship depends on eligibility conditions, such as major or degree required. ConditionalQA was proposed to evaluate models' capability of reading a document and answering eligibility questions, considering unmentioned conditions. However, it is limited to questions on single documents, neglecting harder cases that may require cross-document reasoning and optimization, for example,\"What is the maximum number of scholarships attainable?\"Such questions over multiple documents are not only more challenging due to more context having to understand, but also because the model has to (1) explore all possible combinations of unmentioned conditions and (2) understand the relationship between conditions across documents, to reason about the optimal outcome. To evaluate models' capability of answering such questions, we propose a new dataset MDCR, which can reflect real-world challenges and serve as a new test bed for complex conditional reasoning that requires optimization. We evaluate this dataset using the most recent LLMs and demonstrate their limitations in solving this task. We believe this dataset will facilitate future research in answering optimization questions with unknown conditions."
    },
    {
        "paperId": "6dd415a07a6c304a78e191bbb09f50ee04c03d89",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "A Notion of Complexity for Theory of Mind via Discrete World Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11911, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-16",
        "authors": [
            {
                "authorId": "2307221704",
                "name": "X. A. Huang"
            },
            {
                "authorId": "1582740100",
                "name": "Emanuele La Malfa"
            },
            {
                "authorId": "2151859640",
                "name": "Samuele Marro"
            },
            {
                "authorId": "2262814370",
                "name": "A. Asperti"
            },
            {
                "authorId": "2248060862",
                "name": "Anthony G. Cohn"
            },
            {
                "authorId": "2247963153",
                "name": "Michael Wooldridge"
            }
        ],
        "abstract": "Theory of Mind (ToM) can be used to assess the capabilities of Large Language Models (LLMs) in complex scenarios where social reasoning is required. While the research community has proposed many ToM benchmarks, their hardness varies greatly, and their complexity is not well defined. This work proposes a framework inspired by cognitive load theory to measure the complexity of ToM tasks. We quantify a problem's complexity as the number of states necessary to solve it correctly. Our complexity measure also accounts for spurious states of a ToM problem designed to make it apparently harder. We use our method to assess the complexity of five widely adopted ToM benchmarks. On top of this framework, we design a prompting technique that augments the information available to a model with a description of how the environment changes with the agents' interactions. We name this technique Discrete World Models (DWM) and show how it elicits superior performance on ToM tasks."
    },
    {
        "paperId": "c45236298d1274d225e0f54e9e33d67755583a08",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-17",
        "authors": [
            {
                "authorId": "1519088854",
                "name": "Jiasheng Zhang"
            },
            {
                "authorId": "2263472358",
                "name": "Jialin Chen"
            },
            {
                "authorId": "2301207729",
                "name": "Menglin Yang"
            },
            {
                "authorId": "2268782997",
                "name": "Aosong Feng"
            },
            {
                "authorId": "122563825",
                "name": "Shuang Liang"
            },
            {
                "authorId": "2241184642",
                "name": "Jie Shao"
            },
            {
                "authorId": "2301161297",
                "name": "Rex Ying"
            }
        ],
        "abstract": "Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world scenarios, where each node and edge are associated with text descriptions, and both the graph structure and text descriptions evolve over time. Despite their broad applicability, there is a notable scarcity of benchmark datasets tailored to DyTAGs, which hinders the potential advancement in many research fields. To address this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB), a collection of large-scale, time-evolving graphs from diverse domains, with nodes and edges enriched by dynamically changing text attributes and categories. To facilitate the use of DTGB, we design standardized evaluation procedures based on four real-world use cases: future link prediction, destination node retrieval, edge classification, and textual relation generation. These tasks require models to understand both dynamic graph structures and natural language, highlighting the unique challenges posed by DyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB, evaluating 7 popular dynamic graph learning algorithms and their variants of adapting to text attributes with LLM embeddings, along with 6 powerful large language models (LLMs). Our results show the limitations of existing models in handling DyTAGs. Our analysis also demonstrates the utility of DTGB in investigating the incorporation of structural and textual dynamics. The proposed DTGB fosters research on DyTAGs and their broad applications. It offers a comprehensive benchmark for evaluating and advancing models to handle the interplay between dynamic graph structures and natural language. The dataset and source code are available at https://github.com/zjs123/DTGB."
    },
    {
        "paperId": "de4f65ea0c4b029beb2317c0bdc5c16149c18024",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Community-Cross-Instruct: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12074, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-17",
        "authors": [
            {
                "authorId": "2279405154",
                "name": "Zihao He"
            },
            {
                "authorId": "152955538",
                "name": "Rebecca Dorn"
            },
            {
                "authorId": "2110835750",
                "name": "Siyi Guo"
            },
            {
                "authorId": "2279811056",
                "name": "Minh Duc Hoang Chu"
            },
            {
                "authorId": "2073018730",
                "name": "Kristina Lerman"
            }
        ],
        "abstract": "Social scientists use surveys to probe the opinions and beliefs of populations, but these methods are slow, costly, and prone to biases. Recent advances in large language models (LLMs) enable the creating of computational representations or \u201cdigital twins\u201d of populations that generate human-like responses mimicking the population\u2019s language, styles, and attitudes. We introduce Community-Cross-Instruct, an unsupervised framework for aligning LLMs to online communities to elicit their beliefs. Given a corpus of a community\u2019s online discussions, Community-Cross-Instruct automatically generates instruction-output pairs by an advanced LLM to (1) finetune a foundational LLM to faithfully represent that community, and (2) evaluate the alignment of the finetuned model to the community. We demonstrate the method\u2019s utility in accurately representing political and diet communities on Reddit. Unlike prior methods requiring human-authored instructions, Community-Cross-Instruct generates instructions in a fully unsupervised manner, enhancing scalability and generalization across domains. This work enables cost-effective and automated surveying of diverse online communities."
    },
    {
        "paperId": "77dc09e14d0faa39998d9988e2b46731e859ea2f",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "AI \"News\" Content Farms Are Easy to Make and Hard to Detect: A Case Study in Italian",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2024.acl-long.817.pdf",
            "status": "GREEN",
            "license": "other-oa",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12128, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-17",
        "authors": [
            {
                "authorId": "2237787076",
                "name": "Giovanni Puccetti"
            },
            {
                "authorId": "2307075696",
                "name": "Anna Rogers"
            },
            {
                "authorId": "8168830",
                "name": "Chiara Alzetta"
            },
            {
                "authorId": "2187131",
                "name": "F. Dell\u2019Orletta"
            },
            {
                "authorId": "2293611403",
                "name": "Andrea Esuli"
            }
        ],
        "abstract": "Large Language Models (LLMs) are increasingly used as\"content farm\"models (CFMs), to generate synthetic text that could pass for real news articles. This is already happening even for languages that do not have high-quality monolingual LLMs. We show that fine-tuning Llama (v1), mostly trained on English, on as little as 40K Italian news articles, is sufficient for producing news-like texts that native speakers of Italian struggle to identify as synthetic. We investigate three LLMs and three methods of detecting synthetic texts (log-likelihood, DetectGPT, and supervised classification), finding that they all perform better than human raters, but they are all impractical in the real world (requiring either access to token likelihood information or a large dataset of CFM texts). We also explore the possibility of creating a proxy CFM: an LLM fine-tuned on a similar dataset to one used by the real\"content farm\". We find that even a small amount of fine-tuning data suffices for creating a successful detector, but we need to know which base LLM is used, which is a major challenge. Our results suggest that there are currently no practical methods for detecting synthetic news-like texts 'in the wild', while generating them is too easy. We highlight the urgency of more NLP research on this problem."
    },
    {
        "paperId": "a8a2a15b5d51b51c62eeed6045ab1e67130e0867",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "LLMs Are Prone to Fallacies in Causal Inference",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12158, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-18",
        "authors": [
            {
                "authorId": "134516087",
                "name": "Nitish Joshi"
            },
            {
                "authorId": "2407368",
                "name": "Abulhair Saparov"
            },
            {
                "authorId": "2307188271",
                "name": "Yixin Wang"
            },
            {
                "authorId": "2263869572",
                "name": "He He"
            }
        ],
        "abstract": "Recent work shows that causal facts can be effectively extracted from LLMs through prompting, facilitating the creation of causal graphs for causal inference tasks. However, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which the model can memorize. Thus, this work investigates: Can LLMs infer causal relations from other relational data in text? To disentangle the role of memorized causal facts vs inferred causal relations, we finetune LLMs on synthetic data containing temporal, spatial and counterfactual relations, and measure whether the LLM can then infer causal relations. We find that: (a) LLMs are susceptible to inferring causal relations from the order of two entity mentions in text (e.g. X mentioned before Y implies X causes Y); (b) if the order is randomized, LLMs still suffer from the post hoc fallacy, i.e. X occurs before Y (temporal relation) implies X causes Y. We also find that while LLMs can correctly deduce the absence of causal relations from temporal and spatial relations, they have difficulty inferring causal relations from counterfactuals, questioning their understanding of causality."
    },
    {
        "paperId": "7c649618873fb6e72261183507a09324c722e788",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "InterIntent: Investigating Social Intelligence of LLMs via Intention Understanding in an Interactive Game Context",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12203, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-18",
        "authors": [
            {
                "authorId": "2267011746",
                "name": "Ziyi Liu"
            },
            {
                "authorId": "2307073011",
                "name": "Abhishek Anand"
            },
            {
                "authorId": "2307400300",
                "name": "Pei Zhou"
            },
            {
                "authorId": "2307221612",
                "name": "Jen-tse Huang"
            },
            {
                "authorId": "2267002049",
                "name": "Jieyu Zhao"
            }
        ],
        "abstract": "Large language models (LLMs) have demonstrated the potential to mimic human social intelligence. However, most studies focus on simplistic and static self-report or performance-based tests, which limits the depth and validity of the analysis. In this paper, we developed a novel framework, InterIntent, to assess LLMs\u2019 social intelligence by mapping their ability to understand and manage intentions in a game setting. We focus on four dimensions of social intelligence: situational awareness, self-regulation, self-awareness, and theory of mind. Each dimension is linked to a specific game task: intention selection, intention following, intention summarization, and intention guessing. Our findings indicate that while LLMs exhibit high proficiency in selecting intentions, achieving an accuracy of 88%, their ability to infer the intentions of others is significantly weaker, trailing human performance by 20%. Additionally, game performance correlates with intention understanding, highlighting the importance of the four components towards success in this game. These findings underline the crucial role of intention understanding in evaluating LLMs\u2019 social intelligence and highlight the potential of using social deduction games as a complex testbed to enhance LLM evaluation. InterIntent contributes a structured approach to bridging the evaluation gap in social intelligence within multiplayer LLM-based games."
    },
    {
        "paperId": "4b743d624ede1b1cbdb22c570ef62be0aefe33e8",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "\u201cYou Gotta be a Doctor, Lin\u201d : An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12232, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-18",
        "authors": [
            {
                "authorId": "1413110592",
                "name": "H. Nghiem"
            },
            {
                "authorId": "3410976",
                "name": "John J. Prindle"
            },
            {
                "authorId": "2307215850",
                "name": "Jieyu Zhao"
            },
            {
                "authorId": "2200167546",
                "name": "Hal Daum'e"
            }
        ],
        "abstract": "Social science research has shown that candidates with names indicative of certain races or genders often face discrimination in employment practices. Similarly, Large Language Models (LLMs) have demonstrated racial and gender biases in various applications. In this study, we utilize GPT-3.5-Turbo and Llama 3-70B-Instruct to simulate hiring decisions and salary recommendations for candidates with 320 first names that strongly signal their race and gender, across over 750,000 prompts. Our empirical results indicate a preference among these models for hiring candidates with White female-sounding names over other demographic groups across 40 occupations. Additionally, even among candidates with identical qualifications, salary recommendations vary by as much as 5% between different subgroups. A comparison with real-world labor data reveals inconsistent alignment with U.S. labor market characteristics, underscoring the necessity of risk investigation of LLM-powered systems."
    },
    {
        "paperId": "349dde7c0b8d45e7aab6516588e4ca764d8a4bda",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "From Instance Training to Instruction Learning: Task Adapters Generation from Instructions",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-18",
        "authors": [
            {
                "authorId": "2232817310",
                "name": "Huanxuan Liao"
            },
            {
                "authorId": "2223168343",
                "name": "Yao Xu"
            },
            {
                "authorId": "1954845",
                "name": "Shizhu He"
            },
            {
                "authorId": "2145784135",
                "name": "Yuanzhe Zhang"
            },
            {
                "authorId": "8362138",
                "name": "Yanchao Hao"
            },
            {
                "authorId": "2238165288",
                "name": "Shengping Liu"
            },
            {
                "authorId": "2283274776",
                "name": "Kang Liu"
            },
            {
                "authorId": "2257315249",
                "name": "Jun Zhao"
            }
        ],
        "abstract": "Large language models (LLMs) have acquired the ability to solve general tasks by utilizing instruction finetuning (IFT). However, IFT still relies heavily on instance training of extensive task data, which greatly limits the adaptability of LLMs to real-world scenarios where labeled task instances are scarce and broader task generalization becomes paramount. Contrary to LLMs, humans acquire skills and complete tasks not merely through repeated practice but also by understanding and following instructional guidelines. This paper is dedicated to simulating human learning to address the shortcomings of instance training, focusing on instruction learning to enhance cross-task generalization. Within this context, we introduce Task Adapters Generation from Instructions (TAGI), which automatically constructs the task-specific model in a parameter generation manner based on the given task instructions without retraining for unseen tasks. Specifically, we utilize knowledge distillation to enhance the consistency between TAGI developed through Learning with Instruction and task-specific models developed through Training with Instance, by aligning the labels, output logits, and adapter parameters between them. TAGI is endowed with cross-task generalization capabilities through a two-stage training process that includes hypernetwork pretraining and finetuning. We evaluate TAGI on the Super-Natural Instructions and P3 datasets. The experimental results demonstrate that TAGI can match or even outperform traditional meta-trained models and other hypernetwork models, while significantly reducing computational requirements."
    },
    {
        "paperId": "367e43d1561fce27c919e2d370e42399a40846bd",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Ask-before-Plan: Proactive Language Agents for Real-World Planning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12639, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-18",
        "authors": [
            {
                "authorId": "2286379501",
                "name": "Xuan Zhang"
            },
            {
                "authorId": "145843537",
                "name": "Yang Deng"
            },
            {
                "authorId": "2158173064",
                "name": "Zifeng Ren"
            },
            {
                "authorId": "2241348826",
                "name": "See-Kiong Ng"
            },
            {
                "authorId": "2257036129",
                "name": "Tat-Seng Chua"
            }
        ],
        "abstract": "The evolution of large language models (LLMs) has enhanced the planning capabilities of language agents in diverse real-world scenarios. Despite these advancements, the potential of LLM-powered agents to comprehend ambiguous user instructions for reasoning and decision-making is still under exploration. In this work, we introduce a new task, Proactive Agent Planning, which requires language agents to predict clarification needs based on user-agent conversation and agent-environment interaction, invoke external tools to collect valid information, and generate a plan to fulfill the user's demands. To study this practical problem, we establish a new benchmark dataset, Ask-before-Plan. To tackle the deficiency of LLMs in proactive planning, we propose a novel multi-agent framework, Clarification-Execution-Planning (\\texttt{CEP}), which consists of three agents specialized in clarification, execution, and planning. We introduce the trajectory tuning scheme for the clarification agent and static execution agent, as well as the memory recollection mechanism for the dynamic execution agent. Extensive evaluations and comprehensive analyses conducted on the Ask-before-Plan dataset validate the effectiveness of our proposed framework."
    },
    {
        "paperId": "881af971d00621709b4c772750cd3ea9d0fb11fd",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Estimating Knowledge in Large Language Models Without Generating a Single Token",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12673, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-18",
        "authors": [
            {
                "authorId": "2307080108",
                "name": "Daniela Gottesman"
            },
            {
                "authorId": "22245981",
                "name": "Mor Geva"
            }
        ],
        "abstract": "To evaluate knowledge in large language models (LLMs), current methods query the model and then evaluate its generated responses. In this work, we ask whether evaluation can be done before the model has generated any text. Concretely, is it possible to estimate how knowledgeable a model is about a certain entity, only from its internal computation? We study this question with two tasks: given a subject entity, the goal is to predict (a) the ability of the model to answer common questions about the entity, and (b) the factuality of open-ended responses generated by the model about the entity. Experiments with a variety of LLMs show that KEEN, a simple probe trained over internal subject representations, succeeds at both tasks - correlating with both the QA accuracy of the model per-subject and FActScore, a recent factuality metric in open-ended generation. Moreover, KEEN naturally aligns with the model's hedging behavior and faithfully reflects changes in the model's knowledge after fine-tuning. Lastly, we show a more interpretable yet equally performant variant of KEEN, which highlights a small set of tokens indicative of clusters and gaps in the model's knowledge. Being simple and lightweight, KEEN can be leveraged to guide decisions such as when it is appropriate to apply further training or augment queries with retrieval."
    },
    {
        "paperId": "de69c6b8b5eae28d45bbac31364175a9abfb826c",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Measuring Psychological Depth in Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12680, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-18",
        "authors": [
            {
                "authorId": "1576480073",
                "name": "Fabrice Harel-Canada"
            },
            {
                "authorId": "2290149447",
                "name": "Hanyu Zhou"
            },
            {
                "authorId": "1986355219",
                "name": "Sreya Muppalla"
            },
            {
                "authorId": "2307077874",
                "name": "Zeynep Yildiz"
            },
            {
                "authorId": "2307082830",
                "name": "Amit Sahai"
            },
            {
                "authorId": "2266840227",
                "name": "Nanyun Peng"
            }
        ],
        "abstract": "Evaluations of creative stories generated by large language models (LLMs) often focus on objective properties of the text, such as its style, coherence, and diversity. While these metrics are indispensable, they do not speak to a story\u2019s subjective, psychological impact from a reader\u2019s perspective. We introduce the Psychological Depth Scale (PDS), a novel framework rooted in literary theory that measures an LLM\u2019s ability to produce authentic and narratively complex stories that provoke emotion, empathy, and engagement. We empirically validate our framework by showing that humans can consistently evaluate stories based on PDS (0.72 Krippendorff\u2019s alpha). We also explore techniques for automating the PDS to easily scale future analyses. GPT-4o, combined with a novel Mixture-of-Personas (MoP) prompting strategy, achieves an average Spearman correlation of 0.51 with human judgment while Llama-3-70B with constrained decoding scores as high as 0.68 for empathy. Finally, we compared the depth of stories authored by both humans and LLMs. Surprisingly, GPT-4 stories either surpassed or were statistically indistinguishable from highly-rated human-written stories sourced from Reddit. By shifting the focus from text to reader, the Psychological Depth Scale is a validated, automated, and systematic means of measuring the capacity of LLMs to connect with humans through the stories they tell."
    },
    {
        "paperId": "5889a56c5a006e66b97ef55a47b88bd7509e8f39",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12707, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-18",
        "authors": [
            {
                "authorId": "2307074584",
                "name": "Haoqiu Yan"
            },
            {
                "authorId": "2116513069",
                "name": "Yongxin Zhu"
            },
            {
                "authorId": "2307073385",
                "name": "Kai Zheng"
            },
            {
                "authorId": "2307323733",
                "name": "Bing Liu"
            },
            {
                "authorId": "2280336439",
                "name": "Haoyu Cao"
            },
            {
                "authorId": "2302373566",
                "name": "Deqiang Jiang"
            },
            {
                "authorId": "2261952235",
                "name": "Linli Xu"
            }
        ],
        "abstract": "Large Language Model (LLM)-enhanced agents become increasingly prevalent in Human-AI communication, offering vast potential from entertainment to professional domains. However, current multi-modal dialogue systems overlook the acoustic information present in speech, which is crucial for understanding human communication nuances. This oversight can lead to misinterpretations of speakers' intentions, resulting in inconsistent or even contradictory responses within dialogues. To bridge this gap, in this paper, we propose PerceptiveAgent, an empathetic multi-modal dialogue system designed to discern deeper or more subtle meanings beyond the literal interpretations of words through the integration of speech modality perception. Employing LLMs as a cognitive core, PerceptiveAgent perceives acoustic information from input speech and generates empathetic responses based on speaking styles described in natural language. Experimental results indicate that PerceptiveAgent excels in contextual understanding by accurately discerning the speakers' true intentions in scenarios where the linguistic meaning is either contrary to or inconsistent with the speaker's true feelings, producing more nuanced and expressive spoken dialogues. Code is publicly available at: \\url{https://github.com/Haoqiu-Yan/PerceptiveAgent}."
    },
    {
        "paperId": "9348b7b95982d0a675a767e92c23647aa6915a94",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "AgentReview: Exploring Peer Review Dynamics with LLM Agents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12708, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-18",
        "authors": [
            {
                "authorId": "2087723977",
                "name": "Yiqiao Jin"
            },
            {
                "authorId": "2261935625",
                "name": "Qinlin Zhao"
            },
            {
                "authorId": "2307187280",
                "name": "Yiyang Wang"
            },
            {
                "authorId": "2261741520",
                "name": "Hao Chen"
            },
            {
                "authorId": "2543684",
                "name": "Kaijie Zhu"
            },
            {
                "authorId": "95289709",
                "name": "Yijia Xiao"
            },
            {
                "authorId": "2285254341",
                "name": "Jindong Wang"
            }
        ],
        "abstract": "Peer review is fundamental to the integrity and advancement of scientific publication. Traditional methods of peer review analyses often rely on exploration and statistics of existing peer review data, which do not adequately address the multivariate nature of the process, account for the latent variables, and are further constrained by privacy concerns due to the sensitive nature of the data. We introduce AgentReview, the first large language model (LLM) based peer review simulation framework, which effectively disentangles the impacts of multiple latent factors and addresses the privacy issue. Our study reveals significant insights, including a notable 37.1% variation in paper decisions due to reviewers\u2019 biases, supported by sociological theories such as the social influence theory, altruism fatigue, and authority bias. We believe that this study could offer valuable insights to improve the design of peer review mechanisms."
    },
    {
        "paperId": "b0ce75fca254a50e5b6e1eca757a67706d2f91db",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12753, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-18",
        "authors": [
            {
                "authorId": "2307164191",
                "name": "Zhen Huang"
            },
            {
                "authorId": "3196852",
                "name": "Zengzhi Wang"
            },
            {
                "authorId": "2295644924",
                "name": "Shijie Xia"
            },
            {
                "authorId": "2284732801",
                "name": "Xuefeng Li"
            },
            {
                "authorId": "2284719949",
                "name": "Haoyang Zou"
            },
            {
                "authorId": "2299105285",
                "name": "Ruijie Xu"
            },
            {
                "authorId": "1657674644",
                "name": "Run-Ze Fan"
            },
            {
                "authorId": "2047330635",
                "name": "Lyumanshan Ye"
            },
            {
                "authorId": "2273658317",
                "name": "Ethan Chern"
            },
            {
                "authorId": "2307408759",
                "name": "Yixin Ye"
            },
            {
                "authorId": "2279654115",
                "name": "Yikai Zhang"
            },
            {
                "authorId": "2145435513",
                "name": "Yuqing Yang"
            },
            {
                "authorId": "2307640367",
                "name": "Ting Wu"
            },
            {
                "authorId": "2307185797",
                "name": "Binjie Wang"
            },
            {
                "authorId": "2256995981",
                "name": "Shichao Sun"
            },
            {
                "authorId": "2307314606",
                "name": "Yang Xiao"
            },
            {
                "authorId": "2307218360",
                "name": "Yiyuan Li"
            },
            {
                "authorId": "2153433679",
                "name": "Fan Zhou"
            },
            {
                "authorId": "2224851117",
                "name": "Steffi Chern"
            },
            {
                "authorId": "2143528602",
                "name": "Yiwei Qin"
            },
            {
                "authorId": "2148986814",
                "name": "Yan Ma"
            },
            {
                "authorId": "2307998894",
                "name": "Jiadi Su"
            },
            {
                "authorId": "2307217460",
                "name": "Yixiu Liu"
            },
            {
                "authorId": "2307444742",
                "name": "Yuxiang Zheng"
            },
            {
                "authorId": "2237786370",
                "name": "Shaoting Zhang"
            },
            {
                "authorId": "2237734015",
                "name": "Dahua Lin"
            },
            {
                "authorId": "2270064219",
                "name": "Yu Qiao"
            },
            {
                "authorId": "2276753486",
                "name": "Pengfei Liu"
            }
        ],
        "abstract": "The evolution of Artificial Intelligence (AI) has been significantly accelerated by advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), gradually showcasing potential cognitive reasoning abilities in problem-solving and scientific discovery (i.e., AI4Science) once exclusive to human intellect. To comprehensively evaluate current models' performance in cognitive reasoning abilities, we introduce OlympicArena, which includes 11,163 bilingual problems across both text-only and interleaved text-image modalities. These challenges encompass a wide range of disciplines spanning seven fields and 62 international Olympic competitions, rigorously examined for data leakage. We argue that the challenges in Olympic competition problems are ideal for evaluating AI's cognitive reasoning due to their complexity and interdisciplinary nature, which are essential for tackling complex scientific challenges and facilitating discoveries. Beyond evaluating performance across various disciplines using answer-only criteria, we conduct detailed experiments and analyses from multiple perspectives. We delve into the models' cognitive reasoning abilities, their performance across different modalities, and their outcomes in process-level evaluations, which are vital for tasks requiring complex reasoning with lengthy solutions. Our extensive evaluations reveal that even advanced models like GPT-4o only achieve a 39.97% overall accuracy, illustrating current AI limitations in complex reasoning and multimodal integration. Through the OlympicArena, we aim to advance AI towards superintelligence, equipping it to address more complex challenges in science and beyond. We also provide a comprehensive set of resources to support AI research, including a benchmark dataset, an open-source annotation platform, a detailed evaluation tool, and a leaderboard with automatic submission features."
    },
    {
        "paperId": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-18",
        "authors": [
            {
                "authorId": "2266194421",
                "name": "Haoxiang Wang"
            },
            {
                "authorId": "2275119437",
                "name": "Wei Xiong"
            },
            {
                "authorId": "51437774",
                "name": "Tengyang Xie"
            },
            {
                "authorId": "2188204871",
                "name": "Han Zhao"
            },
            {
                "authorId": "2301173100",
                "name": "Tong Zhang"
            }
        ],
        "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. The RLHF process typically starts by training a reward model (RM) using human preference data. Conventional RMs are trained on pairwise responses to the same user request, with relative ratings indicating which response humans prefer. The trained RM serves as a proxy for human preferences. However, due to the black-box nature of RMs, their outputs lack interpretability, as humans cannot intuitively understand why an RM thinks a response is good or not. As RMs act as human preference proxies, we believe they should be human-interpretable to ensure that their internal decision processes are consistent with human preferences and to prevent reward hacking in LLM alignment. To build RMs with interpretable preferences, we propose a two-stage approach: i) train an Absolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional absolute-rating data, each dimension corresponding to a human-interpretable objective (e.g., honesty, verbosity, safety); ii) employ a Mixture-of-Experts (MoE) strategy with a gating network that automatically selects the most suitable reward objectives based on the context. We efficiently trained an ArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top of the ArmoRM. Our trained model, ArmoRM-Llama3-8B, obtains state-of-the-art performance on RewardBench, a benchmark evaluating RMs for language modeling. Notably, the performance of our model surpasses the LLM-as-a-judge method with GPT-4 judges by a margin, and approaches the performance of the much larger Nemotron-4 340B reward model."
    },
    {
        "paperId": "2fddf16c0a5092f0c8f804fdeb0038996766c385",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond Words",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.13340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-19",
        "authors": [
            {
                "authorId": "2052108603",
                "name": "Junyi Ao"
            },
            {
                "authorId": "2275036526",
                "name": "Yuancheng Wang"
            },
            {
                "authorId": "2339600",
                "name": "Xiaohai Tian"
            },
            {
                "authorId": "2307482725",
                "name": "Dekun Chen"
            },
            {
                "authorId": "2305803685",
                "name": "Jun Zhang"
            },
            {
                "authorId": "2307556627",
                "name": "Lu Lu"
            },
            {
                "authorId": "2307556125",
                "name": "Yuxuan Wang"
            },
            {
                "authorId": "2274941014",
                "name": "Haizhou Li"
            },
            {
                "authorId": "2290126835",
                "name": "Zhizheng Wu"
            }
        ],
        "abstract": "Speech encompasses a wealth of information, including but not limited to content, paralinguistic, and environmental information. This comprehensive nature of speech significantly impacts communication and is crucial for human-computer interaction. Chat-Oriented Large Language Models (LLMs), known for their general-purpose assistance capabilities, have evolved to handle multi-modal inputs, including speech. Although these models can be adept at recognizing and analyzing speech, they often fall short of generating appropriate responses. We argue that this is due to the lack of principles on task definition and model development, which requires open-source datasets and metrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a benchmark dataset aimed at multidimensional evaluation of spoken dialogue understanding and generation. SD-Eval focuses on paralinguistic and environmental information and includes 7,303 utterances, amounting to 8.76 hours of speech data. The data is aggregated from eight public datasets, representing four perspectives: emotion, accent, age, and background sound. To assess the SD-Eval benchmark dataset, we implement three different models and construct a training set following a process similar to that of SD-Eval. The training set contains 1,052.72 hours of speech data and 724.4k utterances. We also conduct a comprehensive evaluation using objective evaluation methods (e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the generated responses. Models conditioned with paralinguistic and environmental information outperform their counterparts in both objective and subjective measures. Moreover, experiments demonstrate that LLM-based metrics show a higher correlation with human evaluation compared to traditional metrics. We open-source SD-Eval at https://github.com/amphionspace/SD-Eval."
    },
    {
        "paperId": "45a92d8483c80ccfc0ae497ce308d882a11a368e",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.13805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-19",
        "authors": [
            {
                "authorId": "2307897753",
                "name": "Yufang Hou"
            },
            {
                "authorId": "2307463172",
                "name": "Alessandra Pascale"
            },
            {
                "authorId": "2307467605",
                "name": "Javier Carnerero-Cano"
            },
            {
                "authorId": "3235883",
                "name": "T. Tchrakian"
            },
            {
                "authorId": "2808565",
                "name": "Radu Marinescu"
            },
            {
                "authorId": "2307465399",
                "name": "Elizabeth Daly"
            },
            {
                "authorId": "8350409",
                "name": "Inkit Padhi"
            },
            {
                "authorId": "1706272",
                "name": "P. Sattigeri"
            }
        ],
        "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising solution to mitigate the limitations of large language models (LLMs), such as hallucinations and outdated information. However, it remains unclear how LLMs handle knowledge conflicts arising from different augmented retrieved passages, especially when these passages originate from the same source and have equal trustworthiness. In this work, we conduct a comprehensive evaluation of LLM-generated answers to questions that have varying answers based on contradictory passages from Wikipedia, a dataset widely regarded as a high-quality pre-training resource for most LLMs. Specifically, we introduce WikiContradict, a benchmark consisting of 253 high-quality, human-annotated instances designed to assess LLM performance when augmented with retrieved passages containing real-world knowledge conflicts. We benchmark a diverse range of both closed and open-source LLMs under different QA scenarios, including RAG with a single passage, and RAG with 2 contradictory passages. Through rigorous human evaluations on a subset of WikiContradict instances involving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and limitations of these models. For instance, when provided with two passages containing contradictory facts, all models struggle to generate answers that accurately reflect the conflicting nature of the context, especially for implicit conflicts requiring reasoning. Since human evaluation is costly, we also introduce an automated model that estimates LLM performance using a strong open-source language model, achieving an F-score of 0.8. Using this automated metric, we evaluate more than 1,500 answers from seven LLMs across all WikiContradict instances. To facilitate future work, we release WikiContradict on: https://ibm.biz/wikicontradict."
    },
    {
        "paperId": "156f1a58661814f60ab4abaf027f64e47d1c4fdf",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Persuasiveness of Generated Free-Text Rationales in Subjective Decisions: A Case Study on Pairwise Argument Ranking",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.13905, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-20",
        "authors": [
            {
                "authorId": "41134103",
                "name": "Mohamed S. Elaraby"
            },
            {
                "authorId": "2161470060",
                "name": "Diane Litman"
            },
            {
                "authorId": "2307982465",
                "name": "Xiang Lorraine Li"
            },
            {
                "authorId": "2905973",
                "name": "Ahmed Magooda"
            }
        ],
        "abstract": "Generating free-text rationales is among the emergent capabilities of Large Language Models (LLMs). These rationales have been found to enhance LLM performance across various NLP tasks. Recently, there has been growing interest in using these rationales to provide insights for various important downstream tasks. In this paper, we analyze generated free-text rationales in tasks with subjective answers, emphasizing the importance of rationalization in such scenarios. We focus on pairwise argument ranking, a highly subjective task with significant potential for real-world applications, such as debate assistance. We evaluate the persuasiveness of rationales generated by nine LLMs to support their subjective choices. Our findings suggest that open-source LLMs, particularly Llama2-70B-chat, are capable of providing highly persuasive rationalizations, surpassing even GPT models. Additionally, our experiments show that rationale persuasiveness can be improved by controlling its parameters through prompting or through self-refinement."
    },
    {
        "paperId": "232469e4195759c6f2e3f2ff383d0f7e8dbe433c",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Aligning Large Language Models with Diverse Political Viewpoints",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-20",
        "authors": [
            {
                "authorId": "146552774",
                "name": "Dominik Stammbach"
            },
            {
                "authorId": "147856060",
                "name": "Philine Widmer"
            },
            {
                "authorId": "2349014004",
                "name": "Eunjung Cho"
            },
            {
                "authorId": "2288101023",
                "name": "Caglar Gulcehre"
            },
            {
                "authorId": "2261279066",
                "name": "Elliott Ash"
            }
        ],
        "abstract": "Large language models such as ChatGPT exhibit striking political biases. If users query them about political information, they often take a normative stance. To overcome this, we align LLMs with diverse political viewpoints from 100,000 comments written by candidates running for national parliament in Switzerland. Models aligned with this data can generate more accurate political viewpoints from Swiss parties, compared to commercial models such as ChatGPT. We also propose a procedure to generate balanced overviews summarizing multiple viewpoints using such models. The replication package contains all code and data."
    },
    {
        "paperId": "661c0467b442479a3891bbfd9a6ef3924af2aa70",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human Factors in Personas",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2406.14462",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-20",
        "authors": [
            {
                "authorId": "2306632156",
                "name": "Salvatore Giorgi"
            },
            {
                "authorId": "2115432852",
                "name": "Tingting Liu"
            },
            {
                "authorId": "72295693",
                "name": "A. Aich"
            },
            {
                "authorId": "2307072274",
                "name": "Kelsey Isman"
            },
            {
                "authorId": "17010256",
                "name": "G. Sherman"
            },
            {
                "authorId": "2307466980",
                "name": "Zachary Fried"
            },
            {
                "authorId": "2662374",
                "name": "Jo\u00e3o Sedoc"
            },
            {
                "authorId": "143857273",
                "name": "Pallavi V. Kulkarni"
            },
            {
                "authorId": "2307070926",
                "name": "Brenda Curtis"
            }
        ],
        "abstract": "Large language models (LLMs) are increasingly being used in human-centered social scientific tasks, such as data annotation, synthetic data creation, and engaging in dialog. However, these tasks are highly subjective and dependent on human factors, such as one's environment, attitudes, beliefs, and lived experiences. Thus, it may be the case that employing LLMs (which do not have such human factors) in these tasks results in a lack of variation in data, failing to reflect the diversity of human experiences. In this paper, we examine the role of prompting LLMs with human-like personas and asking the models to answer as if they were a specific human. This is done explicitly, with exact demographics, political beliefs, and lived experiences, or implicitly via names prevalent in specific populations. The LLM personas are then evaluated via (1) subjective annotation task (e.g., detecting toxicity) and (2) a belief generation task, where both tasks are known to vary across human factors. We examine the impact of explicit vs. implicit personas and investigate which human factors LLMs recognize and respond to. Results show that explicit LLM personas show mixed results when reproducing known human biases, but generally fail to demonstrate implicit biases. We conclude that LLMs may capture the statistical patterns of how people speak, but are generally unable to model the complex interactions and subtleties of human perceptions, potentially limiting their effectiveness in social science applications."
    },
    {
        "paperId": "5da64e4d1d9ad1322223d81d3133188a54619b0f",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-20",
        "authors": [
            {
                "authorId": "2029885131",
                "name": "Gabriel Sarch"
            },
            {
                "authorId": "2307917842",
                "name": "Lawrence Jang"
            },
            {
                "authorId": "2237792290",
                "name": "Michael J. Tarr"
            },
            {
                "authorId": "2307918432",
                "name": "William W. Cohen"
            },
            {
                "authorId": "2316561978",
                "name": "Kenneth Marino"
            },
            {
                "authorId": "1705557",
                "name": "Katerina Fragkiadaki"
            }
        ],
        "abstract": "Large-scale LLMs and VLMs excel at few-shot learning but require high-quality examples. We introduce In-Context Abstraction Learning (ICAL), which iteratively refines suboptimal trajectories into high-quality data with optimized actions and detailed reasoning. Given an inefficient demonstration, a VLM corrects actions and annotates causal relationships, object states, subgoals, and task-relevant visuals, forming\"programs of thought.\"With human feedback, these programs are improved as the agent executes them in a similar environment. The resulting examples, used as prompt context or fine-tuning data, significantly boost decision-making while reducing human feedback needs. ICAL surpasses state-of-the-art in TEACh (dialogue-based instruction following), VisualWebArena (multimodal web agents), and Ego4D (egocentric video action anticipation). In TEACh, combining fine-tuning and retrieval on ICAL examples outperforms raw human demonstrations and expert examples, achieving a 17.5% increase in goal-condition success. In VisualWebArena, retrieval-augmented GPT-4V with ICAL improves task success rate 1.6x over GPT-4V, while fine-tuning Qwen2-VL achieves a 2.8x improvement. In Ego4D, ICAL outperforms few-shot GPT-4V and remains competitive with supervised models. Overall, ICAL scales 2x better than raw human demonstrations and reduces manual prompt engineering."
    },
    {
        "paperId": "670f9c71de3480d1ae4629b9e6b1775bb02e5ea4",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "An LLM Feature-based Framework for Dialogue Constructiveness Assessment",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14760, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-20",
        "authors": [
            {
                "authorId": "2307982883",
                "name": "Lexin Zhou"
            },
            {
                "authorId": "22202718",
                "name": "Youmna Farag"
            },
            {
                "authorId": "2266465838",
                "name": "Andreas Vlachos"
            }
        ],
        "abstract": "Research on dialogue constructiveness assessment focuses on (i) analysing conversational factors that influence individuals to take specific actions, win debates, change their perspectives or broaden their open-mindedness and (ii) predicting constructiveness outcomes following dialogues for such use cases. These objectives can be achieved by training either interpretable feature-based models (which often involve costly human annotations) or neural models such as pre-trained language models (which have empirically shown higher task accuracy but lack interpretability). In this paper we propose an LLM feature-based framework for dialogue constructiveness assessment that combines the strengths of feature-based and neural approaches, while mitigating their downsides. The framework first defines a set of dataset-independent and interpretable linguistic features, which can be extracted by both prompting an LLM and simple heuristics. Such features are then used to train LLM feature-based models. We apply this framework to three datasets of dialogue constructiveness and find that our LLM feature-based models outperform or performs at least as well as standard feature-based models and neural models. We also find that the LLM feature-based model learns more robust prediction rules instead of relying on superficial shortcuts, which often trouble neural models."
    },
    {
        "paperId": "2816c49543ec0392627de3f0cf05c2107c2f0031",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "OATH-Frames: Characterizing Online Attitudes Towards Homelessness with LLM Assistants",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-21",
        "authors": [
            {
                "authorId": "1819649550",
                "name": "Jaspreet Ranjit"
            },
            {
                "authorId": "2307915799",
                "name": "Brihi Joshi"
            },
            {
                "authorId": "2307916142",
                "name": "Rebecca Dorn"
            },
            {
                "authorId": "2065263416",
                "name": "Laura Petry"
            },
            {
                "authorId": "2335040948",
                "name": "Olga Koumoundouros"
            },
            {
                "authorId": "2307915789",
                "name": "Jayne Bottarini"
            },
            {
                "authorId": "2308011069",
                "name": "Peichen Liu"
            },
            {
                "authorId": "2262110285",
                "name": "Eric Rice"
            },
            {
                "authorId": "2705113",
                "name": "Swabha Swayamdipta"
            }
        ],
        "abstract": "Warning: Contents of this paper may be upsetting.Public attitudes towards key societal issues, expressed on online media, are of immense value in policy and reform efforts, yet challenging to understand at scale. We study one such social issue: homelessness in the U.S., by leveraging the remarkable capabilities of large language models to assist social work experts in analyzing millions of posts from Twitter. We introduce a framing typology: Online Attitudes Towards Homelessness (OATH) Frames: nine hierarchical frames capturing critiques, responses and perceptions. We release annotations with varying degrees of assistance from language models, with immense benefits in scaling: 6.5\u00d7 speedup in annotation time while only incurring a 3 point F1 reduction in performance with respect to the domain experts. Our experiments demonstrate the value of modeling OATH-Frames over existing sentiment and toxicity classifiers. Our large-scale analysis with predicted OATH-Frames on 2.4M posts on homelessness reveal key trends in attitudes across states, time periods and vulnerable populations, enabling new insights on the issue. Our work provides a general framework to understand nuanced public attitudes at scale, on issues beyond homelessness."
    },
    {
        "paperId": "c2f0c080588f6622410ef02ef034eae373b43529",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Autonomous Agents for Collaborative Task under Information Asymmetry",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14928, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-21",
        "authors": [
            {
                "authorId": "39059473",
                "name": "Wei Liu"
            },
            {
                "authorId": "2307988547",
                "name": "Chenxi Wang"
            },
            {
                "authorId": "2300250179",
                "name": "Yifei Wang"
            },
            {
                "authorId": "2300249571",
                "name": "Zihao Xie"
            },
            {
                "authorId": "2307916117",
                "name": "Rennai Qiu"
            },
            {
                "authorId": "2276606698",
                "name": "Yufan Dang"
            },
            {
                "authorId": "2305747470",
                "name": "Zhuoyun Du"
            },
            {
                "authorId": "2306423185",
                "name": "Weize Chen"
            },
            {
                "authorId": "2257052321",
                "name": "Cheng Yang"
            },
            {
                "authorId": "2214580084",
                "name": "Cheng Qian"
            }
        ],
        "abstract": "Large Language Model Multi-Agent Systems (LLM-MAS) have achieved great progress in solving complex tasks. It performs communication among agents within the system to collaboratively solve tasks, under the premise of shared information. However, when agents' collaborations are leveraged to perform multi-person tasks, a new challenge arises due to information asymmetry, since each agent can only access the information of its human user. Previous MAS struggle to complete tasks under this condition. To address this, we propose a new MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems. In iAgents, the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry. iAgents employs a novel agent reasoning mechanism, InfoNav, to navigate agents' communication toward effective information exchange. Together with InfoNav, iAgents organizes human information in a mixed memory to provide agents with accurate and comprehensive information for exchange. Additionally, we introduce InformativeBench, the first benchmark tailored for evaluating LLM agents' task-solving ability under information asymmetry. Experimental results show that iAgents can collaborate within a social network of 140 individuals and 588 relationships, autonomously communicate over 30 turns, and retrieve information from nearly 70,000 messages to complete tasks within 3 minutes."
    },
    {
        "paperId": "202f3a1341a9e614c2611de4809dd12fe7850f36",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14952, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-21",
        "authors": [
            {
                "authorId": "2306149464",
                "name": "Haiquan Zhao"
            },
            {
                "authorId": "2308044439",
                "name": "Lingyu Li"
            },
            {
                "authorId": "2244180854",
                "name": "Shisong Chen"
            },
            {
                "authorId": "2307914771",
                "name": "Shuqi Kong"
            },
            {
                "authorId": "2118328782",
                "name": "Jiaan Wang"
            },
            {
                "authorId": "2266421899",
                "name": "Kexin Huang"
            },
            {
                "authorId": "2279024315",
                "name": "Tianle Gu"
            },
            {
                "authorId": "2266363141",
                "name": "Yixu Wang"
            },
            {
                "authorId": "2305830148",
                "name": "Dandan Liang"
            },
            {
                "authorId": "2243457917",
                "name": "Zhixu Li"
            },
            {
                "authorId": "2266238818",
                "name": "Yan Teng"
            },
            {
                "authorId": "2265724350",
                "name": "Yanghua Xiao"
            },
            {
                "authorId": "2266364817",
                "name": "Yingchun Wang"
            }
        ],
        "abstract": "Emotion Support Conversation (ESC) is a crucial application, which aims to reduce human stress, offer emotional guidance, and ultimately enhance human mental and physical well-being. With the advancement of Large Language Models (LLMs), many researchers have employed LLMs as the ESC models. However, the evaluation of these LLM-based ESCs remains uncertain. In detail, we first re-organize 2,801 role-playing cards from seven existing datasets to define the roles of the role-playing agent. Second, we train a specific role-playing model called ESC-Role which behaves more like a confused person than GPT-4. Third, through ESC-Role and organized role cards, we systematically conduct experiments using 14 LLMs as the ESC models, including general AI-assistant LLMs (e.g., ChatGPT) and ESC-oriented LLMs (e.g., ExTES-Llama). We conduct comprehensive human annotations on interactive multi-turn dialogues of different ESC models. The results show that ESC-oriented LLMs exhibit superior ESC abilities compared to general AI-assistant LLMs, but there is still a gap behind human performance. Moreover, to automate the scoring process for future ESC models, we developed ESC-RANK, which trained on the annotated data, achieving a scoring performance surpassing 35 points of GPT-4."
    },
    {
        "paperId": "3bc7877cc7e49af87f0b3f2e525aace6b49d0bd1",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15053, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-21",
        "authors": [
            {
                "authorId": "2297767230",
                "name": "Ishaan Watts"
            },
            {
                "authorId": "2140408530",
                "name": "Varun Gumma"
            },
            {
                "authorId": "151482275",
                "name": "Aditya Yadavalli"
            },
            {
                "authorId": "2281746966",
                "name": "Vivek Seshadri"
            },
            {
                "authorId": "30576065",
                "name": "Manohar Swaminathan"
            },
            {
                "authorId": "2256989615",
                "name": "Sunayana Sitaram"
            }
        ],
        "abstract": "Evaluation of multilingual Large Language Models (LLMs) is challenging due to a variety of factors \u2013 the lack of benchmarks with sufficient linguistic diversity, contamination of popular benchmarks into LLM pre-training data and the lack of local, cultural nuances in translated benchmarks. In this work, we study human and LLM-based evaluation in a multilingual, multi-cultural setting. We evaluate 30 models across 10 Indic languages by conducting 90K human evaluations and 30K LLM-based evaluations and find that models such as GPT-4o and Llama-3 70B consistently perform best for most Indic languages. We build leaderboards for two evaluation settings - pairwise comparison and direct assessment and analyse the agreement between humans and LLMs. We find that humans and LLMs agree fairly well in the pairwise setting but the agreement drops for direct assessment evaluation especially for languages such as Bengali and Odia. We also check for various biases in human and LLM-based evaluation and find evidence of self-bias in the GPT-based evaluator. Our work presents a significant step towards scaling up multilingual evaluation of LLMs."
    },
    {
        "paperId": "74f1b67fa18bc9c4033a9e8fd4e11ed38b178a37",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Hybrid Alignment Training for Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15178, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-21",
        "authors": [
            {
                "authorId": "2109452621",
                "name": "Chenglong Wang"
            },
            {
                "authorId": "2227993964",
                "name": "Hang Zhou"
            },
            {
                "authorId": "2295485360",
                "name": "Kaiyan Chang"
            },
            {
                "authorId": "2291200534",
                "name": "Bei Li"
            },
            {
                "authorId": "2040853265",
                "name": "Yongyu Mu"
            },
            {
                "authorId": "2261739712",
                "name": "Tong Xiao"
            },
            {
                "authorId": "2111149004",
                "name": "Tongran Liu"
            },
            {
                "authorId": "2240940961",
                "name": "Jingbo Zhu"
            }
        ],
        "abstract": "Alignment training is crucial for enabling large language models (LLMs) to cater to human intentions and preferences. It is typically performed based on two stages with different objectives: instruction-following alignment and human-preference alignment. However, aligning LLMs with these objectives in sequence suffers from an inherent problem: the objectives may conflict, and the LLMs cannot guarantee to simultaneously align with the instructions and human preferences well. To response to these, in this work, we propose a Hybrid Alignment Training (Hbat) approach, based on alternating alignment and modified elastic weight consolidation methods. The basic idea is to alternate between different objectives during alignment training, so that better collaboration can be achieved between the two alignment tasks.We experiment with Hbat on summarization and dialogue tasks. Experimental results show that the proposed \\textsc{Hbat} can significantly outperform all baselines. Notably, Hbat yields consistent performance gains over the traditional two-stage alignment training when using both proximal policy optimization and direct preference optimization."
    },
    {
        "paperId": "fac4e94cb7f0813cbccbddc6e792ececd2814ca2",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Evaluating Diversity in Automatic Poetry Generation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15267, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-21",
        "authors": [
            {
                "authorId": "2109307862",
                "name": "Yanran Chen"
            },
            {
                "authorId": "2307918070",
                "name": "Hannes Groner"
            },
            {
                "authorId": "1721364",
                "name": "Sina Zarrie\u00df"
            },
            {
                "authorId": "2265897653",
                "name": "Steffen Eger"
            }
        ],
        "abstract": "Natural Language Generation (NLG), and more generally generative AI, are among the currently most impactful research fields. Creative NLG, such as automatic poetry generation, is a fascinating niche in this area. While most previous research has focused on forms of the Turing test when evaluating automatic poetry generation \u2014 can humans distinguish between automatic and human generated poetry \u2014 we evaluate the diversity of automatically generated poetry (with a focus on quatrains), by comparing distributions of generated poetry to distributions of human poetry along structural, lexical, semantic and stylistic dimensions, assessing different model types (word vs. character-level, general purpose LLMs vs. poetry-specific models), including the very recent LLaMA3-8B, and types of fine-tuning (conditioned vs. unconditioned). We find that current automatic poetry systems are considerably underdiverse along multiple dimensions \u2014 they often do not rhyme sufficiently, are semantically too uniform and even do not match the length distribution of human poetry. Our experiments reveal, however, that style-conditioning and character-level modeling clearly increases diversity across virtually all dimensions we explore. Our identified limitations may serve as the basis for more genuinely diverse future poetry generation models."
    },
    {
        "paperId": "1610e78380da2890a081892ac52a9c57f2b8e01f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "A SMART Mnemonic Sounds like \u201cGlue Tonic\u201d: Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-21",
        "authors": [
            {
                "authorId": "2216486213",
                "name": "Nishant Balepur"
            },
            {
                "authorId": "2138314131",
                "name": "Matthew Shu"
            },
            {
                "authorId": "2307915196",
                "name": "Alexander Hoyle"
            },
            {
                "authorId": "2307914904",
                "name": "Alison Robey"
            },
            {
                "authorId": "2284701194",
                "name": "Shi Feng"
            },
            {
                "authorId": "1405375772",
                "name": "Seraphina Goldfarb-Tarrant"
            },
            {
                "authorId": "2260114875",
                "name": "Jordan L. Boyd-Graber"
            }
        ],
        "abstract": "Keyword mnemonics are memorable explanations that link new terms to simpler keywords.Prior work generates mnemonics for students, but they do not train models using mnemonics students prefer and aid learning.We build SMART, a mnemonic generator trained on feedback from real students learning new terms.To train SMART, we first fine-tune LLaMA-2 on a curated set of user-written mnemonics.We then use LLM alignment to enhance SMART: we deploy mnemonics generated by SMART in a flashcard app to find preferences on mnemonics students favor.We gather 2684 preferences from 45 students across two types: **expressed** (inferred from ratings) and **observed** (inferred from student learning), yielding three key findings.First, expressed and observed preferences disagree; what students *think* is helpful does not always capture what is *truly* helpful.Second, Bayesian models can synthesize complementary data from multiple preference types into a single effectiveness signal.SMART is tuned via Direct Preference Optimization on this signal, which resolves ties and missing labels in the typical method of pairwise comparisons, augmenting data for LLM output quality gains. Third, mnemonic experts assess SMART as matching GPT-4 at much lower deployment costs, showing the utility of capturing diverse student feedback to align LLMs in education."
    },
    {
        "paperId": "9ab5cf4423547ac0ff37db234ff5d65290c11c8f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Mental Disorder Classification via Temporal Representation of Text",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15470, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-15",
        "authors": [
            {
                "authorId": "2273917115",
                "name": "Raja Kumar"
            },
            {
                "authorId": "2273459762",
                "name": "Kishan Maharaj"
            },
            {
                "authorId": "2273570942",
                "name": "Ashita Saxena"
            },
            {
                "authorId": "2270239819",
                "name": "Pushpak Bhattacharyya"
            }
        ],
        "abstract": "Mental disorders pose a global challenge, aggravated by the shortage of qualified mental health professionals. Mental disorder prediction from social media posts by current LLMs is challenging due to the complexities of sequential text data and the limited context length of language models. Current language model-based approaches split a single data instance into multiple chunks to compensate for limited context size. The predictive model is then applied to each chunk individually, and the most voted output is selected as the final prediction. This results in the loss of inter-post dependencies and important time variant information, leading to poor performance. We propose a novel framework which first compresses the large sequence of chronologically ordered social media posts into a series of numbers. We then use this time variant representation for mental disorder classification. We demonstrate the generalization capabilities of our framework by outperforming the current SOTA in three different mental conditions: depression, self-harm, and anorexia, with an absolute improvement of 5% in the F1 score. We investigate the situation where current data instances fall within the context length of language models and present empirical results highlighting the importance of temporal properties of textual data. Furthermore, we utilize the proposed framework for a cross-domain study, exploring commonalities across disorders and the possibility of inter-domain data usage."
    },
    {
        "paperId": "5b403776ac18823500865fb84c32f03bbdc4e8b2",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2406.15484",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15484, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-17",
        "authors": [
            {
                "authorId": "2308046804",
                "name": "Ze Wang"
            },
            {
                "authorId": "2284029866",
                "name": "Zekun Wu"
            },
            {
                "authorId": "2308070610",
                "name": "Xin Guan"
            },
            {
                "authorId": "2308039312",
                "name": "Michael Thaler"
            },
            {
                "authorId": "2268316579",
                "name": "A. Koshiyama"
            },
            {
                "authorId": "2308072394",
                "name": "Skylar Lu"
            },
            {
                "authorId": "2308039064",
                "name": "Sachin Beepath"
            },
            {
                "authorId": "2308039749",
                "name": "Ediz Ertekin"
            },
            {
                "authorId": "2308039428",
                "name": "Mar\u00eda P\u00e9rez-Ortiz"
            }
        ],
        "abstract": "The use of Large Language Models (LLMs) in hiring has led to legislative actions to protect vulnerable demographic groups. This paper presents a novel framework for benchmarking hierarchical gender hiring bias in Large Language Models (LLMs) for resume scoring, revealing significant issues of reverse gender hiring bias and overdebiasing. Our contributions are fourfold: Firstly, we introduce a new construct grounded in labour economics, legal principles, and critiques of current bias benchmarks: hiring bias can be categorized into two types: Level bias (difference in the average outcomes between demographic counterfactual groups) and Spread bias (difference in the variance of outcomes between demographic counterfactual groups); Level bias can be further subdivided into statistical bias (i.e. changing with non-demographic content) and taste-based bias (i.e. consistent regardless of non-demographic content). Secondly, the framework includes rigorous statistical and computational hiring bias metrics, such as Rank After Scoring (RAS), Rank-based Impact Ratio, Permutation Test, and Fixed Effects Model. Thirdly, we analyze gender hiring biases in ten state-of-the-art LLMs. Seven out of ten LLMs show significant biases against males in at least one industry. An industry-effect regression reveals that the healthcare industry is the most biased against males. Moreover, we found that the bias performance remains invariant with resume content for eight out of ten LLMs. This indicates that the bias performance measured in this paper might apply to other resume datasets with different resume qualities. Fourthly, we provide a user-friendly demo and resume dataset to support the adoption and practical use of the framework, which can be generalized to other social traits and tasks."
    },
    {
        "paperId": "125ff68732c441e143cf33587bebd2ab01372e49",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15718, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-22",
        "authors": [
            {
                "authorId": "2254576790",
                "name": "Xinrong Zhang"
            },
            {
                "authorId": "2109274417",
                "name": "Yingfa Chen"
            },
            {
                "authorId": "1576223501",
                "name": "Shengding Hu"
            },
            {
                "authorId": "48506411",
                "name": "Xu Han"
            },
            {
                "authorId": "2284948588",
                "name": "Zihang Xu"
            },
            {
                "authorId": "2308071031",
                "name": "Yuanwei Xu"
            },
            {
                "authorId": "2150606888",
                "name": "Weilin Zhao"
            },
            {
                "authorId": "2273551430",
                "name": "Maosong Sun"
            },
            {
                "authorId": "2301534001",
                "name": "Zhiyuan Liu"
            }
        ],
        "abstract": "As large language models (LLMs) increasingly permeate daily lives, there is a growing demand for real-time interactions that mirror human conversations. Traditional turn-based chat systems driven by LLMs prevent users from verbally interacting with the system while generating responses.To overcome these limitations, we adapt existing LLMs to duplex models so that they can listen to users while generating output and dynamically adjust themselves to provide instant feedback.Specifically, we divide the queries and responses of conversations into several time slices and then adopt a time-division-multiplexing (TDM) encoding-decoding strategy to process these slices pseudo-simultaneously.Furthermore, to make LLMs proficient enough to handle real-time conversations, we build a fine-tuning dataset consisting of alternating time slices of queries and responses and covering typical feedback types in instantaneous interactions.Our experiments show that although the queries and responses of conversations are segmented into incomplete slices for processing, LLMs can preserve their original performance on standard benchmarks with a few fine-tuning steps on our dataset. Automatic and human evaluation indicate that duplex models make user-AI interactions more natural and human-like, and greatly improve user satisfaction compared to vanilla LLMs. Our duplex model and dataset will be released soon."
    },
    {
        "paperId": "939cbdc260d6c2b02e72fd871ebb0f26d643ce7d",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15765, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-22",
        "authors": [
            {
                "authorId": "10385489",
                "name": "Zhongzhi Yu"
            },
            {
                "authorId": "2308276343",
                "name": "Zheng Wang"
            },
            {
                "authorId": "2116099023",
                "name": "Yonggan Fu"
            },
            {
                "authorId": "2310569409",
                "name": "Huihong Shi"
            },
            {
                "authorId": "2308034263",
                "name": "Khalid Shaikh"
            },
            {
                "authorId": "2305737055",
                "name": "Y. Lin"
            }
        ],
        "abstract": "Attention is a fundamental component behind the remarkable achievements of large language models (LLMs). However, our current understanding of the attention mechanism, especially regarding how attention distributions are established, remains limited. Inspired by recent studies that explore the presence of attention sink in the initial token, which receives disproportionately large attention scores despite their lack of semantic importance, this work delves deeper into this phenomenon. We aim to provide a more profound understanding of the existence of attention sinks within LLMs and to uncover ways to enhance the achievable accuracy of LLMs by directly optimizing the attention distributions, without the need for weight finetuning. Specifically, this work begins with comprehensive visualizations of the attention distributions in LLMs during inference across various inputs and tasks. Based on these visualizations, to the best of our knowledge, we are the first to discover that (1) attention sinks occur not only at the start of sequences but also within later tokens of the input, and (2) not all attention sinks have a positive impact on the achievable accuracy of LLMs. Building upon our findings, we propose a training-free Attention Calibration Technique (ACT) that automatically optimizes the attention distributions on the fly during inference in an input-adaptive manner. Extensive experiments validate that ACT consistently enhances the accuracy of various LLMs across different applications. Specifically, ACT achieves an average improvement of up to 7.30% in accuracy across different datasets when applied to Llama-30B. Our code is available at https://github.com/GATECH-EIC/ACT."
    },
    {
        "paperId": "ab0fa7d219908c04a7b8dde51c1bb5a2a4caec3b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "CaT-Bench: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15823, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-22",
        "authors": [
            {
                "authorId": "27604402",
                "name": "Yash Kumar Lal"
            },
            {
                "authorId": "2308037278",
                "name": "Vanya Cohen"
            },
            {
                "authorId": "2271320422",
                "name": "Nathanael Chambers"
            },
            {
                "authorId": "2270415749",
                "name": "Niranjan Balasubramanian"
            },
            {
                "authorId": "2270005530",
                "name": "Raymond J. Mooney"
            }
        ],
        "abstract": "Understanding the abilities of LLMs to reason about natural language plans, such as instructional text and recipes, is critical to reliably using them in decision-making systems. A fundamental aspect of plans is the temporal order in which their steps need to be executed, which reflects the underlying causal dependencies between them. We introduce CaT-Bench, a benchmark of Step Order Prediction questions, which test whether a step must necessarily occur before or after another in cooking recipe plans. We use this to evaluate how well frontier LLMs understand causal and temporal dependencies. We find that SOTA LLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased towards predicting dependence more often, perhaps relying on temporal order of steps as a heuristic. While prompting for explanations and using few-shot examples improve performance, the best F1 result is only 0.73. Further, human evaluation of explanations along with answer correctness show that, on average, humans do not agree with model reasoning. Surprisingly, we also find that explaining after answering leads to better performance than normal chain-of-thought prompting, and LLM answers are not consistent across questions about the same step pairs. Overall, results show that LLMs\u2019 ability to detect dependence between steps has significant room for improvement."
    },
    {
        "paperId": "833a2fb379e9d7faabcb55650fc6db39653277a1",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "UNO Arena for Evaluating Sequential Decision-Making Capability of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.16382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-24",
        "authors": [
            {
                "authorId": "2308095876",
                "name": "Zhanyue Qin"
            },
            {
                "authorId": "2277689256",
                "name": "Haochuan Wang"
            },
            {
                "authorId": "2293768616",
                "name": "Deyuan Liu"
            },
            {
                "authorId": "2308101887",
                "name": "Ziyang Song"
            },
            {
                "authorId": "117639306",
                "name": "Cunhang Fan"
            },
            {
                "authorId": "2308631023",
                "name": "Zhao Lv"
            },
            {
                "authorId": "49387921",
                "name": "Jinlin Wu"
            },
            {
                "authorId": "2308079421",
                "name": "Zhen Lei"
            },
            {
                "authorId": "2282568037",
                "name": "Zhiying Tu"
            },
            {
                "authorId": "2249633557",
                "name": "Dianhui Chu"
            },
            {
                "authorId": "2308391764",
                "name": "Xiaoyan Yu"
            },
            {
                "authorId": "2273504274",
                "name": "Dianbo Sui"
            }
        ],
        "abstract": "Sequential decision-making refers to algorithms that take into account the dynamics of the environment, where early decisions affect subsequent decisions. With large language models (LLMs) demonstrating powerful capabilities between tasks, we can\u2019t help but ask: Can Current LLMs Effectively Make Sequential Decisions? In order to answer this question, we propose the UNO Arena based on the card game UNO to evaluate the sequential decision-making capability of LLMs and explain in detail why we choose UNO. In UNO Arena, We evaluate the sequential decision-making capability of LLMs dynamically with novel metrics based Monte Carlo methods. We set up random players, DQN-based reinforcement learning players, and LLM players (e.g. GPT-4, Gemini-pro) for comparison testing. Furthermore, in order to improve the sequential decision-making capability of LLMs, we propose the TUTRI player, which can involves having LLMs reflect their own actions with the summary of game history and the game strategy. Numerous experiments demonstrate that the TUTRI player achieves a notable breakthrough in the performance of sequential decision-making compared to the vanilla LLM player."
    },
    {
        "paperId": "04e5c40f897098d1781e2d6ee721f5fafcbf0417",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.17232, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-25",
        "authors": [
            {
                "authorId": "1996191947",
                "name": "Yun-Shiuan Chuang"
            },
            {
                "authorId": "2308100885",
                "name": "Zach Studdiford"
            },
            {
                "authorId": "2179397781",
                "name": "Krirk Nirunwiroj"
            },
            {
                "authorId": "2266839000",
                "name": "Agam Goyal"
            },
            {
                "authorId": "2308100004",
                "name": "Vincent V. Frigo"
            },
            {
                "authorId": "2267007696",
                "name": "Sijia Yang"
            },
            {
                "authorId": "2266839576",
                "name": "Dhavan Shah"
            },
            {
                "authorId": "2267032106",
                "name": "Junjie Hu"
            },
            {
                "authorId": "2266838658",
                "name": "Timothy T. Rogers"
            }
        ],
        "abstract": "Creating human-like large language model (LLM) agents is crucial for faithful social simulation. Having LLMs role-play based on demographic information sometimes improves human likeness but often does not. This study assessed whether LLM alignment with human behavior can be improved by integrating information from empirically-derived human belief networks. Using data from a human survey, we estimated a belief network encompassing 64 topics loading on nine non-overlapping latent factors. We then seeded LLM-based agents with an opinion on one topic, and assessed the alignment of its expressed opinions on remaining test topics with corresponding human data. Role-playing based on demographic information alone did not align LLM and human opinions, but seeding the agent with a single belief greatly improved alignment for topics related in the belief network, and not for topics outside the network. These results suggest a novel path for human-LLM belief alignment in work seeking to simulate and understand patterns of belief distributions in society."
    },
    {
        "paperId": "f0932282d573ed6a5745e5498fc1437fa66d47eb",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.17453, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-25",
        "authors": [
            {
                "authorId": "2243229271",
                "name": "Davide Mazzaccara"
            },
            {
                "authorId": "50829868",
                "name": "A. Testoni"
            },
            {
                "authorId": "2276333243",
                "name": "Raffaella Bernardi"
            }
        ],
        "abstract": "Questions are essential tools for acquiring the necessary information to complete information-seeking tasks. However, large language models (LLMs), especially open-source models, often perform poorly in generating informative questions, as measured by expected information gain (EIG). In this paper, we propose a method to enhance the informativeness of LLM-generated questions in 20-question game dialogues. We sample multiple questions from the same model (LLAMA 2-CHAT 7B) for each game and create pairs of low-EIG and high-EIG questions to apply a Direct Preference Optimization (DPO) algorithm. Our results show that this method produces more effective questions (in terms of EIG), even in domains different from those used to train the DPO model."
    },
    {
        "paperId": "38fd1616d4e1e2e94ed3b5713ecfd60a8442a3bf",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models with Personality-Indicative Data",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.18921, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-27",
        "authors": [
            {
                "authorId": "2308466794",
                "name": "Yiting Ran"
            },
            {
                "authorId": "2108003209",
                "name": "Xintao Wang"
            },
            {
                "authorId": "2284900134",
                "name": "Rui Xu"
            },
            {
                "authorId": "2295749596",
                "name": "Xinfeng Yuan"
            },
            {
                "authorId": "3366523",
                "name": "Jiaqing Liang"
            },
            {
                "authorId": "2267005966",
                "name": "Yanghua Xiao"
            },
            {
                "authorId": "2300429503",
                "name": "Deqing Yang"
            }
        ],
        "abstract": "Role-playing agents (RPA) have been a popular application area for large language models (LLMs), attracting significant interest from both industry and academia.While existing RPAs well portray the characters' knowledge and tones, they face challenges in capturing their minds, especially for small role-playing language models (RPLMs). In this paper, we propose to enhance RPLMs via personality-indicative data. Specifically, we leverage questions from psychological scales and distill advanced RPAs to generate dialogues that grasp the minds of characters. Experimental results validate that RPLMs trained with our dataset exhibit advanced role-playing capabilities for both general and personality-related evaluations. Code and data are available at \\href{https://github.com/alienet1109/RolePersonality}{this URL}."
    },
    {
        "paperId": "dde7da078f5acb6fadcfb7830a0608a4b5f52e20",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.19223, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-27",
        "authors": [
            {
                "authorId": "2905059",
                "name": "Bjorn Deiseroth"
            },
            {
                "authorId": "2166299958",
                "name": "Manuel Brack"
            },
            {
                "authorId": "40896023",
                "name": "P. Schramowski"
            },
            {
                "authorId": "2066493115",
                "name": "K. Kersting"
            },
            {
                "authorId": "2024731554",
                "name": "Samuel Weinbach"
            }
        ],
        "abstract": "Tokenizers are crucial for encoding information in Large Language Models, but their development has recently stagnated, and they contain inherent weaknesses. Major limitations include computational overhead, ineffective vocabulary use, and unnecessarily large embedding and head layers. Additionally, their performance is biased towards a reference corpus, leading to reduced effectiveness for underrepresented languages.To remedy these issues, we propose T-Free, which directly embeds words through sparse activation patterns over character triplets and does not require a reference corpus. T-Free inherently exploits morphological similarities and allows for strong compression of embedding layers. In our exhaustive experimental evaluation, we achieve competitive downstream performance with a parameter reduction of more than 85% on these layers. Further, T-Free shows significant improvements in cross-lingual transfer learning."
    },
    {
        "paperId": "370205dcfdab72c5551ea59b516097d5c3ee414a",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Jump Starting Bandits with LLM-Generated Prior Knowledge",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.19317, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-27",
        "authors": [
            {
                "authorId": "1716200148",
                "name": "P. A. Alamdari"
            },
            {
                "authorId": "2308726522",
                "name": "Yanshuai Cao"
            },
            {
                "authorId": "2308469928",
                "name": "Kevin H. Wilson"
            }
        ],
        "abstract": "We present substantial evidence demonstrating the benefits of integrating Large Language Models (LLMs) with a Contextual Multi-Armed Bandit framework. Contextual bandits have been widely used in recommendation systems to generate personalized suggestions based on user-specific contexts. We show that LLMs, pre-trained on extensive corpora rich in human knowledge and preferences, can simulate human behaviours well enough to jump-start contextual multi-armed bandits to reduce online learning regret. We propose an initialization algorithm for contextual bandits by prompting LLMs to produce a pre-training dataset of approximate human preferences for the bandit. This significantly reduces online learning regret and data-gathering costs for training such models. Our approach is validated empirically through two sets of experiments with different bandit setups: one which utilizes LLMs to serve as an oracle and a real-world experiment utilizing data from a conjoint survey experiment."
    },
    {
        "paperId": "516d20473dc74e089f1be5d7b2aa61c120263788",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.19356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-27",
        "authors": [
            {
                "authorId": "39022996",
                "name": "Nigel Fernandez"
            },
            {
                "authorId": "1976128281",
                "name": "Alexander Scarlatos"
            },
            {
                "authorId": "2289844523",
                "name": "Simon Woodhead"
            },
            {
                "authorId": "2289844808",
                "name": "Andrew S. Lan"
            }
        ],
        "abstract": "High-quality distractors are crucial to both the assessment and pedagogical value of multiple-choice questions (MCQs), where manually crafting ones that anticipate knowledge deficiencies or misconceptions among real students is difficult. Meanwhile, automated distractor generation, even with the help of large language models (LLMs), remains challenging for subjects like math. It is crucial to not only identify plausible distractors but also understand the error behind them. In this paper, we introduce DiVERT (Distractor Generation with Variational Errors Represented as Text), a novel variational approach that learns an interpretable representation of errors behind distractors in math MCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions used by hundreds of thousands of students, we show that DiVERT, despite using a base open-source LLM with 7B parameters, outperforms state-of-the-art approaches using GPT-4o on downstream distractor generation. We also conduct a human evaluation with math educators and find that DiVERT leads to error labels that are of comparable quality to human-authored ones."
    },
    {
        "paperId": "a814b096f45746626cf53ebf146d232e24924edc",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Hierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework for Analyzing Knowledge Utilization",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2406.19502",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.19502, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-27",
        "authors": [
            {
                "authorId": "2266468965",
                "name": "Miyoung Ko"
            },
            {
                "authorId": "2266436197",
                "name": "Sue Hyun Park"
            },
            {
                "authorId": "2309075956",
                "name": "Joonsuk Park"
            },
            {
                "authorId": "2266393906",
                "name": "Minjoon Seo"
            }
        ],
        "abstract": "Despite the advances in large language models (LLMs), how they use their knowledge for reasoning is not yet well understood.In this study, we propose a method that deconstructs complex real-world questions into a graph, representing each question as a node with predecessors of background knowledge needed to solve the question. We develop the DepthQA dataset, deconstructing questions into three depths: (i) recalling conceptual knowledge, (ii) applying procedural knowledge, and (iii) analyzing strategic knowledge. Based on a hierarchical graph, we quantify forward discrepancy, a discrepancy in LLM performance on simpler sub-problems versus complex questions. We also measure backward discrepancy where LLMs answer complex questions but struggle with simpler ones. Our analysis shows that smaller models exhibit more discrepancies than larger models. Distinct patterns of discrepancies are observed across model capacity and possibility of training data memorization. Additionally, guiding models from simpler to complex questions through multi-turn interactions improves performance across model sizes, highlighting the importance of structured intermediate steps in knowledge reasoning. This work enhances our understanding of LLM reasoning and suggests ways to improve their problem-solving abilities."
    },
    {
        "paperId": "032b6040bacdcbf7c1a416faf0c225f9dc97dd76",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Leveraging Machine-Generated Rationales to Facilitate Social Meaning Detection in Conversations",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.19545, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-27",
        "authors": [
            {
                "authorId": "36662010",
                "name": "Ritam Dutt"
            },
            {
                "authorId": "2309115264",
                "name": "Zhen Wu"
            },
            {
                "authorId": "2309007485",
                "name": "Kelly Shi"
            },
            {
                "authorId": "2147325409",
                "name": "Divyanshu Sheth"
            },
            {
                "authorId": "2309448475",
                "name": "Prakhar Gupta"
            },
            {
                "authorId": "2309004987",
                "name": "Carolyn Rose"
            }
        ],
        "abstract": "We present a generalizable classification approach that leverages Large Language Models (LLMs) to facilitate the detection of implicitly encoded social meaning in conversations. We design a multi-faceted prompt to extract a textual explanation of the reasoning that connects visible cues to underlying social meanings. These extracted explanations or rationales serve as augmentations to the conversational text to facilitate dialogue understanding and transfer. Our empirical results over 2,340 experimental settings demonstrate the significant positive impact of adding these rationales. Our findings hold true for in-domain classification, zero-shot, and few-shot domain transfer for two different social meaning detection tasks, each spanning two different corpora."
    },
    {
        "paperId": "84c7ca14b9dc95fc8a21cf6f4a2194ffa99579cc",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.19949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-28",
        "authors": [
            {
                "authorId": "92861741",
                "name": "Jiazheng Li"
            },
            {
                "authorId": "2283763073",
                "name": "Hainiu Xu"
            },
            {
                "authorId": "1390477672",
                "name": "ZHAOYUE SUN"
            },
            {
                "authorId": "2314888686",
                "name": "Yuxiang Zhou"
            },
            {
                "authorId": "2218327140",
                "name": "David West"
            },
            {
                "authorId": "117934075",
                "name": "Cesare Aloisi"
            },
            {
                "authorId": "2265996676",
                "name": "Yulan He"
            }
        ],
        "abstract": "Generating rationales that justify scoring decisions has been a promising way to facilitate explainability in automated scoring systems. However, existing methods do not match the accuracy of classifier-based methods. Plus, the generated rationales often contain hallucinated information. To address these issues, we propose a novel framework capable of generating more faithful rationales and, more importantly, matching performance with classifier-based black-box scoring systems. We first mimic the human assessment process by querying Large Language Models (LLMs) to generate a thought tree. We then summarise intermediate assessment decisions from each thought tree path for creating synthetic rationale data and rationale preference data. Finally, we utilise the generated synthetic data to calibrate LLMs through a two-step training process: supervised fine-tuning and preference optimization. Extensive experimental results demonstrate that our framework achieves a 38% assessment performance improvement in the QWK score compared to prior work while producing higher-quality rationales, as recognised by human evaluators and LLMs. Our work sheds light on the effectiveness of performing preference optimization using synthetic preference data obtained from thought tree paths. Data and code are available at https://github.com/lijiazheng99/thought_tree_assessment."
    },
    {
        "paperId": "1f874c94d531f6776f65e0aa807adbf432ea35cf",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Understanding and Mitigating Language Confusion in LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.20052, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-28",
        "authors": [
            {
                "authorId": "1396188646",
                "name": "Kelly Marchisio"
            },
            {
                "authorId": "2309005865",
                "name": "Wei-Yin Ko"
            },
            {
                "authorId": "2309172370",
                "name": "Alexandre B'erard"
            },
            {
                "authorId": "2309005484",
                "name": "Th'eo Dehaze"
            },
            {
                "authorId": "2884561",
                "name": "Sebastian Ruder"
            }
        ],
        "abstract": "We investigate a surprising limitation of LLMs: their inability to consistently generate text in a user\u2019s desired language. We create the Language Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically diverse languages with existing and newly-created English and multilingual prompts. We evaluate a range of LLMs on monolingual and cross-lingual generation reflecting practical use cases, finding that Llama Instruct and Mistral models exhibit high degrees of language confusion and even the strongest models fail to consistently respond in the correct language. We observe that base and English-centric instruct models are more prone to language confusion, which is aggravated by complex prompts and high sampling temperatures. We find that language confusion can be partially mitigated via few-shot prompting, multilingual SFT and preference tuning. We release our language confusion benchmark, which serves as a first layer of efficient, scalable multilingual evaluation."
    },
    {
        "paperId": "5f6dc4904849f675ce3154fb87c0955503b27683",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "ProgressGym: Alignment with a Millennium of Moral Progress",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.20087, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-28",
        "authors": [
            {
                "authorId": "2263267720",
                "name": "Tianyi Qiu"
            },
            {
                "authorId": "2309065069",
                "name": "Yang Zhang"
            },
            {
                "authorId": "2309173012",
                "name": "Xuchuan Huang"
            },
            {
                "authorId": "2309076210",
                "name": "Jasmine Xinze Li"
            },
            {
                "authorId": "2273548793",
                "name": "Jiaming Ji"
            },
            {
                "authorId": "2260432856",
                "name": "Yaodong Yang"
            }
        ],
        "abstract": "Frontier AI systems, including large language models (LLMs), hold increasing influence over the epistemology of human users. Such influence can reinforce prevailing societal values, potentially contributing to the lock-in of misguided moral beliefs and, consequently, the perpetuation of problematic moral practices on a broad scale. We introduce progress alignment as a technical solution to mitigate this imminent risk. Progress alignment algorithms learn to emulate the mechanics of human moral progress, thereby addressing the susceptibility of existing alignment methods to contemporary moral blindspots. To empower research in progress alignment, we introduce ProgressGym, an experimental framework allowing the learning of moral progress mechanics from history, in order to facilitate future progress in real-world moral decisions. Leveraging 9 centuries of historical text and 18 historical LLMs, ProgressGym enables codification of real-world progress alignment challenges into concrete benchmarks. Specifically, we introduce three core challenges: tracking evolving values (PG-Follow), preemptively anticipating moral progress (PG-Predict), and regulating the feedback loop between human and AI value shifts (PG-Coevolve). Alignment methods without a temporal dimension are inapplicable to these tasks. In response, we present lifelong and extrapolative algorithms as baseline methods of progress alignment, and build an open leaderboard soliciting novel algorithms and challenges. The framework and the leaderboard are available at https://github.com/PKU-Alignment/ProgressGym and https://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard respectively."
    },
    {
        "paperId": "80587a0674a93944cca396fb6abc573a3dc53c70",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.00256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-28",
        "authors": [
            {
                "authorId": "1655400088",
                "name": "Ruochen Wang"
            },
            {
                "authorId": "2277548482",
                "name": "Sohyun An"
            },
            {
                "authorId": "2424698",
                "name": "Minhao Cheng"
            },
            {
                "authorId": "2284940649",
                "name": "Tianyi Zhou"
            },
            {
                "authorId": "2310398889",
                "name": "Sung Ju Hwang"
            },
            {
                "authorId": "2284761748",
                "name": "Cho-Jui Hsieh"
            }
        ],
        "abstract": "Large Language Models (LLMs) exhibit strong generalization capabilities to novel tasks when prompted with language instructions and in-context demos. Since this ability sensitively depends on the quality of prompts, various methods have been explored to automate the instruction design. While these methods demonstrated promising results, they also restricted the searched prompt to one instruction. Such simplification significantly limits their capacity, as a single demo-free instruction might not be able to cover the entire complex problem space of the targeted task. To alleviate this issue, we adopt the Mixture-of-Expert paradigm and divide the problem space into a set of sub-regions; Each sub-region is governed by a specialized expert, equipped with both an instruction and a set of demos. A two-phase process is developed to construct the specialized expert for each region: (1) demo assignment: Inspired by the theoretical connection between in-context learning and kernel regression, we group demos into experts based on their semantic similarity; (2) instruction assignment: A region-based joint search of an instruction per expert complements the demos assigned to it, yielding a synergistic effect. The resulting method, codenamed Mixture-of-Prompts (MoP), achieves an average win rate of 81% against prior arts across several major benchmarks."
    },
    {
        "paperId": "70b3ae0480e795608ce0b2a4b555d002c5d6f4d7",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.00497, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-29",
        "authors": [
            {
                "authorId": "2249532957",
                "name": "Jiahao Ying"
            },
            {
                "authorId": "49352079",
                "name": "Mingbao Lin"
            },
            {
                "authorId": "2275189609",
                "name": "Yixin Cao"
            },
            {
                "authorId": "2308631156",
                "name": "Wei Tang"
            },
            {
                "authorId": "2217713470",
                "name": "Bo Wang"
            },
            {
                "authorId": "2306912740",
                "name": "Qianru Sun"
            },
            {
                "authorId": "2284750473",
                "name": "Xuanjing Huang"
            },
            {
                "authorId": "2284684403",
                "name": "Shuicheng Yan"
            }
        ],
        "abstract": "This paper introduces the innovative\"LLMs-as-Instructors\"framework, which leverages the advanced Large Language Models (LLMs) to autonomously enhance the training of smaller target models. Inspired by the theory of\"Learning from Errors\", this framework employs an instructor LLM to meticulously analyze the specific errors within a target model, facilitating targeted and efficient training cycles. Within this framework, we implement two strategies:\"Learning from Error,\"which focuses solely on incorrect responses to tailor training data, and\"Learning from Error by Contrast\", which uses contrastive learning to analyze both correct and incorrect responses for a deeper understanding of errors. Our empirical studies, conducted with several open-source models, demonstrate significant improvements across multiple benchmarks, including mathematical reasoning, coding abilities, and factual knowledge. Notably, the refined Llama-3-8b-Instruction has outperformed ChatGPT, illustrating the effectiveness of our approach. By leveraging the strengths of both strategies, we have attained a more balanced performance improvement on both in-domain and out-of-domain benchmarks. Our code can be found at https://yingjiahao14.github.io/LLMs-as-Instructors-pages/."
    },
    {
        "paperId": "2e4774b6feff0ed8af83e31bf99fc1cd3c24c5cf",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.00870, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-01",
        "authors": [
            {
                "authorId": "2293313556",
                "name": "Ryan Louie"
            },
            {
                "authorId": "2221286767",
                "name": "Ananjan Nandi"
            },
            {
                "authorId": "2309178836",
                "name": "William Fang"
            },
            {
                "authorId": "2309218970",
                "name": "Cheng Chang"
            },
            {
                "authorId": "2563117",
                "name": "E. Brunskill"
            },
            {
                "authorId": "2309214619",
                "name": "Diyi Yang"
            }
        ],
        "abstract": "Recent works leverage LLMs to roleplay realistic social scenarios, aiding novices in practicing their social skills. However, simulating sensitive interactions, such as in the domain of mental health, is challenging. Privacy concerns restrict data access, and collecting expert feedback, although vital, is laborious. To address this, we develop Roleplay-doh, a novel human-LLM collaboration pipeline that elicits qualitative feedback from a domain-expert, which is transformed into a set of principles, or natural language rules, that govern an LLM-prompted roleplay. We apply this pipeline to enable senior mental health supporters to create customized AI patients as simulated practice partners for novice counselors. After uncovering issues with basic GPT-4 simulations not adhering to expert-defined principles, we also introduce a novel principle-adherence prompting pipeline which shows a 30% improvement in response quality and principle following for the downstream task. Through a user study with 25 counseling experts, we demonstrate that the pipeline makes it easy and effective to create AI patients that more faithfully resemble real patients, as judged by both creators and third-party counselors. We provide access to the code and data on our project website: https://roleplay-doh.github.io/."
    },
    {
        "paperId": "0c3dd1f837349f46f1b77e4035f7984f8c56f9fe",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MalAlgoQA: Pedagogical Evaluation of Counterfactual Reasoning in Large Language Models and Implications for AI in Education",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.00938, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-01",
        "authors": [
            {
                "authorId": "2143066732",
                "name": "Naiming Liu"
            },
            {
                "authorId": "1720691070",
                "name": "Shashank Sonkar"
            },
            {
                "authorId": "2243334456",
                "name": "Myco Le"
            },
            {
                "authorId": "2324697572",
                "name": "Richard Baraniuk"
            }
        ],
        "abstract": "This paper introduces MalAlgoQA, a novel dataset designed to evaluate the counterfactual reasoning capabilities of Large Language Models (LLMs) through a pedagogical approach. The dataset comprises mathematics and reading comprehension questions, each accompanied by four answer choices and their corresponding rationales. At the heart of MalAlgoQA are ``malgorithms'' - rationales behind incorrect answer choices that represent flawed yet logically coherent reasoning paths. These malgorithms serve as counterfactual scenarios, allowing us to assess an LLM's ability to identify and analyze flawed reasoning patterns. We propose the Malgorithm Identification task, where LLMs are assessed based on their ability to identify corresponding malgorithm given an incorrect answer choice. To evaluate the model performance, we introduce two metrics: Algorithm Identification Accuracy (AIA) for correct answer rationale identification, and Malgorithm Identification Accuracy (MIA) for incorrect answer rationale identification. Our experiments reveal that state-of-the-art LLMs exhibit significant performance drops in MIA compared to AIA, highlighting the challenges in counterfactual reasoning. Surprisingly, we find that the chain-of-thought prompting technique not only fails to consistently enhance MIA but can sometimes lead to underperformance compared to simple prompting. These findings have important implications for developing LLMs with improved counterfactual reasoning, particularly relevant for AI-powered tutoring systems, where identifying and addressing student misconceptions is essential. MalAlgoQA dataset is available \\href{https://github.com/luffycodes/MalAlgoQA-Dataset}{here}."
    },
    {
        "paperId": "fa9b0b55aa937e12f521ed2364e8299307450c37",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01009, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-01",
        "authors": [
            {
                "authorId": "2311112233",
                "name": "Jiabao Pan"
            },
            {
                "authorId": "2308223415",
                "name": "Yan Zhang"
            },
            {
                "authorId": "2297642863",
                "name": "Chen Zhang"
            },
            {
                "authorId": "2258784763",
                "name": "Zuozhu Liu"
            },
            {
                "authorId": "2266008822",
                "name": "Hongwei Wang"
            },
            {
                "authorId": "2302787029",
                "name": "Haizhou Li"
            }
        ],
        "abstract": "Large language models (LLMs) have demonstrated emergent capabilities across diverse reasoning tasks via popular Chains-of-Thought (COT) prompting. However, such a simple and fast COT approach often encounters limitations in dealing with complicated problems, while a thorough method, which considers multiple reasoning pathways and verifies each step carefully, results in slower inference. This paper addresses the challenge of enabling LLMs to autonomously select between fast and slow inference methods, thereby optimizing both efficiency and effectiveness. We introduce a dynamic decision-making framework that categorizes tasks into two distinct pathways: \u2018Fast,\u2019 designated for tasks where the LLM quickly identifies a high-confidence solution, and \u2018Slow,\u2019 allocated for tasks that the LLM perceives as complex and for which it has low confidence in immediate solutions as well as requiring more reasoning paths to verify. Experiments on five popular reasoning benchmarks demonstrated the superiority of the DynaThink over baselines. For example, when we compared it to strong COT with self-consistency baseline on the complicated MATH dataset, DynaThink achieved more than 3% increase in accuracy with lower cost. The code will be made available upon publication."
    },
    {
        "paperId": "7c434f0243c1fc6f68150e4af0997636c3336084",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "M2QA: Multi-domain Multilingual Question Answering",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01091, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-01",
        "authors": [
            {
                "authorId": "2267338500",
                "name": "Leon Arne Engl\u00e4nder"
            },
            {
                "authorId": "2160470871",
                "name": "Hannah Sterz"
            },
            {
                "authorId": "1816752514",
                "name": "Clifton A. Poth"
            },
            {
                "authorId": "153733568",
                "name": "Jonas Pfeiffer"
            },
            {
                "authorId": "145775250",
                "name": "Ilia Kuznetsov"
            },
            {
                "authorId": "2260340390",
                "name": "Iryna Gurevych"
            }
        ],
        "abstract": "Generalization and robustness to input variation are core desiderata of machine learning research. Language varies along several axes, most importantly, language instance (e.g. French) and domain (e.g. news). While adapting NLP models to new languages within a single domain, or to new domains within a single language, is widely studied, research in joint adaptation is hampered by the lack of evaluation datasets. This prevents the transfer of NLP systems from well-resourced languages and domains to non-dominant language-domain combinations. To address this gap, we introduce M2QA, a multi-domain multilingual question answering benchmark. M2QA includes 13,500 SQuAD 2.0-style question-answer instances in German, Turkish, and Chinese for the domains of product reviews, news, and creative writing. We use M2QA to explore cross-lingual cross-domain performance of fine-tuned models and state-of-the-art LLMs and investigate modular approaches to domain and language adaptation. We witness 1) considerable performance variations across domain-language combinations within model classes and 2) considerable performance drops between source and target language-domain combinations across all model sizes. We demonstrate that M2QA is far from solved, and new methods to effectively transfer both linguistic and domain-specific information are necessary. We make M2QA publicly available at https://github.com/UKPLab/m2qa."
    },
    {
        "paperId": "187eefa5167705cf916751c135aa650c545a87a8",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2407.01119",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01119, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-01",
        "authors": [
            {
                "authorId": "2075206254",
                "name": "Guillermo Marco"
            },
            {
                "authorId": "2309178337",
                "name": "Julio Gonzalo"
            },
            {
                "authorId": "2309177556",
                "name": "Ram'on del Castillo"
            },
            {
                "authorId": "2237672515",
                "name": "M. Girona"
            }
        ],
        "abstract": "Are LLMs ready to compete in creative writing skills with a top (rather than average) novelist? To provide an initial answer for this question, we have carried out a contest between Patricio Pron (an awarded novelist, considered one of the best of his generation) and GPT-4 (one of the top performing LLMs), in the spirit of AI-human duels such as DeepBlue vs Kasparov and AlphaGo vs Lee Sidol. We asked Pron and GPT-4 to provide thirty titles each, and then to write short stories for both their titles and their opponent\u2019s. Then, we prepared an evaluation rubric inspired by Boden\u2019s definition of creativity, and we collected several detailed expert assessments of the texts, provided by literature critics and scholars. The results of our experimentation indicate that LLMs are still far from challenging a top human creative writer. We also observed that GPT-4 writes more creatively using Pron\u2019s titles than its own titles (which is an indication of the potential for human-machine co-creation). Additionally, we found that GPT-4 has a more creative writing style in English than in Spanish."
    },
    {
        "paperId": "4c51430adfe11ff9f56399b710178d8b12411430",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Collaborative Performance Prediction for Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01300, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-01",
        "authors": [
            {
                "authorId": "2124959429",
                "name": "Qiyuan Zhang"
            },
            {
                "authorId": "1704274486",
                "name": "Fuyuan Lyu"
            },
            {
                "authorId": "2188246843",
                "name": "Xue Liu"
            },
            {
                "authorId": "2261661544",
                "name": "Chen Ma"
            }
        ],
        "abstract": "Comprehensively understanding and accurately predicting the performance of large language models across diverse downstream tasks has emerged as a pivotal challenge in NLP research. The pioneering scaling law on downstream works demonstrated intrinsic similarities within model families and utilized such similarities for performance prediction. However, they tend to overlook the similarities between model families and only consider design factors listed in the original scaling law. To overcome these limitations, we introduce a novel framework, Collaborative Performance Prediction (CPP), which significantly enhances prediction accuracy by leveraging the historical performance of various models on downstream tasks and other design factors for both model and task. We also collect a collaborative data sourced from online platforms containing both historical performance and additional design factors. With the support of the collaborative data, CPP not only surpasses traditional scaling laws in predicting the performance of scaled LLMs but also facilitates a detailed analysis of factor importance, an area previously overlooked."
    },
    {
        "paperId": "8c9c05f40819c34c713efea897c141d718dc12e7",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "TimeToM: Temporal Space is the Key to Unlocking the Door of Large Language Models' Theory-of-Mind",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01455, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-01",
        "authors": [
            {
                "authorId": "2273676684",
                "name": "Guiyang Hou"
            },
            {
                "authorId": "2135282890",
                "name": "Wenqi Zhang"
            },
            {
                "authorId": "1471660296",
                "name": "Yongliang Shen"
            },
            {
                "authorId": "2273585338",
                "name": "Linjuan Wu"
            },
            {
                "authorId": "1776903",
                "name": "Weiming Lu"
            }
        ],
        "abstract": "Theory of Mind (ToM)-the cognitive ability to reason about mental states of ourselves and others, is the foundation of social interaction. Although ToM comes naturally to humans, it poses a significant challenge to even the most advanced Large Language Models (LLMs). Due to the complex logical chains in ToM reasoning, especially in higher-order ToM questions, simply utilizing reasoning methods like Chain of Thought (CoT) will not improve the ToM capabilities of LLMs. We present TimeToM, which constructs a temporal space and uses it as the foundation to improve the ToM capabilities of LLMs in multiple scenarios. Specifically, within the temporal space, we construct Temporal Belief State Chain (TBSC) for each character and inspired by the cognition perspective of the social world model, we divide TBSC into self-world beliefs and social world beliefs, aligning with first-order ToM (first-order beliefs) and higher-order ToM (higher-order beliefs) questions, respectively. Moreover, we design a novel tool-belief solver that, by considering belief communication between characters in temporal space, can transform a character's higher-order beliefs into another character's first-order beliefs under belief communication period. Experimental results indicate that TimeToM can dramatically improve the reasoning performance of LLMs on ToM questions while taking a big step towards coherent and robust ToM reasoning."
    },
    {
        "paperId": "d9fb3f98c870891bae33aa5161fd222fb21cf1a5",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-01",
        "authors": [
            {
                "authorId": "116606529",
                "name": "Akshara Prabhakar"
            },
            {
                "authorId": "2267240667",
                "name": "Thomas L. Griffiths"
            },
            {
                "authorId": "145534175",
                "name": "R. Thomas McCoy"
            }
        ],
        "abstract": "Chain-of-Thought (CoT) prompting has been shown to enhance the multi-step reasoning capabilities of Large Language Models (LLMs). However, debates persist about whether LLMs exhibit abstract generalization or rely on shallow heuristics when given CoT prompts. To understand the factors influencing CoT reasoning we provide a detailed case study of the symbolic reasoning task of decoding shift ciphers, where letters are shifted forward some number of steps in the alphabet. We analyze the pattern of results produced by three LLMs -- GPT-4, Claude 3, and Llama 3.1 -- performing this task using CoT prompting. By focusing on a single relatively simple task, we are able to identify three factors that systematically affect CoT performance: the probability of the task's expected output (probability), what the model has implicitly learned during pre-training (memorization), and the number of intermediate operations involved in reasoning (noisy reasoning). We show that these factors can drastically influence task accuracy across all three LLMs; e.g., when tested with GPT-4, varying the output's probability of occurrence shifts accuracy from 26% to 70%. Overall, we conclude that CoT prompting performance reflects both memorization and a probabilistic version of genuine reasoning. Code and data at this https://github.com/aksh555/deciphering_cot"
    },
    {
        "paperId": "9669862f69b2e4cf05ecf02b7e6f38c2899acf28",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01920, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-02",
        "authors": [
            {
                "authorId": "2064522174",
                "name": "Bo Tian"
            },
            {
                "authorId": "2153398295",
                "name": "Xiaozhuan Liang"
            },
            {
                "authorId": "2258034882",
                "name": "Siyuan Cheng"
            },
            {
                "authorId": "2258682951",
                "name": "Qingbin Liu"
            },
            {
                "authorId": "2218346459",
                "name": "Meng Wang"
            },
            {
                "authorId": "2273504274",
                "name": "Dianbo Sui"
            },
            {
                "authorId": "48283576",
                "name": "Xi Chen"
            },
            {
                "authorId": "2144200945",
                "name": "Huajun Chen"
            },
            {
                "authorId": "2153010067",
                "name": "Ningyu Zhang"
            }
        ],
        "abstract": "Large Language Models (LLMs) trained on extensive corpora inevitably retain sensitive data, such as personal privacy information and copyrighted material. Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters. Experimental results show that MemFlex is superior to existing methods in both precise knowledge unlearning and general knowledge retaining of LLMs. Code and dataset are released at https://github.com/zjunlp/KnowUnDo."
    },
    {
        "paperId": "a3a723d149c6f5d7a17616029e6491570abaa9f1",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.02352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-02",
        "authors": [
            {
                "authorId": "4643090",
                "name": "Pritish Sahu"
            },
            {
                "authorId": "39707211",
                "name": "Karan Sikka"
            },
            {
                "authorId": "2258962662",
                "name": "Ajay Divakaran"
            }
        ],
        "abstract": "Large Visual Language Models (LVLMs) struggle with hallucinations in visual instruction following task(s). These issues hinder their trustworthiness and real-world applicability. We propose Pelican \u2013 a novel framework designed to detect and mitigate hallucinations through claim verification. Pelican first decomposes the visual claim into a chain of sub-claims based on first-order predicates. These sub-claims consists of (predicate, question) pairs and can be conceptualized as nodes of a computational graph. We then use use Program-of-Thought prompting to generate Python code for answering these questions through flexible composition of external tools. Pelican improves over prior work by introducing (1) intermediate variables for precise grounding of object instances, and (2) shared computation for answering the sub-question to enable adaptive corrections and inconsistency identification. We finally use reasoning abilities of LLM to verify the correctness of the the claim by considering the consistency and confidence of the (question, answer) pairs from each sub-claim. Our experiments demonstrate consistent performance improvements over various baseline LVLMs and existing hallucination mitigation approaches across several benchmarks."
    },
    {
        "paperId": "dab57aa71794781ae305218fce9fd004ecdfc4f0",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.02518, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-23",
        "authors": [
            {
                "authorId": "2258548905",
                "name": "Hung Le"
            },
            {
                "authorId": "2118860628",
                "name": "Yingbo Zhou"
            },
            {
                "authorId": "2292439981",
                "name": "Caiming Xiong"
            },
            {
                "authorId": "2238207181",
                "name": "Silvio Savarese"
            },
            {
                "authorId": "36187119",
                "name": "Doyen Sahoo"
            }
        ],
        "abstract": "Large language models (LLMs) for code are typically trained to align with natural language instructions to closely follow their intentions and requirements. However, in many practical scenarios, it becomes increasingly challenging for these models to navigate the intricate boundary between helpfulness and safety, especially against highly complex yet potentially malicious instructions. In this work, we introduce INDICT: a new framework that empowers LLMs with Internal Dialogues of Critiques for both safety and helpfulness guidance. The internal dialogue is a dual cooperative system between a safety-driven critic and a helpfulness-driven critic. Each critic provides analysis against the given task and corresponding generated response, equipped with external knowledge queried through relevant code snippets and tools like web search and code interpreter. We engage the dual critic system in both code generation stage as well as code execution stage, providing preemptive and post-hoc guidance respectively to LLMs. We evaluated INDICT on 8 diverse tasks across 8 programming languages from 5 benchmarks, using LLMs from 7B to 70B parameters. We observed that our approach can provide an advanced level of critiques of both safety and helpfulness analysis, significantly improving the quality of output codes ($+10\\%$ absolute improvements in all models)."
    },
    {
        "paperId": "378702db60e4e9761dcdb0b73f0b9a1549bbbb58",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Cactus: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.03103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-03",
        "authors": [
            {
                "authorId": "2287822199",
                "name": "Suyeon Lee"
            },
            {
                "authorId": "2295169937",
                "name": "Sunghwan Kim"
            },
            {
                "authorId": "2261077753",
                "name": "Minju Kim"
            },
            {
                "authorId": "2266420525",
                "name": "Dongjin Kang"
            },
            {
                "authorId": "2309659264",
                "name": "Dongil Yang"
            },
            {
                "authorId": "2287875032",
                "name": "Harim Kim"
            },
            {
                "authorId": "2309787428",
                "name": "Minseok Kang"
            },
            {
                "authorId": "2309481707",
                "name": "Dayi Jung"
            },
            {
                "authorId": "2309504727",
                "name": "Min Hee Kim"
            },
            {
                "authorId": "2307987815",
                "name": "Seungbeen Lee"
            },
            {
                "authorId": "2287828497",
                "name": "Kyoung-Mee Chung"
            },
            {
                "authorId": "2258802492",
                "name": "Youngjae Yu"
            },
            {
                "authorId": "2258907627",
                "name": "Dongha Lee"
            },
            {
                "authorId": "2258712913",
                "name": "Jinyoung Yeo"
            }
        ],
        "abstract": "Recently, the demand for psychological counseling has significantly increased as more individuals express concerns about their mental health. This surge has accelerated efforts to improve the accessibility of counseling by using large language models (LLMs) as counselors. To ensure client privacy, training open-source LLMs faces a key challenge: the absence of realistic counseling datasets. To address this, we introduce Cactus, a multi-turn dialogue dataset that emulates real-life interactions using the goal-oriented and structured approach of Cognitive Behavioral Therapy (CBT). We create a diverse and realistic dataset by designing clients with varied, specific personas, and having counselors systematically apply CBT techniques in their interactions. To assess the quality of our data, we benchmark against established psychological criteria used to evaluate real counseling sessions, ensuring alignment with expert evaluations. Experimental results demonstrate that Camel, a model trained with Cactus, outperforms other models in counseling skills, highlighting its effectiveness and potential as a counseling agent. We make our data, model, and code publicly available."
    },
    {
        "paperId": "540937fd9669776b9a2234ea584435aa0f3bd163",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.03585, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-04",
        "authors": [
            {
                "authorId": "1394308358",
                "name": "Kazuaki Furumai"
            },
            {
                "authorId": "2292327728",
                "name": "Roberto Legaspi"
            },
            {
                "authorId": "2310240640",
                "name": "Julio Vizcarra"
            },
            {
                "authorId": "2310231686",
                "name": "Yudai Yamazaki"
            },
            {
                "authorId": "2184251149",
                "name": "Yasutaka Nishimura"
            },
            {
                "authorId": "1922611796",
                "name": "Sina J. Semnani"
            },
            {
                "authorId": "2256156246",
                "name": "Kazushi Ikeda"
            },
            {
                "authorId": "2310562080",
                "name": "Weiyan Shi"
            },
            {
                "authorId": "2266839752",
                "name": "Monica S. Lam"
            }
        ],
        "abstract": "Persuasion plays a pivotal role in a wide range of applications from health intervention to the promotion of social good. Persuasive chatbots employed responsibly for social good can be an enabler of positive individual and social change. Existing methods rely on fine-tuning persuasive chatbots with task-specific training data which is costly, if not infeasible, to collect. Furthermore, they employ only a handful of pre-defined persuasion strategies. We propose PersuaBot, a zero-shot chatbot based on Large Language Models (LLMs) that is factual and more persuasive by leveraging many more nuanced strategies. PersuaBot uses an LLM to first generate natural responses, from which the strategies used are extracted. To combat hallucination of LLMs, Persuabot replace any unsubstantiated claims in the response with retrieved facts supporting the extracted strategies. We applied our chatbot, PersuaBot, to three significantly different domains needing persuasion skills: donation solicitation, recommendations, and health intervention. Our experiments on simulated and human conversations show that our zero-shot approach is more persuasive than prior work, while achieving factual accuracy surpassing state-of-the-art knowledge-oriented chatbots."
    },
    {
        "paperId": "472251f534ddfae14d163874c7114a1894156efd",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.05216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-07",
        "authors": [
            {
                "authorId": "2165055951",
                "name": "Cheng-Han Chiang"
            },
            {
                "authorId": "2310393313",
                "name": "Wei-Chih Chen"
            },
            {
                "authorId": "2243185910",
                "name": "Chun-Yi Kuan"
            },
            {
                "authorId": "2310396374",
                "name": "Chienchou Yang"
            },
            {
                "authorId": "2279957914",
                "name": "Hung-yi Lee"
            }
        ],
        "abstract": "Using large language models (LLMs) for automatic evaluation has become an important evaluation method in NLP research. However, it is unclear whether these LLM-based evaluators can be effectively applied in real-world classrooms to assess student assignments. This empirical report shares how we use GPT-4 as an automatic assignment evaluator in a university course with over 1000 students. Based on student responses, we found that LLM-based assignment evaluators are generally acceptable to students when they have free access to these tools. However, students also noted that the LLM sometimes fails to adhere to the evaluation instructions, resulting in unreasonable assessments. Additionally, we observed that students can easily manipulate the LLM to output specific strings, allowing them to achieve high scores without meeting the assignment rubric. Based on student feedback and our experience, we offer several recommendations for effectively integrating LLMs into future classroom evaluations. Our observation also highlights potential directions for improving LLM-based evaluators, including their instruction-following ability and vulnerability to prompt hacking."
    },
    {
        "paperId": "68934dde90630c2d2f232f5a87ca192df0080dc8",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.05399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-07",
        "authors": [
            {
                "authorId": "2066733182",
                "name": "Abhinav Joshi"
            },
            {
                "authorId": "2114177988",
                "name": "Shounak Paul"
            },
            {
                "authorId": "2310403771",
                "name": "Akshat Sharma"
            },
            {
                "authorId": "2310340737",
                "name": "Pawan Goyal"
            },
            {
                "authorId": "2310394493",
                "name": "Saptarshi Ghosh"
            },
            {
                "authorId": "2477939",
                "name": "Ashutosh Modi"
            }
        ],
        "abstract": "Legal systems worldwide are inundated with exponential growth in cases and documents. There is an imminent need to develop NLP and ML techniques for automatically processing and understanding legal documents to streamline the legal system. However, evaluating and comparing various NLP models designed specifically for the legal domain is challenging. This paper addresses this challenge by proposing IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning. IL-TUR contains monolingual (English, Hindi) and multi-lingual (9 Indian languages) domain-specific tasks that address different aspects of the legal system from the point of view of understanding and reasoning over Indian legal documents. We present baseline models (including LLM-based) for each task, outlining the gap between models and the ground truth. To foster further research in the legal domain, we create a leaderboard (available at: https://exploration-lab.github.io/IL-TUR/) where the research community can upload and compare legal text understanding systems."
    },
    {
        "paperId": "eceb07cb158b96e053fb1ef39e3c6f0aa1e73d2e",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.05474, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-07",
        "authors": [
            {
                "authorId": "2310548546",
                "name": "Dongxu Zhang"
            },
            {
                "authorId": "2126048085",
                "name": "Varun Gangal"
            },
            {
                "authorId": "2186115342",
                "name": "B. Lattimer"
            },
            {
                "authorId": "2261254006",
                "name": "Yi Yang"
            }
        ],
        "abstract": "Detecting hallucinations in large language model (LLM) outputs is pivotal, yet traditional fine-tuning for this classification task is impeded by the expensive and quickly outdated annotation process, especially across numerous vertical domains and in the face of rapid LLM advancements. In this study, we introduce an approach that automatically generates both faithful and hallucinated outputs by rewriting system responses. Experimental findings demonstrate that a T5-base model, fine-tuned on our generated dataset, surpasses state-of-the-art zero-shot detectors and existing synthetic generation methods in both accuracy and latency, indicating efficacy of our approach."
    },
    {
        "paperId": "cd4f264a72bcdd6ab519fe7d0a407679d85278c7",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Retrieved In-Context Principles from Previous Mistakes",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.05682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-08",
        "authors": [
            {
                "authorId": "2292123797",
                "name": "Hao Sun"
            },
            {
                "authorId": "2256747040",
                "name": "Yong Jiang"
            },
            {
                "authorId": "2217713470",
                "name": "Bo Wang"
            },
            {
                "authorId": "2246638730",
                "name": "Yingyan Hou"
            },
            {
                "authorId": "2274264502",
                "name": "Yan Zhang"
            },
            {
                "authorId": "35930962",
                "name": "Pengjun Xie"
            },
            {
                "authorId": "2276428076",
                "name": "Fei Huang"
            }
        ],
        "abstract": "In-context learning (ICL) has been instrumental in adapting large language models (LLMs) to downstream tasks using correct input-output examples. Recent advances have attempted to improve model performance through principles derived from mistakes, yet these approaches suffer from lack of customization and inadequate error coverage. To address these limitations, we propose Retrieved In-Context Principles (RICP), a novel teacher-student framework. In RICP, the teacher model analyzes mistakes from the student model to generate reasons and insights for preventing similar mistakes. These mistakes are clustered based on their underlying reasons for developing task-level principles, enhancing the error coverage of principles. During inference, the most relevant mistakes for each question are retrieved to create question-level principles, improving the customization of the provided guidance. RICP is orthogonal to existing prompting methods and does not require intervention from the teacher model during inference. Experimental results across seven reasoning benchmarks reveal that RICP effectively enhances performance when applied to various prompting strategies."
    },
    {
        "paperId": "3b3082520e59974f9f43c852ee37a7003911e953",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.05965, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-08",
        "authors": [
            {
                "authorId": "2188993538",
                "name": "Yibo Miao"
            },
            {
                "authorId": "2274197467",
                "name": "Yifan Zhu"
            },
            {
                "authorId": "3431029",
                "name": "Yinpeng Dong"
            },
            {
                "authorId": "2217216194",
                "name": "Lijia Yu"
            },
            {
                "authorId": "2270894724",
                "name": "Jun Zhu"
            },
            {
                "authorId": "2304024668",
                "name": "Xiao-Shan Gao"
            }
        ],
        "abstract": "The recent development of Sora leads to a new era in text-to-video (T2V) generation. Along with this comes the rising concern about its security risks. The generated videos may contain illegal or unethical content, and there is a lack of comprehensive quantitative understanding of their safety, posing a challenge to their reliability and practical deployment. Previous evaluations primarily focus on the quality of video generation. While some evaluations of text-to-image models have considered safety, they cover fewer aspects and do not address the unique temporal risk inherent in video generation. To bridge this research gap, we introduce T2VSafetyBench, a new benchmark designed for conducting safety-critical assessments of text-to-video models. We define 12 critical aspects of video generation safety and construct a malicious prompt dataset including real-world prompts, LLM-generated prompts and jailbreak attack-based prompts. Based on our evaluation results, we draw several important findings, including: 1) no single model excels in all aspects, with different models showing various strengths; 2) the correlation between GPT-4 assessments and manual reviews is generally high; 3) there is a trade-off between the usability and safety of text-to-video generative models. This indicates that as the field of video generation rapidly advances, safety risks are set to surge, highlighting the urgency of prioritizing video safety. We hope that T2VSafetyBench can provide insights for better understanding the safety of video generation in the era of generative AI."
    },
    {
        "paperId": "31aa31257a50530f817a9d35b971758da55d72d0",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Perceptions to Beliefs: Exploring Precursory Inferences for Theory of Mind in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.06004, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-08",
        "authors": [
            {
                "authorId": "2150179781",
                "name": "Chani Jung"
            },
            {
                "authorId": "2154916175",
                "name": "Dongkwan Kim"
            },
            {
                "authorId": "2165259166",
                "name": "Jiho Jin"
            },
            {
                "authorId": "2294595219",
                "name": "Jiseon Kim"
            },
            {
                "authorId": "80450954",
                "name": "Yeon Seonwoo"
            },
            {
                "authorId": "2310393103",
                "name": "Yejin Choi"
            },
            {
                "authorId": "2286063145",
                "name": "Alice Oh"
            },
            {
                "authorId": "32609381",
                "name": "Hyunwoo Kim"
            }
        ],
        "abstract": "While humans naturally develop theory of mind (ToM), the capability to understand other people\u2019s mental states and beliefs, state-of-the-art large language models (LLMs) underperform on simple ToM benchmarks. We posit that we can extend our understanding of LLMs\u2019 ToM abilities by evaluating key human ToM precursors-perception inference and perception-to-belief inference-in LLMs. We introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate these precursory inferences for ToM in LLMs by annotating characters\u2019 perceptions on ToMi and FANToM, respectively.Our evaluation of eight state-of-the-art LLMs reveals that the models generally perform well in perception inference while exhibiting limited capability in perception-to-belief inference (e.g., lack of inhibitory control).Based on these results, we present PercepToM, a novel ToM method leveraging LLMs\u2019 strong perception inference capability while supplementing their limited perception-to-belief inference. Experimental results demonstrate that PercepToM significantly enhances LLM\u2019s performance, especially in false belief scenarios."
    },
    {
        "paperId": "0b41a6899c29a04e1217e6cc80a3d915ea18e2d8",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.06567, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-09",
        "authors": [
            {
                "authorId": "2238569503",
                "name": "Yangyang Yu"
            },
            {
                "authorId": "2284912441",
                "name": "Zhiyuan Yao"
            },
            {
                "authorId": "2238311785",
                "name": "Haohang Li"
            },
            {
                "authorId": "2284946604",
                "name": "Zhiyang Deng"
            },
            {
                "authorId": "2285587055",
                "name": "Yupeng Cao"
            },
            {
                "authorId": "2268374009",
                "name": "Zhi Chen"
            },
            {
                "authorId": "2176225",
                "name": "Jordan W. Suchow"
            },
            {
                "authorId": "2268379593",
                "name": "Rong Liu"
            },
            {
                "authorId": "2310901718",
                "name": "Zhenyu Cui"
            },
            {
                "authorId": "2268373308",
                "name": "Denghui Zhang"
            },
            {
                "authorId": "2297992872",
                "name": "K. Subbalakshmi"
            },
            {
                "authorId": "2284763655",
                "name": "Guojun Xiong"
            },
            {
                "authorId": "2284803928",
                "name": "Yueru He"
            },
            {
                "authorId": "2555230",
                "name": "Jimin Huang"
            },
            {
                "authorId": "2284864662",
                "name": "Dong Li"
            },
            {
                "authorId": "2249763955",
                "name": "Qianqian Xie"
            }
        ],
        "abstract": "Large language models (LLMs) have demonstrated notable potential in conducting complex tasks and are increasingly utilized in various financial applications. However, high-quality sequential financial investment decision-making remains challenging. These tasks require multiple interactions with a volatile environment for every decision, demanding sufficient intelligence to maximize returns and manage risks. Although LLMs have been used to develop agent systems that surpass human teams and yield impressive investment returns, opportunities to enhance multi-sourced information synthesis and optimize decision-making outcomes through timely experience refinement remain unexplored. Here, we introduce the FinCon, an LLM-based multi-agent framework with CONceptual verbal reinforcement tailored for diverse FINancial tasks. Inspired by effective real-world investment firm organizational structures, FinCon utilizes a manager-analyst communication hierarchy. This structure allows for synchronized cross-functional agent collaboration towards unified goals through natural language interactions and equips each agent with greater memory capacity than humans. Additionally, a risk-control component in FinCon enhances decision quality by episodically initiating a self-critiquing mechanism to update systematic investment beliefs. The conceptualized beliefs serve as verbal reinforcement for the future agent's behavior and can be selectively propagated to the appropriate node that requires knowledge updates. This feature significantly improves performance while reducing unnecessary peer-to-peer communication costs. Moreover, FinCon demonstrates strong generalization capabilities in various financial tasks, including single stock trading and portfolio management."
    },
    {
        "paperId": "685b50b738e4597cfc9eb99b30691b0b40a1034b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Virtual Personas for Language Models via an Anthology of Backstories",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.06576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-09",
        "authors": [
            {
                "authorId": "2296745700",
                "name": "Suhong Moon"
            },
            {
                "authorId": "2362620141",
                "name": "Marwa Abdulhai"
            },
            {
                "authorId": "2297002762",
                "name": "Minwoo Kang"
            },
            {
                "authorId": "2310434561",
                "name": "Joseph Suh"
            },
            {
                "authorId": "2129992983",
                "name": "Widyadewi Soedarmadji"
            },
            {
                "authorId": "2310433962",
                "name": "Eran Kohen Behar"
            },
            {
                "authorId": "2310435285",
                "name": "David M. Chan"
            }
        ],
        "abstract": "Large language models (LLMs) are trained from vast repositories of text authored by millions of distinct authors, reflecting an enormous diversity of human traits. While these models bear the potential to be used as approximations of human subjects in behavioral studies, prior efforts have been limited in steering model responses to match individual human users. In this work, we introduce Anthology, a method for conditioning LLMs to particular virtual personas by harnessing open-ended life narratives, which we refer to as backstories. We show that our methodology enhances the consistency and reliability of experimental outcomes while ensuring better representation of diverse sub-populations. Across three nationally representative human surveys conducted as part of Pew Research Center\u2019s American Trends Panel (ATP), we demonstrate that Anthology achieves up to 18% improvement in matching the response distributions of human respondents and 27% improvement in consistency metrics."
    },
    {
        "paperId": "b3ff72674ce1c849a2cd9110f6d217927dcf7402",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "ChatGPT Doesn\u2019t Trust Chargers Fans: Guardrail Sensitivity in Context",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.06866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-09",
        "authors": [
            {
                "authorId": "2310439068",
                "name": "Victoria Li"
            },
            {
                "authorId": "16099092",
                "name": "Yida Chen"
            },
            {
                "authorId": "2307011894",
                "name": "Naomi Saphra"
            }
        ],
        "abstract": "While the biases of language models in production are extensively documented, the biases of their guardrails have been neglected. This paper studies how contextual information about the user influences the likelihood of an LLM to refuse to execute a request. By generating user biographies that offer ideological and demographic information, we find a number of biases in guardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas are more likely to trigger a refusal guardrail when requesting censored or illegal information. Guardrails are also sycophantic, refusing to comply with requests for a political position the user is likely to disagree with. We find that certain identity groups and seemingly innocuous information, e.g., sports fandom, can elicit changes in guardrail sensitivity similar to direct statements of political ideology. For each demographic category and even for American football team fandom, we find that ChatGPT appears to infer a likely political ideology and modify guardrail behavior accordingly."
    },
    {
        "paperId": "30993590c5dd8e5d2f37be1d565b7538250a7b5b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.06908, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-09",
        "authors": [
            {
                "authorId": "3455118",
                "name": "Flor Miriam Plaza del Arco"
            },
            {
                "authorId": "3451318",
                "name": "A. C. Curry"
            },
            {
                "authorId": "2310435572",
                "name": "Susanna Paoli"
            },
            {
                "authorId": "2188052353",
                "name": "Alba Curry"
            },
            {
                "authorId": "2267334203",
                "name": "Dirk Hovy"
            }
        ],
        "abstract": "Emotions play important epistemological and cognitive roles in our lives, revealing our values and guiding our actions. Previous work has shown that LLMs display biases in emotion attribution along gender lines. However, unlike gender, which says little about our values, religion, as a socio-cultural system, prescribes a set of beliefs and values for its followers. Religions, therefore, cultivate certain emotions. Moreover, these rules are explicitly laid out and interpreted by religious leaders. Using emotion attribution, we explore how different religions are represented in LLMs. We find that: Major religions in the US and European countries are represented with more nuance, displaying a more shaded model of their beliefs. Eastern religions like Hinduism and Buddhism are strongly stereotyped. Judaism and Islam are stigmatized -- the models' refusal skyrocket. We ascribe these to cultural bias in LLMs and the scarcity of NLP literature on religion. In the rare instances where religion is discussed, it is often in the context of toxic language, perpetuating the perception of these religions as inherently toxic. This finding underscores the urgent need to address and rectify these biases. Our research underscores the crucial role emotions play in our lives and how our values influence them."
    },
    {
        "paperId": "243dbf643ee5f7233ecc2bb315d00afd8e935e42",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.06917, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-09",
        "authors": [
            {
                "authorId": "2329739308",
                "name": "Zara Siddique"
            },
            {
                "authorId": "2310441319",
                "name": "Liam D. Turner"
            },
            {
                "authorId": "2258950306",
                "name": "Luis Espinosa Anke"
            }
        ],
        "abstract": "Large language models (LLMs) have been shown to propagate and amplify harmful stereotypes, particularly those that disproportionately affect marginalised communities. To understand the effect of these stereotypes more comprehensively, we introduce GlobalBias, a dataset of 876k sentences incorporating 40 distinct gender-by-ethnicity groups alongside descriptors typically used in bias literature, which enables us to study a broad set of stereotypes from around the world. We use GlobalBias to directly probe a suite of LMs via perplexity, which we use as a proxy to determine how certain stereotypes are represented in the model\u2019s internal representations. Following this, we generate character profiles based on given names and evaluate the prevalence of stereotypes in model outputs. We find that the demographic groups associated with various stereotypes remain consistent across model likelihoods and model outputs. Furthermore, larger models consistently display higher levels of stereotypical outputs, even when explicitly instructed not to."
    },
    {
        "paperId": "2018a64911680af735172e3bab2719a80927279f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.08699, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-11",
        "authors": [
            {
                "authorId": "2275054902",
                "name": "Anton Alexandrov"
            },
            {
                "authorId": "1967472",
                "name": "Veselin Raychev"
            },
            {
                "authorId": "2116235329",
                "name": "Mark Niklas M\u00fcller"
            },
            {
                "authorId": "2310779228",
                "name": "Ce Zhang"
            },
            {
                "authorId": "1736447",
                "name": "Martin T. Vechev"
            },
            {
                "authorId": "2319416116",
                "name": "Kristina Toutanova"
            }
        ],
        "abstract": "As open-weight large language models (LLMs) achieve ever more impressive performances across a wide range of tasks in English, practitioners aim to adapt these models to different languages. However, such language adaptation is often accompanied by catastrophic forgetting of the base model's capabilities, severely limiting the usefulness of the resulting model. We address this issue by proposing Branch-and-Merge (BaM), a new adaptation method based on iteratively merging multiple models, fine-tuned on a subset of the available training data. BaM is based on the insight that this yields lower magnitude but higher quality weight changes, reducing forgetting of the source domain while maintaining learning on the target domain. We demonstrate in an extensive empirical study on Bulgarian and German that BaM can significantly reduce forgetting while matching or even improving target domain performance compared to both standard continued pretraining and instruction finetuning across different model architectures."
    },
    {
        "paperId": "f89da2b36bd28c7b86f0318cc6b4e4b2f90d3799",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Self-Evolving GPT: A Lifelong Autonomous Experiential Learner",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.08937, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-12",
        "authors": [
            {
                "authorId": "2115557647",
                "name": "Jin-Fang Gao"
            },
            {
                "authorId": "2117434160",
                "name": "Xiao Ding"
            },
            {
                "authorId": "3043830",
                "name": "Yiming Cui"
            },
            {
                "authorId": "2311564408",
                "name": "Jianbai Zhao"
            },
            {
                "authorId": "2311729192",
                "name": "Hepeng Wang"
            },
            {
                "authorId": "2140034231",
                "name": "Ting Liu"
            },
            {
                "authorId": "2277468940",
                "name": "Bing Qin"
            }
        ],
        "abstract": "To improve the performance of large language models (LLMs), researchers have explored providing LLMs with textual task-solving experience via prompts. However, they rely on manual efforts to acquire and apply such experience for each task, which is not feasible for the growing demand for LLMs and the variety of user questions. To address this issue, we design a lifelong autonomous experiential learning framework based on LLMs to explore whether LLMs can imitate human ability for learning and utilizing experience. It autonomously learns and accumulates experience through experience transfer and induction, categorizing the types of input questions to select which accumulated experience to employ for them. Experimental results on six widely used NLP datasets show that our framework performs reliably in each intermediate step and effectively improves the performance of GPT-3.5 and GPT-4. This validates the feasibility of using LLMs to mimic human experiential learning and application capabilities. Additionally, we provide a detailed analysis of the behavior of our framework at each step."
    },
    {
        "paperId": "696bc486d84cb33a152491676b6237e0e5e43061",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.09136, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-12",
        "authors": [
            {
                "authorId": "2048028927",
                "name": "Nico Daheim"
            },
            {
                "authorId": "23126830",
                "name": "Jakub Macina"
            },
            {
                "authorId": "2311114019",
                "name": "Manu Kapur"
            },
            {
                "authorId": "69033154",
                "name": "Iryna Gurevych"
            },
            {
                "authorId": "2790926",
                "name": "Mrinmaya Sachan"
            }
        ],
        "abstract": "Large language models (LLMs) offer many opportunities to scale high-quality personalized tutoring. A promising approach is to build dialog tutoring models to scaffold students\u2019 problem-solving. However, even though existing models perform well in solving reasoning questions, they can struggle to precisely detect student\u2019s errors and tailor their feedback to these errors. Inspired by real-world teaching practice where teachers identify student errors and customize their response based on them, we focus on verifying student solutions and show how grounding to such verification improves the overall quality of tutor response generation. We collect a dataset of 1,002 stepwise math reasoning chains with the first error step annotated by teachers. We show empirically that finding the mistake in a student solution is challenging for current models. We propose and evaluate several verifiers for detecting these errors. Using both automatic and human evaluation we show that the student solution verifiers steer the generation model towards highly targeted responses to student error which are more often correct with less hallucinations compared to existing baselines. The benchmark dataset and code will be released openly."
    },
    {
        "paperId": "23cd60c214d2f76c6f80a35d077cea337433b084",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "A Training Data Recipe to Accelerate A* Search with Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2407.09985?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2407.09985, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-13",
        "authors": [
            {
                "authorId": "2311398997",
                "name": "Devaansh Gupta"
            },
            {
                "authorId": "2311528282",
                "name": "Boyang Li"
            }
        ],
        "abstract": "Recent works in AI planning have proposed to combine LLMs with iterative tree-search algorithms like A* and MCTS, where LLMs are typically used to calculate the heuristic, guiding the planner towards the goal. However, combining these techniques is not trivial : LM-based heuristics are quite weak, incurring a high computational cost without a significant performance improvement. Existing methods to learn these heuristics do not consider the requirements of the planner, and typically need a lot of compute. Thus, in this work, we pro-pose a distribution to downsample training data by identifying relevant data points to learn a performant heuristic, while constraining computational costs. To arrive at this model, we disentangle the requirements of the planner, in our case A* search, from that of the language model to generalise on this task. Surprisingly, we find an overlap between their requirements; A* requires more accurate predictions on nodes near the goal, and LMs need the same set of nodes for effective generalisation. With these insights, we can quantify the contribution of each node towards accelerating A* search, and subsequently derive a training distribution for learning LM-based heuristics. Following a recent work, we conduct our experiments on two classical planning domains, maze navigation and sokoban, with two test splits per domain, and two conventional loss functions. We reduce the number of iterations required to find the solutions by upto 13 \u00d7 , with a wall-clock speed-up of upto 5 \u00d7 ."
    },
    {
        "paperId": "2a712ff7fb099214e38fb8ac73d797a10ddcf708",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.10725, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-15",
        "authors": [
            {
                "authorId": "2237129499",
                "name": "Jing Yao"
            },
            {
                "authorId": "2258961742",
                "name": "Xiaoyuan Yi"
            },
            {
                "authorId": "2187555771",
                "name": "Xing Xie"
            }
        ],
        "abstract": "The rapid progress in Large Language Models (LLMs) poses potential risks such as generating unethical content. Assessing LLMs' values can help expose their misalignment, but relies on reference-free evaluators, e.g., fine-tuned LLMs or close-source ones like GPT-4, to identify values reflected in generated responses. Nevertheless, these evaluators face two challenges in open-ended value evaluation: they should align with changing human value definitions with minimal annotation, against their own bias (adaptability), and detect varying value expressions and scenarios robustly (generalizability). To handle these challenges, we introduce CLAVE, a novel framework which integrates two complementary LLMs, a large one to extract high-level value concepts from a few human labels, leveraging its extensive knowledge and generalizability, and a smaller one fine-tuned on such concepts to better align with human value understanding. This dual-model approach enables calibration with any value systems using<100 human-labeled samples per value type. Then we present ValEval, a comprehensive dataset comprising 13k+ (text,value,label) tuples across diverse domains, covering three major value systems. We benchmark the capabilities of 12+ popular LLM evaluators and analyze their strengths and weaknesses. Our findings reveal that combining fine-tuned small models and prompt-based large ones serves as a superior balance in value evaluation."
    },
    {
        "paperId": "7e1cccd15c0f5aa511495abe0e0757a8f2962a89",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.11549, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-16",
        "authors": [
            {
                "authorId": "2311574200",
                "name": "Yin Jou Huang"
            },
            {
                "authorId": "2320944",
                "name": "Rafik Hadfi"
            }
        ],
        "abstract": "Psychological evidence reveals the influence of personality traits on decision-making. For instance, agreeableness is generally associated with positive outcomes in negotiations, whereas neuroticism is often linked to less favorable outcomes. This paper introduces a simulation framework centered on Large Language Model (LLM) agents endowed with synthesized personality traits. The agents negotiate within bargaining domains and possess customizable personalities and objectives. The experimental results show that the behavioral tendencies of LLM-based simulations could reproduce behavioral patterns observed in human negotiations. The contribution is twofold. First, we propose a simulation methodology that investigates the alignment between the linguistic and economic capabilities of LLM agents. Secondly, we offer empirical insights into the strategic impact of Big-Five personality traits on the outcomes of bilateral negotiations. We also provide a case study based on synthesized bargaining dialogues to reveal intriguing behaviors, including deceitful and compromising behaviors."
    },
    {
        "paperId": "ccd819bcacc0a37a799a0c50136f94f3d5da2a04",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MASIVE: Open-Ended Affective State Identification in English and Spanish",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.12196, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-16",
        "authors": [
            {
                "authorId": "2303366796",
                "name": "Nicholas Deas"
            },
            {
                "authorId": "1402934614",
                "name": "Elsbeth Turcan"
            },
            {
                "authorId": "2311698389",
                "name": "Iv'an P'erez Mej'ia"
            },
            {
                "authorId": "2284760379",
                "name": "Kathleen McKeown"
            }
        ],
        "abstract": "In the field of emotion analysis, much NLP research focuses on identifying a limited number of discrete emotion categories, often applied across languages. These basic sets, however, are rarely designed with textual data in mind, and culture, language, and dialect can influence how particular emotions are interpreted. In this work, we broaden our scope to a practically unbounded set of affective states, which includes any terms that humans use to describe their experiences of feeling. We collect and publish MASIVE, a dataset of Reddit posts in English and Spanish containing over 1,000 unique affective states each. We then define the new problem of affective state identification for language generation models framed as a masked span prediction task. On this task, we find that smaller finetuned multilingual models outperform much larger LLMs, even on region-specific Spanish affective states. Additionally, we show that pretraining on MASIVE improves model performance on existing emotion benchmarks. Finally, through machine translation experiments, we find that native speaker-written data is vital to good performance on this task."
    },
    {
        "paperId": "cc3f599bb0c0939c648494cd2426a3f9b4e84e83",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MERLIN: Multimodal Embedding Refinement via LLM-based Iterative Navigation for Text-Video Retrieval-Rerank Pipeline",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.12508, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-17",
        "authors": [
            {
                "authorId": "34190344",
                "name": "D. Han"
            },
            {
                "authorId": "2294809568",
                "name": "Eunhwan Park"
            },
            {
                "authorId": "2311818328",
                "name": "Gisang Lee"
            },
            {
                "authorId": "2311736317",
                "name": "Adam Lee"
            },
            {
                "authorId": "101880623",
                "name": "N. Kwak"
            }
        ],
        "abstract": "The rapid expansion of multimedia content has made accurately retrieving relevant videos from large collections increasingly challenging. Recent advancements in text-video retrieval have focused on cross-modal interactions, large-scale foundation model training, and probabilistic modeling, yet often neglect the crucial user perspective, leading to discrepancies between user queries and the content retrieved. To address this, we introduce MERLIN (Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel, training-free pipeline that leverages Large Language Models (LLMs) for iterative feedback learning. MERLIN refines query embeddings from a user perspective, enhancing alignment between queries and video content through a dynamic question answering process. Experimental results on datasets like MSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves Recall@1, outperforming existing systems and confirming the benefits of integrating LLMs into multimodal retrieval systems for more responsive and context-aware multimedia retrieval."
    },
    {
        "paperId": "b169c56cbe546d3e1ff12003ff5ab6f57cba608e",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Truth is Universal: Robust Detection of Lies in LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.12831, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-03",
        "authors": [
            {
                "authorId": "2316632834",
                "name": "Lennart B\u00fcrger"
            },
            {
                "authorId": "1685187",
                "name": "F. Hamprecht"
            },
            {
                "authorId": "1786884",
                "name": "B. Nadler"
            }
        ],
        "abstract": "Large Language Models (LLMs) have revolutionised natural language processing, exhibiting impressive human-like capabilities. In particular, LLMs are capable of\"lying\", knowingly outputting false statements. Hence, it is of interest and importance to develop methods to detect when LLMs lie. Indeed, several authors trained classifiers to detect LLM lies based on their internal model activations. However, other researchers showed that these classifiers may fail to generalise, for example to negated statements. In this work, we aim to develop a robust method to detect when an LLM is lying. To this end, we make the following key contributions: (i) We demonstrate the existence of a two-dimensional subspace, along which the activation vectors of true and false statements can be separated. Notably, this finding is universal and holds for various LLMs, including Gemma-7B, LLaMA2-13B, Mistral-7B and LLaMA3-8B. Our analysis explains the generalisation failures observed in previous studies and sets the stage for more robust lie detection; (ii) Building upon (i), we construct an accurate LLM lie detector. Empirically, our proposed classifier achieves state-of-the-art performance, attaining 94% accuracy in both distinguishing true from false factual statements and detecting lies generated in real-world scenarios."
    },
    {
        "paperId": "3c7bcbc6c978f39a5db2d687906e7c7a8014102d",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "LLM-Empowered State Representation for Reinforcement Learning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.13237, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-18",
        "authors": [
            {
                "authorId": "2223444420",
                "name": "Boyuan Wang"
            },
            {
                "authorId": "2244622868",
                "name": "Yun Qu"
            },
            {
                "authorId": "2116320200",
                "name": "Yuhang Jiang"
            },
            {
                "authorId": "1631174448",
                "name": "Jianzhun Shao"
            },
            {
                "authorId": "2312096447",
                "name": "Chang Liu"
            },
            {
                "authorId": "2312117608",
                "name": "Wenming Yang"
            },
            {
                "authorId": "2305263249",
                "name": "Xiangyang Ji"
            }
        ],
        "abstract": "Conventional state representations in reinforcement learning often omit critical task-related details, presenting a significant challenge for value networks in establishing accurate mappings from states to task rewards. Traditional methods typically depend on extensive sample learning to enrich state representations with task-specific information, which leads to low sample efficiency and high time costs. Recently, surging knowledgeable large language models (LLM) have provided promising substitutes for prior injection with minimal human intervention. Motivated by this, we propose LLM-Empowered State Representation (LESR), a novel approach that utilizes LLM to autonomously generate task-related state representation codes which help to enhance the continuity of network mappings and facilitate efficient training. Experimental results demonstrate LESR exhibits high sample efficiency and outperforms state-of-the-art baselines by an average of 29% in accumulated reward in Mujoco tasks and 30% in success rates in Gym-Robotics tasks."
    },
    {
        "paperId": "a9b04034e31a8c8ec5adff3c4665eda58b1f3013",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.13998, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-19",
        "authors": [
            {
                "authorId": "2312206172",
                "name": "Rujun Han"
            },
            {
                "authorId": "2285401140",
                "name": "Yuhao Zhang"
            },
            {
                "authorId": "50531624",
                "name": "Peng Qi"
            },
            {
                "authorId": "2312273705",
                "name": "Yumo Xu"
            },
            {
                "authorId": "2312269233",
                "name": "Jenyuan Wang"
            },
            {
                "authorId": "2197484384",
                "name": "Lan Liu"
            },
            {
                "authorId": "2312264839",
                "name": "William Yang Wang"
            },
            {
                "authorId": "2290849554",
                "name": "Bonan Min"
            },
            {
                "authorId": "2287929856",
                "name": "Vittorio Castelli"
            }
        ],
        "abstract": "Question answering based on retrieval augmented generation (RAG-QA) is an important research topic in NLP and has a wide range of real-world applications. However, most existing datasets for this task are either constructed using a single source corpus or consist of short extractive answers, which fall short of evaluating large language model (LLM) based RAG-QA systems on cross-domain generalization. To address these limitations, we create Long-form RobustQA (LFRQA), a new dataset comprising human-written long-form answers that integrate short extractive answers from multiple documents into a single, coherent narrative, covering 26K queries and large corpora across seven different domains. We further propose RAG-QA Arena by directly comparing model-generated answers against LFRQA\u2019s answers using LLMs as evaluators. We show via extensive experiments that RAG-QA Arena and human judgments on answer quality are highly correlated. Moreover, only 41.3% of the most competitive LLM\u2019s answers are preferred to LFRQA\u2019s answers, demonstrating RAG-QA Arena as a challenging evaluation platform for future research."
    },
    {
        "paperId": "d977733d9c842d38adc1bf3fcba0f7044cdb92ab",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Evaluating language models as risk scores",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.14614, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-19",
        "authors": [
            {
                "authorId": "50704606",
                "name": "Andr\u00e9 F. Cruz"
            },
            {
                "authorId": "2284219836",
                "name": "Moritz Hardt"
            },
            {
                "authorId": "1403842911",
                "name": "Celestine Mendler-D\u00fcnner"
            }
        ],
        "abstract": "Current question-answering benchmarks predominantly focus on accuracy in realizable prediction tasks. Conditioned on a question and answer-key, does the most likely token match the ground truth? Such benchmarks necessarily fail to evaluate LLMs' ability to quantify ground-truth outcome uncertainty. In this work, we focus on the use of LLMs as risk scores for unrealizable prediction tasks. We introduce folktexts, a software package to systematically generate risk scores using LLMs, and evaluate them against US Census data products. A flexible API enables the use of different prompting schemes, local or web-hosted models, and diverse census columns that can be used to compose custom prediction tasks. We evaluate 17 recent LLMs across five proposed benchmark tasks. We find that zero-shot risk scores produced by multiple-choice question-answering have high predictive signal but are widely miscalibrated. Base models consistently overestimate outcome uncertainty, while instruction-tuned models underestimate uncertainty and produce over-confident risk scores. In fact, instruction-tuning polarizes answer distribution regardless of true underlying data uncertainty. This reveals a general inability of instruction-tuned LLMs to express data uncertainty using multiple-choice answers. A separate experiment using verbalized chat-style risk queries yields substantially improved calibration across instruction-tuned models. These differences in ability to quantify data uncertainty cannot be revealed in realizable settings, and highlight a blind-spot in the current evaluation ecosystem that folktexts covers."
    },
    {
        "paperId": "65f20ab2481203e3699792d1f00a78df14ee95a8",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.15017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-22",
        "authors": [
            {
                "authorId": "2218346459",
                "name": "Meng Wang"
            },
            {
                "authorId": "4841460",
                "name": "Yunzhi Yao"
            },
            {
                "authorId": "2277568426",
                "name": "Ziwen Xu"
            },
            {
                "authorId": "2190751119",
                "name": "Shuofei Qiao"
            },
            {
                "authorId": "152931849",
                "name": "Shumin Deng"
            },
            {
                "authorId": "2277701209",
                "name": "Peng Wang"
            },
            {
                "authorId": "2143735911",
                "name": "Xiang Chen"
            },
            {
                "authorId": "2313474195",
                "name": "Jia-Chen Gu"
            },
            {
                "authorId": "2256747040",
                "name": "Yong Jiang"
            },
            {
                "authorId": "35930962",
                "name": "Pengjun Xie"
            },
            {
                "authorId": "2276428076",
                "name": "Fei Huang"
            },
            {
                "authorId": "2144200945",
                "name": "Huajun Chen"
            },
            {
                "authorId": "2288030000",
                "name": "Ningyu Zhang"
            }
        ],
        "abstract": "Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial for advancing towards trustworthy AGI. This paper reviews knowledge mechanism analysis from a novel taxonomy including knowledge utilization and evolution. Knowledge utilization delves into the mechanism of memorization, comprehension and application, and creation. Knowledge evolution focuses on the dynamic progression of knowledge within individual and group LLMs. Moreover, we discuss what knowledge LLMs have learned, the reasons for the fragility of parametric knowledge, and the potential dark knowledge (hypothesis) that will be challenging to address. We hope this work can help understand knowledge in LLMs and provide insights for future research."
    },
    {
        "paperId": "192bd161ca7dbdafc45c4399918cdba97952f8c6",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "A Community-Centric Perspective for Characterizing and Detecting Anti-Asian Violence-Provoking Speech",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.15227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-21",
        "authors": [
            {
                "authorId": "145816931",
                "name": "Gaurav Verma"
            },
            {
                "authorId": "2047998583",
                "name": "Rynaa Grover"
            },
            {
                "authorId": "2312339426",
                "name": "Jiawei Zhou"
            },
            {
                "authorId": "2041989412",
                "name": "Binny Mathew"
            },
            {
                "authorId": "2312326615",
                "name": "Jordan Kraemer"
            },
            {
                "authorId": "2256118804",
                "name": "Munmun De Choudhury"
            },
            {
                "authorId": "2309661459",
                "name": "Srijan Kumar"
            }
        ],
        "abstract": "Violence-provoking speech -- speech that implicitly or explicitly promotes violence against the members of the targeted community, contributed to a massive surge in anti-Asian crimes during the pandemic. While previous works have characterized and built tools for detecting other forms of harmful speech, like fear speech and hate speech, our work takes a community-centric approach to studying anti-Asian violence-provoking speech. Using data from ~420k Twitter posts spanning a 3-year duration (January 1, 2020 to February 1, 2023), we develop a codebook to characterize anti-Asian violence-provoking speech and collect a community-crowdsourced dataset to facilitate its large-scale detection using state-of-the-art classifiers. We contrast the capabilities of natural language processing classifiers, ranging from BERT-based to LLM-based classifiers, in detecting violence-provoking speech with their capabilities to detect anti-Asian hateful speech. In contrast to prior work that has demonstrated the effectiveness of such classifiers in detecting hateful speech ($F_1 = 0.89$), our work shows that accurate and reliable detection of violence-provoking speech is a challenging task ($F_1 = 0.69$). We discuss the implications of our findings, particularly the need for proactive interventions to support Asian communities during public health crises. The resources related to the study are available at https://claws-lab.github.io/violence-provoking-speech/."
    },
    {
        "paperId": "ded7778be3d2ac18765168e04d03a22dd2aed908",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Walking in Others\u2019 Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.15366, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-22",
        "authors": [
            {
                "authorId": "2158524037",
                "name": "Rongwu Xu"
            },
            {
                "authorId": "2312335443",
                "name": "Zi'an Zhou"
            },
            {
                "authorId": "2268671902",
                "name": "Tianwei Zhang"
            },
            {
                "authorId": "2257044451",
                "name": "Zehan Qi"
            },
            {
                "authorId": "2312902257",
                "name": "Su Yao"
            },
            {
                "authorId": "2312401769",
                "name": "Ke Xu"
            },
            {
                "authorId": "2304324554",
                "name": "Wei Xu"
            },
            {
                "authorId": "2268515593",
                "name": "Han Qiu"
            }
        ],
        "abstract": "The common toxicity and societal bias in contents generated by large language models (LLMs) necessitate strategies to reduce harm. Present solutions often demand white-box access to the model or substantial training, which is impractical for cutting-edge commercial LLMs. Moreover, prevailing prompting methods depend on external tool feedback and fail to simultaneously lessen toxicity and bias. Motivated by social psychology principles, we propose a novel strategy named perspective-taking prompting (PeT) that inspires LLMs to integrate diverse human perspectives and self-regulate their responses. This self-correction mechanism can significantly diminish toxicity (up to 89%) and bias (up to 73%) in LLMs\u2019 responses. Rigorous evaluations and ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and three open-source LLMs, revealing PeT\u2019s superiority in producing less harmful responses, outperforming five strong baselines."
    },
    {
        "paperId": "9c6cd9b1b95ff13ecd58447e1fb43c65a9d295a4",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Counter Turing Test (CT2): Investigating AI-Generated Text Detection for Hindi - Ranking LLMs based on Hindi AI Detectability Index (ADIhi)",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.15694, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-22",
        "authors": [
            {
                "authorId": "2284590307",
                "name": "Ishan Kavathekar"
            },
            {
                "authorId": "2106627712",
                "name": "Anku Rani"
            },
            {
                "authorId": "2312328008",
                "name": "Ashmit Chamoli"
            },
            {
                "authorId": "1734731",
                "name": "P. Kumaraguru"
            },
            {
                "authorId": "2064342742",
                "name": "Amit P. Sheth"
            },
            {
                "authorId": "2258322706",
                "name": "Amitava Das"
            }
        ],
        "abstract": "The widespread adoption of Large Language Models (LLMs) and awareness around multilingual LLMs have raised concerns regarding the potential risks and repercussions linked to the misapplication of AI-generated text, necessitating increased vigilance. While these models are primarily trained for English, their extensive training on vast datasets covering almost the entire web, equips them with capabilities to perform well in numerous other languages. AI-Generated Text Detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by the emergence of techniques to bypass detection. In this paper, we report our investigation on AGTD for an indic language Hindi. Our major contributions are in four folds: i) examined 26 LLMs to evaluate their proficiency in generating Hindi text, ii) introducing the AI-generated news article in Hindi ($AG_{hi}$) dataset, iii) evaluated the effectiveness of five recently proposed AGTD techniques: ConDA, J-Guard, RADAR, RAIDAR and Intrinsic Dimension Estimation for detecting AI-generated Hindi text, iv) proposed Hindi AI Detectability Index ($ADI_{hi}$) which shows a spectrum to understand the evolving landscape of eloquence of AI-generated text in Hindi. The code and dataset is available at https://github.com/ishank31/Counter_Turing_Test"
    },
    {
        "paperId": "33b8824f3c8fc3035afabaa77ecee3afe1c9753c",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.16148, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-23",
        "authors": [
            {
                "authorId": "23608432",
                "name": "Chao-Chun Hsu"
            },
            {
                "authorId": "2203427167",
                "name": "Erin Bransom"
            },
            {
                "authorId": "2052201732",
                "name": "Jenna Sparks"
            },
            {
                "authorId": "2003338023",
                "name": "Bailey Kuehl"
            },
            {
                "authorId": "2269860394",
                "name": "Chenhao Tan"
            },
            {
                "authorId": "30051202",
                "name": "David Wadden"
            },
            {
                "authorId": "31860505",
                "name": "Lucy Lu Wang"
            },
            {
                "authorId": "2275210271",
                "name": "Aakanksha Naik"
            }
        ],
        "abstract": "Literature review requires researchers to synthesize a large amount of information and is increasingly challenging as the scientific literature expands. In this work, we investigate the potential of LLMs for producing hierarchical organizations of scientific studies to assist researchers with literature review. We define hierarchical organizations as tree structures where nodes refer to topical categories and every node is linked to the studies assigned to that category. Our naive LLM-based pipeline for hierarchy generation from a set of studies produces promising yet imperfect hierarchies, motivating us to collect CHIME, an expert-curated dataset for this task focused on biomedicine. Given the challenging and time-consuming nature of building hierarchies from scratch, we use a human-in-the-loop process in which experts correct errors (both links between categories and study assignment) in LLM-generated hierarchies. CHIME contains 2,174 LLM-generated hierarchies covering 472 topics, and expert-corrected hierarchies for a subset of 100 topics. Expert corrections allow us to quantify LLM performance, and we find that while they are quite good at generating and organizing categories, their assignment of studies to categories could be improved. We attempt to train a corrector model with human feedback which improves study assignment by 12.6 F1 points. We release our dataset and models to encourage research on developing better assistive tools for literature review."
    },
    {
        "paperId": "a1b876e5a02f2f2d39189c6d24da822bb36bfc3e",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Enhancing LLM's Cognition via Structurization",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.16434, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-23",
        "authors": [
            {
                "authorId": "2283150812",
                "name": "Kai Liu"
            },
            {
                "authorId": "26910528",
                "name": "Zhihang Fu"
            },
            {
                "authorId": "2282974473",
                "name": "Chao Chen"
            },
            {
                "authorId": "2267067881",
                "name": "Wei Zhang"
            },
            {
                "authorId": "3115947",
                "name": "Rongxin Jiang"
            },
            {
                "authorId": "2114904538",
                "name": "Fan Zhou"
            },
            {
                "authorId": "2279072073",
                "name": "Yao-Shen Chen"
            },
            {
                "authorId": "2282998795",
                "name": "Yue Wu"
            },
            {
                "authorId": "2283451411",
                "name": "Jieping Ye"
            }
        ],
        "abstract": "When reading long-form text, human cognition is complex and structurized. While large language models (LLMs) process input contexts through a causal and sequential perspective, this approach can potentially limit their ability to handle intricate and complex inputs effectively. To enhance LLM's cognition capability, this paper presents a novel concept of context structurization. Specifically, we transform the plain, unordered contextual sentences into well-ordered and hierarchically structurized elements. By doing so, LLMs can better grasp intricate and extended contexts through precise attention and information-seeking along the organized structures. Extensive evaluations are conducted across various model architectures and sizes (including a series of auto-regressive LLMs as well as BERT-like masking models) on a diverse set of NLP tasks (e.g., context-based question-answering, exhaustive hallucination evaluation, and passage-level dense retrieval). Empirical results show consistent and significant performance gains afforded by a single-round structurization. In particular, we boost the open-sourced LLaMA2-70B model to achieve comparable performance against GPT-3.5-Turbo as the hallucination evaluator. Besides, we show the feasibility of distilling advanced LLMs' language processing abilities to a smaller yet effective StruXGPT-7B to execute structurization, addressing the practicality of our approach. Code is available at https://github.com/alibaba/struxgpt."
    },
    {
        "paperId": "f6c0a84ac1f1fe79cfde96f4e163d0d69f9c06cb",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Fine-Tuning Large Language Models for Stock Return Prediction Using Newsflow",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.18103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-25",
        "authors": [
            {
                "authorId": "2313101701",
                "name": "Tian Guo"
            },
            {
                "authorId": "2064941286",
                "name": "E. Hauptmann"
            }
        ],
        "abstract": "Large language models (LLMs) and their fine-tuning techniques have demonstrated superior performance in various language understanding and generation tasks.This paper explores fine-tuning LLMs for predicting stock returns with financial newsflow.Return prediction is fundamental for subsequent tasks like portfolio construction and optimization in quantitative investing. We formulate the model to include a text representation and forecasting modules. We propose to compare the encoder-only and decoder-only LLMs, considering they generate text representations in distinct ways.The impact of these different representations on return forecasting remains an open question.Meanwhile, we compare two simple methods of integrating LLMs\u2019 token-level representations into the forecasting module.The experiments on real investment universes reveal that:(1) aggregated representations from LLMs\u2019 token-level embeddings generally produce return predictions that enhance the performance of long-only and long-short portfolios;(2) in the relatively large investment universe, the decoder LLMs-based prediction model leads to stronger portfolios, whereas in the small universes, there are no consistent winners;(3) return predictions derived from LLMs\u2019 text representations are a strong signal for portfolio construction, outperforming conventional sentiment scores.These findings shed light on developing suitable LLM fine-tuning methods for return prediction-based portfolio construction."
    },
    {
        "paperId": "914a5521aa1ccd1813695e651c31a64be59b263a",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Are LLMs Good Annotators for Discourse-level Event Relation Extraction?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.19568, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-07-28",
        "authors": [
            {
                "authorId": "2313634652",
                "name": "Kangda Wei"
            },
            {
                "authorId": "2350091609",
                "name": "Aayush Gautam"
            },
            {
                "authorId": "2313624455",
                "name": "Ruihong Huang"
            }
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated proficiency in a wide array of natural language processing tasks. However, its effectiveness over discourse-level event relation extraction (ERE) tasks remains unexplored. In this paper, we assess the effectiveness of LLMs in addressing discourse-level ERE tasks characterized by lengthy documents and intricate relations encompassing coreference, temporal, causal, and subevent types. Evaluation is conducted using an commercial model, GPT-3.5, and an open-source model, LLaMA-2. Our study reveals a notable underperformance of LLMs compared to the baseline established through supervised learning. Although Supervised Fine-Tuning (SFT) can improve LLMs performance, it does not scale well compared to the smaller supervised baseline model. Our quantitative and qualitative analysis shows that LLMs have several weaknesses when applied for extracting event relations, including a tendency to fabricate event mentions, and failures to capture transitivity rules among relations, detect long distance relations, or comprehend contexts with dense event mentions. Code available at: https://github.com/WeiKangda/LLM-ERE.git."
    },
    {
        "paperId": "29d33aefc064ae90e9d92664fad525996b2cd30b",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.03281, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-06",
        "authors": [
            {
                "authorId": "2113252896",
                "name": "Boxi Cao"
            },
            {
                "authorId": "2280137459",
                "name": "Mengjie Ren"
            },
            {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
            },
            {
                "authorId": "2118233348",
                "name": "Xianpei Han"
            },
            {
                "authorId": "2315685414",
                "name": "Feng Zhang"
            },
            {
                "authorId": "2314918498",
                "name": "Junfeng Zhan"
            },
            {
                "authorId": "2110832778",
                "name": "Le Sun"
            }
        ],
        "abstract": "Evaluation is the baton for the development of large language models. Current evaluations typically employ a single-item assessment paradigm for each atomic test objective, which struggles to discern whether a model genuinely possesses the required capabilities or merely memorizes/guesses the answers to specific questions. To this end, we propose a novel evaluation framework referred to as StructEval. Starting from an atomic test objective, StructEval deepens and broadens the evaluation by conducting a structured assessment across multiple cognitive levels and critical concepts, and therefore offers a comprehensive, robust and consistent evaluation for LLMs. Experiments on three widely-used benchmarks demonstrate that StructEval serves as a reliable tool for resisting the risk of data contamination and reducing the interference of potential biases, thereby providing more reliable and consistent conclusions regarding model capabilities. Our framework also sheds light on the design of future principled and trustworthy LLM evaluation protocols."
    },
    {
        "paperId": "7e7bd0d0b42012ed1950ec8f78954e005e10799d",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "PAGED: A Benchmark for Procedural Graphs Extraction from Documents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.03630, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-07",
        "authors": [
            {
                "authorId": "2219279771",
                "name": "Weihong Du"
            },
            {
                "authorId": "2315103188",
                "name": "Wenrui Liao"
            },
            {
                "authorId": "2272276724",
                "name": "Hongru Liang"
            },
            {
                "authorId": "2315096119",
                "name": "Wenqiang Lei"
            }
        ],
        "abstract": "Automatic extraction of procedural graphs from documents creates a low-cost way for users to easily understand a complex procedure by skimming visual graphs. Despite the progress in recent studies, it remains unanswered: whether the existing studies have well solved this task (Q1) and whether the emerging large language models (LLMs) can bring new opportunities to this task (Q2). To this end, we propose a new benchmark PAGED, equipped with a large high-quality dataset and standard evaluations. It investigates five state-of-the-art baselines, revealing that they fail to extract optimal procedural graphs well because of their heavy reliance on hand-written rules and limited available data. We further involve three advanced LLMs in PAGED and enhance them with a novel self-refine strategy. The results point out the advantages of LLMs in identifying textual elements and their gaps in building logical structures. We hope PAGED can serve as a major landmark for automatic procedural graph extraction and the investigations in PAGED can offer insights into the research on logic reasoning among non-sequential elements."
    },
    {
        "paperId": "7ceffe4241106529a06eb64ada74addd751a0096",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Simplifying Translations for Children: Iterative Simplification Considering Age of Acquisition with LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.04217, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-08",
        "authors": [
            {
                "authorId": "2261495009",
                "name": "Masashi Oshika"
            },
            {
                "authorId": "2315303279",
                "name": "Makoto Morishita"
            },
            {
                "authorId": "2296710119",
                "name": "Tsutomu Hirao"
            },
            {
                "authorId": "2293543",
                "name": "Ryohei Sasano"
            },
            {
                "authorId": "2261497771",
                "name": "Koichi Takeda"
            }
        ],
        "abstract": "In recent years, neural machine translation (NMT) has been widely used in everyday life. However, the current NMT lacks a mechanism to adjust the difficulty level of translations to match the user's language level. Additionally, due to the bias in the training data for NMT, translations of simple source sentences are often produced with complex words. In particular, this could pose a problem for children, who may not be able to understand the meaning of the translations correctly. In this study, we propose a method that replaces words with high Age of Acquisitions (AoA) in translations with simpler words to match the translations to the user's level. We achieve this by using large language models (LLMs), providing a triple of a source sentence, a translation, and a target word to be replaced. We create a benchmark dataset using back-translation on Simple English Wikipedia. The experimental results obtained from the dataset show that our method effectively replaces high-AoA words with lower-AoA words and, moreover, can iteratively replace most of the high-AoA words while still maintaining high BLEU and COMET scores."
    },
    {
        "paperId": "ac13ca1957cd566eddc45112cd9386fd7ba76499",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Exploring Reasoning Biases in Large Language Models Through Syllogism: Insights from the NeuBAROCO Dataset",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.04403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-08",
        "authors": [
            {
                "authorId": "2315300128",
                "name": "Kentaro Ozeki"
            },
            {
                "authorId": "2220404504",
                "name": "Risako Ando"
            },
            {
                "authorId": "2220403840",
                "name": "Takanobu Morishita"
            },
            {
                "authorId": "2069103180",
                "name": "Hirohiko Abe"
            },
            {
                "authorId": "2106670",
                "name": "K. Mineshima"
            },
            {
                "authorId": "2315297512",
                "name": "Mitsuhiro Okada"
            }
        ],
        "abstract": "This paper explores the question of how accurately current large language models can perform logical reasoning in natural language, with an emphasis on whether these models exhibit reasoning biases similar to humans. Specifically, our study focuses on syllogistic reasoning, a form of deductive reasoning extensively studied in cognitive science as a natural form of human reasoning. We present a syllogism dataset called NeuBAROCO, which consists of syllogistic reasoning problems in English and Japanese. This dataset was originally designed for psychological experiments to assess human reasoning capabilities using various forms of syllogisms. Our experiments with leading large language models indicate that these models exhibit reasoning biases similar to humans, along with other error tendencies. Notably, there is significant room for improvement in reasoning problems where the relationship between premises and hypotheses is neither entailment nor contradiction. We also present experimental results and in-depth analysis using a new Chain-of-Thought prompting method, which asks LLMs to translate syllogisms into abstract logical expressions and then explain their reasoning process. Our analysis using this method suggests that the primary limitations of LLMs lie in the reasoning process itself rather than the interpretation of syllogisms."
    },
    {
        "paperId": "d0973f15e23e41caa665bf5cec6164de4ddf2e7a",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Communicate to Play: Pragmatic Reasoning for Efficient Cross-Cultural Communication in Codenames",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.04900, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-09",
        "authors": [
            {
                "authorId": "2315808287",
                "name": "Isadora White"
            },
            {
                "authorId": "2218587021",
                "name": "Sashrika Pandey"
            },
            {
                "authorId": "2315808733",
                "name": "Michelle Pan"
            }
        ],
        "abstract": "Cultural differences in common ground may result in pragmatic failure and misunderstandings during communication. We develop our method Rational Speech Acts for Cross-Cultural Communication (RSA+C3) to resolve cross-cultural differences in common ground. To measure the success of our method, we study RSA+C3 in the collaborative referential game of Codenames Duet and show that our method successfully improves collaboration between simulated players of different cultures. Our contributions are threefold: (1) creating Codenames players using contrastive learning of an embedding space and LLM prompting that are aligned with human patterns of play, (2) studying culturally induced differences in common ground reflected in our trained models, and (3) demonstrating that our method RSA+C3 can ease cross-cultural communication in gameplay by inferring sociocultural context from interaction. Our code is publicly available at github.com/icwhite/codenames."
    },
    {
        "paperId": "bf746159ec6008fa8e4d4134c848f8611066d62d",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "DataNarrative: Automated Data-Driven Storytelling with Visualizations and Texts",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.05346, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-09",
        "authors": [
            {
                "authorId": "2304464834",
                "name": "Mohammed Saidul Islam"
            },
            {
                "authorId": "2274022429",
                "name": "Enamul Hoque"
            },
            {
                "authorId": "2708940",
                "name": "Shafiq R. Joty"
            },
            {
                "authorId": "46437970",
                "name": "Md Tahmid Rahman Laskar"
            },
            {
                "authorId": "3405393",
                "name": "Md. Rizwan Parvez"
            }
        ],
        "abstract": "Data-driven storytelling is a powerful method for conveying insights by combining narrative techniques with visualizations and text. These stories integrate visual aids, such as highlighted bars and lines in charts, along with textual annotations explaining insights. However, creating such stories requires a deep understanding of the data and meticulous narrative planning, often necessitating human intervention, which can be time-consuming and mentally taxing. While Large Language Models (LLMs) excel in various NLP tasks, their ability to generate coherent and comprehensive data stories remains underexplored. In this work, we introduce a novel task for data story generation and a benchmark containing 1,449 stories from diverse sources. To address the challenges of crafting coherent data stories, we propose a multi-agent framework employing two LLM agents designed to replicate the human storytelling process: one for understanding and describing the data (Reflection), generating the outline, and narration, and another for verification at each intermediary step. While our agentic framework generally outperforms non-agentic counterparts in both model-based and human evaluations, the results also reveal unique challenges in data story generation."
    },
    {
        "paperId": "51d862461bccad7fe48c09c8ad85ddfbd092764c",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.06574, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-13",
        "authors": [
            {
                "authorId": "2118208508",
                "name": "Dayong Wu"
            },
            {
                "authorId": "2316004713",
                "name": "Jiaqi Li"
            },
            {
                "authorId": "2118640235",
                "name": "Baoxin Wang"
            },
            {
                "authorId": "2316025657",
                "name": "Honghong Zhao"
            },
            {
                "authorId": "2315980688",
                "name": "Siyuan Xue"
            },
            {
                "authorId": "2316008753",
                "name": "Yanjie Yang"
            },
            {
                "authorId": "2299046547",
                "name": "Zhijun Chang"
            },
            {
                "authorId": "2257226680",
                "name": "Rui Zhang"
            },
            {
                "authorId": "2315981640",
                "name": "Li Qian"
            },
            {
                "authorId": "2257266055",
                "name": "Bo Wang"
            },
            {
                "authorId": "2257065753",
                "name": "Shijin Wang"
            },
            {
                "authorId": "2299102032",
                "name": "Zhixiong Zhang"
            },
            {
                "authorId": "2316008068",
                "name": "Guoping Hu"
            }
        ],
        "abstract": "Large language models (LLMs) have shown remarkable achievements across various language tasks. To enhance the performance of LLMs in scientific literature services, we developed the scientific literature LLM (SciLit-LLM) through pre-training and supervised fine-tuning on scientific literature, building upon the iFLYTEK Spark LLM. Furthermore, we present a knowledge service system Spark Research Assistant (SparkRA) based on our SciLit-LLM. SparkRA is accessible online and provides three primary functions: literature investigation, paper reading, and academic writing. As of July 30, 2024, SparkRA has garnered over 50,000 registered users, with a total usage count exceeding 1.3 million."
    },
    {
        "paperId": "dbbf7bff411811a9bf715c0ab1526f9757782ebf",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Pragmatic inference of scalar implicature by LLMs",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2408.06673",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.06673, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-13",
        "authors": [
            {
                "authorId": "2316012331",
                "name": "Ye-eun Cho"
            },
            {
                "authorId": "2316101810",
                "name": "Seong mook Kim"
            }
        ],
        "abstract": "This study investigates how Large Language Models (LLMs), particularly BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), engage in pragmatic inference of scalar implicature, such as some. Two sets of experiments were conducted using cosine similarity and next sentence/token prediction as experimental methods. The results in experiment 1 showed that, both models interpret some as pragmatic implicature not all in the absence of context, aligning with human language processing. In experiment 2, in which Question Under Discussion (QUD) was presented as a contextual cue, BERT showed consistent performance regardless of types of QUDs, while GPT-2 encountered processing difficulties since a certain type of QUD required pragmatic inference for implicature. The findings revealed that, in terms of theoretical approaches, BERT inherently incorporates pragmatic implicature not all within the term some, adhering to Default model (Levinson, 2000). In contrast, GPT-2 seems to encounter processing difficulties in inferring pragmatic implicature within context, consistent with Context-driven model (Sperber and Wilson, 2002)."
    },
    {
        "paperId": "8d4c1bbffd6a0144278103ce112217677e10d281",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Words Matter: Reducing Stigma in Online Conversations about Substance Use with Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.07873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-15",
        "authors": [
            {
                "authorId": "35345716",
                "name": "Layla A. Bouzoubaa"
            },
            {
                "authorId": "2307070497",
                "name": "Elham Aghakhani"
            },
            {
                "authorId": "3342374",
                "name": "R. Rezapour"
            }
        ],
        "abstract": "Stigma is a barrier to treatment for individuals struggling with substance use disorders (SUD), which leads to significantly lower treatment engagement rates. With only 7% of those affected receiving any form of help, societal stigma not only discourages individuals with SUD from seeking help but isolates them, hindering their recovery journey and perpetuating a cycle of shame and self-doubt. This study investigates how stigma manifests on social media, particularly Reddit, where anonymity can exacerbate discriminatory behaviors. We analyzed over 1.2 million posts, identifying 3,207 that exhibited stigmatizing language related to people who use substances (PWUS). Of these, 1,649 posts were classified as containing directed stigma towards PWUS, which became the focus of our de-stigmatization efforts. Using Informed and Stylized LLMs, we developed a model to transform these instances into more empathetic language.Our paper contributes to the field by proposing a computational framework for analyzing stigma and de-stigmatizing online content, and delving into the linguistic features that propagate stigma towards PWUS. Our work not only enhances understanding of stigma\u2019s manifestations online but also provides practical tools for fostering a more supportive environment for those affected by SUD."
    },
    {
        "paperId": "0455396a569ed3a9c6db69858c70cf457e58fb70",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.08210, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-15",
        "authors": [
            {
                "authorId": "2264999269",
                "name": "Javier Gonz'alez"
            },
            {
                "authorId": "2264522015",
                "name": "Aditya V. Nori"
            }
        ],
        "abstract": "Recent advances in AI have been significantly driven by the capabilities of large language models (LLMs) to solve complex problems in ways that resemble human thinking. However, there is an ongoing debate about the extent to which LLMs are capable of actual reasoning. Central to this debate are two key probabilistic concepts that are essential for connecting causes to their effects: the probability of necessity (PN) and the probability of sufficiency (PS). This paper introduces a framework that is both theoretical and practical, aimed at assessing how effectively LLMs are able to replicate real-world reasoning mechanisms using these probabilistic measures. By viewing LLMs as abstract machines that process information through a natural language interface, we examine the conditions under which it is possible to compute suitable approximations of PN and PS. Our research marks an important step towards gaining a deeper understanding of when LLMs are capable of reasoning, as illustrated by a series of math examples."
    },
    {
        "paperId": "b70062cd24b30cc0c7698d65da8c155ef7109f46",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "CMoralEval: A Moral Evaluation Benchmark for Chinese Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2408.09819",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.09819, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-19",
        "authors": [
            {
                "authorId": "2217579802",
                "name": "Linhao Yu"
            },
            {
                "authorId": "2301582549",
                "name": "Yongqi Leng"
            },
            {
                "authorId": "2263648395",
                "name": "Yufei Huang"
            },
            {
                "authorId": "2310313146",
                "name": "Shang Wu"
            },
            {
                "authorId": "2316413956",
                "name": "Haixin Liu"
            },
            {
                "authorId": "2292143863",
                "name": "Xinmeng Ji"
            },
            {
                "authorId": "2316426703",
                "name": "Jiahui Zhao"
            },
            {
                "authorId": "2292413369",
                "name": "Jinwang Song"
            },
            {
                "authorId": "2292271761",
                "name": "Tingting Cui"
            },
            {
                "authorId": "2364659406",
                "name": "Xiaoqing Cheng"
            },
            {
                "authorId": "2316926274",
                "name": "Liutao Liutao"
            },
            {
                "authorId": "2310434485",
                "name": "Deyi Xiong"
            }
        ],
        "abstract": "What a large language model (LLM) would respond in ethically relevant context? In this paper, we curate a large benchmark CMoralEval for morality evaluation of Chinese LLMs. The data sources of CMoralEval are two-fold: 1) a Chinese TV program discussing Chinese moral norms with stories from the society and 2) a collection of Chinese moral anomies from various newspapers and academic papers on morality. With these sources, we aim to create a moral evaluation dataset characterized by diversity and authenticity. We develop a morality taxonomy and a set of fundamental moral principles that are not only rooted in traditional Chinese culture but also consistent with contemporary societal norms. To facilitate efficient construction and annotation of instances in CMoralEval, we establish a platform with AI-assisted instance generation to streamline the annotation process. These help us curate CMoralEval that encompasses both explicit moral scenarios (14,964 instances) and moral dilemma scenarios (15,424 instances), each with instances from different data sources. We conduct extensive experiments with CMoralEval to examine a variety of Chinese LLMs. Experiment results demonstrate that CMoralEval is a challenging benchmark for Chinese LLMs. The dataset is publicly available at \\url{https://github.com/tjunlp-lab/CMoralEval}."
    },
    {
        "paperId": "dda8031682684655744c7001374e6cb88c9503bd",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.10902, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-20",
        "authors": [
            {
                "authorId": "2007581062",
                "name": "John Mendon\u00e7a"
            },
            {
                "authorId": "2268558660",
                "name": "Isabel Trancoso"
            },
            {
                "authorId": "1784914",
                "name": "A. Lavie"
            }
        ],
        "abstract": "Although human evaluation remains the gold standard for open-domain dialogue evaluation, the growing popularity of automated evaluation using Large Language Models (LLMs) has also extended to dialogue. However, most frameworks leverage benchmarks that assess older chatbots on aspects such as fluency and relevance, which are not reflective of the challenges associated with contemporary models. In fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset, suggests that current chatbots may exhibit several recurring issues related to coherence and commonsense knowledge, but generally produce highly fluent and relevant responses. Noting the aforementioned limitations, this paper introduces Soda-Eval, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, where the annotations were generated by GPT-4. Using Soda-Eval as a benchmark, we then study the performance of several open-access instruction-tuned LLMs, finding that dialogue evaluation remains challenging. Fine-tuning these models improves performance over few-shot inferences, both in terms of correlation and explanation."
    },
    {
        "paperId": "ef5f651db16112d4e01480d412c2915bf416f1cd",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "CHECKWHY: Causal Fact Verification via Argument Structure",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.10918, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-20",
        "authors": [
            {
                "authorId": "94631629",
                "name": "Jiasheng Si"
            },
            {
                "authorId": "2189417684",
                "name": "Yibo Zhao"
            },
            {
                "authorId": "2189015031",
                "name": "Yingjie Zhu"
            },
            {
                "authorId": "2261484618",
                "name": "Haiyang Zhu"
            },
            {
                "authorId": "2307453346",
                "name": "Wenpeng Lu"
            },
            {
                "authorId": "2306758944",
                "name": "Deyu Zhou"
            }
        ],
        "abstract": "With the growing complexity of fact verification tasks, the concern with\"thoughtful\"reasoning capabilities is increasing. However, recent fact verification benchmarks mainly focus on checking a narrow scope of semantic factoids within claims and lack an explicit logical reasoning process. In this paper, we introduce CheckWhy, a challenging dataset tailored to a novel causal fact verification task: checking the truthfulness of the causal relation within claims through rigorous reasoning steps. CheckWhy consists of over 19K\"why\"claim-evidence-argument structure triplets with supports, refutes, and not enough info labels. Each argument structure is composed of connected evidence, representing the reasoning process that begins with foundational evidence and progresses toward claim establishment. Through extensive experiments on state-of-the-art models, we validate the importance of incorporating the argument structure for causal fact verification. Moreover, the automated and human evaluation of argument structure generation reveals the difficulty in producing satisfying argument structure by fine-tuned models or Chain-of-Thought prompted LLMs, leaving considerable room for future improvements."
    },
    {
        "paperId": "12a52a817bf1495d872cfde09ce071c1a995ba3d",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "StyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.15666, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-28",
        "authors": [
            {
                "authorId": "33772445",
                "name": "Jillian R. Fisher"
            },
            {
                "authorId": "1474550731",
                "name": "Skyler Hallinan"
            },
            {
                "authorId": "50085131",
                "name": "Ximing Lu"
            },
            {
                "authorId": "2283139413",
                "name": "Mitchell Gordon"
            },
            {
                "authorId": "2265540561",
                "name": "Zaid Harchaoui"
            },
            {
                "authorId": "2258807987",
                "name": "Yejin Choi"
            }
        ],
        "abstract": "Authorship obfuscation, rewriting a text to intentionally obscure the identity of the author, is important yet challenging. Current methods using large language models (LLMs) lack interpretability and controllability, often ignoring author-specific stylistic features, resulting in less robust performance overall.To address this, we develop StyleRemix, an adaptive and interpretable obfuscation method that perturbs specific, fine-grained style elements of the original input text. StyleRemix uses pre-trained Low Rank Adaptation (LoRA) modules to rewrite inputs along various stylistic axes (e.g., formality, length) while maintaining low computational costs. StyleRemix outperforms state-of-the-art baselines and much larger LLMs on an array of domains on both automatic and human evaluation.Additionally, we release AuthorMix, a large set of 30K high-quality, long-form texts from a diverse set of 14 authors and 4 domains, and DiSC, a parallel corpus of 1,500 texts spanning seven style axes in 16 unique directions."
    },
    {
        "paperId": "917a8cb617c7ab0a9c1c681f5c9d4c0f28161e9a",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "SSDM: Scalable Speech Dysfluency Modeling",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.16221, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-29",
        "authors": [
            {
                "authorId": "46243665",
                "name": "Jiachen Lian"
            },
            {
                "authorId": "2317094352",
                "name": "Xuanru Zhou"
            },
            {
                "authorId": "2189687642",
                "name": "Z. Ezzes"
            },
            {
                "authorId": "2282150524",
                "name": "Jet M J Vonk"
            },
            {
                "authorId": "2317114703",
                "name": "Brittany Morin"
            },
            {
                "authorId": "2310772005",
                "name": "David Baquirin"
            },
            {
                "authorId": "2317112320",
                "name": "Zachary Miller"
            },
            {
                "authorId": "1397979606",
                "name": "M. Gorno-Tempini"
            },
            {
                "authorId": "1692246",
                "name": "G. Anumanchipalli"
            }
        ],
        "abstract": "Speech dysfluency modeling is the core module for spoken language learning, and speech therapy. However, there are three challenges. First, current state-of-the-art solutions\\cite{lian2023unconstrained-udm, lian-anumanchipalli-2024-towards-hudm} suffer from poor scalability. Second, there is a lack of a large-scale dysfluency corpus. Third, there is not an effective learning framework. In this paper, we propose \\textit{SSDM: Scalable Speech Dysfluency Modeling}, which (1) adopts articulatory gestures as scalable forced alignment; (2) introduces connectionist subsequence aligner (CSA) to achieve dysfluency alignment; (3) introduces a large-scale simulated dysfluency corpus called Libri-Dys; and (4) develops an end-to-end system by leveraging the power of large language models (LLMs). We expect SSDM to serve as a standard in the area of dysfluency modeling. Demo is available at \\url{https://berkeley-speech-group.github.io/SSDM/}."
    },
    {
        "paperId": "da5d2425cdef713b105ceb92cd86e9528aa079d5",
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "title": "Interpreting and Improving Large Language Models in Arithmetic Calculation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.01659, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-03",
        "authors": [
            {
                "authorId": "2319188665",
                "name": "Wei Zhang"
            },
            {
                "authorId": "29001337",
                "name": "Chaoqun Wan"
            },
            {
                "authorId": "2319178563",
                "name": "Yonggang Zhang"
            },
            {
                "authorId": "2319152002",
                "name": "Yiu-ming Cheung"
            },
            {
                "authorId": "2257165685",
                "name": "Xinmei Tian"
            },
            {
                "authorId": "2319393988",
                "name": "Xu Shen"
            },
            {
                "authorId": "2316672136",
                "name": "Jieping Ye"
            }
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable potential across numerous applications and have shown an emergent ability to tackle complex reasoning tasks, such as mathematical computations. However, even for the simplest arithmetic calculations, the intrinsic mechanisms behind LLMs remain mysterious, making it challenging to ensure reliability. In this work, we delve into uncovering a specific mechanism by which LLMs execute calculations. Through comprehensive experiments, we find that LLMs frequently involve a small fraction (<5%) of attention heads, which play a pivotal role in focusing on operands and operators during calculation processes. Subsequently, the information from these operands is processed through multi-layer perceptrons (MLPs), progressively leading to the final solution. These pivotal heads/MLPs, though identified on a specific dataset, exhibit transferability across different datasets and even distinct tasks. This insight prompted us to investigate the potential benefits of selectively fine-tuning these essential heads/MLPs to boost the LLMs' computational performance. We empirically find that such precise tuning can yield notable enhancements on mathematical prowess, without compromising the performance on non-mathematical tasks. Our work serves as a preliminary exploration into the arithmetic calculation abilities inherent in LLMs, laying a solid foundation to reveal more intricate mathematical tasks."
    },
    {
        "paperId": "636b9e5bc46d2f249a5655e374aafe61a55412a7",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.05197, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-08",
        "authors": [
            {
                "authorId": "2320299930",
                "name": "Neeladri Bhuiya"
            },
            {
                "authorId": "71034258",
                "name": "Viktor Schlegel"
            },
            {
                "authorId": "2057271731",
                "name": "Stefan Winkler"
            }
        ],
        "abstract": "State-of-the-art Large Language Models (LLMs) are accredited with an increasing number of different capabilities, ranging from reading comprehension over advanced mathematical and reasoning skills to possessing scientific knowledge. In this paper we focus on multi-hop reasoning\u2014the ability to identify and integrate information from multiple textual sources.Given the concerns with the presence of simplifying cues in existing multi-hop reasoning benchmarks, which allow models to circumvent the reasoning requirement, we set out to investigate whether LLMs are prone to exploiting such simplifying cues. We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors. We propose a challenging multi-hop reasoning benchmark by generating seemingly plausible multi-hop reasoning chains that ultimately lead to incorrect answers. We evaluate multiple open and proprietary state-of-the-art LLMs and show that their multi-hop reasoning performance is affected, as indicated by up to 45% relative decrease in F1 score when presented with such seemingly plausible alternatives. We also find that\u2014while LLMs tend to ignore misleading lexical cues\u2014misleading reasoning paths indeed present a significant challenge. The code and data are made available at https://github.com/zawedcvg/Are-Large-Language-Models-Attentive-Readers"
    },
    {
        "paperId": "c7d43357593ec96c4a18845a413ffe5073a47589",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Efficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.12656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-19",
        "authors": [
            {
                "authorId": "107973475",
                "name": "Furkan \u015eahinu\u00e7"
            },
            {
                "authorId": "151502102",
                "name": "Thy Thy Tran"
            },
            {
                "authorId": "2321870873",
                "name": "Yulia Grishina"
            },
            {
                "authorId": "2282504397",
                "name": "Yufang Hou"
            },
            {
                "authorId": "2321885930",
                "name": "Bei Chen"
            },
            {
                "authorId": "2260340390",
                "name": "Iryna Gurevych"
            }
        ],
        "abstract": "Scientific leaderboards are standardized ranking systems that facilitate evaluating and comparing competitive methods. Typically, a leaderboard is defined by a task, dataset, and evaluation metric (TDM) triple, allowing objective performance assessment and fostering innovation through benchmarking. However, the exponential increase in publications has made it infeasible to construct and maintain these leaderboards manually. Automatic leaderboard construction has emerged as a solution to reduce manual labor. Existing datasets for this task are based on the community-contributed leaderboards without additional curation. Our analysis shows that a large portion of these leaderboards are incomplete, and some of them contain incorrect information. In this work, we present SciLead, a manually-curated Scientific Leaderboard dataset that overcomes the aforementioned problems. Building on this dataset, we propose three experimental settings that simulate real-world scenarios where TDM triples are fully defined, partially defined, or undefined during leaderboard construction. While previous research has only explored the first setting, the latter two are more representative of real-world applications. To address these diverse settings, we develop a comprehensive LLM-based framework for constructing leaderboards. Our experiments and analysis reveal that various LLMs often correctly identify TDM triples while struggling to extract result values from publications. We make our code and data publicly available."
    },
    {
        "paperId": "36c61f368c18356dd4d32022c553ef75cf560202",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Debiasing Text Safety Classifiers through a Fairness-Aware Ensemble",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.13705, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-05",
        "authors": [
            {
                "authorId": "2313911859",
                "name": "Olivia Sturman"
            },
            {
                "authorId": "2117799324",
                "name": "Aparna Joshi"
            },
            {
                "authorId": "2219919981",
                "name": "Bhaktipriya Radharapu"
            },
            {
                "authorId": "2313970527",
                "name": "Piyush Kumar"
            },
            {
                "authorId": "2322446040",
                "name": "Renee Shelby"
            }
        ],
        "abstract": "Increasing use of large language models (LLMs) demand performant guardrails to ensure the safety of inputs and outputs of LLMs. When these safeguards are trained on imbalanced data, they can learn the societal biases. We present a light-weight, post-processing method for mitigating counterfactual fairness in closed-source text safety classifiers. Our approach involves building an ensemble that not only outperforms the input classifiers and policy-aligns them, but also acts as a debiasing regularizer. We introduce two threshold-agnostic metrics to assess the counterfactual fairness of a model, and demonstrate how combining these metrics with Fair Data Reweighting (FDW) helps mitigate biases. We create an expanded Open AI dataset, and a new templated LLM-generated dataset based on user-prompts, both of which are counterfactually balanced across identity groups and cover four key areas of safety; we will work towards publicly releasing these datasets. Our results show that our approach improves counterfactual fairness with minimal impact on model performance."
    },
    {
        "paperId": "f522c8e7261daf215e23b9c251ee145f72b432de",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.13843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-20",
        "authors": [
            {
                "authorId": "93972200",
                "name": "Robert D Morabito"
            },
            {
                "authorId": "2322441817",
                "name": "Sangmitra Madhusudan"
            },
            {
                "authorId": "2322353514",
                "name": "Tyler McDonald"
            },
            {
                "authorId": "2322341774",
                "name": "Ali Emami"
            }
        ],
        "abstract": "Mitigating explicit and implicit biases in Large Language Models (LLMs) has become a critical focus in the field of natural language processing. However, many current methodologies evaluate scenarios in isolation, without considering the broader context or the spectrum of potential biases within each situation. To address this, we introduce the Sensitivity Testing on Offensive Progressions (STOP) dataset, which includes 450 offensive progressions containing 2,700 unique sentences of varying severity that progressively escalate from less to more explicitly offensive. Covering a broad spectrum of 9 demographics and 46 sub-demographics, STOP ensures inclusivity and comprehensive coverage. We evaluate several leading closed- and open-source models, including GPT-4, Mixtral, and Llama 3. Our findings reveal that even the best-performing models detect bias inconsistently, with success rates ranging from 19.3% to 69.8%. Furthermore, we demonstrate how aligning models with human judgments on STOP can improve model answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs by up to 191%, while maintaining or even improving performance. STOP presents a novel framework for assessing the complex nature of biases in LLMs, which will enable more effective bias mitigation strategies and facilitates the creation of fairer language models."
    },
    {
        "paperId": "fa9becea34e062a04a1c153e7417d7d75e195da9",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "\"I Never Said That\": A dataset, taxonomy and baselines on response clarity classification",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.13879, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-20",
        "authors": [
            {
                "authorId": "2105847128",
                "name": "Konstantinos Thomas"
            },
            {
                "authorId": "2080432906",
                "name": "Giorgos Filandrianos"
            },
            {
                "authorId": "2184294391",
                "name": "Maria Lymperaiou"
            },
            {
                "authorId": "36259430",
                "name": "Chrysoula Zerva"
            },
            {
                "authorId": "1719165",
                "name": "G. Stamou"
            }
        ],
        "abstract": "Equivocation and ambiguity in public speech are well-studied discourse phenomena, especially in political science and analysis of political interviews. Inspired by the well-grounded theory on equivocation, we aim to resolve the closely related problem of response clarity in questions extracted from political interviews, leveraging the capabilities of Large Language Models (LLMs) and human expertise. To this end, we introduce a novel taxonomy that frames the task of detecting and classifying response clarity and a corresponding clarity classification dataset which consists of question-answer (QA) pairs drawn from political interviews and annotated accordingly. Our proposed two-level taxonomy addresses the clarity of a response in terms of the information provided for a given question (high-level) and also provides a fine-grained taxonomy of evasion techniques that relate to unclear, ambiguous responses (lower-level). We combine ChatGPT and human annotators to collect, validate and annotate discrete QA pairs from political interviews, to be used for our newly introduced response clarity task. We provide a detailed analysis and conduct several experiments with different model architectures, sizes and adaptation methods to gain insights and establish new baselines over the proposed dataset and task."
    },
    {
        "paperId": "3e033a81c97a00c73066394948f48b022da93c91",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.13935, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-20",
        "authors": [
            {
                "authorId": "2302806171",
                "name": "Sarfaroz Yunusov"
            },
            {
                "authorId": "2322441711",
                "name": "Hamza Sidat"
            },
            {
                "authorId": "2302806420",
                "name": "Ali Emami"
            }
        ],
        "abstract": "This study explores the effectiveness of Large Language Models (LLMs) in creating personalized \u201cmirror stories\u201d that reflect and resonate with individual readers\u2019 identities, addressing the significant lack of diversity in literature. We present MirrorStories, a corpus of 1,500 personalized short stories generated by integrating elements such as name, gender, age, ethnicity, reader interest, and story moral. We demonstrate that LLMs can effectively incorporate diverse identity elements into narratives, with human evaluators identifying personalized elements in the stories with high accuracy. Through a comprehensive evaluation involving 26 diverse human judges, we compare the effectiveness of MirrorStories against generic narratives. We find that personalized LLM-generated stories not only outscore generic human-written and LLM-generated ones across all metrics of engagement (with average ratings of 4.22 versus 3.37 on a 5-point scale), but also achieve higher textual diversity while preserving the intended moral. We also provide analyses that include bias assessments and a study on the potential for integrating images into personalized stories."
    },
    {
        "paperId": "76bc7cb435a1b3de79a4f3da7b06b275329baa45",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Exploring Automated Keyword Mnemonics Generation with Large Language Models via Overgenerate-and-Rank",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.13952, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-21",
        "authors": [
            {
                "authorId": "2294669289",
                "name": "Jaewook Lee"
            },
            {
                "authorId": "2216714825",
                "name": "Hunter McNichols"
            },
            {
                "authorId": "2289844808",
                "name": "Andrew S. Lan"
            }
        ],
        "abstract": "In this paper, we study an under-explored area of language and vocabulary learning: keyword mnemonics, a technique for memorizing vocabulary through memorable associations with a target word via a verbal cue. Typically, creating verbal cues requires extensive human effort and is quite time-consuming, necessitating an automated method that is more scalable. We propose a novel overgenerate-and-rank method via prompting large language models (LLMs) to generate verbal cues and then ranking them according to psycholinguistic measures and takeaways from a pilot user study. To assess cue quality, we conduct both an automated evaluation of imageability and coherence, as well as a human evaluation involving English teachers and learners. Results show that LLM-generated mnemonics are comparable to human-generated ones in terms of imageability, coherence, and perceived usefulness, but there remains plenty of room for improvement due to the diversity in background and preference among language learners."
    },
    {
        "paperId": "3f77b6c2def038cf28a8bf81f32d7a26ff3fee5a",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Temporally Consistent Factuality Probing for Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.14065, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-21",
        "authors": [
            {
                "authorId": "2243338012",
                "name": "Ashutosh Bajpai"
            },
            {
                "authorId": "2322447865",
                "name": "Aaryan Goyal"
            },
            {
                "authorId": "9210358",
                "name": "Atif Anwer"
            },
            {
                "authorId": "2256999352",
                "name": "Tanmoy Chakraborty"
            }
        ],
        "abstract": "The prolific use of Large Language Models (LLMs) as an alternate knowledge base requires them to be factually consistent, necessitating both correctness and consistency traits for paraphrased queries. Recently, significant attempts have been made to benchmark datasets and metrics to evaluate LLMs for these traits. However, structural simplicity (subject-relation-object) and contemporary association in their query formulation limit the broader definition of factuality and consistency. In this study, we introduce TeCFaP, a novel Temporally Consistent Factuality Probe task to expand the consistent factuality probe in the temporal dimension. To this end, we propose TEMP-COFAC, a high-quality dataset of prefix-style English query paraphrases. Subsequently, we extend the definitions of existing metrics to represent consistent factuality across temporal dimension. We experiment with a diverse set of LLMs and find most of them performing poorly on TeCFaP. Next, we propose a novel solution CoTSeLF (Consistent-Time-Sensitive Learning Framework) combining multi-task instruction tuning (MT-IT) with consistent-time-sensitive reinforcement learning (CTSRL) to improve temporally consistent factuality in LLMs. Our experiments demonstrate the efficacy of CoTSeLF over several baselines."
    },
    {
        "paperId": "11a6f7d2969e61be206e4fa2914776cd8a509ae6",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Speechworthy Instruction-tuned Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.14672, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-23",
        "authors": [
            {
                "authorId": "91009922",
                "name": "Hyundong Justin Cho"
            },
            {
                "authorId": "2139649061",
                "name": "Nic Jedema"
            },
            {
                "authorId": "10430740",
                "name": "Leonardo F. R. Ribeiro"
            },
            {
                "authorId": "2303002552",
                "name": "Karishma Sharma"
            },
            {
                "authorId": "2278903766",
                "name": "Pedro A. Szekely"
            },
            {
                "authorId": "1719404",
                "name": "Alessandro Moschitti"
            },
            {
                "authorId": "2301582504",
                "name": "Ruben Janssen"
            },
            {
                "authorId": "2267332763",
                "name": "Jonathan May"
            }
        ],
        "abstract": "Current instruction-tuned language models are exclusively trained with textual preference data and thus may not be aligned to the unique requirements of other modalities, such as speech. To better align language models with the speech domain, we explore i) prompting strategies based on radio-industry best practices and ii) preference learning using a novel speech-based preference data of 20K samples collected by annotators who listen to response pairs. Both human and automatic evaluation show that both prompting and preference learning increase the speech-suitability of popular instruction tuned LLMs. More interestingly, we show that these methods are additive; combining them achieves the best win rates in head-to-head comparison, resulting in responses that are preferred or tied to the base model in 76.2% of comparisons on average. Lastly, we share lexical, syntactical, and qualitative analyses that elicit how our studied methods differ with baselines in generating more speech-suitable responses."
    },
    {
        "paperId": "ace1a82d97d024c26e588cef084dcb322f157811",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions with Path Planning and Feedback",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.14826, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-23",
        "authors": [
            {
                "authorId": "2322456257",
                "name": "Qinzhuo Wu"
            },
            {
                "authorId": "2257333016",
                "name": "Wei Liu"
            },
            {
                "authorId": "2257013742",
                "name": "Jian Luan"
            },
            {
                "authorId": "2257388949",
                "name": "Bin Wang"
            }
        ],
        "abstract": "Recently, tool-augmented LLMs have gained increasing attention. Given an instruction, tool-augmented LLMs can interact with various external tools in multiple rounds and provide a final answer. However, previous LLMs were trained on overly detailed instructions, which included API names or parameters, while real users would not explicitly mention these API details. This leads to a gap between trained LLMs and real-world scenarios. In addition, most works ignore whether the interaction process follows the instruction. To address these issues, we constructed a training dataset called MGToolBench, which contains statement and category-level instructions to better reflect real-world scenarios. In addition, we propose ToolPlanner, a two-stage reinforcement learning framework that utilizes path planning and two feedback mechanisms to enhance the LLM\u2019s task completion and instruction-following capabilities. Experimental results show that ToolPlanner significantly improves the Match Rate, Pass Rate and Win Rate by 26.8%, 20.2%, and 5.6% compared to the SOTA model. Human evaluation verifies that the multi-granularity instructions can better align with users\u2019 usage habits. Our data and code will be released upon acceptance."
    },
    {
        "paperId": "f22472b43801f5df67f21683e0578e71958b382a",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "With Ears to See and Eyes to Hear: Sound Symbolism Experiments with Multimodal Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.14917, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-23",
        "authors": [
            {
                "authorId": "2265752970",
                "name": "Tyler Loakman"
            },
            {
                "authorId": "1527099159",
                "name": "Yucheng Li"
            },
            {
                "authorId": "2275288757",
                "name": "Chenghua Lin"
            }
        ],
        "abstract": "Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have demonstrated aptitude as potential substitutes for human participants in experiments testing psycholinguistic phenomena. However, an understudied question is to what extent models that only have access to vision and text modalities are able to implicitly understand sound-based phenomena via abstract reasoning from orthography and imagery alone. To investigate this, we analyse the ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise a non-arbitrary link between sounds and concepts) as well as their ability to \u201chear\u201d via the interplay of the language and vision modules of open and closed-source multimodal models. We perform multiple experiments, including replicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism tasks and comparing human judgements of linguistic iconicity with that of LLMs. Our results show that VLMs demonstrate varying levels of agreement with human labels, and more task information may be required for VLMs versus their human counterparts for in silico experimentation. We additionally see through higher maximum agreement levels that Magnitude Symbolism is an easier pattern for VLMs to identify than Shape Symbolism, and that an understanding of linguistic iconicity is highly dependent on model size."
    },
    {
        "paperId": "44e86f9d889c7309b1fecd273774b2703ae2fa8a",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Irrelevant Alternatives Bias Large Language Model Hiring Decisions",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2409.15299",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.15299, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-04",
        "authors": [
            {
                "authorId": "103526996",
                "name": "Kremena Valkanova"
            },
            {
                "authorId": "2089815731",
                "name": "Pencho Yordanov"
            }
        ],
        "abstract": "We investigate whether LLMs display a well-known human cognitive bias, the attraction effect, in hiring decisions. The attraction effect occurs when the presence of an inferior candidate makes a superior candidate more appealing, increasing the likelihood of the superior candidate being chosen over a non-dominated competitor. Our study finds consistent and significant evidence of the attraction effect in GPT-3.5 and GPT-4 when they assume the role of a recruiter. Irrelevant attributes of the decoy, such as its gender, further amplify the observed bias. GPT-4 exhibits greater bias variation than GPT-3.5. Our findings remain robust even when warnings against the decoy effect are included and the recruiter role definition is varied."
    },
    {
        "paperId": "21b0e9cf7966f3641f0fed59bae6e11e3cc85930",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.15454, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-23",
        "authors": [
            {
                "authorId": "2320307700",
                "name": "Pengrui Han"
            },
            {
                "authorId": "2297671110",
                "name": "Peiyang Song"
            },
            {
                "authorId": "2322610315",
                "name": "Haofei Yu"
            },
            {
                "authorId": "2261495159",
                "name": "Jiaxuan You"
            }
        ],
        "abstract": "Recent advancements in artificial intelligence have led to the creation of highly capable large language models (LLMs) that can perform tasks in a human-like manner. However, LLMs exhibit only infant-level cognitive abilities in certain areas. One such area is the A-Not-B error, a phenomenon seen in infants where they repeat a previously rewarded behavior despite well-observed changed conditions. This highlights their lack of inhibitory control -- the ability to stop a habitual or impulsive response. In our work, we design a text-based multi-choice QA scenario similar to the A-Not-B experimental settings to systematically test the inhibitory control abilities of LLMs. We found that state-of-the-art LLMs (like Llama3-8b) perform consistently well with in-context learning (ICL) but make errors and show a significant drop of as many as 83.3% in reasoning tasks when the context changes trivially. This suggests that LLMs only have inhibitory control abilities on par with human infants in this regard, often failing to suppress the previously established response pattern during ICL."
    },
    {
        "paperId": "77c12b7565774bc18e420f91336d02ba5bd6309f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.15594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-23",
        "authors": [
            {
                "authorId": "40959535",
                "name": "Bandhav Veluri"
            },
            {
                "authorId": "31634668",
                "name": "Benjamin Peloquin"
            },
            {
                "authorId": "2266439496",
                "name": "Bokai Yu"
            },
            {
                "authorId": "2322505217",
                "name": "Hongyu Gong"
            },
            {
                "authorId": "145254865",
                "name": "Shyamnath Gollakota"
            }
        ],
        "abstract": "Despite broad interest in modeling spoken dialogue agents, most approaches are inherently \u201chalf-duplex\u201d \u2013 restricted to turn-based interaction with responses requiring explicit prompting by the user or implicit tracking of interruption or silence events. Human dialogue, by contrast, is \u201cfull-duplex\u201d allowing for rich synchronicity in the form of quick and dynamic turn-taking, overlapping speech, and backchanneling. Technically, the challenge of achieving full-duplex dialogue with LLMs lies in modeling synchrony as pre-trained LLMs do not have a sense of \u201ctime\u201d. To bridge this gap, we propose Synchronous LLMs for full-duplex spoken dialogue modeling. We design a novel mechanism to integrate time information into Llama3-8b so that they run synchronously with the real-world clock. We also introduce a training recipe that uses 212k hours of synthetic spoken dialogue data generated from text dialogue data to create a model that generates meaningful and natural spoken dialogue, with just 2k hours of real-world spoken dialogue data. Synchronous LLMs outperform state-of-the-art in dialogue meaningfulness while maintaining naturalness. Finally, we demonstrate the model\u2019s ability to participate in full-duplex dialogue by simulating interaction between two agents trained on different datasets, while considering Internet-scale latencies of up to 240 ms."
    },
    {
        "paperId": "8ff7c0df7742646b89172d5d989acc927e667ec9",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.17508, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-26",
        "authors": [
            {
                "authorId": "2201808434",
                "name": "Xun Zhu"
            },
            {
                "authorId": "2286188981",
                "name": "Ying Hu"
            },
            {
                "authorId": "2298270857",
                "name": "Fanbin Mo"
            },
            {
                "authorId": "2302340470",
                "name": "Miao Li"
            },
            {
                "authorId": "2280532239",
                "name": "Ji Wu"
            }
        ],
        "abstract": "Multi-modal large language models (MLLMs) have shown impressive capabilities as a general-purpose interface for various visual and linguistic tasks. However, building a unified MLLM for multi-task learning in the medical field remains a thorny challenge. To mitigate the tug-of-war problem of multi-modal multi-task optimization in MLLMs, recent advances primarily focus on improving the LLM components, while neglecting the connector that bridges the gap between modalities. In this paper, we introduce Uni-Med, a novel medical generalist foundation model which consists of a universal visual feature extraction module, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting from the proposed CMoE that leverages a well-designed router with a mixture of projection experts at the connector, Uni-Med achieves efficient solution to the tug-of-war problem and can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification. To the best of our knowledge, Uni-Med is the first effort to tackle multi-task interference at the connector in MLLMs. Extensive ablation experiments validate the effectiveness of introducing CMoE under any configuration, with up to an average 8% performance gains. We further provide interpretation analysis of the tug-of-war problem from the perspective of gradient optimization and parameter statistics. Compared to previous state-of-the-art medical MLLMs, Uni-Med achieves competitive or superior evaluation metrics on diverse tasks. Code and resources are available at https://github.com/tsinghua-msiip/Uni-Med."
    },
    {
        "paperId": "32bee919a779c601881e8257988e6dabe10383c1",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.17515, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-26",
        "authors": [
            {
                "authorId": "2325204201",
                "name": "Xinlei Wang"
            },
            {
                "authorId": "2325154129",
                "name": "Maike Feng"
            },
            {
                "authorId": "2327174861",
                "name": "Jing Qiu"
            },
            {
                "authorId": "2325724708",
                "name": "Jinjin Gu"
            },
            {
                "authorId": "2145804583",
                "name": "Junhua Zhao"
            }
        ],
        "abstract": "This paper introduces a novel approach that leverages Large Language Models (LLMs) and Generative Agents to enhance time series forecasting by reasoning across both text and time series data. With language as a medium, our method adaptively integrates social events into forecasting models, aligning news content with time series fluctuations to provide richer insights. Specifically, we utilize LLM-based agents to iteratively filter out irrelevant news and employ human-like reasoning to evaluate predictions. This enables the model to analyze complex events, such as unexpected incidents and shifts in social behavior, and continuously refine the selection logic of news and the robustness of the agent's output. By integrating selected news events with time series data, we fine-tune a pre-trained LLM to predict sequences of digits in time series. The results demonstrate significant improvements in forecasting accuracy, suggesting a potential paradigm shift in time series forecasting through the effective utilization of unstructured news data."
    },
    {
        "paperId": "1b2888a60fd95eed021bbdfb754e75f32338db83",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2409.17745",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.17745, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-26",
        "authors": [
            {
                "authorId": "71542370",
                "name": "Nilanjan Sinhababu"
            },
            {
                "authorId": "2290916922",
                "name": "Andrew Parry"
            },
            {
                "authorId": "2280136919",
                "name": "Debasis Ganguly"
            },
            {
                "authorId": "2322992323",
                "name": "Debasis Samanta"
            },
            {
                "authorId": "2322986943",
                "name": "Pabitra Mitra"
            }
        ],
        "abstract": "A supervised ranking model, despite its advantage of being effective, usually involves complex processing - typically multiple stages of task-specific pre-training and fine-tuning. This has motivated researchers to explore simpler pipelines leveraging large language models (LLMs) that are capable of working in a zero-shot manner. However, since zero-shot inference does not make use of a training set of pairs of queries and their relevant documents, its performance is mostly worse than that of supervised models, which are trained on such example pairs. Motivated by the existing findings that training examples generally improve zero-shot performance, in our work, we explore if this also applies to ranking models. More specifically, given a query and a pair of documents, the preference prediction task is improved by augmenting examples of preferences for similar queries from a training set. Our proposed pairwise few-shot ranker demonstrates consistent improvements over the zero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset) retrieval benchmarks. Our method also achieves a close performance to that of a supervised model without requiring any complex training pipeline."
    },
    {
        "paperId": "00c5b7dac0259924f5551c248547ea5b8aafa1d6",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.18433, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-27",
        "authors": [
            {
                "authorId": "52184822",
                "name": "Mucong Ding"
            },
            {
                "authorId": "2238209214",
                "name": "Chenghao Deng"
            },
            {
                "authorId": "2323369199",
                "name": "Jocelyn Choo"
            },
            {
                "authorId": "2283840258",
                "name": "Zichu Wu"
            },
            {
                "authorId": "2258553912",
                "name": "Aakriti Agrawal"
            },
            {
                "authorId": "102604362",
                "name": "Avi Schwarzschild"
            },
            {
                "authorId": "2303390941",
                "name": "Tianyi Zhou"
            },
            {
                "authorId": "2294872863",
                "name": "Tom Goldstein"
            },
            {
                "authorId": "2283766631",
                "name": "John Langford"
            },
            {
                "authorId": "2257161858",
                "name": "Anima Anandkumar"
            },
            {
                "authorId": "2303418156",
                "name": "Furong Huang"
            }
        ],
        "abstract": "While generalization over tasks from easy to hard is crucial to profile language models (LLMs), the datasets with fine-grained difficulty annotations for each problem across a broad range of complexity are still blank. Aiming to address this limitation, we present Easy2Hard-Bench, a consistently formatted collection of 6 benchmark datasets spanning various domains, such as mathematics and programming problems, chess puzzles, and reasoning questions. Each problem within these datasets is annotated with numerical difficulty scores. To systematically estimate problem difficulties, we collect abundant performance data on attempts to each problem by humans in the real world or LLMs on the prominent leaderboard. Leveraging the rich performance data, we apply well-established difficulty ranking systems, such as Item Response Theory (IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to problems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from previous collections by a higher proportion of challenging problems. Through extensive experiments with six state-of-the-art LLMs, we provide a comprehensive analysis of their performance and generalization capabilities across varying levels of difficulty, with the aim of inspiring future research in LLM generalization. The datasets are available at https://huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench."
    },
    {
        "paperId": "f9a0b2be896dc0d12f2c5532ab6ecf8fcd983489",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.18892, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-27",
        "authors": [
            {
                "authorId": "2323436398",
                "name": "Fan Lin"
            },
            {
                "authorId": "2266047190",
                "name": "Shuyi Xie"
            },
            {
                "authorId": "2312747365",
                "name": "Yongwei Dai"
            },
            {
                "authorId": "2323748447",
                "name": "Wenlin Yao"
            },
            {
                "authorId": "2323375229",
                "name": "Tianjiao Lang"
            },
            {
                "authorId": "2323433851",
                "name": "Zishan Xu"
            },
            {
                "authorId": "2265953607",
                "name": "Zhichao Hu"
            },
            {
                "authorId": "2323433835",
                "name": "Xiao Xiao"
            },
            {
                "authorId": "2265797902",
                "name": "Yuhong Liu"
            },
            {
                "authorId": "2323723508",
                "name": "Yu Zhang"
            }
        ],
        "abstract": "As Large Language Models (LLMs) grow increasingly adept at managing complex tasks, the evaluation set must keep pace with these advancements to ensure it remains sufficiently discriminative. Item Discrimination (ID) theory, which is widely used in educational assessment, measures the ability of individual test items to differentiate between high and low performers. Inspired by this theory, we propose an ID-induced prompt synthesis framework for evaluating LLMs to ensure the evaluation set can continually update and refine according to model abilities. Our data synthesis framework prioritizes both breadth and specificity. It can generate prompts that comprehensively evaluate the capabilities of LLMs while revealing meaningful performance differences between models, allowing for effective discrimination of their relative strengths and weaknesses across various tasks and domains. To produce high-quality data, we incorporate a self-correct mechanism into our generalization framework, and develop two models to predict prompt discrimination and difficulty score to facilitate our data synthesis framework, contributing valuable tools to evaluation data synthesis research. We apply our generated data to evaluate five SOTA models. Our data achieves an average score of 51.92, accompanied by a variance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and WizardLM) obtain an average score exceeding 67, with a variance below 3.2. The results demonstrate that the data generated by our framework is more challenging and discriminative compared to previous works. We will release a dataset of over 3,000 carefully crafted prompts to facilitate evaluation research of LLMs."
    },
    {
        "paperId": "49f3a7bdaf103d9611621dd85ab8e33ca9cb6e98",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.20288, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-30",
        "authors": [
            {
                "authorId": "2108590438",
                "name": "Haitao Li"
            },
            {
                "authorId": "2294390078",
                "name": "You Chen"
            },
            {
                "authorId": "2256982003",
                "name": "Qingyao Ai"
            },
            {
                "authorId": "2257126236",
                "name": "Yueyue Wu"
            },
            {
                "authorId": "2127867375",
                "name": "Ruizhe Zhang"
            },
            {
                "authorId": "2260835922",
                "name": "Yiqun Liu"
            }
        ],
        "abstract": "Large language models (LLMs) have made significant progress in natural language processing tasks and demonstrate considerable potential in the legal domain. However, legal applications demand high standards of accuracy, reliability, and fairness. Applying existing LLMs to legal systems without careful evaluation of their potential and limitations could pose significant risks in legal practice. To this end, we introduce a standardized comprehensive Chinese legal benchmark LexEval. This benchmark is notable in the following three aspects: (1) Ability Modeling: We propose a new taxonomy of legal cognitive abilities to organize different tasks. (2) Scale: To our knowledge, LexEval is currently the largest Chinese legal evaluation dataset, comprising 23 tasks and 14,150 questions. (3) Data: we utilize formatted existing datasets, exam datasets and newly annotated datasets by legal experts to comprehensively evaluate the various capabilities of LLMs. LexEval not only focuses on the ability of LLMs to apply fundamental legal knowledge but also dedicates efforts to examining the ethical issues involved in their application. We evaluated 38 open-source and commercial LLMs and obtained some interesting findings. The experiments and findings offer valuable insights into the challenges and potential solutions for developing Chinese legal systems and LLM evaluation pipelines. The LexEval dataset and leaderboard are publicly available at \\url{https://github.com/CSHaitao/LexEval} and will be continuously updated."
    },
    {
        "paperId": "a316bfbc982cded2b854984fe5d62ea2985cc45c",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.00131, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-30",
        "authors": [
            {
                "authorId": "2118971193",
                "name": "Ji Liu"
            },
            {
                "authorId": "2261386155",
                "name": "Jiaxiang Ren"
            },
            {
                "authorId": "2275201759",
                "name": "Ruoming Jin"
            },
            {
                "authorId": "48806049",
                "name": "Zijie Zhang"
            },
            {
                "authorId": "2261386556",
                "name": "Yang Zhou"
            },
            {
                "authorId": "2182066961",
                "name": "Patrick Valduriez"
            },
            {
                "authorId": "2246391421",
                "name": "Dejing Dou"
            }
        ],
        "abstract": "As a promising paradigm to collaboratively train models with decentralized data, Federated Learning (FL) can be exploited to fine-tune Large Language Models (LLMs). While LLMs correspond to huge size, the scale of the training data significantly increases, which leads to tremendous amounts of computation and communication costs. The training data is generally non-Independent and Identically Distributed (non-IID), which requires adaptive data processing within each device. Although Low-Rank Adaptation (LoRA) can significantly reduce the scale of parameters to update in the fine-tuning process, it still takes unaffordable time to transfer the low-rank parameters of all the layers in LLMs. In this paper, we propose a Fisher Information-based Efficient Curriculum Federated Learning framework (FibecFed) with two novel methods, i.e., adaptive federated curriculum learning and efficient sparse parameter update. First, we propose a fisher information-based method to adaptively sample data within each device to improve the effectiveness of the FL fine-tuning process. Second, we dynamically select the proper layers for global aggregation and sparse parameters for local update with LoRA so as to improve the efficiency of the FL fine-tuning process. Extensive experimental results based on 10 datasets demonstrate that FibecFed yields excellent performance (up to 45.35% in terms of accuracy) and superb fine-tuning speed (up to 98.61% faster) compared with 17 baseline approaches)."
    },
    {
        "paperId": "947e39b1e28e6ac948cf8abe7db2a0aeeb50537f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Adaptable Moral Stances of Large Language Models on Sexist Content: Implications for Society and Gender Discourse",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.00175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-09-30",
        "authors": [
            {
                "authorId": "2324868887",
                "name": "Rongchen Guo"
            },
            {
                "authorId": "3163125",
                "name": "Isar Nejadgholi"
            },
            {
                "authorId": "49044849",
                "name": "Hillary Dawkins"
            },
            {
                "authorId": "2243977370",
                "name": "Kathleen C. Fraser"
            },
            {
                "authorId": "2886725",
                "name": "S. Kiritchenko"
            }
        ],
        "abstract": "This work provides an explanatory view of how LLMs can apply moral reasoning to both criticize and defend sexist language. We assessed eight large language models, all of which demonstrated the capability to provide explanations grounded in varying moral perspectives for both critiquing and endorsing views that reflect sexist assumptions. With both human and automatic evaluation, we show that all eight models produce comprehensible and contextually relevant text, which is helpful in understanding diverse views on how sexism is perceived. Also, through analysis of moral foundations cited by LLMs in their arguments, we uncover the diverse ideological perspectives in models\u2019 outputs, with some models aligning more with progressive or conservative views on gender roles and sexism.Based on our observations, we caution against the potential misuse of LLMs to justify sexist language. We also highlight that LLMs can serve as tools for understanding the roots of sexist beliefs and designing well-informed interventions. Given this dual capacity, it is crucial to monitor LLMs and design safety mechanisms for their use in applications that involve sensitive societal topics, such as sexism."
    },
    {
        "paperId": "385d3c4a5c571aafd42e51b7de2e226ddb478be4",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended Responses",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.01246, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-02",
        "authors": [
            {
                "authorId": "2152840093",
                "name": "Xiaotian Lu"
            },
            {
                "authorId": "2301628749",
                "name": "Jiyi Li"
            },
            {
                "authorId": "2243408877",
                "name": "Koh Takeuchi"
            },
            {
                "authorId": "2247886893",
                "name": "Hisashi Kashima"
            }
        ],
        "abstract": "Question answering (QA) tasks have been extensively studied in the field of natural language processing (NLP). Answers to open-ended questions are highly diverse and difficult to quantify, and cannot be simply evaluated as correct or incorrect, unlike close-ended questions with definitive answers. While large language models (LLMs) have demonstrated strong capabilities across various tasks, they exhibit relatively weaker performance in evaluating answers to open-ended questions. In this study, we propose a method that leverages LLMs and the analytic hierarchy process (AHP) to assess answers to open-ended questions. We utilized LLMs to generate multiple evaluation criteria for a question. Subsequently, answers were subjected to pairwise comparisons under each criterion with LLMs, and scores for each answer were calculated in the AHP. We conducted experiments on four datasets using both ChatGPT-3.5-turbo and GPT-4. Our results indicate that our approach more closely aligns with human judgment compared to the four baselines. Additionally, we explored the impact of the number of criteria, variations in models, and differences in datasets on the results."
    },
    {
        "paperId": "00fec110cb998e344af8fa9ddb37aca2caec4c71",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "PCQPR: Proactive Conversational Question Planning with Reflection",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.01363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-02",
        "authors": [
            {
                "authorId": "2119113081",
                "name": "Shasha Guo"
            },
            {
                "authorId": "2294564280",
                "name": "Lizi Liao"
            },
            {
                "authorId": "2155700347",
                "name": "Jing Zhang"
            },
            {
                "authorId": "2287979930",
                "name": "Cuiping Li"
            },
            {
                "authorId": "2191043357",
                "name": "Hong Chen"
            }
        ],
        "abstract": "Conversational Question Generation (CQG) enhances the interactivity of conversational question-answering systems in fields such as education, customer service, and entertainment. However, traditional CQG, focusing primarily on the immediate context, lacks the conversational foresight necessary to guide conversations toward specified conclusions. This limitation significantly restricts their ability to achieve conclusion-oriented conversational outcomes. In this work, we redefine the CQG task as Conclusion-driven Conversational Question Generation (CCQG) by focusing on proactivity, not merely reacting to the unfolding conversation but actively steering it towards a conclusion-oriented question-answer pair. To address this, we propose a novel approach, called Proactive Conversational Question Planning with self-Refining (PCQPR). Concretely, by integrating a planning algorithm inspired by Monte Carlo Tree Search (MCTS) with the analytical capabilities of large language models (LLMs), PCQPR predicts future conversation turns and continuously refines its questioning strategies. This iterative self-refining mechanism ensures the generation of contextually relevant questions strategically devised to reach a specified outcome. Our extensive evaluations demonstrate that PCQPR significantly surpasses existing CQG methods, marking a paradigm shift towards conclusion-oriented conversational question-answering systems."
    },
    {
        "paperId": "f1d7d80f26e469fa05faf85666c071d62561463b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "ACE: A LLM-based Negotiation Coaching System",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.01555, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-02",
        "authors": [
            {
                "authorId": "2258962652",
                "name": "Ryan Shea"
            },
            {
                "authorId": "2323787083",
                "name": "Aymen Kallala"
            },
            {
                "authorId": "2324290984",
                "name": "Xin Lucy Liu"
            },
            {
                "authorId": "2310926866",
                "name": "Michael W. Morris"
            },
            {
                "authorId": "2259619279",
                "name": "Zhou Yu"
            }
        ],
        "abstract": "The growing prominence of LLMs has led to an increase in the development of AI tutoring systems. These systems are crucial in providing underrepresented populations with improved access to valuable education. One important area of education that is unavailable to many learners is strategic bargaining related to negotiation. To address this, we develop a LLM-based Assistant for Coaching nEgotiation (ACE). ACE not only serves as a negotiation partner for users but also provides them with targeted feedback for improvement. To build our system, we collect a dataset of negotiation transcripts between MBA students. These transcripts come from trained negotiators and emulate realistic bargaining scenarios. We use the dataset, along with expert consultations, to design an annotation scheme for detecting negotiation mistakes. ACE employs this scheme to identify mistakes and provide targeted feedback to users. To test the effectiveness of ACE-generated feedback, we conducted a user experiment with two consecutive trials of negotiation and found that it improves negotiation performances significantly compared to a system that doesn\u2019t provide feedback and one which uses an alternative method of providing feedback."
    },
    {
        "paperId": "49eee6c544b1fcf1185c5e1f958be924300b988b",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "MARPLE: A Benchmark for Long-Horizon Inference",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.01926, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-02",
        "authors": [
            {
                "authorId": "2107015633",
                "name": "Emily Jin"
            },
            {
                "authorId": "2253878052",
                "name": "Zhuoyi Huang"
            },
            {
                "authorId": "2278437215",
                "name": "Jan-Philipp Fr\u00e4nken"
            },
            {
                "authorId": "2266440298",
                "name": "Weiyu Liu"
            },
            {
                "authorId": "2301815121",
                "name": "Hannah Cha"
            },
            {
                "authorId": "2299339953",
                "name": "Erik Brockbank"
            },
            {
                "authorId": "2262087889",
                "name": "Sarah A. Wu"
            },
            {
                "authorId": "2248277538",
                "name": "Ruohan Zhang"
            },
            {
                "authorId": "2248288176",
                "name": "Jiajun Wu"
            },
            {
                "authorId": "2264218532",
                "name": "Tobias Gerstenberg"
            }
        ],
        "abstract": "Reconstructing past events requires reasoning across long time horizons. To figure out what happened, we need to use our prior knowledge about the world and human behavior and draw inferences from various sources of evidence including visual, language, and auditory cues. We introduce MARPLE, a benchmark for evaluating long-horizon inference capabilities using multi-modal evidence. Our benchmark features agents interacting with simulated households, supporting vision, language, and auditory stimuli, as well as procedurally generated environments and agent behaviors. Inspired by classic ``whodunit'' stories, we ask AI models and human participants to infer which agent caused a change in the environment based on a step-by-step replay of what actually happened. The goal is to correctly identify the culprit as early as possible. Our findings show that human participants outperform both traditional Monte Carlo simulation methods and an LLM baseline (GPT-4) on this task. Compared to humans, traditional inference models are less robust and performant, while GPT-4 has difficulty comprehending environmental changes. We analyze what factors influence inference performance and ablate different modes of evidence, finding that all modes are valuable for performance. Overall, our experiments demonstrate that the long-horizon, multimodal inference tasks in our benchmark present a challenge to current models."
    },
    {
        "paperId": "1c3f56d41bdaf1f196dd99a9b7fa6c95322df04f",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Towards Comprehensive Detection of Chinese Harmful Memes",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02378, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-03",
        "authors": [
            {
                "authorId": "152319891",
                "name": "Junyu Lu"
            },
            {
                "authorId": "2287795948",
                "name": "Bo Xu"
            },
            {
                "authorId": "2118365880",
                "name": "Xiaokun Zhang"
            },
            {
                "authorId": "2257104312",
                "name": "Hongbo Wang"
            },
            {
                "authorId": "2289122709",
                "name": "Hao Zhu"
            },
            {
                "authorId": "2266077328",
                "name": "Dongyu Zhang"
            },
            {
                "authorId": "2143920912",
                "name": "Liang Yang"
            },
            {
                "authorId": "2257133443",
                "name": "Hongfei Lin"
            }
        ],
        "abstract": "This paper has been accepted in the NeurIPS 2024 D&B Track. Harmful memes have proliferated on the Chinese Internet, while research on detecting Chinese harmful memes significantly lags behind due to the absence of reliable datasets and effective detectors. To this end, we focus on the comprehensive detection of Chinese harmful memes. We construct ToxiCN MM, the first Chinese harmful meme dataset, which consists of 12,000 samples with fine-grained annotations for various meme types. Additionally, we propose a baseline detector, Multimodal Knowledge Enhancement (MKE), incorporating contextual information of meme content generated by the LLM to enhance the understanding of Chinese memes. During the evaluation phase, we conduct extensive quantitative experiments and qualitative analyses on multiple baselines, including LLMs and our MKE. The experimental results indicate that detecting Chinese harmful memes is challenging for existing models while demonstrating the effectiveness of MKE. The resources for this paper are available at https://github.com/DUT-lujunyu/ToxiCN_MM."
    },
    {
        "paperId": "db82caf8d2bab9decfc863e711735839807fa6b8",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Collective Critics for Creative Story Generation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02428, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-03",
        "authors": [
            {
                "authorId": "2324052652",
                "name": "Minwook Bae"
            },
            {
                "authorId": "2324250684",
                "name": "Hyounghun Kim"
            }
        ],
        "abstract": "Generating a long story of several thousand words with narrative coherence using Large Language Models (LLMs) has been a challenging task. Previous research has addressed this challenge by proposing different frameworks that create a story plan and generate a long story based on that plan. However, these frameworks have been mainly focusing on maintaining narrative coherence in stories, often overlooking creativity in story planning and the expressiveness of the stories generated from those plans, which are desirable properties to captivate readers\u2019 interest. In this paper, we propose Collective Critics for Creative Story Generation framework (CritiCS), which is composed of plan refining stage (CrPlan) and story generation stage (CrText), to integrate a collective revision mechanism that promotes those properties into long-form story generation process. Specifically, in each stage, a group of LLM critics and one leader collaborate to incrementally refine drafts of plan and story throughout multiple rounds. Extensive human evaluation shows that the CritiCS can significantly enhance story creativity and reader engagement, while also maintaining narrative coherence. Furthermore, the design of the framework allows active participation from human writers in any role within the critique process, enabling interactive human-machine collaboration in story writing."
    },
    {
        "paperId": "f3b36bbfe20156a336e2fb3ae9736fd49ca68679",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Defining Knowledge: Bridging Epistemology and Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02499, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-03",
        "authors": [
            {
                "authorId": "50110151",
                "name": "Constanza Fierro"
            },
            {
                "authorId": "2311888638",
                "name": "Ruchira Dhar"
            },
            {
                "authorId": "2306968038",
                "name": "Filippos Stamatiou"
            },
            {
                "authorId": "2281953544",
                "name": "Nicolas Garneau"
            },
            {
                "authorId": "2281953769",
                "name": "Anders S\u00f8gaard"
            }
        ],
        "abstract": "Knowledge claims are abundant in the literature on large language models (LLMs); but can we say that GPT-4 truly \u201cknows\u201d the Earth is round? To address this question, we review standard definitions of knowledge in epistemology and we formalize interpretations applicable to LLMs. In doing so, we identify inconsistencies and gaps in how current NLP research conceptualizes knowledge with respect to epistemological frameworks. Additionally, we conduct a survey of 100 professional philosophers and computer scientists to compare their preferences in knowledge definitions and their views on whether LLMs can really be said to know. Finally, we suggest evaluation protocols for testing knowledge in accordance to the most relevant definitions."
    },
    {
        "paperId": "6a4e9df8f0f06713282d1c4d63dc053de450d1e7",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning with Insights from Multi-Agent Collaboration",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02507, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-03",
        "authors": [
            {
                "authorId": "2261186986",
                "name": "Weikang Yuan"
            },
            {
                "authorId": "2324067145",
                "name": "Junjie Cao"
            },
            {
                "authorId": "1695957219",
                "name": "Zhuoren Jiang"
            },
            {
                "authorId": "38753454",
                "name": "Yangyang Kang"
            },
            {
                "authorId": "2110808728",
                "name": "Jun Lin"
            },
            {
                "authorId": "2270983508",
                "name": "Kaisong Song"
            },
            {
                "authorId": "1516806407",
                "name": "Tianqianjin Lin"
            },
            {
                "authorId": "2275255835",
                "name": "Pengwei Yan"
            },
            {
                "authorId": "2060934",
                "name": "Changlong Sun"
            },
            {
                "authorId": "2238387483",
                "name": "Xiaozhong Liu"
            }
        ],
        "abstract": "Large Language Models (LLMs) could struggle to fully understand legal theories and perform complex legal reasoning tasks. In this study, we introduce a challenging task (confusing charge prediction) to better evaluate LLMs' understanding of legal theories and reasoning capabilities. We also propose a novel framework: Multi-Agent framework for improving complex Legal Reasoning capability (MALR). MALR employs non-parametric learning, encouraging LLMs to automatically decompose complex legal tasks and mimic human learning process to extract insights from legal rules, helping LLMs better understand legal theories and enhance their legal reasoning abilities. Extensive experiments on multiple real-world datasets demonstrate that the proposed framework effectively addresses complex reasoning issues in practical scenarios, paving the way for more reliable applications in the legal domain."
    },
    {
        "paperId": "b92ec2ef54e4df2d08cbc66e4dda3e37b6362dbd",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-03",
        "authors": [
            {
                "authorId": "3391272",
                "name": "Ziwei Ji"
            },
            {
                "authorId": "2287917733",
                "name": "Tiezheng Yu"
            },
            {
                "authorId": "2285265406",
                "name": "Yan Xu"
            },
            {
                "authorId": "2314592345",
                "name": "Nayeon Lee"
            },
            {
                "authorId": "2278435713",
                "name": "Albert Q. Jiang"
            },
            {
                "authorId": "2256994781",
                "name": "Alexandre Sablayrolles"
            },
            {
                "authorId": "2319226386",
                "name": "Arthur Men-655"
            },
            {
                "authorId": "2256994975",
                "name": "Chris Bamford"
            },
            {
                "authorId": "2302815701",
                "name": "Devendra Singh"
            },
            {
                "authorId": "2302809975",
                "name": "Diego Chaplot"
            },
            {
                "authorId": "2318358390",
                "name": "laume Lample"
            },
            {
                "authorId": "2318350171",
                "name": "L\u00e9lio Lucile Saulnier"
            },
            {
                "authorId": "2318358843",
                "name": "Renard Lavaud"
            },
            {
                "authorId": "114952298",
                "name": "M. Lachaux"
            },
            {
                "authorId": "2256994779",
                "name": "Pierre Stock"
            },
            {
                "authorId": "1379806208",
                "name": "Teven Le Scao"
            },
            {
                "authorId": "2319259930",
                "name": "Jerry Kang"
            },
            {
                "authorId": "2319227308",
                "name": "Mark W. Bennett"
            },
            {
                "authorId": "2295905607",
                "name": "Devon Carbado"
            },
            {
                "authorId": "2319226400",
                "name": "Pam Casey"
            },
            {
                "authorId": "2310645453",
                "name": "P. Liang"
            },
            {
                "authorId": "2115397918",
                "name": "Chiyu Wu"
            },
            {
                "authorId": "49933077",
                "name": "Louis-philippe Morency"
            },
            {
                "authorId": "21626987",
                "name": "Aman Madaan"
            },
            {
                "authorId": "2261389843",
                "name": "Niket Tandon"
            },
            {
                "authorId": "2302821008",
                "name": "Prakhar Gupta"
            },
            {
                "authorId": null,
                "name": "Skyler Hallinan"
            },
            {
                "authorId": "2267242298",
                "name": "Luyu Gao"
            },
            {
                "authorId": "35823986",
                "name": "Sarah Wiegreffe"
            },
            {
                "authorId": "2268672727",
                "name": "Uri Alon"
            },
            {
                "authorId": "46217681",
                "name": "Nouha Dziri"
            },
            {
                "authorId": "9358910",
                "name": "Shrimai Prabhumoye"
            },
            {
                "authorId": "2315571964",
                "name": "Yiming Yang"
            },
            {
                "authorId": "2302819906",
                "name": "Shashank Gupta"
            },
            {
                "authorId": "3165738",
                "name": "Bodhisattwa Prasad Majumder"
            },
            {
                "authorId": "2273674137",
                "name": "Katherine Hermann"
            },
            {
                "authorId": "2129663",
                "name": "S. Welleck"
            },
            {
                "authorId": "80489277",
                "name": "Amir Yazdan Bakhsh"
            },
            {
                "authorId": "2319225091",
                "name": "ing Bao"
            },
            {
                "authorId": "2275251620",
                "name": "Mo Bavarian"
            },
            {
                "authorId": "2275245092",
                "name": "J. Belgum"
            },
            {
                "authorId": "2309476543",
                "name": "Ir-wan Bello"
            },
            {
                "authorId": "2275245414",
                "name": "Jake Berdine"
            },
            {
                "authorId": "2275245581",
                "name": "Gabriel Bernadett-Shapiro"
            },
            {
                "authorId": "133740015",
                "name": "Christopher Berner"
            },
            {
                "authorId": "2275251674",
                "name": "Lenny Bogdonoff"
            },
            {
                "authorId": "2275246071",
                "name": "Oleg Boiko"
            },
            {
                "authorId": "2275248137",
                "name": "Made-laine Boyd"
            },
            {
                "authorId": "2275245419",
                "name": "Anna-Luisa Brakman"
            },
            {
                "authorId": "2319225043",
                "name": "Greg Brock-724 man"
            },
            {
                "authorId": "2275219628",
                "name": "Tim Brooks"
            },
            {
                "authorId": "2265097787",
                "name": "M. Brundage"
            },
            {
                "authorId": "2146257251",
                "name": "Kevin Button"
            },
            {
                "authorId": "2275157286",
                "name": "Trevor Cai"
            },
            {
                "authorId": "2274782053",
                "name": "Rosie Campbell"
            },
            {
                "authorId": "2275245404",
                "name": "Andrew Cann"
            },
            {
                "authorId": "2275246368",
                "name": "Brittany Carey"
            },
            {
                "authorId": "2275120298",
                "name": "Chelsea Carlson"
            },
            {
                "authorId": "144114446",
                "name": "Rory Carmichael"
            },
            {
                "authorId": "1466431052",
                "name": "Brooke Chan"
            },
            {
                "authorId": "2275545855",
                "name": "Che Chang"
            },
            {
                "authorId": "2057091285",
                "name": "Fotis Chantzis"
            },
            {
                "authorId": "2253841704",
                "name": "Derek Chen"
            },
            {
                "authorId": "2256808607",
                "name": "Su-Hong Chen"
            },
            {
                "authorId": "2275179180",
                "name": "Ruby Chen"
            },
            {
                "authorId": "2275289833",
                "name": "Jason Chen"
            },
            {
                "authorId": "2108828435",
                "name": "Mark Chen"
            },
            {
                "authorId": "1490681878",
                "name": "Benjamin Chess"
            },
            {
                "authorId": "2275251158",
                "name": "Chester Cho"
            },
            {
                "authorId": "2309475703",
                "name": "Hyung Casey Chu"
            },
            {
                "authorId": "2282528643",
                "name": "Won Chung"
            },
            {
                "authorId": "2275231534",
                "name": "Dave Cummings"
            },
            {
                "authorId": "49645091",
                "name": "Jeremiah Currier"
            },
            {
                "authorId": "2276187456",
                "name": "Yunxing Dai"
            },
            {
                "authorId": "2309477435",
                "name": "Tarun Goel"
            },
            {
                "authorId": "2309477420",
                "name": "Gabriel Gogineni"
            },
            {
                "authorId": "2309475753",
                "name": "Rapha Goh"
            },
            {
                "authorId": "2319225953",
                "name": "Jonathan Gontijo-738 Lopes"
            },
            {
                "authorId": "2309478880",
                "name": "Morgan Gordon"
            },
            {
                "authorId": "2309480960",
                "name": "Scott Grafstein"
            },
            {
                "authorId": "2309478956",
                "name": "Ryan Gray"
            },
            {
                "authorId": "2309478103",
                "name": "Joshua Greene"
            },
            {
                "authorId": "2309475937",
                "name": "Shixiang Shane Gross"
            },
            {
                "authorId": "2309896895",
                "name": "Yufei Gu"
            },
            {
                "authorId": "2309804592",
                "name": "Chris Guo"
            },
            {
                "authorId": "2309477491",
                "name": "Jesse Hallacy"
            },
            {
                "authorId": "2309667621",
                "name": "Jeff Han"
            },
            {
                "authorId": "2309475604",
                "name": "Harris Yuchen"
            },
            {
                "authorId": "2310401628",
                "name": "Mike He"
            },
            {
                "authorId": "2309477353",
                "name": "Johannes Heaton"
            },
            {
                "authorId": "2309476458",
                "name": "C. Heidecke"
            },
            {
                "authorId": "2309475602",
                "name": "Alan Hesse"
            },
            {
                "authorId": "2275246148",
                "name": "W. Hickey"
            },
            {
                "authorId": "2309477265",
                "name": "Peter Hickey"
            },
            {
                "authorId": "2309477475",
                "name": "Hoeschele Brandon"
            },
            {
                "authorId": "2309480952",
                "name": "Kenny Houghton"
            },
            {
                "authorId": "2309479202",
                "name": "Shengli Hsu"
            },
            {
                "authorId": "2275777049",
                "name": "Xin Hu"
            },
            {
                "authorId": "2309663799",
                "name": "Joost Hu"
            },
            {
                "authorId": "2309477471",
                "name": "Shantanu Huizinga"
            },
            {
                "authorId": "2309900251",
                "name": "Shawn Jain"
            },
            {
                "authorId": "2309475571",
                "name": "Jain Joanne"
            },
            {
                "authorId": "2309477467",
                "name": "Angela Jang"
            },
            {
                "authorId": "2275172062",
                "name": "Roger Jiang"
            },
            {
                "authorId": "2309830920",
                "name": "Haozhun Jiang"
            },
            {
                "authorId": "2275203081",
                "name": "Denny Jin"
            },
            {
                "authorId": "2309901734",
                "name": "Shino Jin"
            },
            {
                "authorId": "2309481128",
                "name": "Billie Jomoto"
            },
            {
                "authorId": "2309480973",
                "name": "Hee-woo Jonn"
            },
            {
                "authorId": "2309475461",
                "name": "Tomer Jun"
            },
            {
                "authorId": "2309476661",
                "name": "\u0141ukasz Kaftan"
            },
            {
                "authorId": "2309476629",
                "name": "Ali Kaiser"
            },
            {
                "authorId": "2319225089",
                "name": "Ingmar Ka-748 mali"
            },
            {
                "authorId": "2102033721",
                "name": "Kanitscheider"
            },
            {
                "authorId": "2288125393",
                "name": "Nitish Shirish"
            },
            {
                "authorId": "2307452588",
                "name": "Keskar Tabarak"
            },
            {
                "authorId": "2307454011",
                "name": "Logan Khan"
            },
            {
                "authorId": "2307452560",
                "name": "J. Kilpatrick"
            },
            {
                "authorId": "2319227856",
                "name": "Kim"
            },
            {
                "authorId": "2149054292",
                "name": "Christina Kim"
            },
            {
                "authorId": "2275296777",
                "name": "Yongjik Kim"
            },
            {
                "authorId": "2319226271",
                "name": "Jan Hendrik Kirch-751 ner"
            },
            {
                "authorId": "51131802",
                "name": "J. Kiros"
            },
            {
                "authorId": "2146257375",
                "name": "Matthew Knight"
            },
            {
                "authorId": "1485556711",
                "name": "Daniel Kokotajlo"
            },
            {
                "authorId": "2319226462",
                "name": "\u0141ukasz Kondraciuk"
            },
            {
                "authorId": "1666171360",
                "name": "Andrew Kondrich"
            },
            {
                "authorId": "2319225191",
                "name": "Aris Kon-753 stantinidis"
            },
            {
                "authorId": "2275245594",
                "name": "Kyle Kosic"
            },
            {
                "authorId": "2064404342",
                "name": "Gretchen Krueger"
            },
            {
                "authorId": "2275229877",
                "name": "Vishal Kuo"
            },
            {
                "authorId": "2275247085",
                "name": "Michael Lampe"
            },
            {
                "authorId": "2275246287",
                "name": "Ikai Lan"
            },
            {
                "authorId": "2274915115",
                "name": "Teddy Lee"
            },
            {
                "authorId": "2990741",
                "name": "Jan Leike"
            },
            {
                "authorId": "52152632",
                "name": "Jade Leung"
            },
            {
                "authorId": "2319225189",
                "name": "Chak Daniel Levy"
            },
            {
                "authorId": "2319250141",
                "name": "Ming Li"
            },
            {
                "authorId": "2275176375",
                "name": "Rachel Lim"
            },
            {
                "authorId": "2275759230",
                "name": "Molly Lin"
            },
            {
                "authorId": "2253840098",
                "name": "Stephanie Lin"
            },
            {
                "authorId": "1380985420",
                "name": "Ma-teusz Litwin"
            },
            {
                "authorId": "2275248327",
                "name": "Theresa Lopez"
            },
            {
                "authorId": "2257272397",
                "name": "Ryan Lowe"
            },
            {
                "authorId": "2275245628",
                "name": "Patricia Lue"
            },
            {
                "authorId": "119341078",
                "name": "A. Makanju"
            },
            {
                "authorId": "2275245649",
                "name": "Kim Malfacini"
            },
            {
                "authorId": "46430291",
                "name": "Sam Manning"
            },
            {
                "authorId": "14113256",
                "name": "Todor Markov"
            },
            {
                "authorId": "2275245336",
                "name": "Yaniv Markovski"
            },
            {
                "authorId": "2114362965",
                "name": "Bianca Martin"
            },
            {
                "authorId": "2275231822",
                "name": "Katie Mayer"
            },
            {
                "authorId": "2275247045",
                "name": "Andrew Mayne"
            },
            {
                "authorId": "39593364",
                "name": "Bob McGrew"
            },
            {
                "authorId": "2047820455",
                "name": "S. McKinney"
            },
            {
                "authorId": "3028785",
                "name": "Christine McLeavey"
            },
            {
                "authorId": "2274772421",
                "name": "Paul McMillan"
            },
            {
                "authorId": "2275234856",
                "name": "Jake McNeil"
            },
            {
                "authorId": "2275210659",
                "name": "David Medina"
            },
            {
                "authorId": "2275132306",
                "name": "Aalok Mehta"
            },
            {
                "authorId": "10698483",
                "name": "Jacob Menick"
            },
            {
                "authorId": "2275246330",
                "name": "Luke Metz"
            },
            {
                "authorId": "2275252694",
                "name": "An-drey Mishchenko"
            },
            {
                "authorId": "2051714782",
                "name": "Pamela Mishkin"
            },
            {
                "authorId": "2275245453",
                "name": "Vinnie Monaco"
            },
            {
                "authorId": "1404556973",
                "name": "Evan Morikawa"
            },
            {
                "authorId": "3407880",
                "name": "Daniel P. Mossing"
            },
            {
                "authorId": "2319225702",
                "name": "Tong Mu"
            },
            {
                "authorId": "2117715631",
                "name": "Mira Murati"
            },
            {
                "authorId": "147746767",
                "name": "O. Murk"
            },
            {
                "authorId": "2319226404",
                "name": "David M\u00e9ly"
            },
            {
                "authorId": "2319226971",
                "name": "Ashvin Nair"
            },
            {
                "authorId": "7406311",
                "name": "Reiichiro Nakano"
            },
            {
                "authorId": "2057426488",
                "name": "Rajeev Nayak"
            },
            {
                "authorId": "2072676",
                "name": "Arvind Neelakantan"
            },
            {
                "authorId": "2273886618",
                "name": "Richard Ngo"
            },
            {
                "authorId": "2275115983",
                "name": "Hyeonwoo Noh"
            },
            {
                "authorId": "2228518120",
                "name": "Ouyang Long"
            },
            {
                "authorId": "1435765036",
                "name": "Cullen O'Keefe"
            },
            {
                "authorId": "2713380",
                "name": "J. Pachocki"
            },
            {
                "authorId": "34800652",
                "name": "A. Paino"
            },
            {
                "authorId": "2275244652",
                "name": "Joe Palermo"
            },
            {
                "authorId": "2275246178",
                "name": "Ashley Pantuliano"
            },
            {
                "authorId": "2275207240",
                "name": "Carl Ross"
            },
            {
                "authorId": "11150265",
                "name": "Bob Rotsted"
            },
            {
                "authorId": "2275250007",
                "name": "Henri Roussez"
            },
            {
                "authorId": "2319225618",
                "name": "Nick Ry-779 der"
            },
            {
                "authorId": "2252840300",
                "name": "Mario D. Saltarelli"
            },
            {
                "authorId": "2275246803",
                "name": "Ted Sanders"
            },
            {
                "authorId": "2852106",
                "name": "Shibani Santurkar"
            },
            {
                "authorId": "144864359",
                "name": "Girish Sastry"
            },
            {
                "authorId": "2275265666",
                "name": "Heather Schmidt"
            },
            {
                "authorId": "2252874293",
                "name": "David Schnurr"
            },
            {
                "authorId": "2297873691",
                "name": "John Schulman"
            },
            {
                "authorId": "2196579",
                "name": "Daniel Selsam"
            },
            {
                "authorId": "2275244711",
                "name": "Kyla Sheppard"
            },
            {
                "authorId": "102475503",
                "name": "Toki Sherbakov"
            },
            {
                "authorId": "2275246834",
                "name": "Jessica Shieh"
            },
            {
                "authorId": "118335789",
                "name": "Sarah Shoker"
            },
            {
                "authorId": "67311962",
                "name": "Pranav Shyam"
            },
            {
                "authorId": "2700360",
                "name": "Szymon Sidor"
            },
            {
                "authorId": "2064673055",
                "name": "Eric Sigler"
            },
            {
                "authorId": "2151735251",
                "name": "Maddie Simens"
            },
            {
                "authorId": "2275252299",
                "name": "Jordan Sitkin"
            },
            {
                "authorId": "2117680841",
                "name": "Katarina Slama"
            },
            {
                "authorId": "103422608",
                "name": "Ian Sohl"
            },
            {
                "authorId": "2901424",
                "name": "Benjamin Sokolowsky"
            },
            {
                "authorId": "2307592658",
                "name": "Yang Song"
            },
            {
                "authorId": "2275245668",
                "name": "Natalie Staudacher"
            },
            {
                "authorId": "2059411355",
                "name": "Clemens Winter"
            },
            {
                "authorId": "2275244177",
                "name": "Samuel Wolrich"
            },
            {
                "authorId": "2275225207",
                "name": "Hannah Wong"
            },
            {
                "authorId": "2275245771",
                "name": "Lauren Workman"
            },
            {
                "authorId": "2275299848",
                "name": "Sherwin Wu"
            },
            {
                "authorId": "2274911253",
                "name": "Jeff Wu"
            },
            {
                "authorId": "2307456650",
                "name": "Michael Wu"
            },
            {
                "authorId": "2307454769",
                "name": "Kai Xiao"
            },
            {
                "authorId": "2275452480",
                "name": "Tao Xu"
            },
            {
                "authorId": "2275310096",
                "name": "Sarah Yoo"
            },
            {
                "authorId": "2275593618",
                "name": "Kevin Yu"
            },
            {
                "authorId": "2275194186",
                "name": "Qim-ing Yuan"
            },
            {
                "authorId": "2307452791",
                "name": "Wojciech Zaremba"
            },
            {
                "authorId": "49629836",
                "name": "Rowan Zellers"
            },
            {
                "authorId": "2315024566",
                "name": "Chong Zhang"
            },
            {
                "authorId": "2281037751",
                "name": "Marvin Zhang"
            },
            {
                "authorId": "2307453667",
                "name": "Tianhao Shengjia Zhao"
            },
            {
                "authorId": "2298950344",
                "name": "Xu Jiang"
            },
            {
                "authorId": "2275252021",
                "name": "Diogo Almeida"
            },
            {
                "authorId": "2275245962",
                "name": "Carroll L. Wainwright"
            },
            {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
            },
            {
                "authorId": "2319227127",
                "name": "Alex Gray"
            },
            {
                "authorId": "2286540856",
                "name": "Jacob Hilton"
            },
            {
                "authorId": "2151735262",
                "name": "Fraser Kelton"
            },
            {
                "authorId": "2298421583",
                "name": "Luke Miller"
            },
            {
                "authorId": "2220750220",
                "name": "Amanda Askell"
            },
            {
                "authorId": "2930640",
                "name": "P. Welinder"
            },
            {
                "authorId": "2261980896",
                "name": "Paul F. Christiano"
            },
            {
                "authorId": "2197475360",
                "name": "Joon Sung Park"
            },
            {
                "authorId": "2213764034",
                "name": "Joseph C. O\u2019Brien"
            },
            {
                "authorId": "2276794641",
                "name": "C. Cai"
            },
            {
                "authorId": "2319287576",
                "name": "Ringel Morris"
            },
            {
                "authorId": "2256995425",
                "name": "Percy Liang"
            },
            {
                "authorId": "2319226111",
                "name": "Michael S. Bern-814"
            },
            {
                "authorId": "38909097",
                "name": "Alec Radford"
            },
            {
                "authorId": "2285784924",
                "name": "Karthik Narasimhan"
            },
            {
                "authorId": "2887364",
                "name": "Tim Salimans"
            },
            {
                "authorId": "2302559920",
                "name": "Rachel Rudinger"
            },
            {
                "authorId": "2300343",
                "name": "Jason Naradowsky"
            },
            {
                "authorId": "2319225916",
                "name": "Brian Leonard"
            },
            {
                "authorId": "1387983862",
                "name": "Nisan Stiennon"
            },
            {
                "authorId": "2319225774",
                "name": "Ryan Ziegler"
            },
            {
                "authorId": "2314165049",
                "name": "Chelsea Lowe"
            },
            {
                "authorId": "2314160468",
                "name": "Alec Voss"
            },
            {
                "authorId": "2289348718",
                "name": "Radford"
            },
            {
                "authorId": "2698777",
                "name": "Dario Amodei"
            },
            {
                "authorId": "2319225540",
                "name": "Christiano. 2020. Learn-842"
            },
            {
                "authorId": "2319580593",
                "name": "Tony Sun"
            },
            {
                "authorId": "146072982",
                "name": "Andrew Gaut"
            },
            {
                "authorId": "148149462",
                "name": "Shirlyn Tang"
            },
            {
                "authorId": "2154731574",
                "name": "Yuxin Huang"
            },
            {
                "authorId": "2288124187",
                "name": "Mai ElSherief"
            },
            {
                "authorId": "2311580116",
                "name": "Jie Zhao"
            },
            {
                "authorId": "2319225642",
                "name": "Diba Mirza"
            },
            {
                "authorId": "2319226119",
                "name": "Kai-Wei Belding"
            },
            {
                "authorId": "2319225109",
                "name": "Chang William"
            },
            {
                "authorId": "2319563352",
                "name": "Yang Wang"
            },
            {
                "authorId": "2165227666",
                "name": "Yixin Wan"
            },
            {
                "authorId": "2258548444",
                "name": "George Pu"
            },
            {
                "authorId": "2261454711",
                "name": "Jiao Sun"
            },
            {
                "authorId": "31099365",
                "name": "Aparna Garimella"
            },
            {
                "authorId": "2257127887",
                "name": "Kai-Wei Chang"
            },
            {
                "authorId": "2285475879",
                "name": "Nanyun Peng"
            },
            {
                "authorId": "2319226454",
                "name": "\u201ckelly"
            }
        ],
        "abstract": "As Large Language Models (LLMs) continue to evolve, they are increasingly being employed in numerous studies to simulate societies and execute diverse social tasks. However, LLMs are susceptible to societal biases due to their exposure to human-generated data. Given that LLMs are being used to gain insights into various societal aspects, it is essential to mitigate these biases. To that end, our study investigates the presence of implicit gender biases in multi-agent LLM interactions and proposes two strategies to mitigate these biases. We begin by creating a dataset of scenarios where implicit gender biases might arise, and subsequently develop a metric to assess the presence of biases. Our empirical analysis reveals that LLMs generate outputs characterized by strong implicit bias associations (>= 50\\% of the time). Furthermore, these biases tend to escalate following multi-agent interactions. To mitigate them, we propose two strategies: self-reflection with in-context examples (ICE); and supervised fine-tuning. Our research demonstrates that both methods effectively mitigate implicit biases, with the ensemble of fine-tuning and self-reflection proving to be the most successful."
    },
    {
        "paperId": "9d23568fce5806937592a16f0dc598f5872f89e8",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02631, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-03",
        "authors": [
            {
                "authorId": "2257018970",
                "name": "Tianxiang Hu"
            },
            {
                "authorId": "2257107422",
                "name": "Pei Zhang"
            },
            {
                "authorId": "2302785886",
                "name": "Baosong Yang"
            },
            {
                "authorId": "2109935759",
                "name": "Jun Xie"
            },
            {
                "authorId": "2302781657",
                "name": "Derek F. Wong"
            },
            {
                "authorId": "2257198451",
                "name": "Rui Wang"
            }
        ],
        "abstract": "Achieving consistent high-quality machine translation (MT) across diverse domains remains a significant challenge, primarily due to the limited and imbalanced parallel training data available in various domains. While large language models (LLMs) have demonstrated impressive general understanding and generation abilities, their potential in multi-domain MT is under-explored. We establish a comprehensive benchmark for multi-domain translation, featuring 25 German$\\Leftrightarrow$English and 22 Chinese$\\Leftrightarrow$English test sets respectively covering 15 domains. Our evaluation of prominent LLMs reveals a discernible performance gap against traditional MT systems, highlighting domain overfitting and catastrophic forgetting issues after fine-tuning on domain-limited corpora. To mitigate this, we propose a domain Chain of Thought (CoT) fine-tuning technique that utilizes the intrinsic multi-domain intelligence of LLMs to improve translation performance. This method inspires the LLM to perceive domain information from the source text, which then serves as a helpful hint to guide the translation process. Despite being trained on a small dataset of four domains, our CoT fine-tune approach achieves notable enhancements in translation accuracy and domain robustness than traditional fine-tuning, as evidenced by an average 1.53 BLEU score increase in over 20 German$\\rightarrow$English distinct out-of-domain tests."
    },
    {
        "paperId": "b3261c5e1b8b132db2bbab205c4e15f576c7fb07",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Hate Personified: Investigating the role of LLMs in content moderation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02657, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-03",
        "authors": [
            {
                "authorId": "36715403",
                "name": "Sarah Masud"
            },
            {
                "authorId": "2313724159",
                "name": "Sahajpreet Singh"
            },
            {
                "authorId": "2113189",
                "name": "Viktor Hangya"
            },
            {
                "authorId": "2266753399",
                "name": "Alexander Fraser"
            },
            {
                "authorId": "2273556645",
                "name": "Tanmoy Chakraborty"
            }
        ],
        "abstract": "For subjective tasks such as hate detection, where people perceive hate differently, the Large Language Model\u2019s (LLM) ability to represent diverse groups is unclear. By including additional context in prompts, we comprehensively analyze LLM\u2019s sensitivity to geographical priming, persona attributes, and numerical information to assess how well the needs of various groups are reflected. Our findings on two LLMs, five languages, and six datasets reveal that mimicking persona-based attributes leads to annotation variability. Meanwhile, incorporating geographical signals leads to better regional alignment. We also find that the LLMs are sensitive to numerical anchors, indicating the ability to leverage community-based flagging efforts and exposure to adversaries. Our work provides preliminary guidelines and highlights the nuances of applying LLMs in culturally sensitive cases."
    },
    {
        "paperId": "681c02ec8036ecf87499b1f8e5410a532eed0f57",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-04",
        "authors": [
            {
                "authorId": "2294681427",
                "name": "Kyuyoung Kim"
            },
            {
                "authorId": "2324581861",
                "name": "Ah Jeong Seo"
            },
            {
                "authorId": "2324788343",
                "name": "Hao Liu"
            },
            {
                "authorId": "2261688831",
                "name": "Jinwoo Shin"
            },
            {
                "authorId": "3436470",
                "name": "Kimin Lee"
            }
        ],
        "abstract": "Large language models (LLMs) fine-tuned with alignment techniques, such as reinforcement learning from human feedback, have been instrumental in developing some of the most capable AI systems to date. Despite their success, existing methods typically rely on simple binary labels, such as those indicating preferred outputs in pairwise preferences, which fail to capture the subtle differences in relative quality between pairs. To address this limitation, we introduce an approach called Margin Matching Preference Optimization (MMPO), which incorporates relative quality margins into optimization, leading to improved LLM policies and reward models. Specifically, given quality margins in pairwise preferences, we design soft target probabilities based on the Bradley-Terry model, which are then used to train models with the standard cross-entropy objective. Experiments with both human and AI feedback data demonstrate that MMPO consistently outperforms baseline methods, often by a substantial margin, on popular benchmarks including MT-bench and RewardBench. Notably, the 7B model trained with MMPO achieves state-of-the-art performance on RewardBench as of June 2024, outperforming other models of the same scale. Our analysis also shows that MMPO is more robust to overfitting, leading to better-calibrated models."
    },
    {
        "paperId": "c7ef60df82534b63a3577203affc6c9e0feb101f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Kiss up, Kick down: Exploring Behavioral Changes in Multi-modal Large Language Models with Assigned Visual Personas",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03181, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-04",
        "authors": [
            {
                "authorId": "2157738252",
                "name": "Seungjong Sun"
            },
            {
                "authorId": "2288760348",
                "name": "Eungu Lee"
            },
            {
                "authorId": "2304213105",
                "name": "Seo Yeon Baek"
            },
            {
                "authorId": "2324683741",
                "name": "Seunghyun Hwang"
            },
            {
                "authorId": "2288349415",
                "name": "Wonbyung Lee"
            },
            {
                "authorId": "1658326803",
                "name": "Dongyan Nan"
            },
            {
                "authorId": "2287921790",
                "name": "Bernard J. Jansen"
            },
            {
                "authorId": "2261063844",
                "name": "Jang Hyun Kim"
            }
        ],
        "abstract": "This study is the first to explore whether multi-modal large language models (LLMs) can align their behaviors with visual personas, addressing a significant gap in the literature that predominantly focuses on text-based personas. We developed a novel dataset of 5K fictional avatar images for assignment as visual personas to LLMs, and analyzed their negotiation behaviors based on the visual traits depicted in these images, with a particular focus on aggressiveness. The results indicate that LLMs assess the aggressiveness of images in a manner similar to humans and output more aggressive negotiation behaviors when prompted with an aggressive visual persona. Interestingly, the LLM exhibited more aggressive negotiation behaviors when the opponent\u2019s image appeared less aggressive than their own, and less aggressive behaviors when the opponent\u2019s image appeared more aggressive."
    },
    {
        "paperId": "4a9ac49dc374e1f873ff7d993e3afe50097195fc",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "What do Large Language Models Need for Machine Translation Evaluation?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03278, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-04",
        "authors": [
            {
                "authorId": "2165228901",
                "name": "Shenbin Qian"
            },
            {
                "authorId": "2269148551",
                "name": "Archchana Sindhujan"
            },
            {
                "authorId": "2316874047",
                "name": "Minnie Kabra"
            },
            {
                "authorId": "2920443",
                "name": "Diptesh Kanojia"
            },
            {
                "authorId": "2127734199",
                "name": "Constantin Oruasan"
            },
            {
                "authorId": "2264580525",
                "name": "Tharindu Ranasinghe"
            },
            {
                "authorId": "2273692193",
                "name": "Fr\u00e9d\u00e9ric Blain"
            }
        ],
        "abstract": "Leveraging large language models (LLMs) for various natural language processing tasks has led to superlative claims about their performance. For the evaluation of machine translation (MT), existing research shows that LLMs are able to achieve results comparable to fine-tuned multilingual pre-trained language models. In this paper, we explore what translation information, such as the source, reference, translation errors and annotation guidelines, is needed for LLMs to evaluate MT quality. In addition, we investigate prompting techniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for eight language pairs covering high-, medium- and low-resource languages, leveraging varying LLM variants. Our findings indicate the importance of reference translations for an LLM-based evaluation. While larger models do not necessarily fare better, they tend to benefit more from CoT prompting, than smaller models. We also observe that LLMs do not always provide a numerical score when generating evaluations, which poses a question on their reliability for the task. Our work presents a comprehensive analysis for resource-constrained and training-less LLM-based evaluation of machine translation. We release the accrued prompt templates, code and data publicly for reproducibility."
    },
    {
        "paperId": "277d0bd64fd9ef929c66d3d94904d6a9e6840cf8",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-04",
        "authors": [
            {
                "authorId": "2324589944",
                "name": "Min-Hsuan Yeh"
            },
            {
                "authorId": "2324592142",
                "name": "Ruyuan Wan"
            },
            {
                "authorId": "2324775647",
                "name": "Ting-Hao 'Kenneth' Huang"
            }
        ],
        "abstract": "Detecting logical fallacies in texts can help users spot argument flaws, but automating this detection is not easy. Manually annotating fallacies in large-scale, real-world text data to create datasets for developing and validating detection models is costly. This paper introduces CoCoLoFa, the largest known logical fallacy dataset, containing 7,706 comments for 648 news articles, with each comment labeled for fallacy presence and type. We recruited 143 crowd workers to write comments embodying specific fallacy types (e.g., slippery slope) in response to news articles. Recognizing the complexity of this writing task, we built an LLM-powered assistant into the workers\u2019 interface to aid in drafting and refining their comments. Experts rated the writing quality and labeling validity of CoCoLoFa as high and reliable. BERT-based models fine-tuned using CoCoLoFa achieved the highest fallacy detection (F1=0.86) and classification (F1=0.87) performance on its test set, outperforming the state-of-the-art LLMs. Our work shows that combining crowdsourcing and LLMs enables us to more effectively construct datasets for complex linguistic phenomena that crowd workers find challenging to produce on their own."
    },
    {
        "paperId": "e96a40c3cc4fca4731c7e3df413335388370b6f4",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Is Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03466, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-04",
        "authors": [
            {
                "authorId": "2161343118",
                "name": "Helena Bonaldi"
            },
            {
                "authorId": "2322870334",
                "name": "Greta Damo"
            },
            {
                "authorId": "2215623665",
                "name": "Nicol\u00e1s Benjam\u00edn Ocampo"
            },
            {
                "authorId": "2258028581",
                "name": "Elena Cabrio"
            },
            {
                "authorId": "1725656",
                "name": "S. Villata"
            },
            {
                "authorId": "2291136930",
                "name": "Marco Guerini"
            }
        ],
        "abstract": "The potential effectiveness of counterspeech as a hate speech mitigation strategy is attracting increasing interest in the NLG research community, particularly towards the task of automatically producing it. However, automatically generated responses often lack the argumentative richness which characterises expert-produced counterspeech. In this work, we focus on two aspects of counterspeech generation to produce more cogent responses. First, by investigating the tension between helpfulness and harmlessness of LLMs, we test whether the presence of safety guardrails hinders the quality of the generations. Secondly, we assess whether attacking a specific component of the hate speech results in a more effective argumentative strategy to fight online hate. By conducting an extensive human and automatic evaluation, we show how the presence of safety guardrails can be detrimental also to a task that inherently aims at fostering positive social interactions. Moreover, our results show that attacking a specific component of the hate speech, and in particular its implicit negative stereotype and its hateful parts, leads to higher-quality generations."
    },
    {
        "paperId": "261c76cbd1cc6adaf27589091a8720b5ee1cf9d8",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Table Question Answering for Low-resourced Indic Languages",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-04",
        "authors": [
            {
                "authorId": "1453672728",
                "name": "Vaishali Pal"
            },
            {
                "authorId": "2314138767",
                "name": "Evangelos Kanoulas"
            },
            {
                "authorId": "2267295426",
                "name": "Andrew Yates"
            },
            {
                "authorId": "2265490493",
                "name": "M. D. Rijke"
            }
        ],
        "abstract": "TableQA is the task of answering questions over tables of structured information, returning individual cells or tables as output. TableQA research has focused primarily on high-resource languages, leaving medium- and low-resource languages with little progress due to scarcity of annotated data and neural models. We address this gap by introducing a fully automatic large-scale tableQA data generation process for low-resource languages with limited budget. We incorporate our data generation method on two Indic languages, Bengali and Hindi, which have no tableQA datasets or models. TableQA models trained on our large-scale datasets outperform state-of-the-art LLMs. We further study the trained models on different aspects, including mathematical reasoning capabilities and zero-shot cross-lingual transfer. Our work is the first on low-resource tableQA focusing on scalable data generation and evaluation procedures. Our proposed data generation method can be applied to any low-resource language with a web presence. We release datasets, models, and code (https://github.com/kolk/Low-Resource-TableQA-Indic-languages)."
    },
    {
        "paperId": "eb789ac5b8cd4af9ce095917f767a9f73de8b5b1",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "ECON: On the Detection and Resolution of Evidence Conflicts",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.04068, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-05",
        "authors": [
            {
                "authorId": "2308160066",
                "name": "Jiayang Cheng"
            },
            {
                "authorId": "2216598559",
                "name": "Chunkit Chan"
            },
            {
                "authorId": "2324793146",
                "name": "Qianqian Zhuang"
            },
            {
                "authorId": "2260342601",
                "name": "Lin Qiu"
            },
            {
                "authorId": "2257868315",
                "name": "Tianhang Zhang"
            },
            {
                "authorId": "2136108329",
                "name": "Tengxiao Liu"
            },
            {
                "authorId": "2258804099",
                "name": "Yangqiu Song"
            },
            {
                "authorId": "2260818095",
                "name": "Yue Zhang"
            },
            {
                "authorId": "2217850099",
                "name": "Pengfei Liu"
            },
            {
                "authorId": "2260691874",
                "name": "Zheng Zhang"
            }
        ],
        "abstract": "The rise of large language models (LLMs) has significantly influenced the quality of information in decision-making systems, leading to the prevalence of AI-generated content and challenges in detecting misinformation and managing conflicting information, or \u201cinter-evidence conflicts.\u201d This study introduces a method for generating diverse, validated evidence conflicts to simulate real-world misinformation scenarios. We evaluate conflict detection methods, including Natural Language Inference (NLI) models, factual consistency (FC) models, and LLMs, on these conflicts (RQ1) and analyze LLMs\u2019 conflict resolution behaviors (RQ2). Our key findings include: (1) NLI and LLM models exhibit high precision in detecting answer conflicts, though weaker models suffer from low recall; (2) FC models struggle with lexically similar answer conflicts, while NLI and LLM models handle these better; and (3) stronger models like GPT-4 show robust performance, especially with nuanced conflicts. For conflict resolution, LLMs often favor one piece of conflicting evidence without justification and rely on internal knowledge if they have prior beliefs."
    },
    {
        "paperId": "a7a452322c6a0f763f5624445a1fdf55539c8717",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "GlobeSumm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.04087, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-05",
        "authors": [
            {
                "authorId": "2216505879",
                "name": "Yangfan Ye"
            },
            {
                "authorId": "51056442",
                "name": "Xiachong Feng"
            },
            {
                "authorId": "2674998",
                "name": "Xiaocheng Feng"
            },
            {
                "authorId": "2265878959",
                "name": "Weitao Ma"
            },
            {
                "authorId": "2324875482",
                "name": "Libo Qin"
            },
            {
                "authorId": "2284642281",
                "name": "Dongliang Xu"
            },
            {
                "authorId": "2287799372",
                "name": "Qing Yang"
            },
            {
                "authorId": "2291207815",
                "name": "Hongtao Liu"
            },
            {
                "authorId": "2257004102",
                "name": "Bing Qin"
            }
        ],
        "abstract": "News summarization in today\u2019s global scene can be daunting with its flood of multilingual content and varied viewpoints from different sources. However, current studies often neglect such real-world scenarios as they tend to focus solely on either single-language or single-document tasks. To bridge this gap, we aim to unify Multi-lingual, Cross-lingual and Multi-document Summarization into a novel task, i.e., MCMS, which encapsulates the real-world requirements all-in-one. Nevertheless, the lack of a benchmark inhibits researchers from adequately studying this invaluable problem. To tackle this, we have meticulously constructed the GLOBESUMM dataset by first collecting a wealth of multilingual news reports and restructuring them into event-centric format. Additionally, we introduce the method of protocol-guided prompting for high-quality and cost-effective reference annotation. In MCMS, we also highlight the challenge of conflicts between news reports, in addition to the issues of redundancies and omissions, further enhancing the complexity of GLOBESUMM. Through extensive experimental analysis, we validate the quality of our dataset and elucidate the inherent challenges of the task. We firmly believe that GLOBESUMM, given its challenging nature, will greatly contribute to the multilingual communities and the evaluation of LLMs."
    },
    {
        "paperId": "4046475556d334a61612c63622a40676907c7b5d",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.04699, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-07",
        "authors": [
            {
                "authorId": "2324792802",
                "name": "Alexander S. Choi"
            },
            {
                "authorId": "66514001",
                "name": "Syeda Sabrina Akter"
            },
            {
                "authorId": "2324814604",
                "name": "JP Singh"
            },
            {
                "authorId": "2273733474",
                "name": "Antonios Anastasopoulos"
            }
        ],
        "abstract": "Large Language Models (LLMs) have shown capabilities close to human performance in various analytical tasks, leading researchers to use them for time and labor-intensive analyses. However, their capability to handle highly specialized and open-ended tasks in domains like policy studies remains in question. This paper investigates the efficiency and accuracy of LLMs in specialized tasks through a structured user study focusing on Human-LLM partnership. The study, conducted in two stages\u2014Topic Discovery and Topic Assignment\u2014integrates LLMs with expert annotators to observe the impact of LLM suggestions on what is usually human-only analysis. Results indicate that LLM-generated topic lists have significant overlap with human generated topic lists, with minor hiccups in missing document-specific topics. However, LLM suggestions may significantly improve task completion speed, but at the same time introduce anchoring bias, potentially affecting the depth and nuance of the analysis, raising a critical question about the trade-off between increased efficiency and the risk of biased analysis."
    },
    {
        "paperId": "656b36f00a2f6ef98a8032a0beb02c9504a292be",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Precise Model Benchmarking with Only a Few Observations",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.05222, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-07",
        "authors": [
            {
                "authorId": "2257182823",
                "name": "Riccardo Fogliato"
            },
            {
                "authorId": "2257183130",
                "name": "Pratik Patil"
            },
            {
                "authorId": "20917300",
                "name": "Nil-Jana Akpinar"
            },
            {
                "authorId": "2305682013",
                "name": "Mathew Monfort"
            }
        ],
        "abstract": "How can we precisely estimate a large language model\u2019s (LLM) accuracy on questions belonging to a specific topic within a larger question-answering dataset? The standard direct estimator, which averages the model\u2019s accuracy on the questions in each subgroup, may exhibit high variance for subgroups (topics) with small sample sizes. Synthetic regression modeling, which leverages the model\u2019s accuracy on questions about other topics, may yield biased estimates that are too unreliable for large subgroups. We prescribe a simple yet effective solution: an empirical Bayes (EB) estimator that balances direct and regression estimates for each subgroup separately, improving the precision of subgroup-level estimates of model performance. Our experiments on multiple datasets show that this approach consistently provides more precise estimates of the LLM performance compared to the direct and regression approaches, achieving substantial reductions in the mean squared error. Confidence intervals for EB estimates also have near-nominal coverage and are narrower compared to those for the direct estimator. Additional experiments on tabular and vision data validate the benefits of this EB approach."
    },
    {
        "paperId": "03c57348ee00fb28ce20f2e422d49ecd20d1e91d",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Grounding Partially-Defined Events in Multimodal Data",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.05267, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-07",
        "authors": [
            {
                "authorId": "2187060946",
                "name": "Kate Sanders"
            },
            {
                "authorId": "46218926",
                "name": "Reno Kriz"
            },
            {
                "authorId": "2287942787",
                "name": "David Etter"
            },
            {
                "authorId": "2324785460",
                "name": "Hannah Recknor"
            },
            {
                "authorId": "2352036309",
                "name": "Alexander Martin"
            },
            {
                "authorId": "2324786521",
                "name": "Cameron Carpenter"
            },
            {
                "authorId": "2324832157",
                "name": "Jingyang Lin"
            },
            {
                "authorId": "2292194313",
                "name": "Benjamin Van Durme"
            }
        ],
        "abstract": "How are we able to learn about complex current events just from short snippets of video? While natural language enables straightforward ways to represent under-specified, partially observable events, visual data does not facilitate analogous methods and, consequently, introduces unique challenges in event understanding. With the growing prevalence of vision-capable AI agents, these systems must be able to model events from collections of unstructured video data. To tackle robust event modeling in multimodal settings, we introduce a multimodal formulation for partially-defined events and cast the extraction of these events as a three-stage span retrieval task. We propose a corresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours of densely annotated current event videos and 1,168 text documents, containing 22.8K labeled event-centric entities. We propose a collection of LLM-driven approaches to the task of multimodal event analysis, and evaluate them on MultiVENT-G. Results illustrate the challenges that abstract event understanding poses and demonstrates promise in event-centric video-language systems."
    },
    {
        "paperId": "fb2d19fbeb8f7caa89808895066d3c0dbd4dad32",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "The Mystery of Compositional Generalization in Graph-based Generative Commonsense Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.06272, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-08",
        "authors": [
            {
                "authorId": "153000866",
                "name": "Xiyan Fu"
            },
            {
                "authorId": "2240526245",
                "name": "Anette Frank"
            }
        ],
        "abstract": "While LLMs have emerged as performant architectures for reasoning tasks, their compositional generalization capabilities have been questioned. In this work, we introduce a Compositional Generalization Challenge for Graph-based Commonsense Reasoning (CGGC) that goes beyond previous evaluations that are based on sequences or tree structures - and instead involves a reasoning graph: It requires models to generate a natural sentence based on given concepts and a corresponding reasoning graph, where the presented graph involves a previously unseen combination of relation types. To master this challenge, models need to learn how to reason over relation tupels within the graph, and how to compose them when conceptualizing a verbalization. We evaluate seven well-known LLMs using in-context learning and find that performant LLMs still struggle in compositional generalization. We investigate potential causes of this gap by analyzing the structures of reasoning graphs, and find that different structures present varying levels of difficulty for compositional generalization. Arranging the order of demonstrations according to the structures' difficulty shows that organizing samples in an easy-to-hard schema enhances the compositional generalization ability of LLMs."
    },
    {
        "paperId": "bc844991517ac709582ce48d35dfb1e79a49766f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.06524, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-09",
        "authors": [
            {
                "authorId": "3192608",
                "name": "Maharshi Gor"
            },
            {
                "authorId": "2200167546",
                "name": "Hal Daum'e"
            },
            {
                "authorId": "2325732446",
                "name": "Tianyi Zhou"
            },
            {
                "authorId": "2260114875",
                "name": "Jordan L. Boyd-Graber"
            }
        ],
        "abstract": "Recent advancements of large language models (LLMs)have led to claims of AI surpassing humansin natural language processing NLP tasks such as textual understanding and reasoning.%This work investigates these assertions by introducingCAIMIRA, a novel framework rooted in item response theory IRTthat enables quantitative assessment and comparison of problem-solving abilities inquestion-answering QA agents.%Through analysis of over 300,000 responses from ~ 70 AI systemsand 155 humans across thousands of quiz questions, CAIMIRA uncovers distinctproficiency patterns in knowledge domains and reasoning skills. %Humans outperform AI systems in knowledge-grounded abductive and conceptual reasoning,while state-of-the-art LLMs like GPT-4 Turbo and Llama-3-70B demonstrate superior performance ontargeted information retrieval and fact-based reasoning, particularly when information gapsare well-defined and addressable through pattern matching or data retrieval.%These findings identify key areas for future QA tasks and model development,highlighting the critical need for questions that not only challengehigher-order reasoning and scientific thinking, but also demand nuanced linguisticand cross-contextual application."
    },
    {
        "paperId": "8f2da3a0775ec12983308c56a84cee38f02e799c",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.06733, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-09",
        "authors": [
            {
                "authorId": "2282505548",
                "name": "Qi Chen"
            },
            {
                "authorId": "2306989871",
                "name": "Bowen Zhang"
            },
            {
                "authorId": "2325192450",
                "name": "Gang Wang"
            },
            {
                "authorId": "2325114200",
                "name": "Qi Wu"
            }
        ],
        "abstract": "While advancements in NLP have significantly improved the performance of Large Language Models (LLMs) on tasks requiring vertical thinking, their lateral thinking capabilities remain under-explored and challenging to measure due to the complexity of assessing creative thought processes and the scarcity of relevant data. To address these challenges, we introduce SPLAT, a benchmark leveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs. This benchmark, containing 975 graded situation puzzles across three difficulty levels, employs a new multi-turn player-judge framework instead of the traditional model-based evaluation, which often necessitates a stronger evaluation model. This framework simulates an interactive game where the model (player) asks the evaluation model (judge) questions about an incomplete story to infer the full scenario. The judge answers based on a detailed reference scenario or evaluates if the player's predictions align with the reference one. This approach lessens dependence on more robust evaluation models, enabling the assessment of state-of-the-art LLMs. The experiments demonstrate that a robust evaluation model, such as WizardLM-2, closely matches human judgements in both intermediate question-answering and final scenario accuracy, achieving over 80% agreement-similar to the agreement levels among humans. Furthermore, applying data and reasoning processes from our benchmark to other lateral thinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to performance enhancements. This suggests that our benchmark effectively evaluates and elicits the lateral thinking abilities of LLMs. Code is available at: https://github.com/chenqi008/LateralThinking."
    },
    {
        "paperId": "c01038fa4cd80c9a14541d3ece503737780fbe6a",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.07018, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-09",
        "authors": [
            {
                "authorId": "2142035704",
                "name": "Chen Jian"
            },
            {
                "authorId": "2261696406",
                "name": "Kai Yang"
            },
            {
                "authorId": "2055181125",
                "name": "Yang Jiao"
            }
        ],
        "abstract": "Out-of-Distribution (OOD) generalization in machine learning is a burgeoning area of study. Its primary goal is to enhance the adaptability and resilience of machine learning models when faced with new, unseen, and potentially adversarial data that significantly diverges from their original training datasets. In this paper, we investigate time series OOD generalization via pre-trained Large Language Models (LLMs). We first propose a novel \\textbf{T}ri-level learning framework for \\textbf{T}ime \\textbf{S}eries \\textbf{O}OD generalization, termed TTSO, which considers both sample-level and group-level uncertainties. This formula offers a fresh theoretic perspective for formulating and analyzing OOD generalization problem. In addition, we provide a theoretical analysis to justify this method is well motivated. We then develop a stratified localization algorithm tailored for this tri-level optimization problem, theoretically demonstrating the guaranteed convergence of the proposed algorithm. Our analysis also reveals that the iteration complexity to obtain an $\\epsilon$-stationary point is bounded by O($\\frac{1}{\\epsilon^{2}}$). Extensive experiments on real-world datasets have been conducted to elucidate the effectiveness of the proposed method."
    },
    {
        "paperId": "6f5c24e67cae1c93ad31c17046a9f0f621b7cde9",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.07054, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-09",
        "authors": [
            {
                "authorId": "2325105905",
                "name": "Weichuan Wang"
            },
            {
                "authorId": "2273744210",
                "name": "Zhaoyi Li"
            },
            {
                "authorId": "2266241113",
                "name": "Defu Lian"
            },
            {
                "authorId": "2260852073",
                "name": "Chen Ma"
            },
            {
                "authorId": "2285564983",
                "name": "Linqi Song"
            },
            {
                "authorId": "2261250492",
                "name": "Ying Wei"
            }
        ],
        "abstract": "Large Language Models (LLMs) have recently revolutionized the NLP field, while they still fall short in some specific down-stream tasks. In the work, we focus on utilizing LLMs to perform machine translation, where we observe that two patterns of errors frequently occur and drastically affect the translation quality: language mismatch and repetition. The work sets out to explore the potential for mitigating these two issues by leveraging model editing methods, e.g., by locating Feed-Forward Network (FFN) neurons or something that are responsible for the errors and deactivating them in the inference time.We find that directly applying such methods either limited effect on the targeted errors or has significant negative side-effect on the general translation quality, indicating that the located components may also be crucial for ensuring machine translation with LLMs on the rails.To this end, we propose to refine the located components by fetching the intersection of the locating results under different language settings, filtering out the aforementioned information that is irrelevant to targeted errors. The experiment results empirically demonstrate that our methods can effectively reduce the language mismatch and repetition ratios and meanwhile enhance or keep the general translation quality in most cases."
    },
    {
        "paperId": "3857c71b91a3330921bb019af47c23ad1a942beb",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "When and Where Did it Happen? An Encoder-Decoder Model to Identify Scenario Context",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.07567, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-10",
        "authors": [
            {
                "authorId": "1404937995",
                "name": "Enrique Noriega-Atala"
            },
            {
                "authorId": "1725412182",
                "name": "Robert Vacareanu"
            },
            {
                "authorId": "2293530612",
                "name": "Salena Torres Ashton"
            },
            {
                "authorId": "72038514",
                "name": "A. Pyarelal"
            },
            {
                "authorId": "2287939838",
                "name": "Clayton T. Morrison"
            },
            {
                "authorId": "2269457097",
                "name": "Mihai Surdeanu"
            }
        ],
        "abstract": "We introduce a neural architecture finetuned for the task of scenario context generation: The relevant location and time of an event or entity mentioned in text. Contextualizing information extraction helps to scope the validity of automated finings when aggregating them as knowledge graphs. Our approach uses a high-quality curated dataset of time and location annotations in a corpus of epidemiology papers to train an encoder-decoder architecture. We also explored the use of data augmentation techniques during training. Our findings suggest that a relatively small fine-tuned encoder-decoder model performs better than out-of-the-box LLMs and semantic role labeling parsers to accurate predict the relevant scenario information of a particular entity or event."
    },
    {
        "paperId": "344a289365081eafd66afb3c2757f736a087e7a6",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Mars: Situated Inductive Reasoning in an Open-World Environment",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.08126, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-10",
        "authors": [
            {
                "authorId": "2189014540",
                "name": "Xiaojuan Tang"
            },
            {
                "authorId": "2265797420",
                "name": "Jiaqi Li"
            },
            {
                "authorId": "2325728024",
                "name": "Yitao Liang"
            },
            {
                "authorId": "2303322004",
                "name": "Song-chun Zhu"
            },
            {
                "authorId": "2265913927",
                "name": "Muhan Zhang"
            },
            {
                "authorId": "2266461235",
                "name": "Zilong Zheng"
            }
        ],
        "abstract": "Large Language Models (LLMs) trained on massive corpora have shown remarkable success in knowledge-intensive tasks. Yet, most of them rely on pre-stored knowledge. Inducing new general knowledge from a specific environment and performing reasoning with the acquired knowledge -- \\textit{situated inductive reasoning}, is crucial and challenging for machine intelligence. In this paper, we design Mars, an interactive environment devised for situated inductive reasoning. It introduces counter-commonsense game mechanisms by modifying terrain, survival setting and task dependency while adhering to certain principles. In Mars, agents need to actively interact with their surroundings, derive useful rules and perform decision-making tasks in specific contexts. We conduct experiments on various RL-based and LLM-based methods, finding that they all struggle on this challenging situated inductive reasoning benchmark. Furthermore, we explore \\textit{Induction from Reflection}, where we instruct agents to perform inductive reasoning from history trajectory. The superior performance underscores the importance of inductive reasoning in Mars. Through Mars, we aim to galvanize advancements in situated inductive reasoning and set the stage for developing the next generation of AI systems that can reason in an adaptive and context-sensitive way."
    },
    {
        "paperId": "5a83a63fa3a361e85bf26f7b69cedd601c0572eb",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.09220, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-11",
        "authors": [
            {
                "authorId": "2084322185",
                "name": "G. Kumari"
            },
            {
                "authorId": "2325909057",
                "name": "Kirtan Jain"
            },
            {
                "authorId": "2240213656",
                "name": "Asif Ekbal"
            }
        ],
        "abstract": "In recent years, there has been a significant rise in the phenomenon of hate against women on social media platforms, particularly through the use of misogynous memes. These memes often target women with subtle and obscure cues, making their detection a challenging task for automated systems. Recently, Large Language Models (LLMs) have shown promising results in reasoning using Chain-of-Thought (CoT) prompting to generate the intermediate reasoning chains as the rationale to facilitate multimodal tasks, but often neglect cultural diversity and key aspects like emotion and contextual knowledge hidden in the visual modalities. To address this gap, we introduce a **M**ultimodal **M**ulti-hop CoT (M3Hop-CoT) framework for **M**isogynous meme identification, combining a CLIP-based classifier and a multimodal CoT module with entity-object-relationship integration. M3Hop-CoT employs a three-step multimodal prompting principle to induce emotions, target awareness, and contextual knowledge for meme analysis. Our empirical evaluation, including both qualitative and quantitative analysis, validates the efficacy of the M3Hop-CoT framework on the SemEval-2022 Task 5 (**MAMI task**) dataset, highlighting its strong performance in the macro-F1 score. Furthermore, we evaluate the model\u2019s generalizability by evaluating it on various benchmark meme datasets, offering a thorough insight into the effectiveness of our approach across different datasets. Codes are available at this link: https://github.com/Gitanjali1801/LLM_CoT"
    },
    {
        "paperId": "bb03245c2867c4252a4087a4e9a95794e7ed9025",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.09318, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-12",
        "authors": [
            {
                "authorId": "1699604238",
                "name": "Saiful Islam Salim"
            },
            {
                "authorId": "2326527576",
                "name": "Rubin Yuchan Yang"
            },
            {
                "authorId": "2325909734",
                "name": "Alexander Cooper"
            },
            {
                "authorId": "2325888065",
                "name": "Suryashree Ray"
            },
            {
                "authorId": "2325907050",
                "name": "Saumya Debray"
            },
            {
                "authorId": "40242722",
                "name": "Sazzadur Rahaman"
            }
        ],
        "abstract": "While Large language model (LLM)-based programming assistants such as CoPilot and ChatGPT can help improve the productivity of professional software developers, they can also facilitate cheating in introductory computer programming courses. Assuming instructors have limited control over the industrial-strength models, this paper investigates the baseline performance of 5 widely used LLMs on a collection of introductory programming problems, examines adversarial perturbations to degrade their performance, and describes the results of a user study aimed at measuring the efficacy of such perturbations in hindering actual code generation for introductory programming assignments. The user study suggests that i) perturbations combinedly reduced the average correctness score by 77%, ii) the drop in correctness caused by these perturbations was affected based on their detectability."
    },
    {
        "paperId": "cca31f4ca29c46007122564be25ecf8ba99966f3",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MisinfoEval: Generative AI in the Era of \u201cAlternative Facts\u201d",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.09949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-13",
        "authors": [
            {
                "authorId": "2278633302",
                "name": "Saadia Gabriel"
            },
            {
                "authorId": "2277245993",
                "name": "Liang Lyu"
            },
            {
                "authorId": "1571403456",
                "name": "James Siderius"
            },
            {
                "authorId": "2294092232",
                "name": "Marzyeh Ghassemi"
            },
            {
                "authorId": "2325900540",
                "name": "Jacob Andreas"
            },
            {
                "authorId": "2141420607",
                "name": "Asu Ozdaglar"
            }
        ],
        "abstract": "The spread of misinformation on social media platforms threatens democratic processes, contributes to massive economic losses, and endangers public health. Many efforts to address misinformation focus on a knowledge deficit model and propose interventions for improving users\u2019 critical thinking through access to facts. Such efforts are often hampered by challenges with scalability, and by platform users\u2019 personal biases. The emergence of generative AI presents promising opportunities for countering misinformation at scale across ideological barriers. In this paper, we introduce a framework (MisinfoEval) for generating and comprehensively evaluating large language model (LLM) based misinformation interventions. We present (1) an experiment with a simulated social media environment to measure effectiveness of misinformation interventions, and (2) a second experiment with personalized explanations tailored to the demographics and beliefs of users with the goal of countering misinformation by appealing to their pre-existing values. Our findings confirm that LLM-based interventions are highly effective at correcting user behavior (improving overall user accuracy at reliability labeling by up to 41.72%). Furthermore, we find that users favor more personalized interventions when making decisions about news reliability and users shown personalized interventions have significantly higher accuracy at identifying misinformation."
    },
    {
        "paperId": "ff89469c3121e504259b67f0bc96deddc4187cbe",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Evaluating Gender Bias of LLMs in Making Morality Judgements",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.09992, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-13",
        "authors": [
            {
                "authorId": "2325902371",
                "name": "Divij Bajaj"
            },
            {
                "authorId": "2190089066",
                "name": "Yuanyuan Lei"
            },
            {
                "authorId": "2294563583",
                "name": "Jonathan Tong"
            },
            {
                "authorId": "2264613440",
                "name": "Ruihong Huang"
            }
        ],
        "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in a multitude of Natural Language Processing (NLP) tasks. However, these models are still not immune to limitations such as social biases, especially gender bias. This work investigates whether current closed and open-source LLMs possess gender bias, especially when asked to give moral opinions. To evaluate these models, we curate and introduce a new dataset GenMO (Gender-bias in Morality Opinions) comprising parallel short stories featuring male and female characters respectively. Specifically, we test models from the GPT family (GPT-3.5-turbo, GPT-3.5-turbo-instruct, GPT-4-turbo), Llama 3 and 3.1 families (8B/70B), Mistral-7B and Claude 3 families (Sonnet and Opus). Surprisingly, despite employing safety checks, all production-standard models we tested display significant gender bias with GPT-3.5-turbo giving biased opinions in 24% of the samples. Additionally, all models consistently favour female characters, with GPT showing bias in 68-85% of cases and Llama 3 in around 81-85% instances. Additionally, our study investigates the impact of model parameters on gender bias and explores real-world situations where LLMs reveal biases in moral decision-making."
    },
    {
        "paperId": "73e98c367b5e2b95a2965cc5d25f64155e2da887",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "LLM-based Code-Switched Text Generation for Grammatical Error Correction",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10349, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-14",
        "authors": [
            {
                "authorId": "2325905937",
                "name": "Tom Potter"
            },
            {
                "authorId": "2325943415",
                "name": "Zheng Yuan"
            }
        ],
        "abstract": "With the rise of globalisation, code-switching (CSW) has become a ubiquitous part of multilingual conversation, posing new challenges for natural language processing (NLP), especially in Grammatical Error Correction (GEC). This work explores the complexities of applying GEC systems to CSW texts. Our objectives include evaluating the performance of state-of-the-art GEC systems on an authentic CSW dataset from English as a Second Language (ESL) learners, exploring synthetic data generation as a solution to data scarcity, and developing a model capable of correcting grammatical errors in monolingual and CSW texts. We generated synthetic CSW GEC data, resulting in one of the first substantial datasets for this task, and showed that a model trained on this data is capable of significant improvements over existing systems. This work targets ESL learners, aiming to provide educational technologies that aid in the development of their English grammatical correctness without constraining their natural multilingualism."
    },
    {
        "paperId": "d46c1b5f457ab2f37f834ff07cdd1ae15667a648",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10854, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-06",
        "authors": [
            {
                "authorId": "2166311031",
                "name": "Shramay Palta"
            },
            {
                "authorId": "2216486213",
                "name": "Nishant Balepur"
            },
            {
                "authorId": "2325952837",
                "name": "Peter Rankel"
            },
            {
                "authorId": "35823986",
                "name": "Sarah Wiegreffe"
            },
            {
                "authorId": "2954727",
                "name": "Marine Carpuat"
            },
            {
                "authorId": "2302559920",
                "name": "Rachel Rudinger"
            }
        ],
        "abstract": "Questions involving commonsense reasoning about everyday situations often admit many $\\textit{possible}$ or $\\textit{plausible}$ answers. In contrast, multiple-choice question (MCQ) benchmarks for commonsense reasoning require a hard selection of a single correct answer, which, in principle, should represent the $\\textit{most}$ plausible answer choice. On $250$ MCQ items sampled from two commonsense reasoning benchmarks, we collect $5,000$ independent plausibility judgments on answer choices. We find that for over 20% of the sampled MCQs, the answer choice rated most plausible does not match the benchmark gold answers; upon manual inspection, we confirm that this subset exhibits higher rates of problems like ambiguity or semantic mismatch between question and answer choices. Experiments with LLMs reveal low accuracy and high variation in performance on the subset, suggesting our plausibility criterion may be helpful in identifying more reliable benchmark items for commonsense evaluation."
    },
    {
        "paperId": "6560791590bd50bb73afe4b77dd0a8ede486905e",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Gender Bias in Decision-Making with Large Language Models: A Study of Relationship Conflicts",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.11084, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-14",
        "authors": [
            {
                "authorId": "2290491668",
                "name": "Sharon Levy"
            },
            {
                "authorId": "2290490973",
                "name": "William D. Adler"
            },
            {
                "authorId": "4365187",
                "name": "T. Karver"
            },
            {
                "authorId": "1478928280",
                "name": "Mark Dredze"
            },
            {
                "authorId": "2248094851",
                "name": "Michelle R. Kaufman"
            }
        ],
        "abstract": "Large language models (LLMs) acquire beliefs about gender from training data and can therefore generate text with stereotypical gender attitudes. Prior studies have demonstrated model generations favor one gender or exhibit stereotypes about gender, but have not investigated the complex dynamics that can influence model reasoning and decision-making involving gender. We study gender equity within LLMs through a decision-making lens with a new dataset, DeMET Prompts, containing scenarios related to intimate, romantic relationships. We explore nine relationship configurations through name pairs across three name lists (men, women, neutral). We investigate equity in the context of gender roles through numerous lenses: typical and gender-neutral names, with and without model safety enhancements, same and mixed-gender relationships, and egalitarian versus traditional scenarios across various topics. While all models exhibit the same biases (women favored, then those with gender-neutral names, and lastly men), safety guardrails reduce bias. In addition, models tend to circumvent traditional male dominance stereotypes and side with 'traditionally female' individuals more often, suggesting relationships are viewed as a female domain by the models."
    },
    {
        "paperId": "3f07bacdf8f345d9eeb320bb089fed4d61b4cec7",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.11786, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-15",
        "authors": [
            {
                "authorId": "2325955194",
                "name": "Tsz Ting Chung"
            },
            {
                "authorId": "2279792419",
                "name": "Leyang Cui"
            },
            {
                "authorId": "2273767663",
                "name": "Lemao Liu"
            },
            {
                "authorId": "14799547",
                "name": "Xinting Huang"
            },
            {
                "authorId": "2269760215",
                "name": "Shuming Shi"
            },
            {
                "authorId": "1739816",
                "name": "D. Yeung"
            }
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in a wide range of natural language processing tasks when leveraging in-context learning. To mitigate the additional computational and financial costs associated with in-context learning, several prompt compression methods have been proposed to compress the in-context learning prompts. Despite their success, these methods face challenges with transferability due to model-specific compression, or rely on external training data, such as GPT-4. In this paper, we investigate the ability of LLMs to develop a unified compression method that discretizes uninformative tokens, utilizing a self-supervised pre-training technique. By introducing a small number of parameters during the continual pre-training, the proposed Selection-p produces a probability for each input token, indicating whether to preserve or discard it. Experiments show Selection-p achieves state-of-the-art performance across numerous classification tasks, achieving compression rates of up to 10 times while experiencing only a marginal 0.8% decrease in performance. Moreover, it exhibits superior transferability to different models compared to prior work. Additionally, we further analyze how Selection-p helps maintain performance on in-context learning with long contexts."
    },
    {
        "paperId": "9fdc06e719fab6d3fddd10174100332f3a127834",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Boosting Logical Fallacy Reasoning in LLMs via Logical Structure Tree",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.12048, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-15",
        "authors": [
            {
                "authorId": "2190089066",
                "name": "Yuanyuan Lei"
            },
            {
                "authorId": "2264613440",
                "name": "Ruihong Huang"
            }
        ],
        "abstract": "Logical fallacy uses invalid or faulty reasoning in the construction of a statement. Despite the prevalence and harmfulness of logical fallacies, detecting and classifying logical fallacies still remains a challenging task. We observe that logical fallacies often use connective words to indicate an intended logical relation between two arguments, while the argument semantics does not actually support the logical relation. Inspired by this observation, we propose to build a logical structure tree to explicitly represent and track the hierarchical logic flow among relation connectives and their arguments in a statement. Specifically, this logical structure tree is constructed in an unsupervised manner guided by the constituency tree and a taxonomy of connectives for ten common logical relations, with relation connectives as non-terminal nodes and textual arguments as terminal nodes, and the latter are mostly elementary discourse units. We further develop two strategies to incorporate the logical structure tree into LLMs for fallacy reasoning. Firstly, we transform the tree into natural language descriptions and feed the textualized tree into LLMs as a part of the hard text prompt. Secondly, we derive a relation-aware tree embedding and insert the tree embedding into LLMs as a soft prompt. Experiments on benchmark datasets demonstrate that our approach based on logical structure tree significantly improves precision and recall for both fallacy detection and fallacy classification."
    },
    {
        "paperId": "d0f61d14394a4dd8c7a9357252dc4b8058910a04",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection and Argumentative Dialogue Summarization",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.582.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13667, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-17",
        "authors": [
            {
                "authorId": "2273531473",
                "name": "Xiutian Zhao"
            },
            {
                "authorId": "2273574212",
                "name": "Ke Wang"
            },
            {
                "authorId": "2273549661",
                "name": "Wei Peng"
            }
        ],
        "abstract": "Dialogue agents have been receiving increasing attention for years, and this trend has been further boosted by the recent progress of large language models (LLMs). Stance detection and dialogue summarization are two core tasks of dialogue agents in application scenarios that involve argumentative dialogues. However, research on these tasks is limited by the insufficiency of public datasets, especially for non-English languages. To address this language resource gap in Chinese, we present ORCHID (Oral Chinese Debate), the first Chinese dataset for benchmarking target-independent stance detection and debate summarization. Our dataset consists of 1,218 real-world debates that were conducted in Chinese on 476 unique topics, containing 2,436 stance-specific summaries and 14,133 fully annotated utterances. Besides providing a versatile testbed for future research, we also conduct an empirical study on the dataset and propose an integrated task. The results show the challenging nature of the dataset and suggest a potential of incorporating stance detection in summarization for argumentative dialogue."
    },
    {
        "paperId": "ca01cb09c81af738b8108615115e7bfe96f44ec9",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "An Electoral Approach to Diversify LLM-based Multi-Agent Collective Decision-Making",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.15168, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-19",
        "authors": [
            {
                "authorId": "2273531473",
                "name": "Xiutian Zhao"
            },
            {
                "authorId": "2273574212",
                "name": "Ke Wang"
            },
            {
                "authorId": "2273549661",
                "name": "Wei Peng"
            }
        ],
        "abstract": "Modern large language models (LLMs) have exhibited cooperative synergy on complex task-solving, and collective decision-making (CDM) is a pivotal component in LLM-based multi-agent collaboration frameworks. Our survey on 52 recent such systems uncovers a severe lack of diversity, with a heavy reliance on dictatorial and plurality voting for CDM. Through the lens of social choice theory, we scrutinize widely-adopted CDM methods and identify their limitations. To enrich current landscape of LLM-based CDM, we present GEDI, an electoral CDM module that incorporates various ordinal preferential voting mechanisms. Our empirical case study across three benchmarks shows that the integration of certain CDM methods can markedly improve the reasoning capabilities and robustness of some leading LLMs, all without requiring intricate system designs. Additionally, we find that some CDM mechanisms generate positive synergies even with as few as three agents. The voting-based methods also demonstrate robustness against single points of failure, as well as diversity in terms of hit-rate@k and subject-wise impacts."
    },
    {
        "paperId": "6474c64ea92bc06ca3d2fe62a79a82a807699c71",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "The Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.15182, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-19",
        "authors": [
            {
                "authorId": "2273571395",
                "name": "Xiaobo Guo"
            },
            {
                "authorId": "2326986821",
                "name": "Neil Potnis"
            },
            {
                "authorId": "2327005183",
                "name": "Melody Yu"
            },
            {
                "authorId": "35689089",
                "name": "Nabeel Gillani"
            },
            {
                "authorId": "1918441",
                "name": "Soroush Vosoughi"
            }
        ],
        "abstract": "The ability for individuals to constructively engage with one another across lines of difference is a critical feature of a healthy pluralistic society. This is also true in online discussion spaces like social media platforms. To date, much social media research has focused on preventing ills\u2014like political polarization and the spread of misinformation. While this is important, enhancing the quality of online public discourse requires not just reducing ills, but also, promoting foundational human virtues. In this study, we focus on one particular virtue: \u201cintellectual humility\u201d (IH), or acknowledging the potential limitations in one\u2019s own beliefs. Specifically, we explore the development of computational methods for measuring IH at scale. We manually curate and validate an IH codebook on 350 posts about religion drawn from subreddits and use them to develop LLM-based models for automating this measurement. Our best model achieves a Macro-F1 score of 0.64 across labels (and 0.70 when predicting IH/IA/Neutral at the coarse level), higher than an expected naive baseline of 0.51 (0.32 for IH/IA/Neutral) but lower than a human annotator-informed upper bound of 0.85 (0.83 for IH/IA/Neutral). Our results both highlight the challenging nature of detecting IH online\u2014opening the door to new directions in NLP research\u2014and also lay a foundation for computational social science researchers interested in analyzing and fostering more IH in online public discourse."
    },
    {
        "paperId": "6a091c300b0991d7f853c8c2c179577a07b5354e",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Large Language Models Know What To Say But Not When To Speak",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.16044, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-21",
        "authors": [
            {
                "authorId": "2257111534",
                "name": "Muhammad Umair"
            },
            {
                "authorId": "3379438",
                "name": "Vasanth Sarathy"
            },
            {
                "authorId": "113253455",
                "name": "J. Ruiter"
            }
        ],
        "abstract": "Turn-taking is a fundamental mechanism in human communication that ensures smooth and coherent verbal interactions. Recent advances in Large Language Models (LLMs) have motivated their use in improving the turn-taking capabilities of Spoken Dialogue Systems (SDS), such as their ability to respond at appropriate times. However, existing models often struggle to predict opportunities for speaking -- called Transition Relevance Places (TRPs) -- in natural, unscripted conversations, focusing only on turn-final TRPs and not within-turn TRPs. To address these limitations, we introduce a novel dataset of participant-labeled within-turn TRPs and use it to evaluate the performance of state-of-the-art LLMs in predicting opportunities for speaking. Our experiments reveal the current limitations of LLMs in modeling unscripted spoken interactions, highlighting areas for improvement and paving the way for more naturalistic dialogue systems."
    },
    {
        "paperId": "a3357c562e46cbdad632737c73628869c02b95b2",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the US",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.16451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-21",
        "authors": [
            {
                "authorId": "2307006593",
                "name": "Christabel Acquaye"
            },
            {
                "authorId": "1576552123",
                "name": "Haozhe An"
            },
            {
                "authorId": "2034613",
                "name": "Rachel Rudinger"
            }
        ],
        "abstract": "Recent work has highlighted the culturally-contingent nature of commonsense knowledge. We introduce AMAMMER\u03b5, a test set of 525 multiple-choice questions designed to evaluate the commonsense knowledge of English LLMs, relative to the cultural contexts of Ghana and the United States. To create AMAMMER\u03b5, we select a set of multiple-choice questions (MCQs) from existing commonsense datasets and rewrite them in a multi-stage process involving surveys of Ghanaian and U.S. participants. In three rounds of surveys, participants from both pools are solicited to (1) write correct and incorrect answer choices, (2) rate individual answer choices on a 5-point Likert scale, and (3) select the best answer choice from the newly-constructed MCQ items, in a final validation step. By engaging participants at multiple stages, our procedure ensures that participant perspectives are incorporated both in the creation and validation of test items, resulting in high levels of agreement within each pool. We evaluate several off-the-shelf English LLMs on AMAMMER\u03b5. Uniformly, models prefer answers choices that align with the preferences of U.S. annotators over Ghanaian annotators. Additionally, when test items specify a cultural context (Ghana or the U.S.), models exhibit some ability to adapt, but performance is consistently better in U.S. contexts than Ghanaian. As large resources are devoted to the advancement of English LLMs, our findings underscore the need for culturally adaptable models and evaluations to meet the needs of diverse English-speaking populations around the world."
    },
    {
        "paperId": "0082835d32b34decc31812d5f8a89161f1c81e26",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.17509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-23",
        "authors": [
            {
                "authorId": "2051552791",
                "name": "Jinghan Jia"
            },
            {
                "authorId": "2168533640",
                "name": "Jiancheng Liu"
            },
            {
                "authorId": "2155369380",
                "name": "Yihua Zhang"
            },
            {
                "authorId": "2247652289",
                "name": "Parikshit Ram"
            },
            {
                "authorId": "2284064162",
                "name": "Nathalie Baracaldo"
            },
            {
                "authorId": "2254478722",
                "name": "Sijia Liu"
            }
        ],
        "abstract": "The need for effective unlearning mechanisms in large language models (LLMs) is increasingly urgent, driven by the necessity to adhere to data regulations and foster ethical generative AI practices. Despite growing interest of LLM unlearning, much of the existing research has focused on varied unlearning method designs to boost effectiveness and efficiency. However, the inherent relationship between model weights and LLM unlearning has not been extensively examined. In this paper, we systematically explore how model weights interact with unlearning processes in LLMs and we design the weight attribution-guided LLM unlearning method, WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. By strategically guiding the LLM unlearning across different types of unlearning methods and tasks, WAGLE can erase the undesired content, while maintaining the performance of the original tasks. We refer to the weight attribution-guided LLM unlearning method as WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. Our extensive experiments show that WAGLE boosts unlearning performance across a range of LLM unlearning methods such as gradient difference and (negative) preference optimization, applications such as fictitious unlearning, malicious use prevention, and copyrighted information removal, and models including Zephyr-7b-beta and Llama2-7b. To the best of our knowledge, our work offers the first principled method for attributing and pinpointing the influential weights in enhancing LLM unlearning. It stands in contrast to previous methods that lack weight attribution and simpler weight attribution techniques."
    },
    {
        "paperId": "9b90e2251fb866f38ce42c3f39158ea77c751c57",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Delving into the Reversal Curse: How Far Can Large Language Models Generalize?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.18808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-24",
        "authors": [
            {
                "authorId": "2284607292",
                "name": "Zhengkai Lin"
            },
            {
                "authorId": "26910528",
                "name": "Zhihang Fu"
            },
            {
                "authorId": "2283150812",
                "name": "Kai Liu"
            },
            {
                "authorId": "2305301033",
                "name": "Liang Xie"
            },
            {
                "authorId": "2284672601",
                "name": "Binbin Lin"
            },
            {
                "authorId": "2305030728",
                "name": "Wenxiao Wang"
            },
            {
                "authorId": "2299571473",
                "name": "Deng Cai"
            },
            {
                "authorId": "2282998795",
                "name": "Yue Wu"
            },
            {
                "authorId": "2268645316",
                "name": "Jieping Ye"
            }
        ],
        "abstract": "While large language models (LLMs) showcase unprecedented capabilities, they also exhibit certain inherent limitations when facing seemingly trivial tasks. A prime example is the recently debated\"reversal curse\", which surfaces when models, having been trained on the fact\"A is B\", struggle to generalize this knowledge to infer that\"B is A\". In this paper, we examine the manifestation of the reversal curse across various tasks and delve into both the generalization abilities and the problem-solving mechanisms of LLMs. This investigation leads to a series of significant insights: (1) LLMs are able to generalize to\"B is A\"when both A and B are presented in the context as in the case of a multiple-choice question. (2) This generalization ability is highly correlated to the structure of the fact\"A is B\"in the training documents. For example, this generalization only applies to biographies structured in\"[Name] is [Description]\"but not to\"[Description] is [Name]\". (3) We propose and verify the hypothesis that LLMs possess an inherent bias in fact recalling during knowledge application, which explains and underscores the importance of the document structure to successful learning. (4) The negative impact of this bias on the downstream performance of LLMs can hardly be mitigated through training alone. These findings offer a novel perspective on interpreting LLMs' generalization through their intrinsic mechanisms and provide insights for developing more effective learning methods. Our code and data are available at https://github.com/alibaba/thinking_bias.git."
    },
    {
        "paperId": "4fab7f960aa833670a52c2fdbcd4d0943e2f3b67",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.20008, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-25",
        "authors": [
            {
                "authorId": "2318611618",
                "name": "Zheng Zhao"
            },
            {
                "authorId": "7264689",
                "name": "Yftah Ziser"
            },
            {
                "authorId": "2277600097",
                "name": "Shay B. Cohen"
            }
        ],
        "abstract": "Fine-tuning pre-trained large language models (LLMs) on a diverse array of tasks has become a common approach for building models that can solve various natural language processing (NLP) tasks. However, where and to what extent these models retain task-specific knowledge remains largely unexplored. This study investigates the task-specific information encoded in pre-trained LLMs and the effects of instruction tuning on their representations across a diverse set of over 60 NLP tasks. We use a set of matrix analysis tools to examine the differences between the way pre-trained and instruction-tuned LLMs store task-specific information. Our findings reveal that while some tasks are already encoded within the pre-trained LLMs, others greatly benefit from instruction tuning. Additionally, we pinpointed the layers in which the model transitions from high-level general representations to more task-oriented representations. This finding extends our understanding of the governing mechanisms of LLMs and facilitates future research in the fields of parameter-efficient transfer learning and multi-task learning. Our code is available at: https://github.com/zsquaredz/layer_by_layer/"
    },
    {
        "paperId": "b2e80edfe3c0ab90d3c3a251962266b7114ff3ed",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Evaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2410.20763",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.20763, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-28",
        "authors": [
            {
                "authorId": "51896072",
                "name": "Sumit Asthana"
            },
            {
                "authorId": "2516777",
                "name": "Hannah Rashkin"
            },
            {
                "authorId": "2324056615",
                "name": "Elizabeth Clark"
            },
            {
                "authorId": "2174667321",
                "name": "Fantine Huot"
            },
            {
                "authorId": "1747893",
                "name": "Mirella Lapata"
            }
        ],
        "abstract": "One useful application of NLP models is to support people in reading complex text from unfamiliar domains (e.g., scientific articles). Simplifying the entire text makes it understandable but sometimes removes important details. On the contrary, helping adult readers understand difficult concepts in context can enhance their vocabulary and knowledge. In a preliminary human study, we first identify that lack of context and unfamiliarity with difficult concepts is a major reason for adult readers\u2019 difficulty with domain-specific text. We then introduce targeted concept simplification, a simplification task for rewriting text to help readers comprehend text containing unfamiliar concepts. We also introduce WikiDomains, a new dataset of 22k definitions from 13 academic domains paired with a difficult concept within each definition. We benchmark the performance of open-source and commercial LLMs and a simple dictionary baseline on this task across human judgments of ease of understanding and meaning preservation. Interestingly, our human judges preferred explanations about the difficult concept more than simplifications of the concept phrase. Further, no single model achieved superior performance across all quality dimensions, and automated metrics also show low correlations with human evaluations of concept simplification (~0.2), opening up rich avenues for research on personalized human reading comprehension support."
    },
    {
        "paperId": "20838de13350d7b51ccf6c75869f73254a77f517",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "NewTerm: Benchmarking Real-Time New Terms for Large Language Models with Annual Updates",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.20814, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-28",
        "authors": [
            {
                "authorId": "2193388247",
                "name": "Hexuan Deng"
            },
            {
                "authorId": "12386833",
                "name": "Wenxiang Jiao"
            },
            {
                "authorId": "2256344322",
                "name": "Xuebo Liu"
            },
            {
                "authorId": "2269805934",
                "name": "Min Zhang"
            },
            {
                "authorId": "2253396717",
                "name": "Zhaopeng Tu"
            }
        ],
        "abstract": "Despite their remarkable abilities in various tasks, large language models (LLMs) still struggle with real-time information (e.g., new facts and terms) due to the knowledge cutoff in their development process. However, existing benchmarks focus on outdated content and limited fields, facing difficulties in real-time updating and leaving new terms unexplored. To address this problem, we propose an adaptive benchmark, NewTerm, for real-time evaluation of new terms. We design a highly automated construction method to ensure high-quality benchmark construction with minimal human effort, allowing flexible updates for real-time information. Empirical results on various LLMs demonstrate over 20% performance reduction caused by new terms. Additionally, while updates to the knowledge cutoff of LLMs can cover some of the new terms, they are unable to generalize to more distant new terms. We also analyze which types of terms are more challenging and why LLMs struggle with new terms, paving the way for future research. Finally, we construct NewTerm 2022 and 2023 to evaluate the new terms updated each year and will continue updating annually. The benchmark and codes can be found at https://github.com/hexuandeng/NewTerm."
    },
    {
        "paperId": "a3fc4a24cbedbedec8576ddc8e6fbdc11f029e95",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MARCO: Multi-Agent Real-time Chat Orchestration",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.21784, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-29",
        "authors": [
            {
                "authorId": "2007573924",
                "name": "Anubhav Shrimal"
            },
            {
                "authorId": "2328077588",
                "name": "Stanley Kanagaraj"
            },
            {
                "authorId": "2328078563",
                "name": "Kriti Biswas"
            },
            {
                "authorId": "2328077144",
                "name": "Swarnalatha Raghuraman"
            },
            {
                "authorId": "1659057072",
                "name": "Anish Nediyanchath"
            },
            {
                "authorId": "2328285025",
                "name": "Yi Zhang"
            },
            {
                "authorId": "31024864",
                "name": "Promod Yenigalla"
            }
        ],
        "abstract": "Large language model advancements have enabled the development of multi-agent frameworks to tackle complex, real-world problems such as to automate workflows that require interactions with diverse tools, reasoning, and human collaboration. We present MARCO, a Multi-Agent Real-time Chat Orchestration framework for automating workflows using LLMs. MARCO addresses key challenges in utilizing LLMs for complex, multi-step task execution in a production environment. It incorporates robust guardrails to steer LLM behavior, validate outputs, and recover from errors that stem from inconsistent output formatting, function and parameter hallucination, and lack of domain knowledge. Through extensive experiments we demonstrate MARCO\u2019s superior performance with 94.48% and 92.74% accuracy on task execution for Digital Restaurant Service Platform conversations and Retail conversations datasets respectively along with 44.91% improved latency and 33.71% cost reduction in a production setting. We also report effects of guardrails in performance gain along with comparisons of various LLM models, both open-source and proprietary. The modular and generic design of MARCO allows it to be adapted for automating workflows across domains and to execute complex tasks through multi-turn interactions."
    },
    {
        "paperId": "30f612820063bec9cfc16f6d8151b7eafdd6e95f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2411.00492",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.00492, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-01",
        "authors": [
            {
                "authorId": "2060491855",
                "name": "Do Xuan Long"
            },
            {
                "authorId": "2328977209",
                "name": "Duong Ngoc Yen"
            },
            {
                "authorId": "1755919",
                "name": "A. Luu"
            },
            {
                "authorId": "2266466003",
                "name": "Kenji Kawaguchi"
            },
            {
                "authorId": "2257033898",
                "name": "Min-Yen Kan"
            },
            {
                "authorId": "2266471203",
                "name": "Nancy F. Chen"
            }
        ],
        "abstract": "We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple experts, aggregating their responses, and selecting the best among individual and aggregated responses. This process is performed in a single chain of thoughts through our seven carefully designed subtasks derived from the Nominal Group Technique (Ven and Delbecq, 1974), a well-established decision-making framework. Our evaluations demonstrate that Multi-expert Prompting significantly outperforms ExpertPrompting and comparable baselines in enhancing the truthfulness, factuality, informativeness, and usefulness of responses while reducing toxicity and hurtfulness. It further achieves state-of-the-art truthfulness by outperforming the best baseline by 8.69% with ChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable to diverse scenarios, eliminating the need for manual prompt construction."
    },
    {
        "paperId": "246c4153ecdace42c77c7ce70771487984581791",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.03665, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-06",
        "authors": [
            {
                "authorId": "2329551492",
                "name": "Xuelin Liu"
            },
            {
                "authorId": "2330145167",
                "name": "Yanfei Zhu"
            },
            {
                "authorId": "2290240747",
                "name": "Shucheng Zhu"
            },
            {
                "authorId": "2315583182",
                "name": "Pengyuan Liu"
            },
            {
                "authorId": "2290275558",
                "name": "Ying Liu"
            },
            {
                "authorId": "2320840029",
                "name": "Dong Yu"
            }
        ],
        "abstract": "Proper moral beliefs are fundamental for language models, yet assessing these beliefs poses a significant challenge. This study introduces a novel three-module framework to evaluate the moral beliefs of four prominent large language models. Initially, we constructed a dataset containing 472 moral choice scenarios in Chinese, derived from moral words. The decision-making process of the models in these scenarios reveals their moral principle preferences. By ranking these moral choices, we discern the varying moral beliefs held by different language models. Additionally, through moral debates, we investigate the firmness of these models to their moral choices. Our findings indicate that English language models, namely ChatGPT and Gemini, closely mirror moral decisions of the sample of Chinese university students, demonstrating strong adherence to their choices and a preference for individualistic moral beliefs. In contrast, Chinese models such as Ernie and ChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their moral choices and debates. This study also uncovers gender bias embedded within the moral beliefs of all examined language models. Our methodology offers an innovative means to assess moral beliefs in both artificial and human intelligence, facilitating a comparison of moral values across different cultures."
    },
    {
        "paperId": "8b4a317a2765ff0529c639e757f0d4e6011c5d15",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.03865, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-06",
        "authors": [
            {
                "authorId": "2306071296",
                "name": "Yizhe Huang"
            },
            {
                "authorId": "2330102825",
                "name": "Xingbo Wang"
            },
            {
                "authorId": "2329728501",
                "name": "Hao Liu"
            },
            {
                "authorId": "2305774655",
                "name": "Fanqi Kong"
            },
            {
                "authorId": "2220639399",
                "name": "Aoyang Qin"
            },
            {
                "authorId": "2330228594",
                "name": "Min Tang"
            },
            {
                "authorId": "2329425978",
                "name": "Xiaoxi Wang"
            },
            {
                "authorId": "2249929708",
                "name": "Song-Chun Zhu"
            },
            {
                "authorId": "2329368557",
                "name": "Mingjie Bi"
            },
            {
                "authorId": "3390244",
                "name": "Siyuan Qi"
            },
            {
                "authorId": "2305855026",
                "name": "Xue Feng"
            }
        ],
        "abstract": "Traditional interactive environments limit agents' intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety."
    },
    {
        "paperId": "0cec0406b31befeabc69e6dc53b7933a93be21cd",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Explaining Mixtures of Sources in News Articles",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.05192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-07",
        "authors": [
            {
                "authorId": "2257248527",
                "name": "Alexander Spangher"
            },
            {
                "authorId": "2052851219",
                "name": "James Youn"
            },
            {
                "authorId": "2330081839",
                "name": "Matt DeButts"
            },
            {
                "authorId": "2266840227",
                "name": "Nanyun Peng"
            },
            {
                "authorId": "2266840986",
                "name": "Emilio Ferrara"
            },
            {
                "authorId": "2257228316",
                "name": "Jonathan May"
            }
        ],
        "abstract": "Human writers plan, then write. For large language models (LLMs) to play a role in longer-form article generation, we must understand the planning steps humans make before writing. We explore one kind of planning, source-selection in news, as a case-study for evaluating plans in long-form generation. We ask: why do specific stories call for specific kinds of sources? We imagine a generative process for story writing where a source-selection schema is first selected by a journalist, and then sources are chosen based on categories in that schema. Learning the article's plan means predicting the schema initially chosen by the journalist. Working with professional journalists, we adapt five existing schemata and introduce three new ones to describe journalistic plans for the inclusion of sources in documents. Then, inspired by Bayesian latent-variable modeling, we develop metrics to select the most likely plan, or schema, underlying a story, which we use to compare schemata. We find that two schemata: stance and social affiliation best explain source plans in most documents. However, other schemata like textual entailment explain source plans in factually rich topics like\"Science\". Finally, we find we can predict the most suitable schema given just the article's headline with reasonable accuracy. We see this as an important case-study for human planning, and provides a framework and approach for evaluating other kinds of plans. We release a corpora, NewsSources, with annotations for 4M articles."
    },
    {
        "paperId": "43b1c1971976fd6c34857b1c30cf15576b963b0f",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "Tell What You Hear From What You See - Video to Audio Generation Through Text",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.05679, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-08",
        "authors": [
            {
                "authorId": "2109304511",
                "name": "Xiulong Liu"
            },
            {
                "authorId": "2279722068",
                "name": "Kun Su"
            },
            {
                "authorId": "2258448529",
                "name": "Eli Shlizerman"
            }
        ],
        "abstract": "The content of visual and audio scenes is multi-faceted such that a video can be paired with various audio and vice-versa. Thereby, in video-to-audio generation task, it is imperative to introduce steering approaches for controlling the generated audio. While Video-to-Audio generation is a well-established generative task, existing methods lack such controllability. In this work, we propose VATT, a multi-modal generative framework that takes a video and an optional text prompt as input, and generates audio and optional textual description of the audio. Such a framework has two advantages: i) Video-to-Audio generation process can be refined and controlled via text which complements the context of visual information, and ii) The model can suggest what audio to generate for the video by generating audio captions. VATT consists of two key modules: VATT Converter, a LLM that is fine-tuned for instructions and includes a projection layer that maps video features to the LLM vector space; and VATT Audio, a transformer that generates audio tokens from visual frames and from optional text prompt using iterative parallel decoding. The audio tokens are converted to a waveform by pretrained neural codec. Experiments show that when VATT is compared to existing video-to-audio generation methods in objective metrics, it achieves competitive performance when the audio caption is not provided. When the audio caption is provided as a prompt, VATT achieves even more refined performance (lowest KLD score of 1.41). Furthermore, subjective studies show that VATT Audio has been chosen as preferred generated audio than audio generated by existing methods. VATT enables controllable video-to-audio generation through text as well as suggesting text prompts for videos through audio captions, unlocking novel applications such as text-guided video-to-audio generation and video-to-audio captioning."
    },
    {
        "paperId": "c5f5d92e20fec8f95f2ad394fdeaf87c0e26dc32",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Using Language Models to Disambiguate Lexical Choices in Translation",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2411.05781",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.05781, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-08",
        "authors": [
            {
                "authorId": "2329738277",
                "name": "Josh Barua"
            },
            {
                "authorId": "2329740435",
                "name": "Sanjay Subramanian"
            },
            {
                "authorId": "2329737021",
                "name": "Kayo Yin"
            },
            {
                "authorId": "2324792469",
                "name": "Alane Suhr"
            }
        ],
        "abstract": "In translation, a concept represented by a single word in a source language can have multiple variations in a target language. The task of lexical selection requires using context to identify which variation is most appropriate for a source text. We work with native speakers of nine languages to create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual concept variation when translating from English. We evaluate recent LLMs and neural machine translation systems on DTAiLS, with the best-performing model, GPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use language models to generate English rules describing target-language concept variations. Providing weaker models with high-quality lexical rules improves accuracy substantially, in some cases reaching or outperforming GPT-4."
    },
    {
        "paperId": "2898fe37db2b42ff4cdfdd597f4644667e498e13",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Problem-Oriented Segmentation and Retrieval: Case Study on Tutoring Conversations",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2411.07598",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.07598, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2260440783",
                "name": "Rose Wang"
            },
            {
                "authorId": "2220129265",
                "name": "Pawan Wirawarn"
            },
            {
                "authorId": "2330244283",
                "name": "Kenny Lam"
            },
            {
                "authorId": "2290074162",
                "name": "Omar Khattab"
            },
            {
                "authorId": "2258954602",
                "name": "Dorottya Demszky"
            }
        ],
        "abstract": "Many open-ended conversations (e.g., tutoring lessons or business meetings) revolve around pre-defined reference materials, like worksheets or meeting bullets. To provide a framework for studying such conversation structure, we introduce Problem-Oriented Segmentation&Retrieval (POSR), the task of jointly breaking down conversations into segments and linking each segment to the relevant reference item. As a case study, we apply POSR to education where effectively structuring lessons around problems is critical yet difficult. We present LessonLink, the first dataset of real-world tutoring lessons, featuring 3,500 segments, spanning 24,300 minutes of instruction and linked to 116 SAT math problems. We define and evaluate several joint and independent approaches for POSR, including segmentation (e.g., TextTiling), retrieval (e.g., ColBERT), and large language models (LLMs) methods. Our results highlight that modeling POSR as one joint task is essential: POSR methods outperform independent segmentation and retrieval pipelines by up to +76% on joint metrics and surpass traditional segmentation methods by up to +78% on segmentation metrics. We demonstrate POSR's practical impact on downstream education applications, deriving new insights on the language and time use in real-world lesson structures."
    },
    {
        "paperId": "9369ea09276fca4d3a10a760812b717f99735adb",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2411.12828",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.12828, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-19",
        "authors": [
            {
                "authorId": "2330822520",
                "name": "Sonny George"
            },
            {
                "authorId": "2330818444",
                "name": "Chris Sypherd"
            },
            {
                "authorId": "2330818459",
                "name": "Dylan Cashman"
            }
        ],
        "abstract": "Large language model (LLM) agents show promise in an increasing number of domains. In many proposed applications, it is expected that the agent reasons over accumulated experience presented in an input prompt. We propose the OEDD (Operationalize Experience Despite Distraction) corpus, a human-annotator-validated body of scenarios with pre-scripted agent histories where the agent must make a decision based on disparate experiential information in the presence of a distractor. We evaluate three state-of-the-art LLMs (GPT-3.5 Turbo, GPT-4o, and Gemini 1.5 Pro) using a minimal chain-of-thought prompting strategy and observe that when (1) the input context contains over 1,615 tokens of historical interactions, (2) a crucially decision-informing premise is the rightful conclusion over two disparate environment premises, and (3) a trivial, but distracting red herring fact follows, all LLMs perform worse than random choice at selecting the better of two actions. Our code and test corpus are publicly available at: https://github.com/sonnygeorge/OEDD ."
    },
    {
        "paperId": "c2ef8564b206eb8f61e552a6892052431bdbccf6",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling",
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2411.14042",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.14042, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-21",
        "authors": [
            {
                "authorId": "1998963567",
                "name": "Daehoon Gwak"
            },
            {
                "authorId": "152993838",
                "name": "Junwoo Park"
            },
            {
                "authorId": "2269464940",
                "name": "Minho Park"
            },
            {
                "authorId": "1484057728",
                "name": "chaeHun Park"
            },
            {
                "authorId": "2330984151",
                "name": "Hyunchan Lee"
            },
            {
                "authorId": "2257209516",
                "name": "Edward Choi"
            },
            {
                "authorId": "2260653165",
                "name": "Jaegul Choo"
            }
        ],
        "abstract": "Predicting future international events from textual information, such as news articles, has tremendous potential for applications in global policy, strategic decision-making, and geopolitics. However, existing datasets available for this task are often limited in quality, hindering the progress of related research. In this paper, we introduce WORLDREP (WORLD Relationship and Event Prediction), a novel dataset designed to address these limitations by leveraging the advanced reasoning capabilities of large-language models (LLMs). Our dataset features high-quality scoring labels generated through advanced prompt modeling and rigorously validated by domain experts in political science. We showcase the quality and utility of WORLDREP for real-world event prediction tasks, demonstrating its effectiveness through extensive experiments and analysis. Furthermore, we publicly release our dataset along with the full automation source code for data collection, labeling, and benchmarking, aiming to support and advance research in text-based event prediction."
    },
    {
        "paperId": "de1cf0a69f9e33adcf8fd3d33d1197985079129e",
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "title": "A polar coordinate system represents syntax in large language models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.05571, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-12-07",
        "authors": [
            {
                "authorId": "2334481768",
                "name": "Pablo Diego-Sim'on"
            },
            {
                "authorId": "2334480716",
                "name": "St'ephane d'Ascoli"
            },
            {
                "authorId": "2266394399",
                "name": "Emmanuel Chemla"
            },
            {
                "authorId": "3051598",
                "name": "Yair Lakretz"
            },
            {
                "authorId": "2257433111",
                "name": "Jean-R\u00e9mi King"
            }
        ],
        "abstract": "Originally formalized with symbolic representations, syntactic trees may also be effectively represented in the activations of large language models (LLMs). Indeed, a 'Structural Probe' can find a subspace of neural activations, where syntactically related words are relatively close to one-another. However, this syntactic code remains incomplete: the distance between the Structural Probe word embeddings can represent the existence but not the type and direction of syntactic relations. Here, we hypothesize that syntactic relations are, in fact, coded by the relative direction between nearby embeddings. To test this hypothesis, we introduce a 'Polar Probe' trained to read syntactic relations from both the distance and the direction between word embeddings. Our approach reveals three main findings. First, our Polar Probe successfully recovers the type and direction of syntactic relations, and substantially outperforms the Structural Probe by nearly two folds. Second, we confirm that this polar coordinate system exists in a low-dimensional subspace of the intermediate layers of many LLMs and becomes increasingly precise in the latest frontier models. Third, we demonstrate with a new benchmark that similar syntactic relations are coded similarly across the nested levels of syntactic trees. Overall, this work shows that LLMs spontaneously learn a geometry of neural activations that explicitly represents the main symbolic structures of linguistic theory."
    },
    {
        "paperId": "eea9b77b14b02adc9940dd74d329af88d8440770",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.15443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2025-02-21",
        "authors": [
            {
                "authorId": "2290006119",
                "name": "Weilan Wang"
            },
            {
                "authorId": "2290135389",
                "name": "Yu Mao"
            },
            {
                "authorId": "2312895775",
                "name": "Dongdong Tang"
            },
            {
                "authorId": "2290287053",
                "name": "Hongchao Du"
            },
            {
                "authorId": "2290008872",
                "name": "Nan Guan"
            },
            {
                "authorId": "2283153892",
                "name": "Chun Jason Xue"
            }
        ],
        "abstract": "Large language models (LLMs) exhibit excellent performance in various tasks. However, the memory requirements of LLMs present a great challenge when deploying on memory-limited devices, even for quantized LLMs. This paper introduces a framework to compress LLM after quantization further, achieving about 2.2x compression ratio. A compression-aware quantization is first proposed to enhance model weight compressibility by re-scaling the model parameters before quantization, followed by a pruning method to improve further. Upon this, we notice that decompression can be a bottleneck during practical scenarios. We then give a detailed analysis of the trade-off between memory usage and latency brought by the proposed method. A speed-adaptive method is proposed to overcome it. The experimental results show inference with the compressed model can achieve a 40% reduction in memory size with negligible loss in accuracy and inference speed."
    },
    {
        "paperId": "d7d3791f56e5e801f89d92458deb3171abb89c40",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Dependency resolution at the syntax-semantics interface: psycholinguistic and computational insights on control dependencies",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.12.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.12, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2220421974",
                "name": "Iria de-Dios-Flores"
            },
            {
                "authorId": "2223528163",
                "name": "Juan Pablo Garc\u00eda Amboage"
            },
            {
                "authorId": "2124000464",
                "name": "Marcos Garc\u00eda"
            }
        ],
        "abstract": "Using psycholinguistic and computational experiments we compare the ability of humans and several pre-trained masked language models to correctly identify control dependencies in Spanish sentences such as \u2018Jos\u00e9 le prometi\u00f3/orden\u00f3 a Mar\u00eda ser ordenado/a\u2019 (\u2018Joseph promised/ordered Mary to be tidy\u2019). These structures underlie complex anaphoric and agreement relations at the interface of syntax and semantics, allowing us to study lexically-guided antecedent retrieval processes. Our results show that while humans correctly identify the (un)acceptability of the strings, language models often fail to identify the correct antecedent in non-adjacent dependencies, showing their reliance on linearity. Additional experiments on Galician reinforce these conclusions. Our findings are equally valuable for the evaluation of language models\u2019 ability to capture linguistic generalizations, as well as for psycholinguistic theories of anaphor resolution."
    },
    {
        "paperId": "b37a42da2c5673d12816a7ed3b5dd09342c138a7",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Open-ended Long Text Generation via Masked Language Modeling",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.13.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.13, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "48083523",
                "name": "Xiaobo Liang"
            },
            {
                "authorId": "1576234850",
                "name": "Zecheng Tang"
            },
            {
                "authorId": "2109013629",
                "name": "Juntao Li"
            },
            {
                "authorId": "50495870",
                "name": "M. Zhang"
            }
        ],
        "abstract": "Pre-trained autoregressive (AR) language models such as BART and GPTs have dominated OPen-ended Long Text Generation (Open-LTG).However, the AR nature will decrease the inference efficiency along with the increase of generation length, which hinder their application in Open-LTG.To improve inference efficiency, we alternatively explore the potential of the pre-trained masked language models (MLMs) along with a representative iterative non-autoregressive (NAR) decoding strategy for Open-LTG.Our preliminary study shows that pre-trained MLMs can merely generate short text and will collapse for long text modeling.To enhance the long text generation capability of MLMs, we introduce two simple yet effective strategies for the iterative NAR model: dynamic sliding window attention (DSWA) and linear temperature decay (LTD). It can alleviate long-distance collapse problems and achieve longer text generation with a flexible trade-off between performance and inference speedup.Experiments on the storytelling and multi-paragraph opinionated article writing tasks show that pre-trained MLMs can achieve more than 3 \\times \\to 13 \\times speedup with better performance than strong AR models."
    },
    {
        "paperId": "b89cb555179db43fb9b1f87bd7b5031858c62ccd",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Tailor: A Soft-Prompt-Based Approach to Attribute-Based Controlled Text Generation",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.25.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.25, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2004672900",
                "name": "Kexin Yang"
            },
            {
                "authorId": "2004587660",
                "name": "Dayiheng Liu"
            },
            {
                "authorId": "39165620",
                "name": "Wenqiang Lei"
            },
            {
                "authorId": "21299583",
                "name": "Baosong Yang"
            },
            {
                "authorId": "2065790119",
                "name": "Mingfeng Xue"
            },
            {
                "authorId": "2152687324",
                "name": "Boxing Chen"
            },
            {
                "authorId": "2109935759",
                "name": "Jun Xie"
            }
        ],
        "abstract": "Attribute-based Controlled Text Generation (CTG) refers to generating sentences that satisfy desirable attributes (e.g., emotions and topics). Existing work usually utilize fine-tuning or resort to extra attribute classifiers, yet suffer from increases in storage and inference time. To address these concerns, we explore attribute-based CTG in a parameter-efficient manner. In short, the proposed Tailor represents each attribute as a pre-trained continuous vector i.e., single-attribute prompt), which guides the generation of a fixed pre-trained language model (PLM) to satisfy a pre-specified attribute. These prompts can be simply concatenated as a whole for multi-attribute CTG without any re-training. Nevertheless, this may raise problems of fluency downgrading and position sensitivity. To solve this, Tailor provides two solutions to enhance the combination. The former contains a multi-attribute prompt mask and a re-indexing position sequence to bridge the gap between the training (one single-attribute prompt for each task) and the testing stage (concatenating two prompts). The latter introduces a trainable prompt connector to further enhance the combinations. Experiments demonstrate that, only requiring 0.08% extra training parameters of the GPT-2, Tailor can achieve effective and general improvements on eleven attribute-specific generation tasks."
    },
    {
        "paperId": "b34591db135749b106c7105ba075395953a926cd",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "On-the-fly Cross-lingual Masking for Multilingual Pre-training",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.49.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.49, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2101316388",
                "name": "Xi Ai"
            },
            {
                "authorId": "2144213194",
                "name": "Bin Fang"
            }
        ],
        "abstract": "In multilingual pre-training with the objective of MLM (masked language modeling) on multiple monolingual corpora, multilingual models only learn cross-linguality implicitly from isomorphic spaces formed by overlapping different language spaces due to the lack of explicit cross-lingual forward pass. In this work, we present CLPM (Cross-lingual Prototype Masking), a dynamic and token-wise masking scheme, for multilingual pre-training, using a special token [\\mathcal{C}]_{x} to replace a random token x in the input sentence. [\\mathcal{C}]_{x} is a cross-lingual prototype for x and then forms an explicit cross-lingual forward pass. We instantiate CLPM for the multilingual pre-training phase of UNMT (unsupervised neural machine translation), and experiments show that CLPM can consistently improve the performance of UNMT models on \\{De, Ro, Ne \\} \\leftrightarrow En. Beyond UNMT or bilingual tasks, we show that CLPM can consistently improve the performance of multilingual models on cross-lingual classification."
    },
    {
        "paperId": "b31fb03a86cd44860f1c38e5c7032d9aed10d2f2",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Does GPT-3 Grasp Metaphors? Identifying Metaphor Mappings with Generative Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.58.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.58, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2029658284",
                "name": "Lennart Wachowiak"
            },
            {
                "authorId": "2640975",
                "name": "Dagmar Gromann"
            }
        ],
        "abstract": "Conceptual metaphors present a powerful cognitive vehicle to transfer knowledge structures from a source to a target domain. Prior neural approaches focus on detecting whether natural language sequences are metaphoric or literal. We believe that to truly probe metaphoric knowledge in pre-trained language models, their capability to detect this transfer should be investigated. To this end, this paper proposes to probe the ability of GPT-3 to detect metaphoric language and predict the metaphor\u2019s source domain without any pre-set domains. We experiment with different training sample configurations for fine-tuning and few-shot prompting on two distinct datasets. When provided 12 few-shot samples in the prompt, GPT-3 generates the correct source domain for a new sample with an accuracy of 65.15% in English and 34.65% in Spanish. GPT\u2019s most common error is a hallucinated source domain for which no indicator is present in the sentence. Other common errors include identifying a sequence as literal even though a metaphor is present and predicting the wrong source domain based on specific words in the sequence that are not metaphorically related to the target domain."
    },
    {
        "paperId": "385685d0bffce966475604cd3c3d509f3843cada",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Decoding Symbolism in Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.186.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.186, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "48241395",
                "name": "M. Guo"
            },
            {
                "authorId": "1726601",
                "name": "R. Hwa"
            },
            {
                "authorId": "1770205",
                "name": "Adriana Kovashka"
            }
        ],
        "abstract": "This work explores the feasibility of eliciting knowledge from language models (LMs) to decode symbolism, recognizing something (e.g.,roses) as a stand-in for another (e.g., love). We present our evaluative framework, Symbolism Analysis (SymbA), which compares LMs (e.g., RoBERTa, GPT-J) on different types of symbolism and analyze the outcomes along multiple metrics. Our findings suggest that conventional symbols are more reliably elicited from LMs while situated symbols are more challenging. Results also reveal the negative impact of the bias in pre-trained corpora. We further demonstrate that a simple re-ranking strategy can mitigate the bias and significantly improve model performances to be on par with human performances in some cases."
    },
    {
        "paperId": "9dc7b313c91b0364e109c1cdc5f2b432de971b7a",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "We Understand Elliptical Sentences, and Language Models should Too: A New Dataset for Studying Ellipsis and its Interaction with Thematic Fit",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.188.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.188, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "47091489",
                "name": "D. Testa"
            },
            {
                "authorId": "3433809",
                "name": "Emmanuele Chersoni"
            },
            {
                "authorId": "2038285",
                "name": "Alessandro Lenci"
            }
        ],
        "abstract": "Ellipsis is a linguistic phenomenon characterized by the omission of one or more sentence elements. Solving such a linguistic construction is not a trivial issue in natural language processing since it involves the retrieval of non-overtly expressed verbal material, which might in turn require the model to integrate human-like syntactic and semantic knowledge. In this paper, we explored the issue of how the prototypicality of event participants affects the ability of Language Models (LMs) to handle elliptical sentences and to identify the omitted arguments at different degrees of thematic fit, ranging from highly typical participants to semantically anomalous ones. With this purpose in mind, we built ELLie, the first dataset composed entirely of utterances containing different types of elliptical constructions, and structurally suited for evaluating the effect of argument thematic fit in solving ellipsis and reconstructing the missing element. Our tests demonstrated that the probability scores assigned by the models are higher for typical events than for atypical and impossible ones in different elliptical contexts, confirming the influence of prototypicality of the event participants in interpreting such linguistic structures. Finally, we conducted a retrieval task of the elided verb in the sentence in which the low performance of LMs highlighted a considerable difficulty in reconstructing the correct event."
    },
    {
        "paperId": "79b8689f49fb75c303991f86aa821ff63862d1d5",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Social-Group-Agnostic Bias Mitigation via the Stereotype Content Model",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.227.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2070422394",
                "name": "Ali Omrani"
            },
            {
                "authorId": "2181122081",
                "name": "Alireza S. Ziabari"
            },
            {
                "authorId": "2110963190",
                "name": "Charles Yu"
            },
            {
                "authorId": "1724513406",
                "name": "Preni Golazizian"
            },
            {
                "authorId": "2057247331",
                "name": "Brendan Kennedy"
            },
            {
                "authorId": "6888909",
                "name": "M. Atari"
            },
            {
                "authorId": "144016781",
                "name": "Heng Ji"
            },
            {
                "authorId": "2111849697",
                "name": "Morteza Dehghani"
            }
        ],
        "abstract": "Existing bias mitigation methods require social-group-specific word pairs (e.g., \u201cman\u201d \u2013 \u201cwoman\u201d) for each social attribute (e.g., gender), restricting the bias mitigation to only one specified social attribute. Further, this constraint renders such methods impractical and costly for mitigating bias in understudied and/or unmarked social groups. We propose that the Stereotype Content Model (SCM) \u2014 a theoretical framework developed in social psychology for understanding the content of stereotyping \u2014 can help debiasing efforts to become social-group-agnostic by capturing the underlying connection between bias and stereotypes. SCM proposes that the content of stereotypes map to two psychological dimensions of warmth and competence. Using only pairs of terms for these two dimensions (e.g., warmth: \u201cgenuine\u201d \u2013 \u201cfake\u201d; competence: \u201csmart\u201d \u2013 \u201cstupid\u201d), we perform debiasing with established methods on both pre-trained word embeddings and large language models. We demonstrate that our social-group-agnostic, SCM-based debiasing technique performs comparably to group-specific debiasing on multiple bias benchmarks, but has theoretical and practical advantages over existing approaches."
    },
    {
        "paperId": "60999ddb99aea5a93bd2ba16fb7671dc76bf3ba5",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.232.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.232, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "144315735",
                "name": "Fan Zhou"
            },
            {
                "authorId": "2221367541",
                "name": "Yuzhou Mao"
            },
            {
                "authorId": "38057162",
                "name": "Liu Yu"
            },
            {
                "authorId": "2143686211",
                "name": "Yi Yang"
            },
            {
                "authorId": "46456474",
                "name": "Ting Zhong"
            }
        ],
        "abstract": "Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified. Since pretrain&fine-tune is a major paradigm in NLP applications, separating the debiasing procedure of PLMs from fine-tuning would eventually harm the actual downstream utility. In this paper, we propose a unified debiasing framework Causal-Debias to remove unwanted stereotypical associations in PLMs during fine-tuning. Specifically, CausalDebias mitigates bias from a causal invariant perspective by leveraging the specific downstream task to identify bias-relevant and labelrelevant factors. We propose that bias-relevant factors are non-causal as they should have little impact on downstream tasks, while labelrelevant factors are causal. We perform interventions on non-causal factors in different demographic groups and design an invariant risk minimization loss to mitigate bias while maintaining task performance. Experimental results on three downstream tasks show that our proposed method can remarkably reduce unwanted stereotypical associations after PLMs are finetuned, while simultaneously minimizing the impact on PLMs and downstream applications."
    },
    {
        "paperId": "eae3b3f2ec5f362c873d96802fd83b598b8e106e",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Connective Prediction for Implicit Discourse Relation Recognition via Knowledge Distillation",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.325.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2149178202",
                "name": "Hongyi Wu"
            },
            {
                "authorId": "2111824520",
                "name": "Hao Zhou"
            },
            {
                "authorId": "49284832",
                "name": "Man Lan"
            },
            {
                "authorId": "3174675",
                "name": "Yuanbin Wu"
            },
            {
                "authorId": "2108039288",
                "name": "Yadong Zhang"
            }
        ],
        "abstract": "Implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis due to the absence of connectives. Most existing methods utilize one-hot labels as the sole optimization target, ignoring the internal association among connectives. Besides, these approaches spend lots of effort on template construction, negatively affecting the generalization capability. To address these problems,we propose a novel Connective Prediction via Knowledge Distillation (CP-KD) approach to instruct large-scale pre-trained language models (PLMs) mining the latent correlations between connectives and discourse relations, which is meaningful for IDRR. Experimental results on the PDTB 2.0/3.0 and CoNLL2016 datasets show that our method significantly outperforms the state-of-the-art models on coarse-grained and fine-grained discourse relations. Moreover, our approach can be transferred to explicit discourse relation recognition(EDRR) and achieve acceptable performance."
    },
    {
        "paperId": "38f3c4f8bddb7d9eeb9ab5558c716094e9c8c681",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "What social attitudes about gender does BERT encode? Leveraging insights from psycholinguistics",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.375.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "47480290",
                "name": "J. Watson"
            },
            {
                "authorId": "2076212",
                "name": "Barend Beekhuizen"
            },
            {
                "authorId": "145584212",
                "name": "S. Stevenson"
            }
        ],
        "abstract": "Much research has sought to evaluate the degree to which large language models reflect social biases. We complement such work with an approach to elucidating the connections between language model predictions and people\u2019s social attitudes. We show how word preferences in a large language model reflect social attitudes about gender, using two datasets from human experiments that found differences in gendered or gender neutral word choices by participants with differing views on gender (progressive, moderate, or conservative). We find that the language model BERT takes into account factors that shape human lexical choice of such language, but may not weigh those factors in the same way people do. Moreover, we show that BERT\u2019s predictions most resemble responses from participants with moderate to conservative views on gender. Such findings illuminate how a language model: (1) may differ from people in how it deploys words that signal gender, and (2) may prioritize some social attitudes over others."
    },
    {
        "paperId": "5bf9e92343dae42bef58d3af491b41babc892f00",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Are Experts Needed? On Human Evaluation of Counselling Reflection Generation",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.382.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "151480727",
                "name": "Zixiu \"Alex\" Wu"
            },
            {
                "authorId": "40917675",
                "name": "Simone Balloccu"
            },
            {
                "authorId": "2113922820",
                "name": "Ehud Reiter"
            },
            {
                "authorId": "3064543",
                "name": "Rim Helaoui"
            },
            {
                "authorId": "1752584925",
                "name": "Diego Reforgiato Recupero"
            },
            {
                "authorId": "1802743",
                "name": "Daniele Riboni"
            }
        ],
        "abstract": "Reflection is a crucial counselling skill where the therapist conveys to the client their interpretation of what the client said. Language models have recently been used to generate reflections automatically, but human evaluation is challenging, particularly due to the cost of hiring experts. Laypeople-based evaluation is less expensive and easier to scale, but its quality is unknown for reflections. Therefore, we explore whether laypeople can be an alternative to experts in evaluating a fundamental quality aspect: coherence and context-consistency. We do so by asking a group of laypeople and a group of experts to annotate both synthetic reflections and human reflections from actual therapists. We find that both laypeople and experts are reliable annotators and that they have moderate-to-strong inter-group correlation, which shows that laypeople can be trusted for such evaluations. We also discover that GPT-3 mostly produces coherent and consistent reflections, and we explore changes in evaluation results when the source of synthetic reflections changes to GPT-3 from the less powerful GPT-2."
    },
    {
        "paperId": "52f3b181f6361cd85914798273e497264b2bc32d",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Factual or Contextual? Disentangling Error Types in Entity Description Generation",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.463.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.463, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2000637509",
                "name": "Navita Goyal"
            }
        ],
        "abstract": "In the task of entity description generation, given a context and a specified entity, a model must describe that entity correctly and in a contextually-relevant way. In this task, as well as broader language generation tasks, the generation of a nonfactual description (factual error) versus an incongruous description (contextual error) is fundamentally different, yet often conflated. We develop an evaluation paradigm that enables us to disentangle these two types of errors in naturally occurring textual contexts. We find that factuality and congruity are often at odds, and that models specifically struggle with accurate descriptions of entities that are less familiar to people. This shortcoming of language models raises concerns around the trustworthiness of such models, since factual errors on less well-known entities are exactly those that a human reader will not recognize."
    },
    {
        "paperId": "e9480692336efb4930e13184f683f0c001d0e710",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "My side, your side and the evidence: Discovering aligned actor groups and the narratives they weave",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.497.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.497, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "1645199194",
                "name": "Pavan Holur"
            },
            {
                "authorId": "2059291722",
                "name": "David Chong"
            },
            {
                "authorId": "3338615",
                "name": "Timothy R. Tangherlini"
            },
            {
                "authorId": "1686063",
                "name": "V. Roychowdhury"
            }
        ],
        "abstract": "News reports about emerging issues often include several conflicting story lines. Individual stories can be conceptualized as samples from an underlying mixture of competing narratives. The automated identification of these distinct narratives from unstructured text is a fundamental yet difficult task in Computational Linguistics since narratives are often intertwined and only implicitly conveyed in text. In this paper, we consider a more feasible proxy task: Identify the distinct sets of aligned story actors responsible for sustaining the issue-specific narratives. Discovering aligned actors, and the groups these alignments create, brings us closer to estimating the narrative that each group represents. With the help of Large Language Models (LLM), we address this task by: (i) Introducing a corpus of text segments rich in narrative content associated with six different current issues; (ii) Introducing a novel two-step graph-based framework that (a) identifies alignments between actors (INCANT) and (b) extracts aligned actor groups using the network structure (TAMPA). Amazon Mechanical Turk evaluations demonstrate the effectiveness of our framework. Across domains, alignment relationships from INCANT are accurate (macro F1 >= 0.75) and actor groups from TAMPA are preferred over 2 non-trivial baseline models (ACC >= 0.75)."
    },
    {
        "paperId": "85a5c0bb6660990c7b5e348c165c552c1d4a2c2a",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Large-scale Lifelong Learning of In-context Instructions and How to Tackle It",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.703.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.703, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "1659078761",
                "name": "J. Mok"
            },
            {
                "authorId": "36733784",
                "name": "Jaeyoung Do"
            },
            {
                "authorId": "2108230831",
                "name": "Sungjin Lee"
            },
            {
                "authorId": "2221288115",
                "name": "Tara Taghavi"
            },
            {
                "authorId": "1885974",
                "name": "Seunghak Yu"
            },
            {
                "authorId": "2999019",
                "name": "Sungroh Yoon"
            }
        ],
        "abstract": "Jointly fine-tuning a Pre-trained Language Model (PLM) on a pre-defined set of tasks with in-context instructions has been proven to improve its generalization performance, allowing us to build a universal language model that can be deployed across task boundaries. In this work, we explore for the first time whether this attractive property of in-context instruction learning can be extended to a scenario in which tasks are fed to the target PLM in a sequential manner. The primary objective of so-called lifelong in-context instruction learning is to improve the target PLM\u2019s instance- and task-level generalization performance as it observes more tasks. DynaInst, the proposed method to lifelong in-context instruction learning, achieves noticeable improvements in both types of generalization, nearly reaching the upper bound performance obtained through joint training."
    },
    {
        "paperId": "1756110b0dcb787c48d807a2650aa91827eafcbd",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "A Diverse Set of Freely Available Linguistic Resources for Turkish",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.768.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.768, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2678717",
                "name": "Duygu Altinok"
            }
        ],
        "abstract": "This study presents a diverse set of freely available linguistic resources for Turkish natural language processing, including corpora, pretrained models and education material. Although Turkish is spoken by a sizeable population of over 80 million people, Turkish linguistic resources for natural language processing remain scarce. In this study, we provide corpora to allow practitioners to build their own applications and pretrained models that would assist industry researchers in creating quick prototypes. The provided corpora include named entity recognition datasets of diverse genres, including Wikipedia articles and supplement products customer reviews. In addition, crawling e-commerce and movie reviews websites, we compiled several sentiment analysis datasets of different genres. Our linguistic resources for Turkish also include pretrained spaCy language models. To the best of our knowledge, our models are the first spaCy models trained for the Turkish language. Finally, we provide various types of education material, such as video tutorials and code examples, that can support the interested audience on practicing Turkish NLP. The advantages of our linguistic resources are three-fold: they are freely available, they are first of their kind, and they are easy to use in a broad range of implementations. Along with a thorough description of the resource creation process, we also explain the position of our resources in the Turkish NLP world."
    },
    {
        "paperId": "847e106814753600f5029b8dbaa4255bf1f716e2",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.853.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.853, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "47829199",
                "name": "Mario Ezra Arag\u00f3n"
            },
            {
                "authorId": "1400883876",
                "name": "Adrian Pastor Lopez-Monroy"
            },
            {
                "authorId": "2177617077",
                "name": "Luis C. Gonz\u00e1lez"
            },
            {
                "authorId": "2356644",
                "name": "David E. Losada"
            },
            {
                "authorId": "2068628554",
                "name": "M. Montes"
            }
        ],
        "abstract": "Mental disorders affect millions of people worldwide and cause interference with their thinking and behavior. Through the past years, awareness created by health campaigns and other sources motivated the study of these disorders using information extracted from social media platforms. In this work, we aim to contribute to the study of these disorders and to the understanding of how mental problems reflect on social media. To achieve this goal, we propose a double-domain adaptation of a language model. First, we adapted the model to social media language, and then, we adapted it to the mental health domain. In both steps, we incorporated a lexical resource to guide the masking process of the language model and, therefore, to help it in paying more attention to words related to mental disorders. We have evaluated our model in the detection of signs of three major mental disorders: Anorexia, Self-harm, and Depression. Results are encouraging as they show that the proposed adaptation enhances the classification performance and yields competitive results against state-of-the-art methods."
    },
    {
        "paperId": "779541469ba3ea6f4ba5f8d4af944b88d11a7da4",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.878.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.878, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "1455192856",
                "name": "Krithika Ramesh"
            },
            {
                "authorId": "1585915155",
                "name": "Arnav Chavan"
            },
            {
                "authorId": "1824294087",
                "name": "Shrey Pandit"
            },
            {
                "authorId": "3010457",
                "name": "Sunayana Sitaram"
            }
        ],
        "abstract": "Compression techniques for deep learning have become increasingly popular, particularly in settings where latency and memory constraints are imposed. Several methods, such as pruning, distillation, and quantization, have been adopted for compressing models, each providing distinct advantages. However, existing literature demonstrates that compressing deep learning models could affect their fairness. Our analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which we benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification. We also investigate the impact of using multilingual models and evaluation measures. Our findings highlight the significance of considering both the pre-trained model and the chosen compression strategy in developing equitable language technologies. The results also indicate that compression strategies can have an adverse effect on fairness measures."
    },
    {
        "paperId": "2188be5e678375348ada139a74d14c5559f8d2b3",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Ellipsis-Dependent Reasoning: a New Challenge for Large Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-short.4.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-short.4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "144610213",
                "name": "D. Hardt"
            }
        ],
        "abstract": "We propose a novel challenge for large language models: ellipsis-dependent reasoning. We define several structures of paired examples, where an ellipsis example is matched to its non-ellipsis counterpart, and a question is posed which requires resolution of the ellipsis. Test results show that the best models perform well on non-elliptical examples but struggle with all but the simplest ellipsis structures."
    },
    {
        "paperId": "63d8cd208d029d54a3c31f4c47d11ad9b2b16dee",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-short.14.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-short.14, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2038115602",
                "name": "Andrea Gregor de Varda"
            },
            {
                "authorId": "48188880",
                "name": "M. Marelli"
            }
        ],
        "abstract": "Neural language models are increasingly valued in computational psycholinguistics, due to their ability to provide conditional probability distributions over the lexicon that are predictive of human processing times. Given the vast array of available models, it is of both theoretical and methodological importance to assess what features of a model influence its psychometric quality. In this work we focus on parameter size, showing that larger Transformer-based language models generate probabilistic estimates that are less predictive of early eye-tracking measurements reflecting lexical access and early semantic integration. However, relatively bigger models show an advantage in capturing late eye-tracking measurements that reflect the full semantic and syntactic integration of a word into the current language context. Our results are supported by eye movement data in ten languages and consider four models, spanning from 564M to 4.5B parameters."
    },
    {
        "paperId": "f30ec0882223008d898136f31774ac08c2950b92",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Credible without Credit: Domain Experts Assess Generative Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-short.37.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-short.37, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2221286885",
                "name": "Denis Peskoff"
            },
            {
                "authorId": "28924497",
                "name": "Brandon M Stewart"
            }
        ],
        "abstract": "Language models have recently broken into the public consciousness with the release of the wildly popular ChatGPT. Commentators have argued that language models could replace search engines, make college essays obsolete, or even write academic research papers. All of these tasks rely on accuracy of specialized information which can be difficult to assess for non-experts. Using 10 domain experts across science and culture, we provide an initial assessment of the coherence, conciseness, accuracy, and sourcing of two language models across 100 expert-written questions. While we find the results are consistently cohesive and concise, we find that they are mixed in their accuracy. These results raise questions of the role language models should play in general-purpose and expert knowledge seeking."
    },
    {
        "paperId": "a066a8612253031def43f513296f5447ab80b0a0",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Discourse-Level Representations can Improve Prediction of Degree of Anxiety",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-short.128.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-short.128, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2216074970",
                "name": "Swanie Juhng"
            },
            {
                "authorId": "1386902685",
                "name": "Matthew Matero"
            },
            {
                "authorId": "2175480361",
                "name": "Vasudha Varadarajan"
            },
            {
                "authorId": "2615635",
                "name": "J. Eichstaedt"
            },
            {
                "authorId": "145743099",
                "name": "Adithya V Ganesan"
            },
            {
                "authorId": "145035129",
                "name": "H. A. Schwartz"
            }
        ],
        "abstract": "Anxiety disorders are the most common of mental illnesses, but relatively little is known about how to detect them from language. The primary clinical manifestation of anxiety is worry associated cognitive distortions, which are likely expressed at the discourse-level of semantics. Here, we investigate the development of a modern linguistic assessment for degree of anxiety, specifically evaluating the utility of discourse-level information in addition to lexical-level large language model embeddings. We find that a combined lexico-discourse model outperforms models based solely on state-of-the-art contextual embeddings (RoBERTa), with discourse-level representations derived from Sentence-BERT and DiscRE both providing additional predictive power not captured by lexical-level representations. Interpreting the model, we find that discourse patterns of causal explanations, among others, were used significantly more by those scoring high in anxiety, dovetailing with psychological literature."
    },
    {
        "paperId": "9b8652c6e276d33f8ce5ed01102a8642ec548b57",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Logical Transformers: Infusing Logical Structures into Pre-Trained Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.111.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.111?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2203187",
                "name": "Borui Wang"
            },
            {
                "authorId": "2110991956",
                "name": "Qiuyuan Huang"
            },
            {
                "authorId": "48717082",
                "name": "Budhaditya Deb"
            },
            {
                "authorId": "1793302",
                "name": "Aaron L Halfaker"
            },
            {
                "authorId": "5958959",
                "name": "Liqun Shao"
            },
            {
                "authorId": "1801452",
                "name": "Daniel J. McDuff"
            },
            {
                "authorId": "2072795428",
                "name": "A. Awadallah"
            },
            {
                "authorId": "9215251",
                "name": "Dragomir R. Radev"
            },
            {
                "authorId": "48441311",
                "name": "Jianfeng Gao"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "56de9c4c63ee74757be1b203d2ea852690087ded",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Hence, Socrates is mortal: A Benchmark for Natural Language Syllogistic Reasoning",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.148.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.148?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.148, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2115859566",
                "name": "Yongkang Wu"
            },
            {
                "authorId": "2223174680",
                "name": "Meng Han"
            },
            {
                "authorId": "1900406",
                "name": "Yutao Zhu"
            },
            {
                "authorId": "47681096",
                "name": "Lei Li"
            },
            {
                "authorId": null,
                "name": "Xinyu Zhang"
            },
            {
                "authorId": "2128243415",
                "name": "Ruofei Lai"
            },
            {
                "authorId": "2108181908",
                "name": "Xiaoguang Li"
            },
            {
                "authorId": "10710723",
                "name": "Yuanhang Ren"
            },
            {
                "authorId": "1897235",
                "name": "Zhicheng Dou"
            },
            {
                "authorId": "2106400572",
                "name": "Zhao Cao"
            }
        ],
        "abstract": "Syllogistic reasoning, a typical form of deductive reasoning, is a critical capability widely required in natural language understanding tasks, such as text entailment and question answering. To better facilitate research on syllogis-tic reasoning, we develop a benchmark called S YLLO B ASE that differs from existing syllo-gistic datasets in three aspects: (1) Covering a complete taxonomy of syllogism reasoning patterns; (2) Containing both automatically and manually constructed samples; and (3) Involving both the generation and understanding tasks. We automatically construct 50k template-based syllogism samples by mining syllogism patterns from Wikidata and ConceptNet. To improve our dataset\u2019s naturalness and challenge, we apply GPT-3 to paraphrase the template-based data and further manually rewrite 1,000 samples as the test set. State-of-the-art pre-trained language models can achieve the best generation ROUGE-L of 38.72 by T5 and the best multi-choice accuracy of 72.77% by RoBERTa on S YLLO B ASE , which indicates the great challenge of learning diverse syllo-gistic reasoning types on S YLLO B ASE . Our datasets are released at https://github.com/ casually-PYlearner/SYLLOBASE ."
    },
    {
        "paperId": "3b1516e9ed48e773e6acd20b6a130ea3bb6fdc13",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Playing the Part of the Sharp Bully: Generating Adversarial Examples for Implicit Hate Speech Detection",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.173.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.173?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.173, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2215623665",
                "name": "Nicol\u00e1s Benjam\u00edn Ocampo"
            },
            {
                "authorId": "1772891",
                "name": "Elena Cabrio"
            },
            {
                "authorId": "1725656",
                "name": "S. Villata"
            }
        ],
        "abstract": "Research on abusive content detection on social media has primarily focused on explicit forms of hate speech (HS), that are often identifiable by recognizing hateful words and expressions. Messages containing linguistically subtle and implicit forms of hate speech still constitute an open challenge for automatic hate speech detection. In this paper, we propose a new framework for generating adversarial implicit HS short-text messages using Auto-regressive Language Models. Moreover, we propose a strategy to group the generated implicit messages by their complexity levels (EASY, MEDIUM, and HARD categories) characterizing how challenging these messages are for supervised classifiers. Finally, relying on (Dinan et al., 2019; Vidgen et al., 2021), we propose a \u201cbuild it, break it, fix it\u201d, training scheme using HARD messages showing how iteratively retraining on HARD messages substantially leverages SOTA models\u2019 performances on implicit HS benchmarks."
    },
    {
        "paperId": "1a35dd31f56adfd9217e7943f49fa27e446b4149",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Sequential Path Signature Networks for Personalised Longitudinal Language Modeling",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.310.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.310?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.310, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2223156840",
                "name": "Talia Tseriotou"
            },
            {
                "authorId": "32728828",
                "name": "Adam Tsakalidis"
            },
            {
                "authorId": "2067847845",
                "name": "Peter Foster"
            },
            {
                "authorId": "2069014273",
                "name": "T. Lyons"
            },
            {
                "authorId": "48717312",
                "name": "M. Liakata"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "c373c792bcf5d0add8de812425d384ff101ef070",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Unlearning Bias in Language Models by Partitioning Gradients",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.375.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.375?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2110963190",
                "name": "Charles Yu"
            },
            {
                "authorId": "2165226931",
                "name": "Sullam Jeoung"
            },
            {
                "authorId": "1581343094",
                "name": "Anish Kasi"
            },
            {
                "authorId": "144808890",
                "name": "Pengfei Yu"
            },
            {
                "authorId": "2072975661",
                "name": "Heng Ji"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "53831a304fb568aea4548efcef910cc62f2d2dcb",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.411.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.411?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.411, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2057732429",
                "name": "Bolin Lai"
            },
            {
                "authorId": "2118083343",
                "name": "Hongxin Zhang"
            },
            {
                "authorId": "2108511234",
                "name": "Miao Liu"
            },
            {
                "authorId": "2197412970",
                "name": "Aryan Pariani"
            },
            {
                "authorId": "119797486",
                "name": "Fiona Ryan"
            },
            {
                "authorId": "2072957749",
                "name": "Wenqi Jia"
            },
            {
                "authorId": "31998283",
                "name": "Shirley Anugrah Hayati"
            },
            {
                "authorId": "50779871",
                "name": "J. Rehg"
            },
            {
                "authorId": "2143919864",
                "name": "Diyi Yang"
            }
        ],
        "abstract": "Persuasion modeling is a key building block for conversational agents. Existing works in this direction are limited to analyzing textual dialogue corpora. We argue that visual signals also play an important role in understanding human persuasive behaviors. In this paper, we introduce the first multimodal dataset for modeling persuasion behaviors. Our dataset includes 199 dialogue transcriptions and videos captured in a multi-player social deduction game setting, 26 , 647 utterance level annotations of persuasion strategy, and game level annotations of deduction game outcomes. We provide extensive experiments to show how dialogue context and visual signals benefit persuasion strategy prediction. We also explore the generalization ability of language models for persuasion modeling and the role of persuasion strategies in predicting social deduction game outcomes. Our dataset can be found at https://persuasion-deductiongame. socialai-data.org . The codes and models are available at https://github.com/ SALT-NLP/PersuationGames ."
    },
    {
        "paperId": "bb807e1821eb9debd15be620bf63d05c3f371283",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Spontaneous gestures encoded by hand positions improve language models: An Information-Theoretic motivated study",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.600.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.600?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.600, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "39418120",
                "name": "Yang Xu"
            },
            {
                "authorId": "2166314315",
                "name": "Yang Cheng"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "a31c871f6137edd391fc7be0c647a7a208681caa",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Figurative Language Processing: A Linguistically Informed Feature Analysis of the Behavior of Language Models and Humans",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.622.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.622?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2143040728",
                "name": "Hyewon Jang"
            },
            {
                "authorId": "2101193177",
                "name": "Qi Yu"
            },
            {
                "authorId": "2752887",
                "name": "Diego Frassinelli"
            }
        ],
        "abstract": "Recent years have witnessed a growing interest in investigating what Transformer-based language models (TLMs) actually learn from the training data. This is especially relevant for complex tasks such as the understanding of non-literal meaning. In this work, we probe the performance of three black-box TLMs and two intrinsically transparent white-box models on \ufb01gurative language classi\ufb01cation of sar-casm , similes , idioms , and metaphors . We conduct two studies on the classi\ufb01cation results to provide insights into the inner workings of such models. With our \ufb01rst analysis on feature importance, we identify crucial differences in model behavior. With our second analysis using an online experiment with human participants, we inspect different linguistic characteristics of the four \ufb01gurative language types."
    },
    {
        "paperId": "d35d176b2dfda65a8ca3d7f48d6603f9797c7889",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "MTR: A Dataset Fusing Inductive, Deductive, and Defeasible Reasoning",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.640.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.640?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.640, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2124530624",
                "name": "Yitian Li"
            },
            {
                "authorId": "47069009",
                "name": "Jidong Tian"
            },
            {
                "authorId": "2093926420",
                "name": "Caoyun Fan"
            },
            {
                "authorId": "2108997748",
                "name": "Wenqing Chen"
            },
            {
                "authorId": "144111414",
                "name": "Hao He"
            },
            {
                "authorId": "35692109",
                "name": "Yaohui Jin"
            }
        ],
        "abstract": "A long-standing difficulty in AI is the introduction of human-like reasoning in machine reading comprehension. Since algorithmic models can already perform as well as humans on simple quality assurance tasks thanks to the development of deep learning techniques, more difficult reasoning datasets have been presented. However, these datasets mainly focus on a single type of reasoning. There are still significant gaps in the studies when compared to the complex reasoning used in daily life because we can mix and match different types of reasoning un-consciously. In this work, we introduce a brand-new dataset, named MT R . There are two sub-sets of it: (1)the first is mainly used to explore mixed reasoning abilities and combines deductive and inductive reasoning; (2)the second integrates inductive and defeasible reasoning for detecting non-monotonic reasoning ability. It consists of more than 30k instances, requiring models to infer relations between characters in short stories. Compared with the corresponding single reasoning datasets, MT R serves as a more challenging one, highlighting the gap in language models\u2019 ability to handle sophisticated inference."
    },
    {
        "paperId": "bc452daa5f79a258f33cbf6071cc23aabf6cc750",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "NewsMet : A 'do it all' Dataset of Contemporary Metaphors in News Headlines",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.641.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.641?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.641, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2175481052",
                "name": "Rohan Joseph"
            },
            {
                "authorId": "2109992450",
                "name": "Timothy Liu"
            },
            {
                "authorId": "1557327062",
                "name": "Aik Beng Ng"
            },
            {
                "authorId": "144308998",
                "name": "S. See"
            },
            {
                "authorId": "2773814",
                "name": "Sunny Rai"
            }
        ],
        "abstract": "Metaphors are highly creative constructs of human language that grow old and eventually die. Popular datasets used for metaphor processing tasks were constructed from dated source texts. In this paper, we propose NewsMet, a large high-quality contemporary dataset of news headlines hand-annotated with metaphorical verbs. The dataset comprises headlines from various sources including political, satir-ical, reliable and fake. Our dataset serves the purpose of evaluation for the tasks of metaphor interpretation and generation. The experiments reveal several insights and limitations of using LLMs to automate metaphor processing tasks as frequently seen in the recent literature. The dataset is publicly available for research purposes 1 ."
    },
    {
        "paperId": "016c8d91f8a102111dc5eb76ab4ce433b9e2ec53",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "How Well Do Large Language Models Perform on Faux Pas Tests?",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.663.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.663?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2067118898",
                "name": "Natalie Shapira"
            },
            {
                "authorId": "2223157270",
                "name": "Guy Zwirn"
            },
            {
                "authorId": "79775260",
                "name": "Yoav Goldberg"
            }
        ],
        "abstract": "Motivated by the question of the extent to which large language models \u201cunderstand\u201d social intelligence, we investigate the ability of such models to generate correct responses to questions involving descriptions of faux pas situations. The faux pas test is a test used in clinical psychology, which is known to be more challenging for children than individual tests of theory-of-mind or social intelligence. Our re-sults demonstrate that, while the models seem to sometimes offer correct responses, they in fact struggle with this task, and that many of the seemingly correct responses can be attributed to over-interpretation by the human reader (\u201cthe ELIZA effect\u201d). An additional phenomenon observed is the failure of most models to generate a correct response to presupposition questions. Finally, in an experiment in which the models are tasked with generating original faux pas stories, we find that while some models are capable of generating novel faux pas stories, the stories are all explicit, as the models are limited in their abilities to describe situations in an implicit manner."
    },
    {
        "paperId": "da7f9e6416d952355d1d4974f0b73296719915f1",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Coherent or Not? Stressing a Neural Language Model for Discourse Coherence in Multiple Languages",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.680.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.680?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.680, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "3460752",
                "name": "D. Brunato"
            },
            {
                "authorId": "2187131",
                "name": "F. Dell\u2019Orletta"
            },
            {
                "authorId": "2187454874",
                "name": "Luca Dini"
            },
            {
                "authorId": "39356242",
                "name": "Andrea Amelio Ravelli"
            }
        ],
        "abstract": "In this study, we investigate the capability of a Neural Language Model (NLM) to distinguish between coherent and incoherent text, where the latter has been artificially created to gradually undermine local coherence within text. While previous research on coherence assessment using NLMs has primarily focused on English, we extend our investigation to multiple languages. We employ a consistent evaluation framework to compare the performance of monolingual and multilingual models in both in-domain and out-domain settings. Additionally, we explore the model\u2019s performance in a cross-language scenario."
    },
    {
        "paperId": "fd9c1e6f09dc59b9d3fd92d48077f23f38ed3bd2",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "SORTIE: Dependency-Aware Symbolic Reasoning for Logical Data-to-text Generation",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.715.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.715?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.715, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2878414",
                "name": "Xueliang Zhao"
            },
            {
                "authorId": "2156525869",
                "name": "Tingchen Fu"
            },
            {
                "authorId": "2978364",
                "name": "Lemao Liu"
            },
            {
                "authorId": "47648549",
                "name": "Lingpeng Kong"
            },
            {
                "authorId": "2072684668",
                "name": "Shuming Shi"
            },
            {
                "authorId": "2055862968",
                "name": "Rui Yan"
            }
        ],
        "abstract": "Logical data-to-text generation is a representative task in measuring the capabilities of both language generation and complex reasoning. Despite the introduction of reasoning skills in generation, existing works still rely on neural language models to output the final table description. However, due to the inefficacy of neural language models in complex reasoning, these methods inevitably have difficulty working out key entities in the description and might produce unfaithful descriptions. To alleviate these issues, we propose a dependency-aware symbolic reasoning framework that reasons out each entity in the table description with our designed table-compatible programming language. To figure out the dependency relationship among entities, we devise an entity scheduling mechanism to determine the order of programme synthesis such that the reasoning of an entity only relies on other \u201cre-solved\u201d entities. Experiments on three datasets and three backbones show that ours outperforms previous methods not only in surface-level fidelity but also in logical fidelity. No-tably, the proposed framework enhances GPT-2, BART and T5 with an absolute improvement of 5 . 7% \u223c 11 . 5% on SP-Acc."
    },
    {
        "paperId": "6000c002790c2a94cba0a6cbafe88cd962b670d5",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Disfluency Generation for More Robust Dialogue Systems",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.728.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.728?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.728, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "35150510",
                "name": "Benjamin Marie"
            }
        ],
        "abstract": "Disfluencies in user utterances can trigger a chain of errors impacting all the modules of a dialogue system: natural language understanding, dialogue state tracking, and response generation. In this work, we first analyze existing dialogue datasets commonly used in research and show that they only contain a marginal number of disfluent utterances. Due to this relative absence of disfluencies in their training data, dialogue systems may then critically fail when exposed to disfluent utterances. Following this observation, we propose to augment existing datasets with disfluent user utterances by para-phrasing fluent utterances into disfluent ones. Relying on a pre-trained language model, our few-shot disfluent paraphraser guided by a dis-fluency classifier can generate useful disfluent utterances for training better dialogue systems. We report on improvements for both dialogue state tracking and response generation when the dialogue systems are trained on datasets augmented with our disfluent utterances."
    },
    {
        "paperId": "b4e637bc9fd4678b747b07af21ecc435dc1c5441",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "HuaSLIM: Human Attention Motivated Shortcut Learning Identification and Mitigation for Large Language models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.781.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.781?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.781, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "152321611",
                "name": "Yuqi Ren"
            },
            {
                "authorId": "2694222",
                "name": "Deyi Xiong"
            }
        ],
        "abstract": "Large language models have made remarkable progress on a variety of NLP tasks. However, it has been found that they tend to rely on shortcut features that spuriously correlate with labels for prediction, which weakens their generalization on out-of-distribution samples. In this paper, we propose a human attention guided approach to identifying and mitigating short-cut learning, which encourages the LLM-based target model to learn relevant features. We define an attention-based measurement to capture both model and data bias and identify short-cut tokens by exploring both human and neural attention. In a self-distillation framework, we mitigate shortcut learning by dynamically adjusting the distillation temperature according to the detected shortcut tokens and estimated shortcut degree. Additionally, we utilize human attention as a supervisory signal to constrain large language models to pay more attention to relevant tokens. Experimental results on multiple NLP tasks show that our proposed method can effectively identify shortcut tokens, and significantly improve the robustness of large language models on OOD samples, while not undermining the performance on IID data."
    },
    {
        "paperId": "a9d5ebe5281ff6d1660ebd42351f0e5033741811",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "DMLM: Descriptive Masked Language Modeling",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.808.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.808?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "1810690342",
                "name": "Edoardo Barba"
            },
            {
                "authorId": "1810689808",
                "name": "Niccol\u00f2 Campolungo"
            },
            {
                "authorId": "2068519190",
                "name": "Roberto Navigli"
            }
        ],
        "abstract": "Over the last few years, Masked Language Modeling (MLM) pre-training has resulted in remarkable advancements in many Natural Language Understanding (NLU) tasks, which has sparked an interest in researching alternatives and extensions to the MLM objective. In this paper, we tackle the absence of explicit semantic grounding in MLM and propose Descriptive Masked Language Modeling (DMLM), a knowledge-enhanced reading comprehension objective, where the model is required to predict the most likely word in a context, being provided with the word\u2019s definition. For instance, given the sentence \u201cI was going to the _\u201d, if we provided as definition \u201cfinan-cial institution\u201d, the model would have to predict the word \u201cbank\u201d; if, instead, we provided \u201csandy seashore\u201d, the model should predict \u201cbeach\u201d. Our evaluation highlights the effectiveness of DMLM in comparison with standard MLM, showing improvements on a number of well-established NLU benchmarks, as well as other semantics-focused tasks, e.g., Semantic Role Labeling. Furthermore, we also demonstrate how it is possible to take full advantage of DMLM to embed explicit semantics in downstream tasks, explore several properties of DMLM-based contextual representations and suggest a number of future directions to investigate."
    },
    {
        "paperId": "e9ab3d80b728ba888ca61750914d30f7204a3e4b",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Causal Intervention for Mitigating Name Bias in Machine Reading Comprehension",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-acl.812.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.812?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.812, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-07-09",
        "authors": [
            {
                "authorId": "2179691980",
                "name": "Jiazheng Zhu"
            },
            {
                "authorId": "2042696429",
                "name": "Shaojuan Wu"
            },
            {
                "authorId": "1780738",
                "name": "Xiaowang Zhang"
            },
            {
                "authorId": "1785922",
                "name": "Yuexian Hou"
            },
            {
                "authorId": "2113908670",
                "name": "Zhiyong Feng"
            }
        ],
        "abstract": "Machine Reading Comprehension (MRC) is to answer questions based on a given passage, which has made great achievements using pre-trained Language Models (LMs). We study the robustness of MRC models to names which is flexible and repeatability. MRC models based on LMs may overuse the name information to make predictions, which causes the representation of names to be non-interchangeable, called name bias . In this paper, we propose a novel Causal Interventional paradigm for MRC (CI4MRC) to mitigate name bias. Specifically, we uncover that the pre-trained knowledge concerning names is indeed a confounder by analyzing the causalities among the pre-trained knowledge, context representation and answers based on a Structural Causal Model (SCM). We develop effective CI4MRC algorithmic implementations to constrain the confounder based on the neuron-wise and token-wise adjustments. Experiments demonstrate that our proposed CI4MRC effectively mitigates the name bias and achieves competitive performance on the original SQuAD. Moreover, our method is general to various pre-trained LMs and performs robustly on the adversarial datasets."
    },
    {
        "paperId": "a264ff5f64ee14bc5d824c938b88403c7e201799",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Chain-of-Exemplar: Enhancing Distractor Generation for Multimodal Educational Question Generation",
        "openAccessPdf": {
            "url": "https://ink.library.smu.edu.sg/context/sis_research/article/10235/viewcontent/2024.acl_long.432.pdf",
            "status": "GREEN",
            "license": "CCBYNCND",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2024.acl-long.432?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2024.acl-long.432, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-11",
        "authors": [
            {
                "authorId": "2273922151",
                "name": "Haohao Luo"
            },
            {
                "authorId": "2273841947",
                "name": "Yang Deng"
            },
            {
                "authorId": "2273822319",
                "name": "Ying Shen"
            },
            {
                "authorId": "2241348826",
                "name": "See-Kiong Ng"
            },
            {
                "authorId": "2257036129",
                "name": "Tat-Seng Chua"
            }
        ],
        "abstract": "Multiple-choice questions (MCQs) are important in enhancing concept learning and student engagement for educational purposes. Despite the multimodal nature of educational content, current methods focus mainly on text-based inputs and often neglect the integration of visual information. In this work, we study the problem of multimodal educational question generation, which aims at generating subject-specific educational questions with plausible yet incorrect distractors based on multimodal educational content. To tackle this problem, we introduce a novel framework, named Chain-of-Exemplar ( CoE ), which utilizes multimodal large language models (MLLMs) with Chain-of-Thought reasoning to improve the generation of challenging distractors. Furthermore, CoE leverages three-stage contextualized exemplar retrieval to retrieve exemplary questions as guides for generating more subject-specific educational questions. Experimental results on the ScienceQA benchmark demonstrate the superiority of CoE in both question generation and distractor generation over existing methods across various subjects and educational levels."
    },
    {
        "paperId": "8c2e20afc01ca42771f3d749755dae624474f0ba",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Self-chats from Large Language Models Make Small Emotional Support Chatbot Better",
        "openAccessPdf": {
            "url": "https://ink.library.smu.edu.sg/context/sis_research/article/10239/viewcontent/2024.acl_long.611.pdf",
            "status": "GREEN",
            "license": "CCBYNCND",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2024.acl-long.611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2024.acl-long.611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-11",
        "authors": [
            {
                "authorId": "2297730893",
                "name": "Zhonghua Zheng"
            },
            {
                "authorId": "2266754933",
                "name": "Lizi Liao"
            },
            {
                "authorId": "2316667994",
                "name": "Yang Deng"
            },
            {
                "authorId": "2319385907",
                "name": "Libo Qin"
            },
            {
                "authorId": "2316715562",
                "name": "Liqiang Nie"
            }
        ],
        "abstract": "Large Language Models (LLMs) have shown strong generalization abilities to excel in various tasks, including emotion support conversations. However, deploying such LLMs like GPT-3 (175B parameters) is resource-intensive and challenging at scale. In this study, we utilize LLMs as \u201cCounseling Teacher\u201d to enhance smaller models\u2019 emotion support response abilities, significantly reducing the necessity of scaling up model size. To this end, we first introduce an iterative expansion framework, aiming to prompt the large teacher model to curate an expansive emotion support dialogue dataset. This curated dataset, termed ExTES, encompasses a broad spectrum of scenarios and is crafted with meticulous strategies to ensure its quality and comprehensiveness. Based on this, we then devise a Diverse Response Inpainting (DRI) mechanism to harness the teacher model to produce multiple diverse responses by filling in the masked conversation context. This richness and variety serve as instructive examples, providing a robust foundation for fine-tuning smaller student models. Experiments across varied scenarios reveal that the teacher-student scheme with DRI notably improves the response abilities of smaller models, even out-performing the teacher model in some cases. The dataset and codes are available 1 ."
    },
    {
        "paperId": "d64144b7624da749ed8cac860e7f903810cc522c",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Quantifying Generalizations: Exploring the Divide Between Human and LLMs' Sensitivity to Quantification",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2024.acl-long.636.pdf",
            "status": "GREEN",
            "license": "CCBYNCSA",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2024.acl-long.636?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2024.acl-long.636, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-11",
        "authors": [
            {
                "authorId": "3352951",
                "name": "Aur\u00e9lie Herbelot"
            },
            {
                "authorId": "2318359505",
                "name": "Eva Maria Vecchi. 2016"
            },
            {
                "authorId": "2318355049",
                "name": "Many"
            },
            {
                "authorId": "2278435713",
                "name": "Albert Q. Jiang"
            },
            {
                "authorId": "2256994781",
                "name": "Alexandre Sablayrolles"
            },
            {
                "authorId": "2318356354",
                "name": "Arthur Men-793"
            },
            {
                "authorId": "2256994975",
                "name": "Chris Bamford"
            },
            {
                "authorId": "2302815701",
                "name": "Devendra Singh"
            },
            {
                "authorId": "2302809975",
                "name": "Diego Chaplot"
            },
            {
                "authorId": "2318358390",
                "name": "laume Lample"
            },
            {
                "authorId": "2318350171",
                "name": "L\u00e9lio Lucile Saulnier"
            },
            {
                "authorId": "2318358843",
                "name": "Renard Lavaud"
            },
            {
                "authorId": "114952298",
                "name": "M. Lachaux"
            },
            {
                "authorId": "2256994779",
                "name": "Pierre Stock"
            },
            {
                "authorId": "1379806208",
                "name": "Teven Le Scao"
            },
            {
                "authorId": "31887951",
                "name": "A. Kalouli"
            },
            {
                "authorId": "20484090",
                "name": "R. Sevastjanova"
            },
            {
                "authorId": "2318355928",
                "name": "Beck Maribel"
            },
            {
                "authorId": "2318359459",
                "name": "Romero. 2022"
            },
            {
                "authorId": "2318359485",
                "name": "Negation"
            },
            {
                "authorId": "9529535",
                "name": "Nora Kassner"
            },
            {
                "authorId": "2318356338",
                "name": "Hinrich Sch\u00fctze. 2019"
            },
            {
                "authorId": "2288809224",
                "name": "Negated"
            },
            {
                "authorId": "84698136",
                "name": "Carina Kauf"
            },
            {
                "authorId": "2318360962",
                "name": "Anna A Ivanova"
            },
            {
                "authorId": "2277459171",
                "name": "Giulia Rambelli"
            },
            {
                "authorId": "2323625486",
                "name": "Emmanuele Chersoni"
            },
            {
                "authorId": "2318356639",
                "name": "Jingyuan Selena"
            },
            {
                "authorId": "2318359509",
                "name": "She"
            },
            {
                "authorId": "2318359488",
                "name": "Zawad"
            },
            {
                "authorId": "2318357569",
                "name": "Evelina Chowdhury"
            },
            {
                "authorId": "2318358824",
                "name": "Fedorenko Alessandro"
            },
            {
                "authorId": "2281054128",
                "name": "Manfred Krifka"
            },
            {
                "authorId": "2285616129",
                "name": "Francis Jeffry Pelletier"
            },
            {
                "authorId": "2318358776",
                "name": "Gregory Carl-823"
            },
            {
                "authorId": "2263357402",
                "name": "A. T. Meulen"
            },
            {
                "authorId": "2286500490",
                "name": "G. Chierchia"
            },
            {
                "authorId": "2318358077",
                "name": "Greg N. Carlson"
            },
            {
                "authorId": "2281886771",
                "name": "Ken McRae"
            },
            {
                "authorId": "2902015",
                "name": "George S. Cree"
            },
            {
                "authorId": "29941456",
                "name": "M. Seidenberg"
            },
            {
                "authorId": "2318359483",
                "name": "Chris McNorgan. 2005"
            }
        ],
        "abstract": "Generics are expressions used to communicate 001 abstractions about categories. While conveying 002 general truths (e.g., Birds fly ), generics have the 003 interesting property to admit exceptions (e.g., 004 penguins do not fly). Statements of this type 005 help us organizing our knowledge of the world, 006 and form the basis of how we express it (Hamp-007 ton, 2012; Leslie, 2014). 008 This study investigates how Large Language 009 Models (LLMs) interpret generics, drawing 010 upon psycholinguistic experimental method-011 ologies. Understanding how LLMs interpret 012 generic statements serves not only as a mea-013 sure of their ability to abstract but also arguably 014 plays a role in their encoding of stereotypes. 015 Given that generics interpretation necessitates 016 a comparison with explicitly quantified sen-017 tences, we explored i.) whether LLMs can 018 correctly associate a quantifier with the generic 019 structure, and ii.) whether the presence of a 020 generic sentence as context influences the out-021 comes of quantifiers. We evaluated LLMs us-022 ing both Surprisal distributions and prompting 023 techniques. The findings indicate that mod-024 els do not exhibit a strong sensitivity to quan-025 tification. Nevertheless, they seem to encode 026 a meaning linked with the generic structure, 027 which leads them to adjust their answers ac-028 cordingly when a generalization is provided as 029 context. 030"
    },
    {
        "paperId": "9480bb2cfa9c150b08738054e60ea6828bcc16f4",
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "title": "Can Large Language Models Interpret Noun-Noun Compounds? A Linguistically-Motivated Study on Lexicalized and Novel Compounds",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2024.acl-long.637.pdf",
            "status": "GREEN",
            "license": "CCBYNCSA",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2024.acl-long.637?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2024.acl-long.637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-08-11",
        "authors": [
            {
                "authorId": "2265181341",
                "name": "Giulia Rambelli"
            },
            {
                "authorId": "2323625486",
                "name": "Emmanuele Chersoni"
            },
            {
                "authorId": "2277459173",
                "name": "Claudia Collacciani"
            },
            {
                "authorId": "2279949756",
                "name": "Marianna Bolognesi"
            }
        ],
        "abstract": "Noun-noun compounds interpretation is the task where a model is given one of such constructions, and it is asked to provide a para-phrase, making the semantic relation between the nouns explicit, as in carrot cake is \u201ca cake made of carrots.\u201d Such a task requires the ability to understand the implicit structured representation of the compound meaning. In this paper, we test to what extent the recent Large Language Models can interpret the semantic relation between the constituents of lexicalized English compounds and whether they can abstract from such semantic knowledge to predict the semantic relation between the constituents of similar but novel compounds by relying on analogical comparisons (e.g., carrot dessert ). We test both Sur-prisal metrics and prompt-based methods to see whether i.) they can correctly predict the relation between constituents, and ii.) the semantic representation of the relation is robust to paraphrasing. Using a dataset of lexicalized and annotated noun-noun compounds, we find that LLMs can infer some semantic relations better than others (with a preference for compounds involving concrete concepts). When challenged to perform abstractions and transfer their interpretations to semantically similar but novel compounds, LLMs show serious limitations 1 ."
    },
    {
        "paperId": "e78dec2acce8ae78ca640e4dea8841655d2314be",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Language Model Quality Correlates with Psychometric Predictive Power in Multiple Languages",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.466.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.emnlp-main.466?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.emnlp-main.466, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "2268675406",
                "name": "E. Wilcox"
            },
            {
                "authorId": "150953620",
                "name": "Clara Meister"
            },
            {
                "authorId": "2070989574",
                "name": "Ryan Cotterell"
            },
            {
                "authorId": "1388571351",
                "name": "Tiago Pimentel"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "17560ad9f94218d0953dcec8c1c3b7e5bf4fb7bc",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Not all quantifiers are equal: Probing Transformer-based language models' understanding of generalised quantifiers",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.536.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.emnlp-main.536?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.emnlp-main.536, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "2091992333",
                "name": "Tharindu Madusanka"
            },
            {
                "authorId": "90788907",
                "name": "Iqra Zahid"
            },
            {
                "authorId": "2274084219",
                "name": "Hao Li"
            },
            {
                "authorId": "2273535143",
                "name": "Ian Pratt-Hartmann"
            },
            {
                "authorId": "1400900759",
                "name": "R. Batista-Navarro"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "ba16ec572b09189528f554e6ce7586a2d16fa08c",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Deciphering Stereotypes in Pre-Trained Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.697.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.emnlp-main.697?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.emnlp-main.697, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "2227771",
                "name": "Weicheng Ma"
            },
            {
                "authorId": "2273558834",
                "name": "Henry Scheible"
            },
            {
                "authorId": "2273370242",
                "name": "Brian Wang"
            },
            {
                "authorId": "35569233",
                "name": "Goutham Veeramachaneni"
            },
            {
                "authorId": "2362304414",
                "name": "Pratim Chowdhary"
            },
            {
                "authorId": "2273580697",
                "name": "Alan Sun"
            },
            {
                "authorId": "2273551097",
                "name": "Andrew Koulogeorge"
            },
            {
                "authorId": "2117930921",
                "name": "Lili Wang"
            },
            {
                "authorId": "2262514289",
                "name": "Diyi Yang"
            },
            {
                "authorId": "1918441",
                "name": "Soroush Vosoughi"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "913952cc1a679c18e7ff6f21f8a4b9e0833b93e4",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Doolittle: Benchmarks and Corpora for Academic Writing Formalization",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.809.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.emnlp-main.809?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.emnlp-main.809, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "50826757",
                "name": "Shizhe Diao"
            },
            {
                "authorId": "2273549567",
                "name": "Yongyu Lei"
            },
            {
                "authorId": "2273478615",
                "name": "Liangming Pan"
            },
            {
                "authorId": "2273564894",
                "name": "Tianqing Fang"
            },
            {
                "authorId": "2273570946",
                "name": "Wangchunshu Zhou"
            },
            {
                "authorId": "150299584",
                "name": "Sedrick Scott Keh"
            },
            {
                "authorId": "2273380846",
                "name": "Min-Yen Kan"
            },
            {
                "authorId": "2273594734",
                "name": "Tong Zhang"
            }
        ],
        "abstract": "Improving the quality of academic writing is a meaningful but challenging task. Conventional methods of language refinement focus on narrow, specific linguistic features within isolated sentences, such as grammatical errors and improper word use. We propose a more general task, Academic Writing Formalization (AWF) , to improve the overall quality of formal academic writing at the paragraph level. We formulate this language refinement task as a formal text style transfer task which transfers informal-academic text to formal-academic and contribute a large-scale non-parallel dataset 1 , D OOLITTLE , for this purpose. Concurrently, we apply a method named metric-oriented reinforcement learning (MORL) to two pretrained language models (PLM) where we incorporate different levels of automatic feedback into the training process. Our experiments reveal that existing text transfer models and grammatical error correction models address certain aspects of AWF but still have a significant performance gap compared to humans. Meanwhile, language models fine-tuned with our MORL method exhibit considerably improved performance and rival ChatGPT in AWF, but still have a non-negligible gap compared to the ground truth formal-academic texts in D OOLITTLE . 2"
    },
    {
        "paperId": "dffdd1686218128e0284e45c24fdbc36976f31c1",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MEEP: Is this Engaging? Prompting Large Language Models for Dialogue Evaluation in Multilingual Settings",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.137.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.137?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.137, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "2273571194",
                "name": "Amila Ferron"
            },
            {
                "authorId": "2273579338",
                "name": "Amber Shore"
            },
            {
                "authorId": "2273579386",
                "name": "Ekata Mitra"
            },
            {
                "authorId": "2273554659",
                "name": "Ameeta Agrawal"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "2597426d774fa41a03735efa43a670a2aaf33b4d",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Manipulating the Perceived Personality Traits of Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.156.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.156?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.156, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "2197074462",
                "name": "Graham Caron"
            },
            {
                "authorId": "144567335",
                "name": "Shashank Srivastava"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "b0eeb1eb94834d7aec826a53b7ee2a05ff96b069",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Disfluent Cues for Enhanced Speech Understanding in Large Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.238.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.238?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.238, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "66301207",
                "name": "Morteza Rohanian"
            },
            {
                "authorId": "2428180",
                "name": "F. Nooralahzadeh"
            },
            {
                "authorId": "25119982",
                "name": "Omid Rohanian"
            },
            {
                "authorId": "2273678614",
                "name": "David A. Clifton"
            },
            {
                "authorId": "2257357742",
                "name": "Michael Krauthammer"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "6f130cde30b91b5deaf5c57dc2b2b33c245ed79b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Comparing the Evaluation and Production of Loophole Behavior in Humans and Large Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.264.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.264?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.264, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "2329666394",
                "name": "Sonia K. Murthy"
            },
            {
                "authorId": "2047734240",
                "name": "Kiera Parece"
            },
            {
                "authorId": "2273670422",
                "name": "Sophie Bridgers"
            },
            {
                "authorId": "2273672116",
                "name": "Peng Qian"
            },
            {
                "authorId": "37774552",
                "name": "T. Ullman"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "b78d8d0307cb68c15082915c641c0df63620b143",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.286.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.286?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "102489044",
                "name": "Jianing Zhou"
            },
            {
                "authorId": "41048608",
                "name": "Ziheng Zeng"
            },
            {
                "authorId": "2008458",
                "name": "Hongyu Gong"
            },
            {
                "authorId": "2263637139",
                "name": "Suma Bhat"
            }
        ],
        "abstract": "Non-compositional expressions, by virtue of their non-compositionality, are a classic \u2018pain in the neck\u2019 for NLP systems. Different from the general language modeling and generation tasks that are primarily compositional, generating non-compositional expressions is more challenging for current neural models, including large pre-trained language models. The main reasons are 1) their non-compositionality, and 2) the limited data resources. Therefore, to make the best use of available data for modeling non-compositionality, we pro-pose a dynamic curriculum learning framework, which learns training examples from easy ones to harder ones thus optimizing the learning step by step, but suffers from the forgetting problem. To alleviate the forgetting problem brought by the arrangement of training examples, we also apply a continual learning method into our curriculum learning framework. Our proposed method combined curriculum and continual learning, to gradually improve the model\u2019s performance on the task of non-compositional expression generation. Experiments on idiomatic expression generation and metaphor generation affirm the effectiveness of our proposed curriculum learning framework and the application of continual learning. Our codes are available at https: //github.com/zhjjn/CL2Gen.git ."
    },
    {
        "paperId": "2dad9763f8b128da231b3fb9c9fff7ad730b89a1",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Long-Range Language Modeling with Selective Cache",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.321.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.321?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.321, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "2264299110",
                "name": "Xinting Huang"
            },
            {
                "authorId": "2264302081",
                "name": "Nora Hollenstein"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "c58a8f03edfc2425558192b383cd737c3ba6f7e4",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "ClozEx: A Task toward Generation of English Cloze Explanation",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.347.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.347?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.347, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "2116461091",
                "name": "Zizheng Zhang"
            },
            {
                "authorId": "35643168",
                "name": "Masato Mita"
            },
            {
                "authorId": "2936411",
                "name": "Mamoru Komachi"
            }
        ],
        "abstract": "Providing explanations for cloze questions in language assessment (LA) has been recognized as a valuable approach to enhancing the language proficiency of learners. However, there is a noticeable absence of dedicated tasks and datasets specifically designed for generating language learner explanations. In response to this gap, this paper introduces a novel task ClozEx of generating explanations for cloze questions in LA, with a particular focus on English as a Second Language (ESL) learners. To support this task, we present a meticulously curated dataset comprising cloze questions paired with corresponding explanations. This dataset aims to assess language proficiency and facilitates language learning by offering informative and accurate explanations. To tackle the task, we fine-tuned various baseline models with our training data, including encoder-decoder and decoder-only architectures. We also explored whether large language models (LLMs) are able to generate good explanations without fine-tuning, just using pre-defined prompts. The evaluation results demonstrate that encoder-decoder models have the potential to deliver fluent and valid explanations when trained on our dataset. 1"
    },
    {
        "paperId": "3e2e30dead442f92b49873215825f62bfcef2b2f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Efficient Continue Training of Temporal Language Model with Structural Information",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.418.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.418?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.418, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "2720303",
                "name": "Zhao-yu Su"
            },
            {
                "authorId": "2257093356",
                "name": "Juntao Li"
            },
            {
                "authorId": "2273823433",
                "name": "Zikang Zhang"
            },
            {
                "authorId": "2273916007",
                "name": "Zihan Zhou"
            },
            {
                "authorId": "2258690229",
                "name": "Min Zhang"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "a323610c559db0e8d483e4b599a8cd802e8c0a1a",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Towards large language model-based personal agents in the enterprise: Current trends and open problems",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.461.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.461?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.461, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "46761645",
                "name": "Vinod Muthusamy"
            },
            {
                "authorId": "2587541",
                "name": "Yara Rizk"
            },
            {
                "authorId": "2273679923",
                "name": "Kiran Kate"
            },
            {
                "authorId": "2077182665",
                "name": "P. Venkateswaran"
            },
            {
                "authorId": "1482469814",
                "name": "Vatche Isahagian"
            },
            {
                "authorId": "2273680487",
                "name": "Ashu Gulati"
            },
            {
                "authorId": "2273668918",
                "name": "Parijat Dube"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "b7f676060a8faa90b2ae839029499b758112cb97",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Analysis of Style-Shifting on Social Media: Using Neural Language Model Conditioned by Social Meanings",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.531.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.531?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.531, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "1992912155",
                "name": "Seiya Kawano"
            },
            {
                "authorId": "32013892",
                "name": "Shota Kanezaki"
            },
            {
                "authorId": "2273684715",
                "name": "Angel Fernando Garcia Contreras"
            },
            {
                "authorId": "19263795",
                "name": "Akishige Yuguchi"
            },
            {
                "authorId": "2740358",
                "name": "Marie Katsurai"
            },
            {
                "authorId": "2237192",
                "name": "Koichiro Yoshino"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "450a646a084432a30e015cac5fe2acfaa118455f",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Isotropic Representation Can Improve Zero-Shot Cross-Lingual Transfer on Multilingual Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.545.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.545?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.545, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "2274023815",
                "name": "Yixin Ji"
            },
            {
                "authorId": "2273917218",
                "name": "Jikai Wang"
            },
            {
                "authorId": "2257093356",
                "name": "Juntao Li"
            },
            {
                "authorId": "144079478",
                "name": "Hai Ye"
            },
            {
                "authorId": "2258690229",
                "name": "Min Zhang"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "2edafcd8f9257c74466ac9f43a08d04f13acf275",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "SIR-ABSC: Incorporating Syntax into RoBERTa-based Sentiment Analysis Models with a Special Aggregator Token",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.572.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.572?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.572, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "2273678548",
                "name": "Ikhyun Cho"
            },
            {
                "authorId": "2274099804",
                "name": "Yoonhwa Jung"
            },
            {
                "authorId": "3118681",
                "name": "J. Hockenmaier"
            }
        ],
        "abstract": "We present a simple, but effective method to incorporate syntactic dependency information directly into transformer-based language models (e"
    },
    {
        "paperId": "538e7847ca283db2015fbc50b1d28abc0391416d",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Intersectional Stereotypes in Large Language Models: Dataset and Analysis",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.575.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.575?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.575, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "2227771",
                "name": "Weicheng Ma"
            },
            {
                "authorId": "2273687151",
                "name": "Brian Chiang"
            },
            {
                "authorId": "2273716716",
                "name": "Tong Wu"
            },
            {
                "authorId": "2117930921",
                "name": "Lili Wang"
            },
            {
                "authorId": "1918441",
                "name": "Soroush Vosoughi"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "55ad0427954c97d9f9a44469d3431d3803e58de2",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "PivotFEC: Enhancing Few-shot Factual Error Correction with a Pivot Task Approach using Large Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.667.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.667?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.667, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "1754500",
                "name": "Xingwei He"
            },
            {
                "authorId": "15796861",
                "name": "Alex Jin"
            },
            {
                "authorId": "2273909915",
                "name": "Jun Ma"
            },
            {
                "authorId": "2273924496",
                "name": "Yuan Yuan"
            },
            {
                "authorId": "145964453",
                "name": "S. Yiu"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "afef2ca5dc6b0e6b6da85ccad19505d939cb78bf",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Balaur: Language Model Pretraining with Lexical Semantic Relations",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.674.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.674?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.674, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "1683066845",
                "name": "Andrei Mircea"
            },
            {
                "authorId": "2273676374",
                "name": "Jackie C. K. Cheung"
            }
        ],
        "abstract": "Lexical semantic relations (LSRs) characterize meaning relationships between words and play an important role in systematic generalization on lexical inference tasks. Notably, several tasks that require knowledge of hypernymy still pose a challenge for pretrained language models (LMs) such as BERT, underscoring the need to better align their linguistic behavior with our knowledge of LSRs. In this paper, we propose B ALAUR , a model that addresses this challenge by modeling LSRs directly in the LM\u2019s hidden states throughout pretraining. Motivating our approach is the hypothesis that the internal representations of LMs can provide an interface to their observable linguistic behavior, and that by controlling one we can influence the other. We validate our hypothesis and demonstrate that B ALAUR generally improves the performance of large transformer-based LMs on a comprehensive set of hypernymy-informed tasks, as well as on the original LM objective. Code and data are made available at github.com/mirandrom/balaur ."
    },
    {
        "paperId": "a74ea4565bb3e934a32ba1537d7c9230eff322e3",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "ConPrompt: Pre-training a Language Model with Machine-Generated Data for Implicit Hate Speech Detection",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.731.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.731?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.731, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "2116682476",
                "name": "Youngwook Kim"
            },
            {
                "authorId": "2187495217",
                "name": "Shinwoo Park"
            },
            {
                "authorId": "2273681564",
                "name": "Youngsoo Namgoong"
            },
            {
                "authorId": "2264472447",
                "name": "Yo-Sub Han"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "f118f6b947f767a6e3b79ffee65718f03274fde1",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Incorporating Syntactic Knowledge into Pre-trained Language Model using Optimization for Overcoming Catastrophic Forgetting",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.732.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.732?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "1423660946",
                "name": "Ran Iwamoto"
            },
            {
                "authorId": "3313754",
                "name": "Issei Yoshida"
            },
            {
                "authorId": "2273693407",
                "name": "Hiroshi Kanayama"
            },
            {
                "authorId": "2273682875",
                "name": "Takuya Ohko"
            },
            {
                "authorId": "37438942",
                "name": "Masayasu Muraoka"
            }
        ],
        "abstract": "Syntactic knowledge is invaluable information for many tasks which handle complex or long sentences, but typical pre-trained language models do not contain suf\ufb01cient syntactic knowledge. Thus it results in failures in downstream tasks that require syntactic knowledge. In this paper, we explore additional training to incorporate syntactic knowledge to a language model. We designed four pre-training tasks that learn different syntactic perspectives. For adding new syntactic knowledge and keeping a good balance between the original and additional knowledge, we addressed the problem of catastrophic forgetting that prevents the model from keeping semantic information when the model learns additional syntactic knowledge. We demonstrated that additional syntactic training produced consistent performance gains while clearly avoiding catastrophic forgetting."
    },
    {
        "paperId": "32a86327f2820087d17c1e95f641acbb1ec91e94",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Identifying Early Maladaptive Schemas from Mental Health Question Texts",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.792.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.792?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "3091719",
                "name": "Sujatha Das Gollapalli"
            },
            {
                "authorId": "2215618416",
                "name": "Beng Heng Ang"
            },
            {
                "authorId": "2269507001",
                "name": "See-Kiong Ng"
            }
        ],
        "abstract": "In Psychotherapy, maladaptive schemas \u2013 negative perceptions that an individual has of the self, others, or the world that endure despite ob-jective reality \u2013 often lead to resistance to treatments and relapse of mental health issues such as depression, anxiety, panic attacks etc. Identification of early maladaptive schemas (EMS) is thus a crucial step during Schema Therapy-based counseling sessions, where patients go through a detailed and lengthy EMS questionnaire. However, such an approach is not practical in \u2018offline\u2019 counseling scenarios, such as community QA forums which are gaining popularity for people seeking mental health support. In this paper, we investigate both LLM (Large Language Models) and non-LLM approaches for identifying EMS labels using resources from Schema Therapy. Our evaluation indicates that recent LLMs can be effective for identifying EMS but their predictions lack explainability and are too sensitive to precise \u2018prompts\u2019. Both LLM and non-LLM methods are unable to reliably address the null cases, i.e. cases with no EMS labels. However, we posit that the two approaches show complementary properties and together, they can be used to further devise techniques for EMS identification."
    },
    {
        "paperId": "c7027366b5eb90db7a9cc4a87024a56048644b9e",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "BLM-s/lE: A structured dataset of English spray-load verb alternations for testing generalization in LLMs",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.821.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.821?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.821, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "73118979",
                "name": "Giuseppe Samo"
            },
            {
                "authorId": "2273062820",
                "name": "Vivi Nastase"
            },
            {
                "authorId": "1490776107",
                "name": "Chunyang Jiang"
            },
            {
                "authorId": "2273064860",
                "name": "Paola Merlo"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "b9fd3013a6737f4c469ca6eef807f5793f50974b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "NLMs: Augmenting Negation in Language Models",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.873.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.873?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "2273916127",
                "name": "Rituraj Singh"
            },
            {
                "authorId": "2273909845",
                "name": "Rahul Kumar"
            },
            {
                "authorId": "2273680622",
                "name": "Vivek Sridhar"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "06306a7371f6eeeb19fdb80950f9ed78d76a6962",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Is the Answer in the Text? Challenging ChatGPT with Evidence Retrieval from Instructive Text",
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.findings-emnlp.949.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.949?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2023-12-06",
        "authors": [
            {
                "authorId": "2273688788",
                "name": "Sophie Henning"
            },
            {
                "authorId": "2273694540",
                "name": "Talita Anthonio"
            },
            {
                "authorId": "2275910749",
                "name": "Wei Zhou"
            },
            {
                "authorId": "2273686207",
                "name": "Heike Adel"
            },
            {
                "authorId": "2273694538",
                "name": "Mohsen Mesgar"
            },
            {
                "authorId": "2273686231",
                "name": "Annemarie Friedrich"
            }
        ],
        "abstract": "Generative language models have recently shown remarkable success in generating answers to questions in a given textual context. However, these answers may suffer from hal-lucination, wrongly cite evidence, and spread misleading information. In this work, we address this problem by employing ChatGPT, a state-of-the-art generative model, as a machine-reading system. We ask it to retrieve answers to lexically varied and open-ended questions from trustworthy instructive texts. We introduce WHERE ( W iki H ow E vidence RE trieval), a new high-quality evaluation benchmark of a set of WikiHow articles exhaustively annotated with evidence sentences to questions that comes with a special challenge: All questions are about the article\u2019s topic, but not all can be answered using the provided context. We interestingly find that when us-ing a regular question-answering prompt, Chat-GPT neglects to detect the unanswerable cases. When provided with a few examples, it learns to better judge whether a text provides answer evidence. Alongside this important finding, our dataset defines a new benchmark for evidence retrieval in question answering, which we argue is one of the necessary next steps for making large language models more trustworthy."
    },
    {
        "paperId": "5ed0ab9405f9beb75e29c3cec9d03b0850e71e50",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Cross-domain NER with Generated Task-Oriented Knowledge: An Empirical Study from Information Density Perspective",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.95, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2329250713",
                "name": "Zhihao Zhang"
            },
            {
                "authorId": "2329879871",
                "name": "Sophia Lee"
            },
            {
                "authorId": "151485371",
                "name": "Junshuang Wu"
            },
            {
                "authorId": "2257135991",
                "name": "Dong Zhang"
            },
            {
                "authorId": "2323427471",
                "name": "Shoushan Li"
            },
            {
                "authorId": "2329104057",
                "name": "Erik Cambria"
            },
            {
                "authorId": "2257386981",
                "name": "Guodong Zhou"
            }
        ],
        "abstract": "Cross-domain Named Entity Recognition (CDNER) is crucial for Knowledge Graph (KG) construction and natural language processing (NLP), enabling learning from source to target domains with limited data. Previous studies often rely on manually collected entity-relevant sentences from the web or attempt to bridge the gap between tokens and entity labels across domains. These approaches are time-consuming and inefficient, as these data are often weakly correlated with the target task and require extensive pre-training.To address these issues, we propose automatically generating task-oriented knowledge (GTOK) using large language models (LLMs), focusing on the reasoning process of entity extraction. Then, we employ task-oriented pre-training (TOPT) to facilitate domain adaptation. Additionally, current cross-domain NER methods often lack explicit explanations for their effectiveness. Therefore, we introduce the concept of information density to better evaluate the model\u2019s effectiveness before performing entity recognition.We conduct systematic experiments and analyses to demonstrate the effectiveness of our proposed approach and the validity of using information density for model evaluation."
    },
    {
        "paperId": "981299437248c3032ab347804e6fae9e4a07ba9b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "MTLS: Making Texts into Linguistic Symbols",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2301581242",
                "name": "Wenlong Fei"
            },
            {
                "authorId": "48630507",
                "name": "Xiaohua Wang"
            },
            {
                "authorId": "2295956789",
                "name": "Min Hu"
            },
            {
                "authorId": "2279252175",
                "name": "Qingyu Zhang"
            },
            {
                "authorId": "2330052855",
                "name": "Hongbo Li"
            }
        ],
        "abstract": "In linguistics, all languages can be considered as symbolic systems, with each language relying on symbolic processes to associate specific symbols with meanings. In the same language, there is a fixed correspondence between linguistic symbol and meaning. In different languages, universal meanings follow varying rules of symbolization in one-to-one correspondence with symbols. Most work overlooks the properties of languages as symbol systems. In this paper, we shift the focus to the symbolic properties and introduce MTLS: a pre-training method to improve the multilingual capability of models by Making Texts into Linguistic Symbols. Initially, we replace the vocabulary in pre-trained language models by mapping relations between linguistic symbols and semantics. Subsequently, universal semantics within the symbolic system serve as bridges, linking symbols from different languages to the embedding space of the model, thereby enabling the model to process linguistic symbols. To evaluate the effectiveness of MTLS, we conducted experiments on multilingual tasks using BERT and RoBERTa, respectively, as the backbone. The results indicate that despite having just over 12,000 pieces of English data in pre-training, the improvement that MTLS brings to multilingual capabilities is remarkably significant."
    },
    {
        "paperId": "97126bb7be3797ab21ef938ab886ad66a555aa86",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2243655194",
                "name": "Mayi Xu"
            },
            {
                "authorId": "2243469274",
                "name": "Yongqi Li"
            },
            {
                "authorId": "2273699890",
                "name": "Ke Sun"
            },
            {
                "authorId": "2243466068",
                "name": "Tieyun Qian"
            }
        ],
        "abstract": "Large language models (LLMs) have shown excellent capability for solving reasoning problems. Existing approaches do not differentiate the question difficulty when designing prompting methods for them. Clearly, a simple method cannot elicit sufficient knowledge from LLMs to answer a hard question. Meanwhile, a sophisticated one will force the LLM to generate redundant or even inaccurate intermediate steps toward a simple question. Consequently, the performance of existing methods fluctuates among various questions.In this work, we propose Adaption-of-Thought (AdoT), an adaptive method to improve LLMs for the reasoning problem, which first measures the question difficulty and then tailors demonstration set construction and difficulty-adapted retrieval strategies for the adaptive demonstration construction. Experimental results on three reasoning tasks prove the superiority of our proposed method, showing an absolute improvement of up to 5.5% on arithmetic reasoning, 7.4% on symbolic reasoning, and 2.3% on commonsense reasoning. Our codes and implementation details are available at: https://github.com/NLPGM/AdoT"
    },
    {
        "paperId": "a7099e192bc375868456e708a66d3670127df30c",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Analyzing Key Factors Influencing Emotion Prediction Performance of VLLMs in Conversational Contexts",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.331, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2273563796",
                "name": "Jaewook Lee"
            },
            {
                "authorId": "2329909758",
                "name": "Yeajin Jang"
            },
            {
                "authorId": "2110134563",
                "name": "Hongjin Kim"
            },
            {
                "authorId": "2296232551",
                "name": "Woojin Lee"
            },
            {
                "authorId": "2276657773",
                "name": "Harksoo Kim"
            }
        ],
        "abstract": "Emotional intelligence (EI) in artificial intelligence (AI), which refers to the ability of an AI to understand and respond appropriately to human emotions, has emerged as a crucial research topic. Recent studies have shown that large language models (LLMs) and vision large language models (VLLMs) possess EI and the ability to understand emotional stimuli in the form of text and images, respectively. However, factors influencing the emotion prediction performance of VLLMs in real-world conversational contexts have not been sufficiently explored. This study aims to analyze the key elements affecting the emotion prediction performance of VLLMs in conversational contexts systematically. To achieve this, we reconstructed the MELD dataset, which is based on the popular TV series Friends, and conducted experiments through three sub-tasks: overall emotion tone prediction, character emotion prediction, and contextually appropriate emotion expression selection. We evaluated the performance differences based on various model architectures (e.g., image encoders, modality alignment, and LLMs) and image scopes (e.g., entire scene, person, and facial expression). In addition, we investigated the impact of providing persona information on the emotion prediction performance of the models and analyzed how personality traits and speaking styles influenced the emotion prediction process. We conducted an in-depth analysis of the impact of various other factors, such as gender and regional biases, on the emotion prediction performance of VLLMs. The results revealed that these factors significantly influenced the model performance."
    },
    {
        "paperId": "65548314c5b8480d42bdc2c39d30fd176cbdd632",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Verifiable, Debuggable, and Repairable Commonsense Logical Reasoning via LLM-based Theory Resolution",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.379, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "1646622849",
                "name": "Armin Toroghi"
            },
            {
                "authorId": "2290061685",
                "name": "Willis Guo"
            },
            {
                "authorId": "2699756",
                "name": "Ali Pesaranghader"
            },
            {
                "authorId": "2273670268",
                "name": "Scott Sanner"
            }
        ],
        "abstract": "Recent advances in Large Language Models (LLM) have led to substantial interest in their application to commonsense reasoning tasks. Despite their potential, LLMs are susceptible to reasoning errors and hallucinations that may be harmful in use cases where accurate reasoning is critical. This challenge underscores the need for verifiable, debuggable, and repairable LLM reasoning. Recent works have made progress toward verifiable reasoning with LLMs by using them as either (i) a reasoner over an axiomatic knowledge base, or (ii) a semantic parser for use in existing logical inference systems. However, both settings are unable to extract commonsense axioms from the LLM that are not already formalized in the knowledge base, and also lack a reliable method to repair missed commonsense inferences. In this work, we present LLM-TRes, a logical reasoning framework based on the notion of \u201ctheory resolution\u201d that allows for seamless integration of the commonsense knowledge from LLMs with a verifiable logical reasoning framework that mitigates hallucinations and facilitates debugging of the reasoning procedure as well as repair. We crucially prove that repaired axioms are theoretically guaranteed to be given precedence over flawed ones in our theory resolution inference process. We conclude by evaluating on three diverse language-based reasoning tasks\u2014preference reasoning, deductive reasoning, and causal commonsense reasoning\u2014and demonstrate the superior performance of LLM-TRes vs. state-of-the-art LLM-based reasoning methods in terms of both accuracy and reasoning correctness."
    },
    {
        "paperId": "f082ca739772540f3eb27aa9dad13fcc0efcc364",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Does Large Language Model Contain Task-Specific Neurons?",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2067623237",
                "name": "Ran Song"
            },
            {
                "authorId": "1954845",
                "name": "Shizhu He"
            },
            {
                "authorId": "2310524615",
                "name": "Shuting Jiang"
            },
            {
                "authorId": "2850064",
                "name": "Yantuan Xian"
            },
            {
                "authorId": "2409659",
                "name": "Shengxiang Gao"
            },
            {
                "authorId": "2283274776",
                "name": "Kang Liu"
            },
            {
                "authorId": "2281298707",
                "name": "Zhengtao Yu"
            }
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in comprehensively handling various types of natural language processing (NLP) tasks. However, there are significant differences in the knowledge and abilities required for different tasks. Therefore, it is important to understand whether the same LLM processes different tasks in the same way. Are there specific neurons in a LLM for different tasks? Inspired by neuroscience, this paper pioneers the exploration of whether distinct neurons are activated when a LLM handles different tasks. Compared with current research exploring the neurons of language and knowledge, task-specific neurons present a greater challenge due to their abstractness, diversity, and complexity. To address these challenges, this paper proposes a method for task-specific neuron localization based on Causal Gradient Variation with Special Tokens (CGVST). CGVST identifies task-specific neurons by concentrating on the most significant tokens during task processing, thereby eliminating redundant tokens and minimizing interference from non-essential neurons. Compared to traditional neuron localization methods, our approach can more effectively identify task-specific neurons. We conduct experiments across eight different public tasks. Experiments involving the inhibition and amplification of identified neurons demonstrate that our method can accurately locate task-specific neurons."
    },
    {
        "paperId": "c6d913a49b180bf0ce23aac30c8e5dba68c16d51",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Leveraging pre-trained language models for linguistic analysis: A case of argument structure constructions",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2008981651",
                "name": "Hakyung Sung"
            },
            {
                "authorId": "2283688467",
                "name": "Kristopher Kyle"
            }
        ],
        "abstract": "This study evaluates the effectiveness of pre-trained language models in identifying argument structure constructions, important for modeling both first and second language learning. We examine three methodologies: (1) supervised training with RoBERTa using a gold-standard ASC treebank, including by-tag accuracy evaluation for sentences from both native and non-native English speakers, (2) prompt-guided annotation with GPT-4, and (3) generating training data through prompts with GPT-4, followed by RoBERTa training. Our findings indicate that RoBERTa trained on gold-standard data shows the best performance. While data generated through GPT-4 enhances training, it does not exceed the benchmarks set by gold-standard data."
    },
    {
        "paperId": "bebe269411729b641c81eac7286fffc619c32ea5",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Rethinking the Reversal Curse of LLMs: a Prescription from Human Knowledge Reversal",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.428, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2274059261",
                "name": "Zhicong Lu"
            },
            {
                "authorId": "2287962858",
                "name": "Li Jin"
            },
            {
                "authorId": "2298213582",
                "name": "Peiguang Li"
            },
            {
                "authorId": "2329921864",
                "name": "Yu Tian"
            },
            {
                "authorId": "2237737472",
                "name": "Linhao Zhang"
            },
            {
                "authorId": "2295934207",
                "name": "Sirui Wang"
            },
            {
                "authorId": "2329748262",
                "name": "Guangluan Xu"
            },
            {
                "authorId": "2161064704",
                "name": "Changyuan Tian"
            },
            {
                "authorId": "2290035990",
                "name": "Xunliang Cai"
            }
        ],
        "abstract": "Large Language Models (LLMs) have exhibited exceptional performance across diverse domains. However, recent studies reveal that LLMs are plagued by the \u201creversal curse\u201d. Most existing methods rely on aggressive sample permutation and pay little attention to delving into the underlying reasons for this issue, resulting in only partial mitigation. In this paper, inspired by human knowledge reversal, we investigate and quantify the individual influence of three potential reasons on the reversal curse: 1) knowledge clarity, 2) entity correlation modeling, and 3) pairwise relationship reasoning capability. Motivated by the analysis of these reasons, we propose a novel **P**airwise entity **O**rder- and **R**elationship-**E**nhanced (**PORE**) data strategy, which facilitates bidirectional entity correlation modeling and pairwise relationship reasoning to overcome the reversal curse. Specifically, PORE augments the samples with entity order-reversal and semantically preserved question-answer pairs, enhancing the encoding of entity correlations in both directions. PORE also employs entity-interleaved pairwise relationship data, which elevates the model\u2019s capability for relationship reasoning. Additionally, to improve the recall of reverse relationships, we leverage knowledge clarity to construct high-clarity data for PORE. Extensive experimental results on available and two newly assembled datasets demonstrate the effectiveness and generalization of our method in both data-sufficient and -constrained situations."
    },
    {
        "paperId": "35371e1450da33d40f1253e783f321981dedfbc4",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "LawBench: Benchmarking Legal Knowledge of Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.452, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2248062587",
                "name": "Zhiwei Fei"
            },
            {
                "authorId": "2249736444",
                "name": "Xiaoyu Shen"
            },
            {
                "authorId": "2308000644",
                "name": "Dawei Zhu"
            },
            {
                "authorId": "2248972766",
                "name": "Fengzhe Zhou"
            },
            {
                "authorId": "2247916656",
                "name": "Zhuo Han"
            },
            {
                "authorId": "2331464965",
                "name": "Alan Huang"
            },
            {
                "authorId": "2266356137",
                "name": "Songyang Zhang"
            },
            {
                "authorId": "2331707034",
                "name": "Kai Chen"
            },
            {
                "authorId": "2331462142",
                "name": "Zhixin Yin"
            },
            {
                "authorId": "2249070589",
                "name": "Zongwen Shen"
            },
            {
                "authorId": "2248015856",
                "name": "Jidong Ge"
            },
            {
                "authorId": "2331427345",
                "name": "Vincent Ng"
            }
        ],
        "abstract": "We present LawBench, the first evaluation benchmark composed of 20 tasks aimed to assess the ability of Large Language Models (LLMs) to perform Chinese legal-related tasks. LawBench is meticulously crafted to enable precise assessment of LLMs\u2019 legal capabilities from three cognitive levels that correspond to the widely accepted Bloom\u2019s cognitive taxonomy. Using LawBench, we present a comprehensive evaluation of 21 popular LLMs and the first comparative analysis of the empirical results in order to reveal their relative strengths and weaknesses. All data, model predictions and evaluation code are accessible from https://github.com/open-compass/LawBench."
    },
    {
        "paperId": "f04e48e16eabfd33846f91ccbdd24f509abc01e6",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Jellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.497, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2269741433",
                "name": "Haochen Zhang"
            },
            {
                "authorId": "2815918",
                "name": "Yuyang Dong"
            },
            {
                "authorId": "2330392877",
                "name": "Chuan Xiao"
            },
            {
                "authorId": "37267314",
                "name": "M. Oyamada"
            }
        ],
        "abstract": "This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format. We instruction-tune local LLMs as universal DP task solvers that operate on a local, single, and low-priced GPU, ensuring data security and enabling further customization. We select a collection of datasets across four representative DP tasks and construct instruction data using data configuration, knowledge injection, and reasoning data distillation techniques tailored to DP. By tuning Mistral-7B, Llama 3-8B, and OpenOrca-Platypus2-13B, our models, Jellyfish-7B/8B/13B, deliver competitiveness compared to GPT-3.5/4 models and strong generalizability to unseen tasks while barely compromising the base models\u2019 abilities in NLP tasks. Meanwhile, Jellyfish offers enhanced reasoning capabilities compared to GPT-3.5. Our models are available at: https://huggingface.co/NECOUDBFM/JellyfishOur instruction dataset is available at: https://huggingface.co/datasets/NECOUDBFM/Jellyfish-Instruct"
    },
    {
        "paperId": "7b88fb2e94a42c5f16c27d744ea5d04eb1f8069b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.521, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2309171393",
                "name": "Lu\u00edsa Shimabucoro"
            },
            {
                "authorId": "2326992780",
                "name": "Sebastian Ruder"
            },
            {
                "authorId": "2301581319",
                "name": "Julia Kreutzer"
            },
            {
                "authorId": "2818759",
                "name": "Marzieh Fadaee"
            },
            {
                "authorId": "2257040307",
                "name": "Sara Hooker"
            }
        ],
        "abstract": "The widespread adoption of synthetic data raises new questions about how models generating the data can influence other large language models (LLMs). To start, our work exhaustively characterizes the impact of passive inheritance of model properties by systematically studying how the source of synthetic data shapes models\u2019 internal biases, calibration and preferences, and their generations\u2019 textual attributes, providing one of the most comprehensive studies to-date. We find that models are surprisingly sensitive towards certain attributes even when the synthetic data prompts appear \u201cneutral\u201d which invites the question: can we explicitly steer the distilled data towards desired properties? We demonstrate how such active inheritance can steer the generation profiles of models towards desirable non-differentiable attributes in both directions, e.g. increasing lexical diversity or reducing toxicity. Overall, our study broadens the understanding of the implicit biases inherited by LLMs and explores how we can leverage them to positive effect."
    },
    {
        "paperId": "8299d750132f2933bdd25a4f8b0b766f9ca49bba",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "5936771",
                "name": "Sougata Saha"
            },
            {
                "authorId": "1748081",
                "name": "R. Srihari"
            }
        ],
        "abstract": "The proliferation of online misinformation presents a significant challenge, requiring scalable strategies for effective mitigation. While detection methods exist, current reactive approaches, like content flagging and banning, are short-term and insufficient. Additionally, advancements like large language models (LLMs) exacerbate the issue by enabling large-scale creation and dissemination of misinformation. Thus, sustainable, scalable solutions that encourage behavior change and broaden perspectives by persuading misinformants against their viewpoints or broadening their perspectives are needed. To this end, we propose persuasive LLM-based dialogue systems to tackle misinformation. However, challenges arise due to the lack of suitable datasets and formal frameworks for generating persuasive responses. Inspired by existing methods for countering online hate speech, we explore adapting counter-hate response strategies for misinformation. Since misinformation and hate speech often coexist despite differing intentions, we develop classifiers to identify and annotate response strategies from hate-speech counter-responses for use in misinformation scenarios. Human evaluations show a 91% agreement on the applicability of these strategies to misinformation. Next, as a scalable counter-misinformation solution, we create an LLM-based argument graph framework that generates persuasive responses, using the strategies as control codes to adjust the style and content. Human evaluations and case studies demonstrate that our framework generates expert-like responses and is 14% more engaging, 21% more natural, and 18% more factual than the best available alternatives."
    },
    {
        "paperId": "82fd866b13d97763e738660f2a077eddc854c3ea",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Pragmatic Norms Are All You Need \u2013 Why The Symbol Grounding Problem Does Not Apply to LLMs",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "119338633",
                "name": "Reto Gubelmann"
            }
        ],
        "abstract": "Do LLMs fall prey to Harnad\u2019s symbol grounding problem (SGP), as it has recently been claimed? We argue that this is not the case. Starting out with countering the arguments of Bender and Koller (2020), we trace the origins of the SGP to the computational theory of mind (CTM), and we show that it only arises with natural language when questionable theories of meaning are presupposed. We conclude by showing that it would apply to LLMs only if they were interpreted in the manner of how the CTM conceives the mind, i.e., by postulating that LLMs rely on a version of a language of thought, or by adopting said questionable theories of meaning; since neither option is rational, we conclude that the SGP does not apply to LLMs."
    },
    {
        "paperId": "3a8ae80c08bc23e89e2fdb66847986fdfa39614e",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "PANDA: Persona Attributes Navigation for Detecting and Alleviating Overuse Problem in Large Language Models",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.670, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2117157876",
                "name": "Jinsung Kim"
            },
            {
                "authorId": "2176399606",
                "name": "Seonmin Koo"
            },
            {
                "authorId": "2276211307",
                "name": "Heuiseok Lim"
            }
        ],
        "abstract": "In the persona-grounded dialogue (PGD) task, it is required not only to respond fluently, but also to ground the attributes according to the current conversation topic properly. However, due to their tendency to overly ground given attributes, LLMs often generate unnatural responses provoked by using attributes that deviate from the flow of the conversation or by exploiting too many attributes at once. We term this phenomenon the *overuse* problem of LLMs. Unfortunately, research devising precise criteria and frameworks to quantitatively verify LLMs\u2019 *overuse* problem is obviously insufficient. To address this issue, we propose **P**ersona **A**ttributes **N**avigation for **D**etecting and **A**lleviating the *overuse* problem (**PANDA**) framework. **PANDA** is the first study to quantify the persona *overuse* problem of LLMs by establishing clear standards of the problem and verifying various LLMs based on them. Moreover, this framework navigates us into understanding persona attributes by introducing diverse and detailed dialogue topics that consider practical conversation situations. We provide insights related to LLMs\u2019 persona attribute *overuse* problem through comprehensive verification and analysis with **PANDA** in the PGD task. Our code and resources can be found at http://github.com/jin62304/PANDA."
    },
    {
        "paperId": "fc8ad2b8bb6d83d977926addf2496c9aa3422bbc",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Story Morals: Surfacing value-driven narrative schemas using large language models",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.723, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2315308979",
                "name": "David Hobson"
            },
            {
                "authorId": "2281982524",
                "name": "Haiqi Zhou"
            },
            {
                "authorId": "2315305786",
                "name": "Derek Ruths"
            },
            {
                "authorId": "2281831542",
                "name": "Andrew Piper"
            }
        ],
        "abstract": "Stories are not only designed to entertain but encode lessons reflecting their authors\u2019 beliefs about the world. In this paper, we propose a new task of narrative schema labelling based on the concept of \u201cstory morals\u201d to identify the values and lessons conveyed in stories. Using large language models (LLMs) such as GPT-4, we develop methods to automatically extract and validate story morals across a diverse set of narrative genres, including folktales, novels, movies and TV, personal stories from social media and the news. Our approach involves a multi-step prompting sequence to derive morals and validate them through both automated metrics and human assessments. The findings suggest that LLMs can effectively approximate human story moral interpretations and offer a new avenue for computational narrative understanding. By clustering the extracted morals on a sample dataset of folktales from around the world, we highlight the commonalities and distinctiveness of narrative values, providing preliminary insights into the distribution of values across cultures. This work opens up new possibilities for studying narrative schemas and their role in shaping human beliefs and behaviors."
    },
    {
        "paperId": "beb6a98e46b9756edeef610ff231b454d7dfc20a",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Adaptive Axes: A Pipeline for In-domain Social Stereotype Analysis",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.872, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2325705952",
                "name": "Qingcheng Zeng"
            },
            {
                "authorId": "2323911406",
                "name": "Mingyu Jin"
            },
            {
                "authorId": "2324597043",
                "name": "Rob Voigt"
            }
        ],
        "abstract": "Prior work has explored the possibility of using the semantic information obtained from embedding representations to quantify social stereotypes, leveraging techniques such as word embeddings combined with a list of traits (Garg et al., 2018; Charlesworth et al., 2022) or semantic axes (An et al., 2018; Lucy et al., 2022). However, these approaches have struggled to fully capture the variability in stereotypes across different conceptual domains for the same social group (e.g., black in science, health, and art), in part because the identity of a word and the associations formed during pre-training can dominate its contextual representation (Field and Tsvetkov, 2019). This study explores the ability to recover stereotypes from the contexts surrounding targeted entities by utilizing state-of-the-art text embedding models and adaptive semantic axes enhanced by large language models (LLMs). Our results indicate that the proposed pipeline not only surpasses token-based methods in capturing in-domain framing but also effectively tracks stereotypes over time and along domain-specific semantic axes for in-domain texts. Our research highlights the potential of employing text embedding models to achieve a deeper understanding of nuanced social stereotypes."
    },
    {
        "paperId": "fee4a6f7735e7cf1117f8f3955d6dd02dca0c2b3",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Global Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2115674177",
                "name": "Dong Won Lee"
            },
            {
                "authorId": "2756001",
                "name": "Hae Won Park"
            },
            {
                "authorId": "2292121692",
                "name": "Yoon Kim"
            },
            {
                "authorId": "2287781906",
                "name": "Cynthia Breazeal"
            },
            {
                "authorId": "49933077",
                "name": "Louis-philippe Morency"
            }
        ],
        "abstract": "We describe an approach for aligning an LLM based dialogue agent for long-term social dialogue, where there is only a single global score given by the user at the end of the session. In this paper, we propose the usage of denser naturally-occurring multimodal communicative signals as local implicit feedback to improve the turn-level utterance generation. Therefore, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session level reward, using Local Implicit (LI) multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the RLHF pipeline to improve an LLM-based dialog agent. We run quantitative and qualitative human studies on two large-scale datasets to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods."
    },
    {
        "paperId": "530642cfa0b674e7e1778a2a229f1a444e6a7146",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Working Memory Identifies Reasoning Limits in Language Models",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.938, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2330091603",
                "name": "Chunhui Zhang"
            },
            {
                "authorId": "19258096",
                "name": "Yiren Jian"
            },
            {
                "authorId": "51253105",
                "name": "Z. Ouyang"
            },
            {
                "authorId": "1918441",
                "name": "Soroush Vosoughi"
            }
        ],
        "abstract": "This study explores the inherent limitations of large language models (LLMs) from a scaling perspective, focusing on the upper bounds of their cognitive capabilities. We integrate insights from cognitive science to quantitatively examine how LLMs perform on n-back tasks\u2014a benchmark used to assess working memory, which involves temporarily holding and manipulating information. Our findings reveal that despite the increased model size, LLMs still face significant challenges in holding and processing information effectively, especially under complex task conditions. We also assess various prompting strategies, revealing their diverse impacts on LLM performance. The results highlight the struggle of current LLMs to autonomously discover optimal problem-solving patterns without heavily relying on manually corrected prompts. To move beyond these constraints, fundamental improvements in the planning and search of LLMs are essential for them to reason autonomously. Improving these capabilities will reduce the reliance on external corrections and enable LLMs to become more autonomous in their problem-solving processes."
    },
    {
        "paperId": "64d9a4fc3d0aa7abc99b88b814fd67fc1dfd6756",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Do LLMs learn a true syntactic universal?",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.950, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2329737582",
                "name": "John Hale"
            },
            {
                "authorId": "2329736239",
                "name": "Milo\u0161 Stanojevi\u0107"
            }
        ],
        "abstract": "Do large multilingual language models learn language universals? We consider a candidate universal much-discussed in the linguistics literature, the Final-over-Final Condition (Sheehan et al., 2017b). This Condition is syntactic in the sense that it can only be stated by reference to abstract sentence properties such as nested phrases and head direction. A study of typologically diverse \u201cmixed head direction\u201d languages confirms that the Condition holds in corpora. But in a targeted syntactic evaluation, Gemini Pro only seems to respect the Condition in German, Russian, Hungarian and Serbian. These relatively high-resource languages contrast with Basque, where Gemini Pro does not seem to have learned the Condition at all. This result suggests that modern language models may need additional sources of bias in order to become truly human-like, within a developmentally-realistic budget of training data."
    },
    {
        "paperId": "4b750ac129be04527dfdad8cbdf85f7d0469271b",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "CURE: Context- and Uncertainty-Aware Mental Disorder Detection",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.994, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2175495659",
                "name": "Migyeong Kang"
            },
            {
                "authorId": "2329740717",
                "name": "Goun Choi"
            },
            {
                "authorId": "2259921005",
                "name": "Hyolim Jeon"
            },
            {
                "authorId": "2306762040",
                "name": "Ji hyun An"
            },
            {
                "authorId": "2169215",
                "name": "Daejin Choi"
            },
            {
                "authorId": "2329395472",
                "name": "Jinyoung Han"
            }
        ],
        "abstract": "As the explainability of mental disorder detection models has become important, symptom-based methods that predict disorders from identified symptoms have been widely utilized. However, since these approaches focused on the presence of symptoms, the context of symptoms can be often ignored, leading to missing important contextual information related to detecting mental disorders. Furthermore, the result of disorder detection can be vulnerable to errors that may occur in identifying symptoms. To address these issues, we propose a novel framework that detects mental disorders by leveraging symptoms and their context while mitigating potential errors in symptom identification. In this way, we propose to use large language models to effectively extract contextual information and introduce an uncertainty-aware decision fusion network that combines predictions of multiple models based on quantified uncertainty values. To evaluate the proposed method, we constructed a new Korean mental health dataset annotated by experts, named KoMOS. Experimental results demonstrate that the proposed model accurately detects mental disorders even in situations where symptom information is incomplete."
    },
    {
        "paperId": "65bd9d3715ad6df3e8eea061780fc053ead1d87c",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "In-Context Compositional Generalization for Large Vision-Language Models",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.996, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2294251099",
                "name": "Chuanhao Li"
            },
            {
                "authorId": "32791167",
                "name": "Chenchen Jing"
            },
            {
                "authorId": "2233231737",
                "name": "Zhen Li"
            },
            {
                "authorId": "2329736724",
                "name": "Mingliang Zhai"
            },
            {
                "authorId": "2302812287",
                "name": "Yuwei Wu"
            },
            {
                "authorId": "2257383625",
                "name": "Yunde Jia"
            }
        ],
        "abstract": "Recent work has revealed that in-context learning for large language models exhibits compositional generalization capacity, which can be enhanced by selecting in-context demonstrations similar to test cases to provide contextual information. However, how to exhibit in-context compositional generalization (ICCG) of large vision-language models (LVLMs) is non-trival. Due to the inherent asymmetry between visual and linguistic modalities, ICCG in LVLMs faces an inevitable challenge\u2014redundant information on the visual modality. The redundant information affects in-context learning from two aspects: (1) Similarity calculation may be dominated by redundant information, resulting in sub-optimal demonstration selection. (2) Redundant information in in-context demonstrations brings misleading contextual information to in-context learning. To alleviate these problems, we propose a demonstration selection method to achieve ICCG for LVLMs, by considering two key factors of demonstrations: content and structure, from a multimodal perspective. Specifically, we design a diversity-coverage-based matching score to select demonstrations with maximum coverage, and avoid selecting demonstrations with redundant information via their content redundancy and structural complexity. We build a GQA-ICCG dataset to simulate the ICCG setting, and conduct experiments on GQA-ICCG and the VQA v2 dataset. Experimental results demonstrate the effectiveness of our method."
    },
    {
        "paperId": "526349670919f8fc5e123a64228f30a9bd86a4c2",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Please note that I\u2019m just an AI: Analysis of Behavior Patterns of LLMs in (Non-)offensive Speech Identification",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.1019, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2329739512",
                "name": "Esra D\u00f6nmez"
            },
            {
                "authorId": "2329741127",
                "name": "Thang Vu"
            },
            {
                "authorId": "2329738331",
                "name": "Agnieszka Falenska"
            }
        ],
        "abstract": "Offensive speech is highly prevalent on online platforms. Being trained on online data, Large Language Models (LLMs) display undesirable behaviors, such as generating harmful text or failing to recognize it. Despite these shortcomings, the models are becoming a part of our everyday lives by being used as tools for information search, content creation, writing assistance, and many more. Furthermore, the research explores using LLMs in applications with immense social risk, such as late-life companions and online content moderators. Despite the potential harms from LLMs in such applications, whether LLMs can reliably identify offensive speech and how they behave when they fail are open questions. This work addresses these questions by probing sixteen widely used LLMs and showing that most fail to identify (non-)offensive online language. Our experiments reveal undesirable behavior patterns in the context of offensive speech detection, such as erroneous response generation, over-reliance on profanity, and failure to recognize stereotypes. Our work highlights the need for extensive documentation of model reliability, particularly in terms of the ability to detect offensive language."
    },
    {
        "paperId": "7d7890407cbafa075061d7bd5b53c765efc67399",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "GOME: Grounding-based Metaphor Binding With Conceptual Elaboration For Figurative Language Illustration",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.1028, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2237737472",
                "name": "Linhao Zhang"
            },
            {
                "authorId": "2326248349",
                "name": "Jintao Liu"
            },
            {
                "authorId": "2287962858",
                "name": "Li Jin"
            },
            {
                "authorId": "2331179097",
                "name": "Hao Wang"
            },
            {
                "authorId": "2257931560",
                "name": "Kaiwen Wei"
            },
            {
                "authorId": "2329748262",
                "name": "Guangluan Xu"
            }
        ],
        "abstract": "The illustration or visualization of figurative language, such as linguistic metaphors, is an emerging challenge for existing Large Language Models (LLMs) and multimodal models. Due to their comparison of seemingly unrelated concepts in metaphors, existing LLMs have a tendency of over-literalization, which illustrates figurative language solely based on literal objects, ignoring the underlying groundings and associations across disparate metaphorical domains. Furthermore, prior approaches have ignored the binding process between visual objects and metaphorical attributes, which further intensifies the infidelity of visual metaphors. To address the issues above, we propose GOME (Grounding-based Metaphor Binding), which illustrates linguistic metaphors from the grounding perspective elaborated through LLMs. GOME consists of two steps for metaphor illustration, including grounding-based elaboration and scenario visualization. In the elaboration step, metaphorical knowledge is integrated into systematic instructions for LLMs, which employs a CoT prompting method rooted in rhetoric. This approach specifies metaphorical devices such as vehicles and groundings, to ensure accurate and faithful descriptions consumed by text-to-image models. In the visualization step, an inference-time metaphor binding method is realized based on elaboration outputs, which register attentional control during the diffusion process, and captures the underlying attributes from the abstract metaphorical domain. Comprehensive evaluations using multiple downstream tasks confirm that, GOME is superior to isolated LLMs, diffusion models, or their direct collaboration."
    },
    {
        "paperId": "56eef417c3b24678963de6478e7a3e46cdf15a77",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "NeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.1050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2298452735",
                "name": "S. Doneva"
            },
            {
                "authorId": "2759740",
                "name": "Tilia Ellendorff"
            },
            {
                "authorId": "2298453030",
                "name": "Beate Sick"
            },
            {
                "authorId": "2298453341",
                "name": "Jean-Philippe Goldman"
            },
            {
                "authorId": "2329739486",
                "name": "Amelia Cannon"
            },
            {
                "authorId": "2274429987",
                "name": "Gerold Schneider"
            },
            {
                "authorId": "2329739198",
                "name": "B. Ineichen"
            }
        ],
        "abstract": "Extracting and aggregating information from clinical trial registries could provide invaluable insights into the drug development landscape and advance the treatment of neurologic diseases. However, achieving this at scale is hampered by the volume of available data and the lack of an annotated corpus to assist in the development of automation tools. Thus, we introduce NeuroTrialNER, a new and fully open corpus for named entity recognition (NER). It comprises 1093 clinical trial summaries sourced from ClinicalTrials.gov, annotated for neurological diseases, therapeutic interventions, and control treatments. We describe our data collection process and the corpus in detail. We demonstrate its utility for NER using large language models and achieve a close-to-human performance. By bridging the gap in data resources, we hope to foster the development of text processing tools that help researchers navigate clinical trials data more easily."
    },
    {
        "paperId": "738cd18ce4f81dbc063381cd82437124697101e3",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "An Empirical Analysis of the Writing Styles of Persona-Assigned LLMs",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.1079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2329737771",
                "name": "Manuj Malik"
            },
            {
                "authorId": "2330076851",
                "name": "Jing Jiang"
            },
            {
                "authorId": "1890071",
                "name": "K. M. Chai"
            }
        ],
        "abstract": ","
    },
    {
        "paperId": "a9e5e94b387da7b59873e728f976e66f4bb91ceb",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Re-Evaluating Evaluation for Multilingual Summarization",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.1085, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2282474780",
                "name": "Jessica Zosa Forde"
            },
            {
                "authorId": "49775305",
                "name": "Ruochen Zhang"
            },
            {
                "authorId": "35566806",
                "name": "Lintang Sutawika"
            },
            {
                "authorId": "8129718",
                "name": "Alham Fikri Aji"
            },
            {
                "authorId": "2220548276",
                "name": "Samuel Cahyawijaya"
            },
            {
                "authorId": "9162688",
                "name": "Genta Indra Winata"
            },
            {
                "authorId": "2329757557",
                "name": "Minghao Wu"
            },
            {
                "authorId": "2257274340",
                "name": "Carsten Eickhoff"
            },
            {
                "authorId": "2273535088",
                "name": "Stella Biderman"
            },
            {
                "authorId": "2949185",
                "name": "Ellie Pavlick"
            }
        ],
        "abstract": "Automatic evaluation approaches (ROUGE, BERTScore, LLM-based evaluators) have been widely used to evaluate summarization tasks. Despite the complexities of script differences and tokenization, these approaches have been indiscriminately applied to summarization across multiple languages. While previous works have argued that these approaches correlate strongly with human ratings in English, it remains unclear whether the conclusion holds for other languages. To answer this question, we construct a small-scale pilot dataset containing article-summary pairs and human ratings in English, Chinese and Indonesian. To measure the strength of summaries, our ratings are measured as head-to-head comparisons with resulting Elo scores across four dimensions. Our analysis reveals that standard metrics are unreliable measures of quality, and that these problems are exacerbated in Chinese and Indonesian. We advocate for more nuanced and careful considerations in designing a robust evaluation framework for multiple languages."
    },
    {
        "paperId": "4d5f870f0fa143ed68adab3c8b03bebcaf4feddc",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "The Empirical Variability of Narrative Perceptions of Social Media Texts",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.1113, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2266840684",
                "name": "Joel Mire"
            },
            {
                "authorId": "34199564",
                "name": "Maria Antoniak"
            },
            {
                "authorId": "2266840467",
                "name": "Elliott Ash"
            },
            {
                "authorId": "2266841012",
                "name": "Andrew Piper"
            },
            {
                "authorId": "2729164",
                "name": "Maarten Sap"
            }
        ],
        "abstract": "Most NLP work on narrative detection has focused on prescriptive definitions of stories crafted by researchers, leaving open the questions: how do crowd workers perceive texts to be a story, and why? We investigate this by building StoryPerceptions, a dataset of 2,496 perceptions of storytelling in 502 social media texts from 255 crowd workers, including categorical labels along with free-text storytelling rationales, authorial intent, and more. We construct a fine-grained bottom-up taxonomy of crowd workers\u2019 varied and nuanced perceptions of storytelling by open-coding their free-text rationales. Through comparative analyses at the label and code level, we illuminate patterns of disagreement among crowd workers and across other annotation contexts, including prescriptive labeling from researchers and LLM-based predictions. Notably, plot complexity, references to generalized or abstract actions, and holistic aesthetic judgments (such as a sense of cohesion) are especially important in disagreements. Our empirical findings broaden understanding of the types, relative importance, and contentiousness of features relevant to narrative detection, highlighting opportunities for future work on reader-contextualized models of narrative reception."
    },
    {
        "paperId": "a30d1846ff94055c7a37f6b408034d17b65b21f7",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "One-to-Many Communication and Compositionality in Emergent Communication",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.1157, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2329900310",
                "name": "Heeyoung Lee"
            }
        ],
        "abstract": "Compositional languages leverage rules that derive meaning from combinations of simpler constituents. This property is considered to be the hallmark of human language as it enables the ability to express novel concepts and ease of learning. As such, numerous studies in the emergent communication field explore the prerequisite conditions for emergence of compositionality. Most of these studies set out one-to-one communication environment wherein a speaker interacts with a single listener during a single round of communication game. However, real-world communications often involve multiple listeners; their interests may vary and they may even need to coordinate among themselves to be successful at a given task. This work investigates the effects of one-to-many communication environment on emergent languages where a single speaker broadcasts its message to multiple listeners to cooperatively solve a task. We observe that simply broadcasting the speaker\u2019s message to multiple listeners does not induce more compositional languages. We then find and analyze two axes of environmental pressures that facilitate emergence of compositionality: listeners of *different interests* and *coordination* among listeners."
    },
    {
        "paperId": "89880e13ef6844bd1f4bede2658144caa89c9e83",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Thoughts to Target: Enhance Planning for Target-driven Conversation",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.1175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2297730893",
                "name": "Zhonghua Zheng"
            },
            {
                "authorId": "2329188582",
                "name": "Lizi Liao"
            },
            {
                "authorId": "2316667994",
                "name": "Yang Deng"
            },
            {
                "authorId": "2329407419",
                "name": "Ee-Peng Lim"
            },
            {
                "authorId": "2329390132",
                "name": "Minlie Huang"
            },
            {
                "authorId": "2316715562",
                "name": "Liqiang Nie"
            }
        ],
        "abstract": "In conversational AI, large-scale models excel in various tasks but struggle with target-driven conversation planning. Current methods, such as chain-of-thought reasoning and tree-search policy learning techniques, either neglect plan rationality or require extensive human simulation procedures. Addressing this, we propose a novel two-stage framework, named EnPL, to improve the LLMs\u2019 capability in planning conversations towards designated targets, including (1) distilling natural language plans from target-driven conversation corpus and (2) generating new plans with demonstration-guided in-context learning. Specifically, we first propose a filter approach to distill a high-quality plan dataset, ConvPlan (Resources of this paper can be found at https://github.com/pandazzh2020/ConvPlan). With the aid of corresponding conversational data and support from relevant knowledge bases, we validate the quality and rationality of these plans. Then, these plans are leveraged to help guide LLMs to further plan for new targets. Empirical results demonstrate that our method significantly improves the planning ability of LLMs, especially in target-driven conversations. Furthermore, EnPL is demonstrated to be quite effective in collecting target-driven conversation datasets and enhancing response generation, paving the way for constructing extensive target-driven conversational models."
    },
    {
        "paperId": "3c069b953413c95bbb60b7472a83870884081161",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Adversarial Text Generation using Large Language Models for Dementia Detection",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.1222, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "50877340",
                "name": "Youxiang Zhu"
            },
            {
                "authorId": "2319369795",
                "name": "Nana Lin"
            },
            {
                "authorId": "2329735500",
                "name": "Kiran Balivada"
            },
            {
                "authorId": "2361207693",
                "name": "Daniel Haehn"
            },
            {
                "authorId": "2153398177",
                "name": "Xiaohui Liang"
            }
        ],
        "abstract": "Although large language models (LLMs) excel in various text classification tasks, regular prompting strategies (e.g., few-shot prompting) do not work well with dementia detection via picture description. The challenge lies in the language marks for dementia are unclear, and LLM may struggle with relating its internal knowledge to dementia detection. In this paper, we present an accurate and interpretable classification approach by Adversarial Text Generation (ATG), a novel decoding strategy that could relate dementia detection with other tasks. We further develop a comprehensive set of instructions corresponding to various tasks and use them to guide ATG, achieving the best accuracy of 85%, >10% improvement compared to the regular prompting strategies. In addition, we introduce feature context, a human-understandable text that reveals the underlying features of LLM used for classifying dementia. From feature contexts, we found that dementia detection can be related to tasks such as assessing attention to detail, language, and clarity with specific features of the environment, character, and other picture content or language-related features. Future work includes incorporating multi-modal LLMs to interpret speech and picture information."
    },
    {
        "paperId": "3cd29c4e5e67cdda63b2ca3b727f7e745c1eb3e1",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Deciphering Cognitive Distortions in Patient-Doctor Mental Health Conversations: A Multimodal LLM-Based Detection and Reasoning Framework",
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.1256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2148283381",
                "name": "G. Singh"
            },
            {
                "authorId": "2329736693",
                "name": "Sai Vemulapalli"
            },
            {
                "authorId": "40195882",
                "name": "Mauajama Firdaus"
            },
            {
                "authorId": "2254291732",
                "name": "Asif Ekbal"
            }
        ],
        "abstract": "Cognitive distortion research holds increasing significance as it sheds light on pervasive errors in thinking patterns, providing crucial insights into mental health challenges and fostering the development of targeted interventions and therapies. This paper delves into the complex domain of cognitive distortions which are prevalent distortions in cognitive processes often associated with mental health issues. Focusing on patient-doctor dialogues, we introduce a pioneering method for detecting and reasoning about cognitive distortions utilizing Large Language Models (LLMs). Operating within a multimodal context encompassing audio, video, and textual data, our approach underscores the critical importance of integrating diverse modalities for a comprehensive understanding of cognitive distortions. By leveraging multimodal information, including audio, video, and textual data, our method offers a nuanced perspective that enhances the accuracy and depth of cognitive distortion detection and reasoning in a zero-shot manner. Our proposed hierarchical framework adeptly tackles both detection and reasoning tasks, showcasing significant performance enhancements compared to current methodologies. Through comprehensive analysis, we elucidate the efficacy of our approach, offering promising insights into the diagnosis and understanding of cognitive distortions in multimodal settings.The code and dataset can be found here: https://github.com/clang1234/ZS-CoDR.git"
    },
    {
        "paperId": "ea052da88d817ea2c7b13b6706b2124965936769",
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "title": "Rethinking Pragmatics in Large Language Models: Towards Open-Ended Evaluation and Preference Tuning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.1258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-11-12",
        "authors": [
            {
                "authorId": "2247967520",
                "name": "Shengguang Wu"
            },
            {
                "authorId": "2329898756",
                "name": "Shusheng Yang"
            },
            {
                "authorId": "2274080133",
                "name": "Zhenglun Chen"
            },
            {
                "authorId": "2266465542",
                "name": "Qi Su"
            }
        ],
        "abstract": "This study addresses the challenges of assessing and enhancing social-pragmatic inference in large language models (LLMs). We first highlight the inadequacy of current accuracy-based multiple choice question answering (MCQA) formats in assessing social-pragmatic reasoning, and propose the direct evaluation of models\u2019 free-form responses as measure, which correlates better with human judgment. Furthermore, we explore methods to improve pragmatic abilities in LLMs, advocating for preference optimization (PO) over supervised finetuning (SFT), given the absence of a definitive \u201cgold\u201d answer in social contexts. Our results show that preferential tuning consistently outperforms SFT across pragmatic phenomena and offers a near-free launch in pragmatic abilities without compromising general capabilities. Lastly, we examine the internal structure of LLMs, revealing that the significant boost in pragmatic reasoning is tied to deeper layer representations, analogous to human high-level thinking. Our experiments span a variety of pragmatic and social reasoning datasets, as well as an image referential game requiring a multimodal theory of mind (ToM). With our refined paradigms for evaluating and enhancing pragmatic inference, this paper offers key insights into building more socially aware language models."
    },
    {
        "paperId": "46d70e3942a331bc4db678a5200c4174025b8b60",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Quantum-inspired Language Model with Lindblad Master Equation and Interference Measurement for Sentiment Analysis",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.naacl-long.116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-16",
        "authors": [
            {
                "authorId": "2306635561",
                "name": "Kehuan Yan"
            },
            {
                "authorId": "2187454266",
                "name": "Peichao Lai"
            },
            {
                "authorId": "2258299780",
                "name": "Yilei Wang"
            }
        ],
        "abstract": "Quantum-inspired models have demonstrated superior performance in many downstream language tasks, such as question answering and sentiment analysis. However, recent models primarily focus on embedding and measurement operations, overlooking the significance of the quantum evolution process. In this work, we present a novel quantum-inspired neural network, LI-QiLM, which integrates the Lindblad Master Equation (LME) to model the evolution process and the interferometry to the measurement process, providing more physical meaning to strengthen the interpretability. We conduct comprehensive experiments on six sentiment analysis datasets. Compared to the traditional neural networks, transformer-based pre-trained models and quantum-inspired models, such as CICWE-QNN and ComplexQNN, the proposed method demonstrates superior performance in accuracy and F1-score on six commonly used datasets for sentiment analysis. Additional ablation tests verify the effectiveness of LME and interferometry."
    },
    {
        "paperId": "f95963baed747bdf5d0336a806f2112664636e88",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "My Heart Skipped a Beat! Recognizing Expressions of Embodied Emotion in Natural Language",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.naacl-long.193, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-16",
        "authors": [
            {
                "authorId": "2302511961",
                "name": "Zhuang Yuan"
            },
            {
                "authorId": "144830318",
                "name": "Tianyu Jiang"
            },
            {
                "authorId": "2301582451",
                "name": "Ellen Riloff"
            }
        ],
        "abstract": "Humans frequently experience emotions. When emotions arise, they affect not only our mental state but can also change our physical state. For example, we often open our eyes wide when we are surprised, or clap our hands when we feel excited. Physical manifestations of emotions are referred to as embodied emotion in the psychology literature. From an NLP perspective, recognizing descriptions of physical movements or physiological responses associated with emotions is a type of implicit emotion recognition. Our work introduces a new task of recognizing expressions of embodied emotion in natural language. We create a dataset of sentences that contains 7,300 body part mentions with human annotations for embodied emotion. We develop a classification model for this task and present two methods to acquire weakly labeled instances of embodied emotion by extracting emotional manner expressions and by prompting a language model. Our experiments show that the weakly labeled data can train an effective classification model without gold data, and can also improve performance when combined with gold data. Our dataset is publicly available at https://github.com/yyzhuang1991/Embodied-Emotions."
    },
    {
        "paperId": "388fea2f6e4dfa6f7469362fffeeaf00f46f652b",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "A Zero-Shot Monolingual Dual Stage Information Retrieval System for Spanish Biomedical Systematic Literature Reviews",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.naacl-long.206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-16",
        "authors": [
            {
                "authorId": "2236997288",
                "name": "Regina Ofori-Boateng"
            },
            {
                "authorId": "1400892315",
                "name": "M. Aceves-Martins"
            },
            {
                "authorId": "1784639",
                "name": "N. Wiratunga"
            },
            {
                "authorId": "1399137626",
                "name": "Carlos Francisco Moreno-Garc\u00eda"
            }
        ],
        "abstract": "Systematic Reviews (SRs) are foundational in healthcare for synthesising evidence to inform clinical practices. Traditionally skewed towards English-language databases, SRs often exclude significant research in other languages, leading to potential biases. This study addresses this gap by focusing on Spanish, a language notably underrepresented in SRs. We present a foundational zero-shot dual information retrieval (IR) baseline system, integrating traditional retrieval methods with pre-trained language models and cross-attention re-rankers for enhanced accuracy in Spanish biomedical literature retrieval. Utilising the LILACS database, known for its comprehensive coverage of Latin American and Caribbean biomedical literature, we evaluate the approach with three real-life case studies in Spanish SRs. The findings demonstrate the system\u2019s efficacy and underscore the importance of query formulation. This study contributes to the field of IR by promoting language inclusivity and supports the development of more comprehensive and globally representative healthcare guidelines."
    },
    {
        "paperId": "6f25ce1be55cf76ff53d21bbcedd572ae3793ee2",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Pedagogically Aligned Objectives Create Reliable Automatic Cloze Tests",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.naacl-long.220, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-16",
        "authors": [
            {
                "authorId": "3144024",
                "name": "Brian D. Ondov"
            },
            {
                "authorId": "122543533",
                "name": "Kush Attal"
            },
            {
                "authorId": "2306633731",
                "name": "Dina Demner-Fushman"
            }
        ],
        "abstract": "The cloze training objective of Masked Language Models makes them a natural choice for generating plausible distractors for human cloze questions. However, distractors must also be both distinct and incorrect, neither of which is directly addressed by existing neural methods. Evaluation of recent models has also relied largely on automated metrics, which cannot demonstrate the reliability or validity of human comprehension tests. In this work, we first formulate the pedagogically motivated objectives of plausibility, incorrectness, and distinctiveness in terms of conditional distributions from language models. Second, we present an unsupervised, interpretable method that uses these objectives to jointly optimize sets of distractors. Third, we test the reliability and validity of the resulting cloze tests compared to other methods with human participants. We find our method has stronger correlation with teacher-created comprehension tests than the state-of-the-art neural method and is more internally consistent. Our implementation is freely available and can quickly create a multiple choice cloze test from any given passage."
    },
    {
        "paperId": "876bcc2576eff53530fa169cc032bb8c1b642d5a",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "No Context Needed: Contextual Quandary In Idiomatic Reasoning With Pre-Trained Language Models",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.naacl-long.272, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-16",
        "authors": [
            {
                "authorId": "2273595246",
                "name": "K. Cheng"
            },
            {
                "authorId": "2263637139",
                "name": "Suma Bhat"
            }
        ],
        "abstract": "Reasoning in the presence of idiomatic expressions (IEs) remains a challenging frontier in natural language understanding (NLU). Unlike standard text, the non-compositional nature of an IE makes it difficult for model comprehension, as their figurative or non-literal mean- ing usually cannot be inferred from the constituent words alone. It stands to reason that in these challenging circumstances, pre-trained language models (PTLMs) should make use of the surrounding context to infer additional in- formation about the IE. In this paper, we investigate the utilization of said context for idiomatic reasoning tasks, which is under-explored relative to arithmetic or commonsense reason- ing (Liu et al., 2022; Yu et al., 2023). Preliminary findings point to a surprising observation: general purpose PTLMs are actually negatively affected by the context, as performance almost always increases with its removal. In these scenarios, models may see gains of up to 3.89%. As a result, we argue that only IE-aware models remain suitable for idiomatic reasoning tasks, given the unexpected and unexplainable manner in which general purpose PTLMs reason over IEs. Additionally, we conduct studies to examine how models utilize the context in various situations, as well as an in-depth analysis on dataset formation and quality. Finally, we provide some explanations and insights into the reasoning process itself based on our results."
    },
    {
        "paperId": "2c1c3704bd512b6989a12cf4d4ac5be5ea030a32",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Generating Mental Health Transcripts with SAPE (Spanish Adaptive Prompt Engineering)",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.naacl-long.285, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-16",
        "authors": [
            {
                "authorId": "2273994353",
                "name": "D. Lozoya"
            },
            {
                "authorId": "2306633153",
                "name": "Alejandro Berazaluce"
            },
            {
                "authorId": "2306632753",
                "name": "Juan Perches"
            },
            {
                "authorId": "2306632750",
                "name": "Eloy L\u00faa"
            },
            {
                "authorId": "2273994312",
                "name": "Mike Conway"
            },
            {
                "authorId": "2306632780",
                "name": "Simon D'alfonso"
            }
        ],
        "abstract": "Large language models have become valuable tools for data augmentation in scenarios with limited data availability, as they can generate synthetic data resembling real-world data. However, their generative performance depends on the quality of the prompt used to instruct the model. Prompt engineering that relies on hand-crafted strategies or requires domain experts to adjust the prompt often yields suboptimal results. In this paper we present SAPE, a Spanish Adaptive Prompt Engineering method utilizing genetic algorithms for prompt generation and selection. Our evaluation of SAPE focuses on a generative task that involves the creation of Spanish therapy transcripts, a type of data that is challenging to collect due to the fact that it typically includes protected health information. Through human evaluations conducted by mental health professionals, our results show that SAPE produces Spanish counselling transcripts that more closely resemble authentic therapy transcripts compared to other prompt engineering techniques that are based on Reflexion and Chain-of-Thought."
    },
    {
        "paperId": "33038d55738f84187bb8ded80851720ceb29a8b4",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "A Theory Guided Scaffolding Instruction Framework for LLM-Enabled Metaphor Reasoning",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.naacl-long.428, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-16",
        "authors": [
            {
                "authorId": "144966427",
                "name": "Yuan Tian"
            },
            {
                "authorId": "2150038278",
                "name": "Nan Xu"
            },
            {
                "authorId": "2264558046",
                "name": "Wenji Mao"
            }
        ],
        "abstract": "Metaphor detection is a challenging task in figurative language processing, which aims to distinguish between metaphorical and literal expressions in text. Existing methods tackle metaphor detection via training or fine-tuning discriminative models on labeled data. However, these approaches struggle to explain the underlying reasoning process behind the metaphorical/literal judgment. Recently, large language models (LLMs) have shown promise in language reasoning tasks. Although promising, LLM-based methods for metaphor detection and reasoning are still faced with the challenging issue of bringing the explainable concepts for metaphor reasoning and their linguistic manifestation. To fill this gap, we propose a novel Theory guided Scaffolding Instruction (TSI) framework that instructs an LLM to infer the underlying reasoning process of metaphor detection guided by metaphor theories for the first time. Our work is inspired by a pedagogical strategy called scaffolding instruction, which encourages educators to provide questioning and support as scaffolding so as to assist learners in constructing the understanding of pedagogical goals step by step. We first construct a metaphor knowledge graph grounded in metaphor theory which serves as the instructional structure to obtain a series of scaffolding questions, directing the LLM to incrementally generate the reasoning process for metaphor understanding through dialogue interactions. During this theory guided instruction process, we explore the LLM\u2019s mastery boundary and provide the relevant knowledge as scaffolding support when the question is beyond the LLM\u2019s capability. Experimental results verify that our method significantly outperforms both the LLM-based reasoning methods and the SOTA methods in metaphor detection, indicating the facilitation of metaphor and instruction theories in guiding LLM-based reasoning process."
    },
    {
        "paperId": "852a84d8d1385b0367e92ff14588d1d3e3782cd3",
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "title": "Automatic, Meta and Human Evaluation for Multimodal Summarization with Multimodal Output",
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.naacl-long.430, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-06-16",
        "authors": [
            {
                "authorId": "3435068",
                "name": "Haojie Zhuang"
            },
            {
                "authorId": "2291469204",
                "name": "Wei Emma Zhang"
            },
            {
                "authorId": "2307547655",
                "name": "Leon Xie"
            },
            {
                "authorId": "2303307115",
                "name": "Weitong Chen"
            },
            {
                "authorId": "2291439407",
                "name": "Jian Yang"
            },
            {
                "authorId": "2264149461",
                "name": "Quan Z. Sheng"
            }
        ],
        "abstract": "Multimodal summarization with multimodal output (MSMO) has attracted increasing research interests recently as multimodal summary could provide more comprehensive information compared to text-only summary, effectively improving the user experience and satisfaction. As one of the most fundamental components for the development of MSMO, evaluation is an emerging yet underexplored research topic. In this paper, we fill this gap and propose a research framework that studies three research questions of MSMO evaluation: (1) Automatic Evaluation: We propose a novel metric mLLM-EVAL, which utilizes multimodal Large Language Model for MSMO EVALuation. (2) Meta-Evaluation: We create a meta-evaluation benchmark dataset by collecting human-annotated scores for multimodal summaries. With our benchmark, we conduct meta-evaluation analysis to assess the quality of different evaluation metrics and show the effectiveness of our proposed mLLM-EVAL. (3) Human Evaluation: To provide more objective and unbiased human annotations for meta-evaluation, we hypothesize and verify three types of cognitive biases in human evaluation. We also incorporate our findings into the human annotation process in the meta-evaluation benchmark. Overall, our research framework provides an evaluation metric, a meta-evaluation benchmark dataset annotated by humans and an analysis of cognitive biases in human evaluation, which we believe would serve as a valuable and comprehensive resource for the MSMO research community."
    },
    {
        "paperId": "f5369f58f77fc987b4153a7225ec4bf5617fb236",
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "title": "BrainLM: A foundation model for brain activity recordings",
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2023/09/13/2023.09.12.557460.full.pdf",
            "status": "GREEN",
            "license": "CCBYNCND",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2023.09.12.557460?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2023.09.12.557460, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-01-13",
        "authors": [
            {
                "authorId": "16217712",
                "name": "J. O. Caro"
            },
            {
                "authorId": "2165414377",
                "name": "Antonio H. O. Fonseca"
            },
            {
                "authorId": "2242365094",
                "name": "Christopher Averill"
            },
            {
                "authorId": "2242367286",
                "name": "S. Rizvi"
            },
            {
                "authorId": "2242365338",
                "name": "Matteo Rosati"
            },
            {
                "authorId": "2242363631",
                "name": "James L. Cross"
            },
            {
                "authorId": "2242365436",
                "name": "Prateek Mittal"
            },
            {
                "authorId": "2242366935",
                "name": "E. Zappala"
            },
            {
                "authorId": "2242367774",
                "name": "Daniel Levine"
            },
            {
                "authorId": "2180409627",
                "name": "R. M. Dhodapkar"
            },
            {
                "authorId": "2268862065",
                "name": "Insu Han"
            },
            {
                "authorId": "2257001335",
                "name": "Amin Karbasi"
            },
            {
                "authorId": "3348652",
                "name": "C. Abdallah"
            },
            {
                "authorId": "2180407901",
                "name": "David van Dijk"
            }
        ],
        "abstract": "We introduce the Brain Language Model (BrainLM), a foundation model for brain activity dynamics trained on 6,700 hours of fMRI recordings. Utilizing self-supervised masked-prediction training, BrainLM demonstrates proficiency in both fine-tuning and zero-shot inference tasks. Fine-tuning allows for the accurate prediction of clinical variables like age, anxiety, and PTSD as well as forecasting of future brain states. Critically, the model generalizes well to entirely new external cohorts not seen during training. In zero-shot inference mode, BrainLM can identify intrinsic functional networks directly from raw fMRI data without any network-based supervision during training. The model also generates interpretable latent representations that reveal relationships between brain activity patterns and cognitive states. Overall, BrainLM offers a versatile and interpretable framework for elucidating the complex spatiotemporal dynamics of human brain activity. It serves as a powerful \u201clens\u201d through which massive repositories of fMRI data can be analyzed in new ways, enabling more effective interpretation and utilization at scale. The work demonstrates the potential of foundation models to advance computational neuroscience research."
    },
    {
        "paperId": "0133a86f206a8dbd1d6800485fbc85cb4b9699f4",
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "title": "Feature Reuse and Scaling: Understanding Transfer Learning with Protein Language Models",
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2024/02/14/2024.02.05.578959.full.pdf",
            "status": "GREEN",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2024.02.05.578959?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2024.02.05.578959, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-02-14",
        "authors": [
            {
                "authorId": "97687559",
                "name": "Francesca-Zhoufan Li"
            },
            {
                "authorId": "40360514",
                "name": "Ava P. Amini"
            },
            {
                "authorId": "2279799975",
                "name": "Yisong Yue"
            },
            {
                "authorId": "2283816103",
                "name": "Kevin K. Yang"
            },
            {
                "authorId": "2241535679",
                "name": "Alex X. Lu"
            }
        ],
        "abstract": "Large pretrained protein language models (PLMs) have improved protein property and structure prediction from sequences via transfer learning, in which weights and representations from PLMs are repurposed for downstream tasks. Although PLMs have shown great promise, currently there is little understanding of how the features learned by pretraining relate to and are useful for downstream tasks. We perform a systematic analysis of transfer learning using PLMs, conducting 370 experiments across a comprehensive suite of factors including different downstream tasks, architectures, model sizes, model depths, and pretraining time. We observe that while almost all down-stream tasks do benefit from pretrained models compared to naive sequence representations, for the majority of tasks performance does not scale with pretraining, and instead relies on low-level features learned early in pretraining. Our results point to a mismatch between current PLM pretraining paradigms and most applications of these models, indicating a need for better pretraining methods."
    },
    {
        "paperId": "587547493b5cf221af4b929cf390ef81e9768937",
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "title": "Cell2Sentence: Teaching Large Language Models the Language of Biology",
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2023/09/14/2023.09.11.557287.full.pdf",
            "status": "GREEN",
            "license": "CCBYNCND",
            "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11565894, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "publicationDate": "2024-10-29",
        "authors": [
            {
                "authorId": "2242367774",
                "name": "Daniel Levine"
            },
            {
                "authorId": "2242367286",
                "name": "S. Rizvi"
            },
            {
                "authorId": "2242797951",
                "name": "Sacha L\u00e9vy"
            },
            {
                "authorId": "2242799949",
                "name": "Nazreen Pallikkavaliyaveetil"
            },
            {
                "authorId": "2284617709",
                "name": "David Zhang"
            },
            {
                "authorId": "2242592769",
                "name": "Xingyu Chen"
            },
            {
                "authorId": "2242800902",
                "name": "Sina Ghadermarzi"
            },
            {
                "authorId": "2244170762",
                "name": "Ruiming Wu"
            },
            {
                "authorId": "2242791064",
                "name": "Zihe Zheng"
            },
            {
                "authorId": "2284602793",
                "name": "Ivan Vrkic"
            },
            {
                "authorId": "2284603015",
                "name": "Anna Zhong"
            },
            {
                "authorId": "2284602411",
                "name": "Daphne Raskin"
            },
            {
                "authorId": "2268862065",
                "name": "Insu Han"
            },
            {
                "authorId": "2284602828",
                "name": "Antonio Henrique de Oliveira Fonseca"
            },
            {
                "authorId": "16217712",
                "name": "J. O. Caro"
            },
            {
                "authorId": "2257001335",
                "name": "Amin Karbasi"
            },
            {
                "authorId": "2180409627",
                "name": "R. M. Dhodapkar"
            },
            {
                "authorId": "2180407901",
                "name": "David van Dijk"
            }
        ],
        "abstract": "We introduce Cell2Sentence (C2S), a novel method to directly adapt large language models to a biological context, specifically single-cell transcriptomics. By transforming gene expression data into \u201ccell sentences,\u201d C2S bridges the gap between natural language processing and biology. We demonstrate cell sentences enable the fine-tuning of language models for diverse tasks in biology, including cell generation, complex cell-type annotation, and direct data-driven text generation. Our experiments reveal that GPT-2, when fine-tuned with C2S, can generate biologically valid cells based on cell type inputs, and accurately predict cell types from cell sentences. This illustrates that language models, through C2S fine-tuning, can acquire a significant understanding of single-cell biology while maintaining robust text generation capabilities. C2S offers a flexible, accessible framework to integrate natural language processing with transcriptomics, utilizing existing models and libraries for a wide range of biological applications."
    }
]